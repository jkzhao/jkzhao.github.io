<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>jkzhao&#39;s blog</title>
  <subtitle>学习 总结 思考</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-01-10T03:44:49.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Zhao Jiankai</name>
    <email>jk.zhaocoder@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Redis安装部署</title>
    <link href="http://yoursite.com/2018/01/10/Redis%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
    <id>http://yoursite.com/2018/01/10/Redis安装部署/</id>
    <published>2018-01-10T01:12:43.000Z</published>
    <updated>2018-01-10T03:44:49.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;安装Redis&quot;&gt;&lt;a href=&quot;#安装Redis&quot; class=&quot;headerlink&quot; title=&quot;安装Redis&quot;&gt;&lt;/a&gt;安装Redis&lt;/h2&gt;&lt;p&gt;我这里安装的版本是 3.2.2&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@log2 ~]# yum install readline-devel pcre-devel openssl-devel -y
[root@log2 local]# tar zxf redis-3.2.2.tar.gz 
[root@log2 local]# cd redis-3.2.2/
[root@log2 redis-3.2.2]# make
[root@log2 redis-3.2.2]# make install
&lt;/code&gt;&lt;/pre&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;配置Redis&quot;&gt;&lt;a href=&quot;#配置Redis&quot; class=&quot;headerlink&quot; title=&quot;配置Redis&quot;&gt;&lt;/a&gt;配置Redis&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;[root@log2 redis-3.2.2]# cp redis.conf /etc/
[root@log2 redis-3.2.2]# vim /etc/redis.conf
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;1.配置redis后台启动&lt;/strong&gt;&lt;br&gt;打开/etc/redis.conf，将daemonize处修改为yes。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;################################# GENERAL #####################################

# By default Redis does not run as a daemon. Use &amp;apos;yes&amp;apos; if you need it.
# Note that Redis will write a pid file in /var/run/redis.pid when daemonized.
daemonize yes
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;2.配置Redis持久化策略&lt;/strong&gt;&lt;br&gt;使用RDB和AOF双持久化策略：其中默认开启了RDB持久化，我们只需要开启AOF持久化。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# AOF and RDB persistence can be enabled at the same time without problems.
# If the AOF is enabled on startup Redis will load the AOF, that is the file
# with the better durability guarantees.
#
# Please check http://redis.io/topics/persistence for more information.

appendonly yes
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;3.配置redis日志文件&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Specify the log file name. Also the empty string can be used to force
# Redis to log on the standard output. Note that if you use standard
# output for logging but daemonize, logs will be sent to /dev/null
logfile &amp;quot;/var/log/redis.log&amp;quot;   
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;4.配置数据文件存放路径，在/目录下面创建/RedisData目录&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@log2 redis-3.2.2]# mkdir /RedisData

 # The working directory.
#
# The DB will be written inside this directory, with the filename specified
# above using the &amp;apos;dbfilename&amp;apos; configuration directive.
#
# The Append Only File will also be created inside this directory.
#
# Note that you must specify a directory here, not a file name.
dir /RedisData
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;5.修改Redis监听地址&lt;/strong&gt;&lt;br&gt;注释掉 #bind 127.0.0.1&lt;br&gt;否则redis只会监听在127.0.0.1的某个端口上。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;6.修改Redis监听端口&lt;/strong&gt;&lt;br&gt;为了安全，强烈建议修改redis的监听端口。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Accept connections on the specified port, default is 6379 (IANA #815344).
# If port 0 is specified Redis will not listen on a TCP socket.
port 6400
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;7.在redis3.2之后，redis增加了protected-mode，在这个模式下，即使注释掉了bind 127.0.0.1，再访问redis的时候还是报错，所以要如下设置：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# By default protected mode is enabled. You should disable it only if
# you are sure you want clients from other hosts to connect to Redis
# even if no authentication is configured, nor a specific set of interfaces
# are explicitly listed using the &amp;quot;bind&amp;quot; directive.
protected-mode no
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;启动-停止&quot;&gt;&lt;a href=&quot;#启动-停止&quot; class=&quot;headerlink&quot; title=&quot;启动/停止&quot;&gt;&lt;/a&gt;启动/停止&lt;/h2&gt;&lt;p&gt;1.启动&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@log2 ~]# /usr/local/bin/redis-server /etc/redis.conf
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.停止&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@log2 ~]# /usr/local/redis-3.2.2/src/redis-cli -p 6400 shutdown
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;Redis简单使用&quot;&gt;&lt;a href=&quot;#Redis简单使用&quot; class=&quot;headerlink&quot; title=&quot;Redis简单使用&quot;&gt;&lt;/a&gt;Redis简单使用&lt;/h2&gt;&lt;h3 id=&quot;使用客户端连接redis-server&quot;&gt;&lt;a href=&quot;#使用客户端连接redis-server&quot; class=&quot;headerlink&quot; title=&quot;使用客户端连接redis-server&quot;&gt;&lt;/a&gt;使用客户端连接redis-server&lt;/h3&gt;&lt;p&gt;在Redis的安装目录中有redis客户端，即redis-cli（Redis Command Line Interface），它是Redis自带的基于命令行的Redis的客户端。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cd /usr/local/bin/
# ./redis-cli -h 172.16.206.30 -p 6400
172.16.206.30:6400&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;向Redis服务器发送命令&quot;&gt;&lt;a href=&quot;#向Redis服务器发送命令&quot; class=&quot;headerlink&quot; title=&quot;向Redis服务器发送命令&quot;&gt;&lt;/a&gt;向Redis服务器发送命令&lt;/h3&gt;&lt;p&gt;redis-cli连上redis服务后，可以在命令行发送命令。&lt;br&gt;&lt;strong&gt;1.ping，测试客户端与redis的连接是否正常，如果正常会收到回复PONG&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# ./redis-cli -h 172.16.206.30 -p 6400
172.16.206.30:6400&amp;gt; ping
PONG
172.16.206.30:6400&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;2.set/get，使用set和get可以向redis设置数据、获取数据&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;172.16.206.30:6400&amp;gt; set name wisedu
OK
172.16.206.30:6400&amp;gt; get name
&amp;quot;wisedu&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;3.del，删除指定key的内容&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;172.16.206.30:6400&amp;gt; del name
(integer) 1
172.16.206.30:6400&amp;gt; get name
(nil)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;4.keys *，查看当前库中所有的key&lt;/strong&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;安装Redis&quot;&gt;&lt;a href=&quot;#安装Redis&quot; class=&quot;headerlink&quot; title=&quot;安装Redis&quot;&gt;&lt;/a&gt;安装Redis&lt;/h2&gt;&lt;p&gt;我这里安装的版本是 3.2.2&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@log2 ~]# yum install readline-devel pcre-devel openssl-devel -y
[root@log2 local]# tar zxf redis-3.2.2.tar.gz 
[root@log2 local]# cd redis-3.2.2/
[root@log2 redis-3.2.2]# make
[root@log2 redis-3.2.2]# make install
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="NoSQL" scheme="http://yoursite.com/categories/NoSQL/"/>
    
    
      <category term="Redis" scheme="http://yoursite.com/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Python操作InfluxDB</title>
    <link href="http://yoursite.com/2017/12/20/Python%E6%93%8D%E4%BD%9CInfluxDB/"/>
    <id>http://yoursite.com/2017/12/20/Python操作InfluxDB/</id>
    <published>2017-12-20T11:50:06.000Z</published>
    <updated>2017-12-22T13:13:55.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;InfluxDB-API-Client-Libraries&quot;&gt;&lt;a href=&quot;#InfluxDB-API-Client-Libraries&quot; class=&quot;headerlink&quot; title=&quot;InfluxDB API Client Libraries&quot;&gt;&lt;/a&gt;InfluxDB API Client Libraries&lt;/h2&gt;&lt;p&gt;上一篇文章介绍了安装部署InfluxDB和它的一些基本概念，接着就得来处理Nginx access.log，并将处理结果存储在InfluxDB中。&lt;br&gt;InfluxDB支持多种语言使用其客户端库来进行交互，具体参见官方文档：&lt;br&gt;&lt;a href=&quot;https://docs.influxdata.com/influxdb/v1.4/tools/api_client_libraries/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://docs.influxdata.com/influxdb/v1.4/tools/api_client_libraries/&lt;/a&gt;&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;br&gt;我这里使用Python语言来处理Nginx access.log并将结果存储于InfluxDB中。&lt;/p&gt;
&lt;h2 id=&quot;下载安装模块influxdb&quot;&gt;&lt;a href=&quot;#下载安装模块influxdb&quot; class=&quot;headerlink&quot; title=&quot;下载安装模块influxdb&quot;&gt;&lt;/a&gt;下载安装模块influxdb&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;# pip install influxdb
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;编写代码&quot;&gt;&lt;a href=&quot;#编写代码&quot; class=&quot;headerlink&quot; title=&quot;编写代码&quot;&gt;&lt;/a&gt;编写代码&lt;/h2&gt;&lt;p&gt;Nginx access.log的一行日志如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;line =&amp;apos;res.wisedu.com 172.16.6.4 [20/Dec/2017:09:20:17 +0800] &amp;quot;GET /statistics/res?/bh_apis/1.0/module-bhMenu.html&amp;amp;callback=__jp0 HTTP/1.1&amp;quot; 200 0 &amp;quot;-&amp;quot; &amp;quot;Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)&amp;quot; 0.000 -&amp;apos;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;代码如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# -*- coding: utf-8 -*-

import re
from influxdb import InfluxDBClient

def read_log(path): # 生成器generator
    &amp;apos;&amp;apos;&amp;apos;一行一行读取日志并返回&amp;apos;&amp;apos;&amp;apos;
    with open(path) as f:
        yield from f

def write_influxDB(lst):
    &amp;apos;&amp;apos;&amp;apos;写入InfluxDB数据库&amp;apos;&amp;apos;&amp;apos;
    client.write_points(lst)

def regular_line(line):
    &amp;apos;&amp;apos;&amp;apos;利用正则分析一行日志，存于字典中&amp;apos;&amp;apos;&amp;apos;
    o = re.compile(pattern)
    m = o.search(line)
    field_dict = m.groupdict()

    return field_dict

def main():
    &amp;apos;&amp;apos;&amp;apos;主函数&amp;apos;&amp;apos;&amp;apos;
    for line in read_log(path):
        field_dict = regular_line(line)
        lst = []
        point_dict = {}
        point_dict[&amp;apos;measurement&amp;apos;] = &amp;apos;res_access_log&amp;apos;
        point_dict[&amp;apos;fields&amp;apos;] = field_dict
        lst.append(point_dict)

        write_influxDB(lst)

if __name__ == &amp;apos;__main__&amp;apos;:
    pattern = &amp;apos;(?P&amp;lt;host&amp;gt;[\w+\.]+\w+) (?P&amp;lt;ip&amp;gt;\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}) \[(?P&amp;lt;time_local&amp;gt;.*)\]&amp;apos;
    pattern += &amp;apos; &amp;quot;(?P&amp;lt;method&amp;gt;\w+) (?P&amp;lt;url&amp;gt;[^\s]*) (?P&amp;lt;version&amp;gt;[\w\/\d\.]*)&amp;quot; (?P&amp;lt;status&amp;gt;\d+) (?P&amp;lt;length&amp;gt;\d+)&amp;apos;
    pattern += &amp;apos; &amp;quot;(?P&amp;lt;http_referer&amp;gt;[^\s]*)&amp;quot; &amp;quot;(?P&amp;lt;ua&amp;gt;.*)&amp;quot; (?P&amp;lt;request_time&amp;gt;[\d\.]*) (?P&amp;lt;upstream_response_time&amp;gt;[\d\.]*)&amp;apos;

    path = &amp;quot;logs/res.statistics.log&amp;quot;
    client = InfluxDBClient(host=&amp;apos;172.16.7.151&amp;apos;, port=8086, username=&amp;apos;root&amp;apos;, password=&amp;apos;wisedu123&amp;apos;, database=&amp;apos;mydb&amp;apos;)

    main()
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;Chronograf查看表数据&quot;&gt;&lt;a href=&quot;#Chronograf查看表数据&quot; class=&quot;headerlink&quot; title=&quot;Chronograf查看表数据&quot;&gt;&lt;/a&gt;Chronograf查看表数据&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/TimeSeriesDatabases/7.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;InfluxDB-API-Client-Libraries&quot;&gt;&lt;a href=&quot;#InfluxDB-API-Client-Libraries&quot; class=&quot;headerlink&quot; title=&quot;InfluxDB API Client Libraries&quot;&gt;&lt;/a&gt;InfluxDB API Client Libraries&lt;/h2&gt;&lt;p&gt;上一篇文章介绍了安装部署InfluxDB和它的一些基本概念，接着就得来处理Nginx access.log，并将处理结果存储在InfluxDB中。&lt;br&gt;InfluxDB支持多种语言使用其客户端库来进行交互，具体参见官方文档：&lt;br&gt;&lt;a href=&quot;https://docs.influxdata.com/influxdb/v1.4/tools/api_client_libraries/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://docs.influxdata.com/influxdb/v1.4/tools/api_client_libraries/&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="时序数据库" scheme="http://yoursite.com/categories/%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="InfluxDB" scheme="http://yoursite.com/tags/InfluxDB/"/>
    
  </entry>
  
  <entry>
    <title>时序数据库InfluxDB</title>
    <link href="http://yoursite.com/2017/12/15/%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93InfluxDB/"/>
    <id>http://yoursite.com/2017/12/15/时序数据库InfluxDB/</id>
    <published>2017-12-15T14:00:36.000Z</published>
    <updated>2017-12-22T12:35:07.000Z</updated>
    
    <content type="html">&lt;p&gt;最近帮助公司前端小伙伴处理他们的nginx访问日志，log的数据是半结构化的数据，同时也是典型的时序数据，每一条数据都带有时间戳。于是考虑使用时间序列数据库存储，而不会去使用mysql或是mongodb(zabbix用的是mysql，它在IO上面遇到了瓶颈)。现在时间序列的数据库是有很多的，比如graphite、opentsdb以及新生的influxdb。这次我使用了InfluxDB，在此记录下学习过程，同时也希望能够帮助到其他学习的同学。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;InfluxDB介绍&quot;&gt;&lt;a href=&quot;#InfluxDB介绍&quot; class=&quot;headerlink&quot; title=&quot;InfluxDB介绍&quot;&gt;&lt;/a&gt;InfluxDB介绍&lt;/h2&gt;&lt;p&gt;什么是时间序列数据？最简单的定义就是数据格式里包含timestamp字段的数据。比如股票市场的价格，环境中的温度，主机的CPU使用率等。但是又有什么数据是不包含timestamp的呢？几乎所有的数据都可以打上一个timestamp字段。时间序列数据更重要的一个属性是如何去查询它。在查询的时候，对于时间序列我们总是会带上一个时间范围去过滤数据。同时查询的结果里也总是会包含timestamp字段。&lt;br&gt;InfluxDB 是一个开源分布式时序、事件和指标数据库。使用 Go 语言编写，无需外部依赖。其设计目标是实现分布式和水平伸缩扩展。&lt;br&gt;它有三大特性：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Time Series （时间序列）：你可以使用与时间有关的相关函数（如最大，最小，求和等）&lt;/li&gt;
&lt;li&gt;Metrics（度量）：你可以实时对大量数据进行计算&lt;/li&gt;
&lt;li&gt;Eevents（事件）：它支持任意的事件数据&lt;br&gt;特点：&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;schemaless(无结构)，可以是任意数量的列&lt;/li&gt;
&lt;li&gt;min, max, sum, count, mean, median 一系列函数，方便统计&lt;/li&gt;
&lt;li&gt;Native HTTP API, 内置http支持，使用http读写&lt;/li&gt;
&lt;li&gt;Powerful Query Language 类似sql&lt;/li&gt;
&lt;li&gt;Built-in Explorer 自带web管理界面。（从1.4版本开始去除了自带的web管理界面）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;安装部署InfluxDB&quot;&gt;&lt;a href=&quot;#安装部署InfluxDB&quot; class=&quot;headerlink&quot; title=&quot;安装部署InfluxDB&quot;&gt;&lt;/a&gt;安装部署InfluxDB&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;influxdb下载地址：&lt;/strong&gt;&lt;br&gt;&lt;a href=&quot;https://portal.influxdata.com/downloads&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://portal.influxdata.com/downloads&lt;/a&gt;&lt;br&gt;&lt;strong&gt;influxdb文档：&lt;/strong&gt;&lt;br&gt;&lt;a href=&quot;http://docs.influxdata.com/influxdb/v1.4/introduction/getting_started/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://docs.influxdata.com/influxdb/v1.4/introduction/getting_started/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.下载安装&lt;/strong&gt;&lt;br&gt;对于在不同操作系统上安装，官网都有说明，我这里使用的是CentOS 7。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/TimeSeriesDatabases/1.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/TimeSeriesDatabases/2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@docdesginer local]# wget https://dl.influxdata.com/influxdb/releases/influxdb-1.4.2.x86_64.rpm
[root@docdesginer local]# yum localinstall influxdb-1.4.2.x86_64.rpm
[root@docdesginer local]# rpm -ql influxdb
/etc/influxdb/influxdb.conf
/etc/logrotate.d/influxdb
/usr/bin/influx
/usr/bin/influx_inspect
/usr/bin/influx_stress
/usr/bin/influx_tsm
/usr/bin/influxd
/usr/lib/influxdb/scripts/influxdb.service
/usr/lib/influxdb/scripts/init.sh
/usr/share/man/man1/influx.1.gz
/usr/share/man/man1/influx_inspect.1.gz
/usr/share/man/man1/influx_stress.1.gz
/usr/share/man/man1/influx_tsm.1.gz
/usr/share/man/man1/influxd-backup.1.gz
/usr/share/man/man1/influxd-config.1.gz
/usr/share/man/man1/influxd-restore.1.gz
/usr/share/man/man1/influxd-run.1.gz
/usr/share/man/man1/influxd-version.1.gz
/usr/share/man/man1/influxd.1.gz
/var/lib/influxdb
/var/log/influxdb
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;2.启动&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@docdesginer local]# systemctl start influxdb 
[root@docdesginer local]# systemctl status influxdb
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;3.登录&lt;/strong&gt;&lt;br&gt;客户端工具：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@docdesginer local]# /usr/bin/influx
Connected to http://localhost:8086 version 1.4.2
InfluxDB shell version: 1.4.2
&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查看数据库：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; SHOW DATABASES
name: databases
name
----
_internal
&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;HTTP API访问：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@docdesginer ~]# curl -G http://localhost:8086/query --data-urlencode &amp;quot;q=SHOW DATABASES&amp;quot;
{&amp;quot;results&amp;quot;:[{&amp;quot;statement_id&amp;quot;:0,&amp;quot;series&amp;quot;:[{&amp;quot;name&amp;quot;:&amp;quot;databases&amp;quot;,&amp;quot;columns&amp;quot;:[&amp;quot;name&amp;quot;],&amp;quot;values&amp;quot;:[[&amp;quot;_internal&amp;quot;]]}]}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;此时数据库中还有没有用户，还没开启认证。&lt;/p&gt;
&lt;h2 id=&quot;开启认证&quot;&gt;&lt;a href=&quot;#开启认证&quot; class=&quot;headerlink&quot; title=&quot;开启认证&quot;&gt;&lt;/a&gt;开启认证&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;1.创建管理员&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@docdesginer local]# /usr/bin/influx
Connected to http://localhost:8086 version 1.4.2
InfluxDB shell version: 1.4.2
&amp;gt; CREATE USER root WITH PASSWORD &amp;apos;wisedu123&amp;apos; WITH ALL PRIVILEGES
&amp;gt; quit
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;2.修改配置文件，开启认证&lt;/strong&gt;&lt;br&gt;By default, authentication is disabled in the configuration file. Enable authentication by setting the auth-enabled option to true in the [http] section of the configuration file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[http]  
  enabled = true  
  bind-address = &amp;quot;:8086&amp;quot;  
  auth-enabled = true 
  log-enabled = true  
  write-tracing = false  
  pprof-enabled = false  
  https-enabled = false  
  https-certificate = &amp;quot;/etc/ssl/influxdb.pem&amp;quot; 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;重启InfluxDB：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@docdesginer ~]# systemctl restart influxdb 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;3.再次登录&lt;/strong&gt;&lt;br&gt;客户端工具连接数据库：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@docdesginer ~]# influx
Connected to http://localhost:8086 version 1.4.2
InfluxDB shell version: 1.4.2
&amp;gt; show databases;
ERR: unable to parse authentication credentials
Warning: It is possible this error is due to not setting a database.
Please set a database with the command &amp;quot;use &amp;lt;database&amp;gt;&amp;quot;.
&amp;gt;

[root@docdesginer ~]# /usr/bin/influx
Connected to http://localhost:8086 version 1.4.2
InfluxDB shell version: 1.4.2
&amp;gt; auth
username: root
password: 
&amp;gt; show databases;
name: databases
name
----
_internal
&amp;gt; 
或者
[root@docdesginer ~]# /usr/bin/influx -username root -password wisedu123 -precision rfc3339 
Connected to http://localhost:8086 version 1.4.2
InfluxDB shell version: 1.4.2
&amp;gt; show databases;
name: databases
name
----
_internal
&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;HTTP API：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@docdesginer ~]# curl -G http://localhost:8086/query --data-urlencode &amp;quot;q=SHOW DATABASES&amp;quot;                  
{&amp;quot;error&amp;quot;:&amp;quot;unable to parse authentication credentials&amp;quot;}
[root@docdesginer ~]# curl -G http://localhost:8086/query -u root:wisedu123 --data-urlencode &amp;quot;q=SHOW DATABASES&amp;quot;
{&amp;quot;results&amp;quot;:[{&amp;quot;statement_id&amp;quot;:0,&amp;quot;series&amp;quot;:[{&amp;quot;name&amp;quot;:&amp;quot;databases&amp;quot;,&amp;quot;columns&amp;quot;:[&amp;quot;name&amp;quot;],&amp;quot;values&amp;quot;:[[&amp;quot;_internal&amp;quot;]]}]}
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;InfluxDB概念&quot;&gt;&lt;a href=&quot;#InfluxDB概念&quot; class=&quot;headerlink&quot; title=&quot;InfluxDB概念&quot;&gt;&lt;/a&gt;InfluxDB概念&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;1.influxdb相关名词（可类比关系型数据库）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;database：数据库。&lt;/li&gt;
&lt;li&gt;measurement：数据库中的表。它就是tag，field，time的容器；对于influxDB的measurement来说，field是必须的，并且不能根据field来排序；Tag是可选的，tag可以用来做索引，tag是以字符串的形式存放的。&lt;/li&gt;
&lt;li&gt;points：表里面的一行数据。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;2.influxDB中独有的概念&lt;/strong&gt;&lt;br&gt;（1）Point由时间戳（time）、数据（field）和标签（tags）组成。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;time：每条数据记录的时间，也是数据库自动生成的主索引；&lt;/li&gt;
&lt;li&gt;fields：各种记录的值；&lt;/li&gt;
&lt;li&gt;tags：各种有索引的属性。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;InfluxDb不需要做schema定义，这意味着你可以随意的添加measurements, tags, and fields at any time。&lt;/p&gt;
&lt;p&gt;（2）series&lt;br&gt;a series is the collection of data that share a retention policy, measurement, and tag set&lt;br&gt;所有在数据库中的数据，都需要通过图表来展示，而这个series表示这个表里面的数据，可以在图表上画成几条线：通过tags排列组合算出来。&lt;br&gt;其实一个series就是一个测点，或者说一条曲线，那么retention policy, measurement, tagset就共同组成了一个定位测点序列的唯一标识。&lt;br&gt;point，就是某个series的同一个时刻的多个field的value，就组成了一个point；其实就是一条曲线上的一个点。&lt;/p&gt;
&lt;p&gt;（3）retention policy&lt;br&gt;保留策略，用于决定要保留多久的数据，保存几个备份，以及集群的策略等。&lt;/p&gt;
&lt;h2 id=&quot;InfluxDB基本操作&quot;&gt;&lt;a href=&quot;#InfluxDB基本操作&quot; class=&quot;headerlink&quot; title=&quot;InfluxDB基本操作&quot;&gt;&lt;/a&gt;InfluxDB基本操作&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;连接数据库：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@docdesginer ~]# /usr/bin/influx -username root -password wisedu123 -precision rfc3339   
Connected to http://localhost:8086 version 1.4.2
InfluxDB shell version: 1.4.2
&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这里的-precision参数指定了时间戳的格式为rfc3339，也可以不使用该参数。&lt;br&gt;&lt;strong&gt;查看数据库：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; SHOW DATABASES
name: databases
name
----
_internal
&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;创建数据库：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; CREATE DATABASE mydb
&amp;gt; SHOW DATABASES
name: databases
name
----
_internal
mydb
&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;进入数据库：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; USE mydb
Using database mydb
&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;插入数据：&lt;/strong&gt;&lt;br&gt;influxDB存储数据采用的是Line Protocol格式。&lt;br&gt;Line Protocol格式：写入数据库的Point的固定格式。格式如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;measurement&amp;gt;[,&amp;lt;tag-key&amp;gt;=&amp;lt;tag-value&amp;gt;...] &amp;lt;field-key&amp;gt;=&amp;lt;field-value&amp;gt;[,&amp;lt;field2-key&amp;gt;=&amp;lt;field2-value&amp;gt;...] [unix-nano-timestamp]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;比如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;weather,location=us-midwest temperature=82 1465839830100400200
  |    -------------------- --------------  |
  |             |             |             |
  |             |             |             |
+-----------+--------+-+---------+-+---------+
|measurement|,tag_set| |field_set| |timestamp|
+-----------+--------+-+---------+-+---------+
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【注意】：最后的timestamp是 unix时间戳*1000000000 的值，或者使用 %Y-%m-%dT%H:%M:%SZ 这种格式。使用其他格式在插入时会报错。&lt;/p&gt;
&lt;p&gt;示例：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; INSERT cpu,host=serverA,region=us_west value=0.64
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cpu是表名&lt;/li&gt;
&lt;li&gt;host=serverA,region=us_west 是tag&lt;/li&gt;
&lt;li&gt;value=0.64是field&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;想对此格式有详细的了解参见&lt;a href=&quot;https://docs.influxdata.com/influxdb/v1.4/write_protocols/line_protocol_tutorial/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;官方文档&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;查询数据：&lt;/strong&gt;&lt;br&gt;influxDB是支持类sql语句的，具体的查询语法都差不多。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; select * from cpu
name: cpu
time                           host    region  value
----                           ----    ------  -----
2017-12-15T13:17:09.660446488Z serverA us_west 0.64
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【注意】：InfluxDB集群功能已经不再开源。要想使用集群服务需要购买企业版。开源版和企业版的主要区别就是企业版的InfluxDB支持集群，而开源版不支持，此外企业版提供了先进的备份/恢复功能，而开源版本没有。但InfluxDB单机版性能也足够支撑中小公司的业务了。&lt;/p&gt;
&lt;h2 id=&quot;InfluxDB数据保存策略（Retention-Policies）&quot;&gt;&lt;a href=&quot;#InfluxDB数据保存策略（Retention-Policies）&quot; class=&quot;headerlink&quot; title=&quot;InfluxDB数据保存策略（Retention Policies）&quot;&gt;&lt;/a&gt;InfluxDB数据保存策略（Retention Policies）&lt;/h2&gt;&lt;p&gt;InfluxDB每秒可以处理成千上万条数据，要将这些数据全部保存下来会占用大量的存储空间，有时我们可能并不需要将所有历史数据进行存储，因此，InfluxDB推出了数据保留策略（Retention Policies），用来让我们自定义数据的保留时间。&lt;br&gt;InfluxDB的数据保留策略（RP） 用来定义数据在InfluxDB中存放的时间，或者定义保存某个期间的数据。&lt;br&gt;一个数据库可以有多个保留策略，但每个策略必须是独一无二的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.查询策略&lt;/strong&gt;&lt;br&gt;可以通过如下语句查看数据库的现有策略：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; SHOW RETENTION POLICIES ON mydb
name    duration shardGroupDuration replicaN default
----    -------- ------------------ -------- -------
autogen 0s       168h0m0s           1        true
&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：数据库mydb只有一个策略，各字段的含义如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;name：名称，此示例名称为 autogen。当你创建一个数据库的时候，InfluxDB会自动为数据库创建一个名叫 autogen 的策略，这个策略会永久保存数据。你可以重命名这个策略，并且在InfluxDB的配置文件中禁止掉自动创建策略。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;duration：数据保存时间，0代表无限制&lt;/li&gt;
&lt;li&gt;shardGroupDuration：shardGroup的存储时间，shardGroup是InfluxDB的一个基本储存结构。&lt;/li&gt;
&lt;li&gt;replicaN：全称是REPLICATION，副本个数&lt;/li&gt;
&lt;li&gt;default：是否是默认策略。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这里有两个概念：&lt;br&gt;&lt;strong&gt;shard：&lt;/strong&gt;&lt;br&gt;    shard 在 InfluxDB 中是一个比较重要的概念，它和 retention policy 相关联。每一个存储策略下会存在许多 shard，每一个 shard 存储一个指定时间段内的数据，并且不重复，例如 7点-8点 的数据落入 shard0 中，8点-9点的数据则落入 shard1 中。每一个 shard 都对应一个底层的 tsm 存储引擎，有独立的 cache、wal、tsm file。&lt;br&gt;创建数据库时会自动创建一个默认存储策略，永久保存数据，对应的在此存储策略下的 shard 所保存的数据的时间段为 7 天，也就是上面查询时看到的168h。计算的函数如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;func shardGroupDuration(d time.Duration) time.Duration {
    if d &amp;gt;= 180*24*time.Hour || d == 0 { // 6 months or 0
        return 7 * 24 * time.Hour
    } else if d &amp;gt;= 2*24*time.Hour { // 2 days
        return 1 * 24 * time.Hour
    }
    return 1 * time.Hour
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果创建一个新的 retention policy 设置数据的保留时间为 1 天，则单个 shard 所存储数据的时间间隔为 1 小时，超过1个小时的数据会被存放到下一个 shard 中。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;shard group：&lt;/strong&gt;&lt;br&gt;    shard group是shards的逻辑容器。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.创建策略&lt;/strong&gt;&lt;br&gt;语法：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CREATE RETENTION POLICY &amp;lt;retention_policy_name&amp;gt; ON &amp;lt;database_name&amp;gt; DURATION &amp;lt;duration&amp;gt; REPLICATION &amp;lt;n&amp;gt; [SHARD DURATION &amp;lt;duration&amp;gt;] [DEFAULT]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其中：SHARD DURATION子句决定了每个shard group存储的时间间隔，在永久存储的策略里这个子句是无效的。这个子句是可选的。shard group duration默认由策略的 duration 决定。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Retention Policy’s DURATION&lt;/th&gt;
&lt;th&gt;Shard Group Duration&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt; 2 days&lt;/td&gt;
&lt;td&gt;1 hour&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;gt;= 2 days and &amp;lt;= 6 months&lt;/td&gt;
&lt;td&gt;1 day&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;gt; 6 months&lt;/td&gt;
&lt;td&gt;7 days&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;示例1：为数据库mydb创建一个策略&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CREATE RETENTION POLICY &amp;quot;one_day_only&amp;quot; ON &amp;quot;mydb&amp;quot; DURATION 1d REPLICATION 1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;示例2：为数据库mydb创建一个默认策略。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CREATE RETENTION POLICY &amp;quot;one_day_only&amp;quot; ON &amp;quot;mydb&amp;quot; DURATION 23h60m REPLICATION 1 DEFAULT
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;3.修改策略&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ALTER RETENTION POLICY &amp;lt;retention_policy_name&amp;gt; ON &amp;lt;database_name&amp;gt; DURATION &amp;lt;duration&amp;gt; REPLICATION &amp;lt;n&amp;gt; SHARD DURATION &amp;lt;duration&amp;gt; DEFAULT
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;4.删除策略&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;DROP RETENTION POLICY &amp;lt;retention_policy_name&amp;gt; ON &amp;lt;database_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【注意】：策略这个关键词“POLICY”在使用是应该大写，小写应该会出错。&lt;br&gt;当一个表使用的策略不是默认策略时，在进行操作时一定要显式的指定策略名称，否则会出现错误。&lt;/p&gt;
&lt;h2 id=&quot;Chronograf介绍&quot;&gt;&lt;a href=&quot;#Chronograf介绍&quot; class=&quot;headerlink&quot; title=&quot;Chronograf介绍&quot;&gt;&lt;/a&gt;Chronograf介绍&lt;/h2&gt;&lt;p&gt;Influxdb在1.3以后版本已经关闭了内置的8086的web管理功能，需要单独的工具来管理。而这个工具就是Chronograf。&lt;br&gt;其实Chronograf是TICK技术栈的一个组成部分。TICK是InfluxdDB公司推出的监控套件，承包指标采集、分析、画图等时序数据库上下游的工作，有点模仿日志分析系统ELK套件的意思。TICK包含：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;T = Telegraf is a plugin-driven server agent for collecting and reporting metrics.&lt;/li&gt;
&lt;li&gt;I = InfluxDB is a time series database built from the ground up to handle high write and query loads.&lt;/li&gt;
&lt;li&gt;C = Chronograf is a graphing and visualization application for performing ad hoc exploration of data.&lt;/li&gt;
&lt;li&gt;K = Kapacitor is a data processing framework proving alerting, anomaly detection and action frameworks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;也就是说：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Telegraf：数据采集&lt;/li&gt;
&lt;li&gt;InfluxDB：数据接收和存储&lt;/li&gt;
&lt;li&gt;Chronograf：数据汇总展示，报警等。&lt;/li&gt;
&lt;li&gt;Kapacitor：数据处理，比如监控策略等&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;安装Chronograf&quot;&gt;&lt;a href=&quot;#安装Chronograf&quot; class=&quot;headerlink&quot; title=&quot;安装Chronograf&quot;&gt;&lt;/a&gt;安装Chronograf&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;1.安装部署&lt;/strong&gt;&lt;br&gt;下载地址：&lt;a href=&quot;https://portal.influxdata.com/downloads&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://portal.influxdata.com/downloads&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/TimeSeriesDatabases/3.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@docdesginer local]# wget https://dl.influxdata.com/chronograf/releases/chronograf-1.3.10.0.x86_64.rpm
[root@docdesginer local]# yum localinstall chronograf-1.3.10.0.x86_64.rpm
[root@docdesginer local]# rpm -ql chronograf
/etc/logrotate.d/chronograf
/usr/bin/chronograf
/usr/lib/chronograf/scripts/chronograf.service
/usr/lib/chronograf/scripts/init.sh
/usr/share/chronograf/canned/apache.json
/usr/share/chronograf/canned/consul.json
/usr/share/chronograf/canned/consul_agent.json
...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;2.启动&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@docdesginer local]# systemctl start chronograf
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;3.访问Chronograf&lt;/strong&gt;&lt;br&gt;浏览器输入&lt;a href=&quot;http://IP:8888&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://IP:8888&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/TimeSeriesDatabases/4.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;查询：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/TimeSeriesDatabases/5.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/TimeSeriesDatabases/6.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;在查询时，最好数据库名和表名都加上引号。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;最近帮助公司前端小伙伴处理他们的nginx访问日志，log的数据是半结构化的数据，同时也是典型的时序数据，每一条数据都带有时间戳。于是考虑使用时间序列数据库存储，而不会去使用mysql或是mongodb(zabbix用的是mysql，它在IO上面遇到了瓶颈)。现在时间序列的数据库是有很多的，比如graphite、opentsdb以及新生的influxdb。这次我使用了InfluxDB，在此记录下学习过程，同时也希望能够帮助到其他学习的同学。&lt;br&gt;
    
    </summary>
    
      <category term="时序数据库" scheme="http://yoursite.com/categories/%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="InfluxDB" scheme="http://yoursite.com/tags/InfluxDB/"/>
    
  </entry>
  
  <entry>
    <title>nexus搭建maven私服</title>
    <link href="http://yoursite.com/2017/11/19/nexus%E6%90%AD%E5%BB%BAmaven%E7%A7%81%E6%9C%8D/"/>
    <id>http://yoursite.com/2017/11/19/nexus搭建maven私服/</id>
    <published>2017-11-19T06:49:41.000Z</published>
    <updated>2017-11-19T08:06:13.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;为什么要搭建私服&quot;&gt;&lt;a href=&quot;#为什么要搭建私服&quot; class=&quot;headerlink&quot; title=&quot;为什么要搭建私服&quot;&gt;&lt;/a&gt;为什么要搭建私服&lt;/h2&gt;&lt;p&gt;私服不是Maven的核心概念，它仅仅是一种衍生出来的特殊的Maven仓库。通过建立自己的私服，就可以降低中央仓库负荷、节省外网带宽、加速Maven构建、自己部署构建等，从而高效地使用Maven。Nexus也是当前最流行的Maven仓库管理软件。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;下载Nexus&quot;&gt;&lt;a href=&quot;#下载Nexus&quot; class=&quot;headerlink&quot; title=&quot;下载Nexus&quot;&gt;&lt;/a&gt;下载Nexus&lt;/h2&gt;&lt;p&gt;最新nexus下载地址：&lt;a href=&quot;http://www.sonatype.org/nexus/go&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://www.sonatype.org/nexus/go&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Maven/1.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;Nexus是典型的Java Web应用，它有两种安装包，一种是包含Jetty容器的Bundle包，另一种是不包含Web容器的war包。&lt;/p&gt;
&lt;h2 id=&quot;安装Nexus&quot;&gt;&lt;a href=&quot;#安装Nexus&quot; class=&quot;headerlink&quot; title=&quot;安装Nexus&quot;&gt;&lt;/a&gt;安装Nexus&lt;/h2&gt;&lt;h3 id=&quot;安装JDK1-8&quot;&gt;&lt;a href=&quot;#安装JDK1-8&quot; class=&quot;headerlink&quot; title=&quot;安装JDK1.8&quot;&gt;&lt;/a&gt;安装JDK1.8&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;# mkdir /usr/java
# tar zxf /usr/local/jdk-8u73-linux-x64.gz -C /usr/java/
# vim /etc/profile
export JAVA_HOME=/usr/java/jdk1.8.0_73
export PATH=$JAVA_HOME/bin:$PATH
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
# source /etc/profile
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;安装Nexus-1&quot;&gt;&lt;a href=&quot;#安装Nexus-1&quot; class=&quot;headerlink&quot; title=&quot;安装Nexus&quot;&gt;&lt;/a&gt;安装Nexus&lt;/h3&gt;&lt;p&gt;上传Nexus安装包到/opt目录下。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@hadoop33 opt]# mkdir nexus
[root@hadoop33 opt]# tar zxf nexus-2.14.0-01-bundle.tar.gz -C /opt/nexus
[root@hadoop33 opt]# cd nexus/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Maven/2.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;nexus-2.6.2-01: 该目录包含了Nexus运行所需要的文件，如启动脚本、依赖jar包等。&lt;br&gt;sonatype-work：该目录包含Nenus生成的配置文件、日志文件、仓库文件等。&lt;br&gt;其中第一个目录是运行Nexus必须的，而第二个不是必须的，Nexus会在运行的时候动态创建该目录。&lt;/p&gt;
&lt;h3 id=&quot;启动Nexus&quot;&gt;&lt;a href=&quot;#启动Nexus&quot; class=&quot;headerlink&quot; title=&quot;启动Nexus&quot;&gt;&lt;/a&gt;启动Nexus&lt;/h3&gt;&lt;p&gt;默认端口为8081，如需修改请查看配置文件 conf/nexus.properties&lt;br&gt;它本身不建议在root用户下使用，如果我们需要在root用户下启动服务，要先配置 bin/nexus 文件中的 RUN_AS_USER=root&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Maven/3.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@hadoop33 nexus]# cd nexus-2.14.0-01/bin/
[root@hadoop33 bin]# ./nexus start
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Maven/4.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;停止-Nexus&quot;&gt;&lt;a href=&quot;#停止-Nexus&quot; class=&quot;headerlink&quot; title=&quot;停止 Nexus&quot;&gt;&lt;/a&gt;停止 Nexus&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@hadoop33 bin]# ./nexus stop
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;访问&quot;&gt;&lt;a href=&quot;#访问&quot; class=&quot;headerlink&quot; title=&quot;访问&quot;&gt;&lt;/a&gt;访问&lt;/h2&gt;&lt;p&gt;启动后访问首页： &lt;a href=&quot;http://172.16.206.33:8081/nexus/index.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.206.33:8081/nexus/index.html&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Maven/5.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;点击右上角的Login in&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Maven/6.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;登录默认账号/密码 admin/admin123。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如何修改 nexus 账号密码：&lt;/strong&gt;先停止 nexus，打开 %NEXUS_HOME%/sonatype-work/nexus/conf/security.xml，修改即可。&lt;br&gt;nexus 的密码采用 SHA1 加密算法 ( 在线加密工具 )，将加密后的 SHA1 串（小写）拷贝覆盖原来的。重启 nexus：./nexus restart。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Maven/7.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;配置Nexus&quot;&gt;&lt;a href=&quot;#配置Nexus&quot; class=&quot;headerlink&quot; title=&quot;配置Nexus&quot;&gt;&lt;/a&gt;配置Nexus&lt;/h2&gt;&lt;p&gt;Nexus常用功能就是：指定私服的中央地址、将自己的Maven项目指定到私服地址、从私服下载中央库的项目索引、从私服仓库下载依赖组件、将第三方项目jar上传到私服供其他项目组使用。&lt;br&gt;登录&lt;a href=&quot;http://172.16.206.33:8081/nexus/index.html，点击左侧菜单栏的“Repositories”。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.206.33:8081/nexus/index.html，点击左侧菜单栏的“Repositories”。&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Maven/8.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;一般用到的仓库种类是hosted、proxy。Hosted代表宿主仓库，用来发布一些第三方不允许的组件，比如oracle驱动、比如商业软件jar包。Proxy代表代理远程的仓库，最典型的就是Maven官方中央仓库、JBoss仓库等等。如果构建的Maven项目本地仓库没有依赖包，那么就会去这个代理站点去下载，那么如果代理站点也没有此依赖包，就回去远程中央仓库下载依赖，这些中央仓库就是proxy。代理站点下载成功后再下载至本机。一般情况下Maven这个自带的默认仓库一般情况下已经够大多数项目使用了。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;hosted   类型的仓库，内部项目的发布仓库&lt;/li&gt;
&lt;li&gt;releases 内部的模块中release模块的发布仓库&lt;/li&gt;
&lt;li&gt;snapshots 发布内部的SNAPSHOT模块的仓库&lt;/li&gt;
&lt;li&gt;3rd party 第三方依赖的仓库，这个数据通常是由内部人员自行下载之后发布上去&lt;/li&gt;
&lt;li&gt;proxy   类型的仓库，从远程中央仓库中寻找数据的仓库&lt;/li&gt;
&lt;li&gt;group   类型的仓库，组仓库用来方便我们开发人员进行设置的仓库&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;maven项目索引&quot;&gt;&lt;a href=&quot;#maven项目索引&quot; class=&quot;headerlink&quot; title=&quot;maven项目索引&quot;&gt;&lt;/a&gt;maven项目索引&lt;/h3&gt;&lt;p&gt;下载Maven项目索引，项目索引是为了使用者能够在私服站点查找依赖使用的功能。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Maven/9.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;保存后后台会运行一个任务，点击菜单栏的Scheduled Tasks选项即可看到有个任务在RUNNING。 下载完成后，Maven索引就可以使用了，在搜索栏输入要搜索的项，就可以查到相关的信息。例如spring-core&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Maven/10.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;就可以检索出它的相关信息，包括怎么配置依赖信息。我们要想使用这个私服仓库，先在项目pom中配置相关私服信息。&lt;/p&gt;
&lt;h2 id=&quot;Maven项目配置&quot;&gt;&lt;a href=&quot;#Maven项目配置&quot; class=&quot;headerlink&quot; title=&quot;Maven项目配置&quot;&gt;&lt;/a&gt;Maven项目配置&lt;/h2&gt;&lt;p&gt;在pom.xml文件中指定仓库：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;repositories&amp;gt;
    &amp;lt;repository&amp;gt;
        &amp;lt;id&amp;gt;nexus&amp;lt;/id&amp;gt;
        &amp;lt;url&amp;gt;http://172.16.206.33:8081/nexus/content/groups/public/&amp;lt;/url&amp;gt;
        &amp;lt;snapshots&amp;gt;&amp;lt;enabled&amp;gt;true&amp;lt;/enabled&amp;gt;&amp;lt;/snapshots&amp;gt;
        &amp;lt;releases&amp;gt;&amp;lt;enabled&amp;gt;true&amp;lt;/enabled&amp;gt;&amp;lt;/releases&amp;gt;
    &amp;lt;/repository&amp;gt;
&amp;lt;/repositories&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在pom.xml文件中指定插件仓库：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;pluginRepositories&amp;gt;
    &amp;lt;pluginRepository&amp;gt;
        &amp;lt;id&amp;gt;nexus&amp;lt;/id&amp;gt;
        &amp;lt;url&amp;gt;http://172.16.206.33:8081/nexus/content/groups/public/&amp;lt;/url&amp;gt;
        &amp;lt;snapshots&amp;gt;&amp;lt;enabled&amp;gt;true&amp;lt;/enabled&amp;gt;&amp;lt;/snapshots&amp;gt;
        &amp;lt;releases&amp;gt;&amp;lt;enabled&amp;gt;true&amp;lt;/enabled&amp;gt;&amp;lt;/releases&amp;gt;
    &amp;lt;/pluginRepository&amp;gt;
&amp;lt;/pluginRepositories&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;以上的配置使得只有本项目才在私服下载组件。这个Maven项目构建的时候会从私服下载相关依赖。&lt;br&gt;上面的配置仅仅是在此项目中生效，对于其他项目还是不起作用。如果相对Maven的其他项目也生效的话。需要修改全局的settings.xml文件。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Maven/11.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;追加激活profile：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;activeProfiles&amp;gt;  
     &amp;lt;activeProfile&amp;gt;central&amp;lt;/activeProfile&amp;gt;  
&amp;lt;/activeProfiles
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;之后所有本机的Maven项目就在私服下载组件。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;为什么要搭建私服&quot;&gt;&lt;a href=&quot;#为什么要搭建私服&quot; class=&quot;headerlink&quot; title=&quot;为什么要搭建私服&quot;&gt;&lt;/a&gt;为什么要搭建私服&lt;/h2&gt;&lt;p&gt;私服不是Maven的核心概念，它仅仅是一种衍生出来的特殊的Maven仓库。通过建立自己的私服，就可以降低中央仓库负荷、节省外网带宽、加速Maven构建、自己部署构建等，从而高效地使用Maven。Nexus也是当前最流行的Maven仓库管理软件。&lt;br&gt;
    
    </summary>
    
      <category term="maven" scheme="http://yoursite.com/categories/maven/"/>
    
    
      <category term="nexus" scheme="http://yoursite.com/tags/nexus/"/>
    
  </entry>
  
  <entry>
    <title>Filebeat日志收集器</title>
    <link href="http://yoursite.com/2017/10/24/Filebeat%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E5%99%A8/"/>
    <id>http://yoursite.com/2017/10/24/Filebeat日志收集器/</id>
    <published>2017-10-24T02:26:16.000Z</published>
    <updated>2017-11-13T07:38:39.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;Beats工具&quot;&gt;&lt;a href=&quot;#Beats工具&quot; class=&quot;headerlink&quot; title=&quot;Beats工具&quot;&gt;&lt;/a&gt;Beats工具&lt;/h2&gt;&lt;p&gt;Beats是elastic公司的一款轻量级数据采集产品，它是从packetbeat发展出来的数据收集器系统，它包含了几个子产品：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Packetbeat(用于监控网络流量)&lt;/li&gt;
&lt;li&gt;Filebeat(用于监听日志数据)&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Metricbeat(用于搜集CPU、内存、磁盘的信息以及Nginx、Redis等服务的数据)&lt;/li&gt;
&lt;li&gt;Winlogbeat(用于搜集windows事件日志)&lt;/li&gt;
&lt;li&gt;Heartbeat(用于监控服务的可用性)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Beats可以直接把数据发送给Elasticsearch或者发送给Logstash，然后Logstash发送给Elasticsearch，然后进行后续的数据分析活动。&lt;br&gt;由于他们都是基于libbeat写出来的，提供了统一的数据发送方法，输入配置解析，日志记录框架等功能。所有的beat工具，在配置上基本相同，只是input输入的地方各有差异。&lt;br&gt;如果有其他特殊需求，可以使用Go语言借助libbeat库方便地开发自己的Beat工具，社区中有很多工具可以参考。&lt;/p&gt;
&lt;h2 id=&quot;Logstash和Filebeat&quot;&gt;&lt;a href=&quot;#Logstash和Filebeat&quot; class=&quot;headerlink&quot; title=&quot;Logstash和Filebeat&quot;&gt;&lt;/a&gt;Logstash和Filebeat&lt;/h2&gt;&lt;p&gt;上一篇文章中介绍过，Logstash是跑在JVM上的，需要消耗较多的系统资源，而Filebeat则是一个轻量级的日志采集工具，占用资源更少。我们完全可以在每台机器上安装启动个Filebeat，由Filebeat来采集日志，将数据传输给Redis或者Kafka，然后logstash去获取，利用filter功能过滤分析，然后存储到elasticsearch中。架构图如下：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/8.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;环境信息&quot;&gt;&lt;a href=&quot;#环境信息&quot; class=&quot;headerlink&quot; title=&quot;环境信息&quot;&gt;&lt;/a&gt;环境信息&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;操作系统版本&lt;/th&gt;
&lt;th&gt;IP地址&lt;/th&gt;
&lt;th&gt;安装软件&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;hadoop16&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.206.16&lt;/td&gt;
&lt;td&gt;elasticsearch-5.6.3.zip、kibana-5.6.3-linux-x86_64.tar.gz&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;spark32&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.206.32&lt;/td&gt;
&lt;td&gt;logstash-5.6.3.tar.gz、filebeat-5.6.3-linux-x86_64.tar.gz、Nginx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;osb30&lt;/td&gt;
&lt;td&gt;Redhat 6.5&lt;/td&gt;
&lt;td&gt;172.16.206.30&lt;/td&gt;
&lt;td&gt;redis-3.2.2.tar.gz&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&quot;安装配置Redis&quot;&gt;&lt;a href=&quot;#安装配置Redis&quot; class=&quot;headerlink&quot; title=&quot;安装配置Redis&quot;&gt;&lt;/a&gt;安装配置Redis&lt;/h2&gt;&lt;h3 id=&quot;安装Redis&quot;&gt;&lt;a href=&quot;#安装Redis&quot; class=&quot;headerlink&quot; title=&quot;安装Redis&quot;&gt;&lt;/a&gt;安装Redis&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;# yum install readline-devel pcre-devel openssl-devel -y
# cd /usr/local/
# tar zxf redis-3.2.2.tar.gz 
# cd redis-3.2.2/
# make
# make install
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;配置Redis&quot;&gt;&lt;a href=&quot;#配置Redis&quot; class=&quot;headerlink&quot; title=&quot;配置Redis&quot;&gt;&lt;/a&gt;配置Redis&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;# cd /usr/local/redis-3.2.2/
# cp redis.conf /etc/
# vim /etc/redis.conf
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;1.打开/etc/redis.conf，将daemonize处修改为yes。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/9.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;2.配置Redis持久化策略&lt;br&gt;使用RDB和AOF双持久化策略：其中默认开启了RDB持久化，我们只需要开启AOF持久化。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/10.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;3.配置redis日志文件&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/11.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;4.配置数据文件存放路径，在/目录下面创建/RedisData目录&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# mkdir /RedisData
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/12.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;5.修改Redis监听地址&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/13.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;6.修改Redis监听端口&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/14.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;7.在redis3.2之后，redis增加了protected-mode，在这个模式下，即使注释掉了bind 127.0.0.1，再访问redis的时候还是报错，所以要如下设置&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/15.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;启动-停止Redis&quot;&gt;&lt;a href=&quot;#启动-停止Redis&quot; class=&quot;headerlink&quot; title=&quot;启动/停止Redis&quot;&gt;&lt;/a&gt;启动/停止Redis&lt;/h3&gt;&lt;p&gt;启动：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# /usr/local/bin/redis-server /etc/redis.conf
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;停止：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# /usr/local/redis-3.2.2/src/redis-cli -p 6400 shutdown
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;安装配置Filebeat&quot;&gt;&lt;a href=&quot;#安装配置Filebeat&quot; class=&quot;headerlink&quot; title=&quot;安装配置Filebeat&quot;&gt;&lt;/a&gt;安装配置Filebeat&lt;/h2&gt;&lt;p&gt;filebeat的工作流程：当开启filebeat程序的时候，它会启动一个或多个探测器（prospectors）去检测指定的日志目录或文件，对于探测器找出的每一个日志文件，filebeat启动收割进程（harvester），每一个收割进程读取一个日志文件的新内容，并发送这些新的日志数据到处理程序（spooler），处理程序会集合这些事件，最后filebeat会发送集合的数据到你指定的地点（Elasticsearch、Logstash、Kafka或者Redis）。&lt;/p&gt;
&lt;h3 id=&quot;安装Filebeat&quot;&gt;&lt;a href=&quot;#安装Filebeat&quot; class=&quot;headerlink&quot; title=&quot;安装Filebeat&quot;&gt;&lt;/a&gt;安装Filebeat&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@spark32 opt]# wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-5.6.3-linux-x86_64.tar.gz
[root@spark32 opt]# ln -sv filebeat-5.6.3-linux-x86_64 filebeat
‘filebeat’ -&amp;gt; ‘filebeat-5.6.3-linux-x86_64’
[root@spark32 opt]# cd filebeat
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;配置Filebeat&quot;&gt;&lt;a href=&quot;#配置Filebeat&quot; class=&quot;headerlink&quot; title=&quot;配置Filebeat&quot;&gt;&lt;/a&gt;配置Filebeat&lt;/h3&gt;&lt;p&gt;1.定义你的日志文件的路径（一个或多个）&lt;br&gt;对于大多数的基本filebeat配置，可以定义一个单一探测器针对一个单一的路径，例如：&lt;br&gt;filebeat.prospectors:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- input_type: log
  paths:
    - /usr/local/openresty/nginx/logs/host.access.log
  #json.keys_under_root: true 若收取日志格式为json的log，请开启此配置
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.定义输出日志，我这里输出到redis中&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;output.redis:
  hosts: [&amp;quot;172.16.206.30:6400&amp;quot;]
  #password: &amp;quot;my_password&amp;quot;
  key: &amp;quot;filebeat&amp;quot;
  db: 0
  timeout: 5
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;启动Filebeat&quot;&gt;&lt;a href=&quot;#启动Filebeat&quot; class=&quot;headerlink&quot; title=&quot;启动Filebeat&quot;&gt;&lt;/a&gt;启动Filebeat&lt;/h3&gt;&lt;p&gt;测试配置文件语法：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cd /opt/filebeat/
# ./filebeat -configtest -e
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;启动：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# ./filebeat &amp;amp;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查看数据是否进入redis：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@osb30 src]# ./redis-cli -p 6400 
127.0.0.1:6400&amp;gt; help @LIST
127.0.0.1:6400&amp;gt; LLEN filebeat
(integer) 15
127.0.0.1:6400&amp;gt; LINDEX filebeat 1
&amp;quot;{\&amp;quot;@timestamp\&amp;quot;:\&amp;quot;2017-10-23T07:13:37.551Z\&amp;quot;,\&amp;quot;beat\&amp;quot;:{\&amp;quot;hostname\&amp;quot;:\&amp;quot;spark32\&amp;quot;,\&amp;quot;name\&amp;quot;:\&amp;quot;spark32\&amp;quot;,\&amp;quot;version\&amp;quot;:\&amp;quot;5.6.3\&amp;quot;},\&amp;quot;input_type\&amp;quot;:\&amp;quot;log\&amp;quot;,\&amp;quot;message\&amp;quot;:\&amp;quot;172.16.4.81 - - [19/Oct/2017:09:47:17 +0800] \\\&amp;quot;GET /favicon.ico HTTP/1.1\\\&amp;quot; 404 576 \\\&amp;quot;http://172.16.206.32:808/\\\&amp;quot; \\\&amp;quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\\\&amp;quot; \\\&amp;quot;-\\\&amp;quot;\&amp;quot;,\&amp;quot;offset\&amp;quot;:439,\&amp;quot;source\&amp;quot;:\&amp;quot;/usr/local/openresty/nginx/logs/host.access.log\&amp;quot;,\&amp;quot;type\&amp;quot;:\&amp;quot;log\&amp;quot;}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;浏览器访问下nginx，再次查看redis数据：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;127.0.0.1:6400&amp;gt; LLEN filebeat
(integer) 16
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;安装配置Elasticsearch&quot;&gt;&lt;a href=&quot;#安装配置Elasticsearch&quot; class=&quot;headerlink&quot; title=&quot;安装配置Elasticsearch&quot;&gt;&lt;/a&gt;安装配置Elasticsearch&lt;/h2&gt;&lt;p&gt;关于Elasticsearch的安装部署详见上一篇博客&lt;a href=&quot;http://jkzhao.github.io/2017/10/20/ELK%E5%AE%9E%E6%88%98/#more&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;《ELK实战》&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&quot;安装配置Logstash&quot;&gt;&lt;a href=&quot;#安装配置Logstash&quot; class=&quot;headerlink&quot; title=&quot;安装配置Logstash&quot;&gt;&lt;/a&gt;安装配置Logstash&lt;/h2&gt;&lt;p&gt;Logstash从redis中取数据，输出结果到Elasticsearch中。关于Logstash的安装部署详见上一篇博客&lt;a href=&quot;http://jkzhao.github.io/2017/10/20/ELK%E5%AE%9E%E6%88%98/#more&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;《ELK实战》&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id=&quot;配置Logstash&quot;&gt;&lt;a href=&quot;#配置Logstash&quot; class=&quot;headerlink&quot; title=&quot;配置Logstash&quot;&gt;&lt;/a&gt;配置Logstash&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;# cd /opt/logstash-5.6.3/conf
# vim logstash_redis.conf
input {
  redis {
    host  =&amp;gt;  &amp;quot;172.16.206.30&amp;quot;
    port  =&amp;gt;  &amp;quot;6400&amp;quot;
    data_type  =&amp;gt;  &amp;quot;list&amp;quot;
    key  =&amp;gt;  &amp;quot;filebeat&amp;quot;
  }
}

filter {
  grok {
    match =&amp;gt; { &amp;quot;message&amp;quot; =&amp;gt; &amp;quot;%{NGINXACCESS}&amp;quot; }
  }
}

output {
  elasticsearch {
    hosts    =&amp;gt;  [&amp;quot;172.16.206.16:9200&amp;quot;]
    action   =&amp;gt;  &amp;quot;index&amp;quot;
    index    =&amp;gt;  &amp;quot;filebeat-%{+YYYY.MM.dd}&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;启动Logstash&quot;&gt;&lt;a href=&quot;#启动Logstash&quot; class=&quot;headerlink&quot; title=&quot;启动Logstash&quot;&gt;&lt;/a&gt;启动Logstash&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@spark32 logstash-5.6.3]# bin/logstash -f /opt/logstash-5.6.3/conf/logstash_redis.conf &amp;amp;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查看redis中的数据，发现已经全部被消费了：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;127.0.0.1:6400&amp;gt; LLEN filebeat
(integer) 0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查看Elasticsearch中的索引：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# curl -XGET &amp;apos;172.16.206.16:9200/_cat/indices&amp;apos;
yellow open filebeat-2017.10.23 PQe6qyVfSSG7GHQJUOPWtA 5 1 18 0 92.8kb 92.8kb
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;安装配置Kibana&quot;&gt;&lt;a href=&quot;#安装配置Kibana&quot; class=&quot;headerlink&quot; title=&quot;安装配置Kibana&quot;&gt;&lt;/a&gt;安装配置Kibana&lt;/h2&gt;&lt;p&gt;关于Kibana的安装部署详见上一篇博客&lt;a href=&quot;http://jkzhao.github.io/2017/10/20/ELK%E5%AE%9E%E6%88%98/#more&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;《ELK实战》&lt;/a&gt;。&lt;br&gt;浏览器访问：&lt;a href=&quot;http://172.16.206.16:5601/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.206.16:5601/&lt;/a&gt;&lt;br&gt;1.左侧菜单点击“Management”，点击“Index Patterns”。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/16.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;2.点击“Create Index Pattern”。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/17.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;3.输入索引名字，点击“Create”&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/18.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;4.点击Discover，选择上一步创建的索引。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/19.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Beats工具&quot;&gt;&lt;a href=&quot;#Beats工具&quot; class=&quot;headerlink&quot; title=&quot;Beats工具&quot;&gt;&lt;/a&gt;Beats工具&lt;/h2&gt;&lt;p&gt;Beats是elastic公司的一款轻量级数据采集产品，它是从packetbeat发展出来的数据收集器系统，它包含了几个子产品：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Packetbeat(用于监控网络流量)&lt;/li&gt;
&lt;li&gt;Filebeat(用于监听日志数据)
    
    </summary>
    
      <category term="日志" scheme="http://yoursite.com/categories/%E6%97%A5%E5%BF%97/"/>
    
    
      <category term="ELK" scheme="http://yoursite.com/tags/ELK/"/>
    
  </entry>
  
  <entry>
    <title>ELK实战</title>
    <link href="http://yoursite.com/2017/10/20/ELK%E5%AE%9E%E6%88%98/"/>
    <id>http://yoursite.com/2017/10/20/ELK实战/</id>
    <published>2017-10-20T01:33:57.000Z</published>
    <updated>2017-11-13T07:38:32.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;ELK介绍&quot;&gt;&lt;a href=&quot;#ELK介绍&quot; class=&quot;headerlink&quot; title=&quot;ELK介绍&quot;&gt;&lt;/a&gt;ELK介绍&lt;/h2&gt;&lt;p&gt;ELK由Elasticsearch、Logstash和Kibana三部分组件组成。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Elasticsearch是个开源分布式搜索引擎，它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制，restful风格接口，多数据源，自动搜索负载等。&lt;/li&gt;
&lt;li&gt;Logstash是一个完全开源的工具，它可以对你的日志进行收集、分析，并将其存储供以后使用。&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;kibana 是一个开源和免费的工具，它可以为 Logstash 和 ElasticSearch 提供的日志分析友好的 Web 界面，可以帮助您汇总、分析和搜索重要数据日志。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;Elasticsearch&quot;&gt;&lt;a href=&quot;#Elasticsearch&quot; class=&quot;headerlink&quot; title=&quot;Elasticsearch&quot;&gt;&lt;/a&gt;Elasticsearch&lt;/h3&gt;&lt;p&gt;Elasticsearch是一个基于Lucene实现的开源、分布式、Restful的全文本搜索引擎。（仅支持文本搜索）此外，它还是一个分布式实时文档存储，其中每个文档的每个field均是被索引的数据，且可被搜索；也是一个带实时分析功能的分布式搜索引擎，能够扩展至数以百计的节点实时处理PB级的数据。&lt;br&gt;Elasticsearch借助于Lucene的API，在Lucene之外又重新封装了一层实现构建搜索引擎中的搜索组件。除此之外，Elasticsearch还新增了更强大的功能。比如把自己构建为分布式，分布式地将Lucene所提供的索引组建成shard形式，分布于多个节点之上，从而构建成分布式实时查询的组件。&lt;/p&gt;
&lt;h3 id=&quot;Logstash&quot;&gt;&lt;a href=&quot;#Logstash&quot; class=&quot;headerlink&quot; title=&quot;Logstash&quot;&gt;&lt;/a&gt;Logstash&lt;/h3&gt;&lt;p&gt;支持多数据获取机制，通过TCP/UDP协议、文件、syslog、windows EventLogs及STDIN等。获取到数据后，它支持对数据执行过滤、修改等操作。&lt;/p&gt;
&lt;p&gt;logstash配置框架：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;input {
  ...
}

filter {
  ...
}

output {
  ...
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Logstash是高度插件化的，支持4种类型的插件(每一类都有数10种具体的实现)：&lt;br&gt;input、filter、codec、output  &lt;/p&gt;
&lt;h4 id=&quot;input插件&quot;&gt;&lt;a href=&quot;#input插件&quot; class=&quot;headerlink&quot; title=&quot;input插件&quot;&gt;&lt;/a&gt;input插件&lt;/h4&gt;&lt;p&gt;下面介绍input插件几种常见的实现。&lt;br&gt;&lt;strong&gt;1.File：&lt;/strong&gt;&lt;br&gt;从指定的文件中读取事件流；其工作特性类似于tail -1，不断将文件的最后一行读出来；不过第一次读取文件时是从第1行开始的；文件中的每一行都被识别为一个事件。对于Logstash而言，每一个独立的信息就是一个事件，而对于文本文件来讲，每一个事件是用一行来表示的，如果期望将多行识别为一个事件的话，就需要codec插件。logstash使用FileWatch(Ruby Gem库)机制来监听文件的变化，FileWatch是Linux内核中提供的一个功能。可以一下子监听多个文件。文件状态记录在.sincedb数据库中。另外，FIle插件还能够自动识别你的日志滚动操作，日志一般达到某个体积、或者满足多少天后会滚动，File也能识别。它会按照上一次那个日志文件所在的位置读取，自动进行滚动，读到最新的文件。&lt;br&gt;&lt;strong&gt;2.udp插件:&lt;/strong&gt;&lt;br&gt;如果我们安装的某个程序，它能够通过udp的某个端口输出自己相关日志信息或者事件。Logstash通过udp协议从网络连接来读取Message。其必备参数为port，用于指明自己监听的端口，别的主机向这个端口发事件，host则用来指明自己监听的地址。&lt;br&gt;&lt;strong&gt;3.redis插件：&lt;/strong&gt;&lt;br&gt;允许Logstash从redis读数据。支持redis channel和lists两种方式来获取数据。&lt;/p&gt;
&lt;h4 id=&quot;filter插件&quot;&gt;&lt;a href=&quot;#filter插件&quot; class=&quot;headerlink&quot; title=&quot;filter插件&quot;&gt;&lt;/a&gt;filter插件&lt;/h4&gt;&lt;p&gt;filter插件主要用于将event通过output发出之前，对其实现某些处理功能。&lt;br&gt;&lt;strong&gt;1.grok：&lt;/strong&gt;用于分析并结构化文本数据。把每一个事件字段切好，做成结构化的形式。使得我们后续可以分析。目前能处理syslog、apache、nginx的日志等。目前Logstash提供120种grok模式，因为我们要分析文本，必须提供模式。&lt;br&gt;简单举个例子说明下什么是模式：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;55.3.244.1 GET /index.html 15824 0.043
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;比如对于上面一条日志，我们定义如下的模式：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;那么经过grok过滤后，这个事件后将会加有分析后的字段：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;client =&amp;gt; 55.3.244.1
method =&amp;gt; GET
request =&amp;gt; /index.html
bytes =&amp;gt; 15824
duration =&amp;gt; 0.043
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Logstash中默认自带了很多pattern，但是如果没有你需要的，就需要自己在文件中定义你需要的模式。&lt;br&gt;&lt;strong&gt;grok语法格式：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;%{SYNTAX:SEMANTIC}
    SYNTAX表示预定义模式名称。就是Logstash解压后pattern相关文件里已经定义好的模式，如果没有的，可以自己写在上面那个文件里，名字得全大写。具体的文件在下面的实战中会讲到。
    SEMANTIC表示匹配到的文本的自定义的标识符。比如识别处理的ip可能是clientip也可能是server端的ip，我们可以自定义名字。
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;output插件&quot;&gt;&lt;a href=&quot;#output插件&quot; class=&quot;headerlink&quot; title=&quot;output插件&quot;&gt;&lt;/a&gt;output插件&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;1.stdout：&lt;/strong&gt;标准输出插件。&lt;br&gt;&lt;strong&gt;2.elasticsearch：&lt;/strong&gt;将结果输出到Elasticsearch中。&lt;br&gt;elasticsearch插件常见配置参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;index：数据存储在ES中的哪个索引中。默认“logstash-%{+YYYY.MM.dd}”，每天使用一个单独的索引。&lt;/li&gt;
&lt;li&gt;workers：执行output的线程数。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;3.redis插件：&lt;/strong&gt;将结果输出到redis中。&lt;br&gt;Logstash使用redis作为输入或输出插件时，尤其是输出插件时，它有两种数据类型可以用来保存Logstash输出的数据。一种是list，一种是channel。一般使用list，list比较简单。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;【注意】：Logstash版本不同，每种插件支持的参数也不同，具体看&lt;a href=&quot;https://www.elastic.co/guide/en/logstash/current/index.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;官方文档&lt;/a&gt;。&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&quot;环境信息&quot;&gt;&lt;a href=&quot;#环境信息&quot; class=&quot;headerlink&quot; title=&quot;环境信息&quot;&gt;&lt;/a&gt;环境信息&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;操作系统版本&lt;/th&gt;
&lt;th&gt;IP地址&lt;/th&gt;
&lt;th&gt;安装软件&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;hadoop16&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.206.16&lt;/td&gt;
&lt;td&gt;elasticsearch-5.6.3.zip、kibana-5.6.3-linux-x86_64.tar.gz&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;spark32&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.206.32&lt;/td&gt;
&lt;td&gt;logstash-5.6.3.tar.gz&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&quot;Elasticsearch-1&quot;&gt;&lt;a href=&quot;#Elasticsearch-1&quot; class=&quot;headerlink&quot; title=&quot;Elasticsearch&quot;&gt;&lt;/a&gt;Elasticsearch&lt;/h2&gt;&lt;h3 id=&quot;安装JDK8&quot;&gt;&lt;a href=&quot;#安装JDK8&quot; class=&quot;headerlink&quot; title=&quot;安装JDK8&quot;&gt;&lt;/a&gt;安装JDK8&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;# tar zxf jdk-8u73-linux-x64.gz -C /usr/java/
# vim /etc/profile
export JAVA_HOME=/usr/java/jdk1.8.0_73
export PATH=$JAVA_HOME/bin:$PATH
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
# source /etc/profile
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;新建用户&quot;&gt;&lt;a href=&quot;#新建用户&quot; class=&quot;headerlink&quot; title=&quot;新建用户&quot;&gt;&lt;/a&gt;新建用户&lt;/h3&gt;&lt;p&gt;【注意】:elasticsearch不能使用root用户去启动。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# groupadd -g 510 es
# useradd -g 510 -u 510 es
# echo &amp;quot;wisedu123&amp;quot; | passwd --stdin es &amp;amp;&amp;gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;安装配置Elasticsearch&quot;&gt;&lt;a href=&quot;#安装配置Elasticsearch&quot; class=&quot;headerlink&quot; title=&quot;安装配置Elasticsearch&quot;&gt;&lt;/a&gt;安装配置Elasticsearch&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@hadoop16 opt]# unzip -oq elasticsearch-5.6.3.zip
[root@hadoop16 opt]# vim elasticsearch-5.6.3/config/elasticsearch.yml 
cluster.name: loges
node.name: hadoop16
network.host: 172.16.206.16
bootstrap.memory_lock: true
[root@hadoop16 opt]# vim /etc/security/limits.conf
es - memlock -1
[root@hadoop16 opt]# chown -R es.es elasticsearch-5.6.3
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;配置说明：&lt;br&gt;bootstrap.memory_lock: true&lt;br&gt;这个配置的作用是保护Elasticsearch使用的内存防止其被swapped。&lt;/p&gt;
&lt;h3 id=&quot;优化Elasticsearch&quot;&gt;&lt;a href=&quot;#优化Elasticsearch&quot; class=&quot;headerlink&quot; title=&quot;优化Elasticsearch&quot;&gt;&lt;/a&gt;优化Elasticsearch&lt;/h3&gt;&lt;p&gt;1.配置操作系统文件描述符数&lt;br&gt;输入下面的命令进行查看：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ ulimit -a
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;找到open files那行：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;open files                      (-n) 1024
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;设置需要修改：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/security/limits.conf
es               -       nofile          65536
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.增大虚拟内存mmap count配置&lt;br&gt;备注：如果你以.deb或.rpm包安装，则默认不需要设置此项，因为已经被自动设置，查看方式为：&lt;br&gt;sysctl vm.max_map_count&lt;br&gt;如果是手动安装，以root身份执行如下命令：&lt;br&gt;sysctl vm.max_map_count=262144&lt;br&gt;并修改文件使设置永久生效：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/sysctl.conf    
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;加一行：vm.max_map_count = 262144&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# sysctl -p
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;改完后，重启elasticsearch。&lt;br&gt;如果需要需改JVM大小，请修改 jvm.options 配置文件。&lt;/p&gt;
&lt;h3 id=&quot;启动Elasticsearch&quot;&gt;&lt;a href=&quot;#启动Elasticsearch&quot; class=&quot;headerlink&quot; title=&quot;启动Elasticsearch&quot;&gt;&lt;/a&gt;启动Elasticsearch&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@hadoop16 opt]# su - es
[es@hadoop16 ~]$ /opt/elasticsearch-5.6.3/bin/elasticsearch -d
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;检验bootstrap.mlockall: true是否生效：&lt;br&gt;curl &lt;a href=&quot;http://172.16.206.16:9200/_nodes/process?pretty&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.206.16:9200/_nodes/process?pretty&lt;/a&gt;&lt;br&gt;关注这个这个请求返回数据中的mlockall的值，如果为false，则说明锁定内存失败，这可能由于运行elasticsearch的用户不具备这样的权限。解决该问题的方法是：&lt;br&gt;在运行elasticsearch之前，以root身份执行&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ulimit -l unlimited
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后再次重启elasticsearch。并查看上面的请求中的mlockall的值是否为true。&lt;br&gt;【注意】:这时候需要在root执行ulimit -l unlimited的shell终端上su - es，然后重启elasticsearch。因为这是命令行设置的ulimit -l unlimited，只对当前会话生效。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# ulimit -l unlimited
# su - es
$ ps -ef|grep elasticsearch
$ kill -9 27189
$ /usr/local/elasticsearch/bin/elasticsearch -d
$ curl http://172.16.206.16:9200/_nodes/process?pretty
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;要想永久修改锁定内存大小无限制，需修改/etc/security/limits.conf，添加下面的内容，改完不需要重启系统，但是需要重新打开一个shell建立会话。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;es - memlock -1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其中，es代表运行elasticsearch的用户，-表示同时设置了soft和hard，memlock代表设置的是”锁定内存”这个类型，-1(unlimited或者infinity)代表没限制。&lt;br&gt;【补充】: 要使 /etc/security/limits.conf 文件配置生效，必须要确保 pam_limits.so 文件被加入到相关的启动文件中，启动文件位于/etc/pam.d路径下，如该路径下sshd、login、system-auth等，一般是system-auth文件负责加载该so文件。只要加载了pam_limits.so，则配置就会生效，无需重启系统。&lt;/p&gt;
&lt;h3 id=&quot;安装配置head插件&quot;&gt;&lt;a href=&quot;#安装配置head插件&quot; class=&quot;headerlink&quot; title=&quot;安装配置head插件&quot;&gt;&lt;/a&gt;安装配置head插件&lt;/h3&gt;&lt;p&gt;项目地址：&lt;a href=&quot;https://github.com/mobz/elasticsearch-head&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/mobz/elasticsearch-head&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;安装head插件&quot;&gt;&lt;a href=&quot;#安装head插件&quot; class=&quot;headerlink&quot; title=&quot;安装head插件&quot;&gt;&lt;/a&gt;安装head插件&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@hadoop16 opt]# git clone git://github.com/mobz/elasticsearch-head.git
[root@hadoop16 opt]# yum -y install epel-release
[root@hadoop16 opt]# yum install nodejs -y
[root@hadoop16 opt]# cd elasticsearch-head/
[root@hadoop16 elasticsearch-head]# npm install
npm: relocation error: npm: symbol SSL_set_cert_cb, version libssl.so.10 not defined in file libssl.so.10 with link time reference
[root@hadoop16 elasticsearch-head]# yum update openssl -y
[root@hadoop16 elasticsearch-head]# npm install 
# 如果下载速度太慢，可以如下方式安装
[root@hadoop16 elasticsearch-head]# npm install -gd express --registry=http://registry.npm.taobao.org
# 为了避免每次安装都需要--registry参数，可以使用如下命令进行永久设置：
[root@hadoop16 elasticsearch-head]# npm config set registry http://registry.npm.taobao.org 
[root@hadoop16 elasticsearch-head]# npm install grunt --save-dev
[root@hadoop16 elasticsearch-head]# npm install grunt-contrib-clean 
[root@hadoop16 elasticsearch-head]# npm install grunt-contrib-concat 
[root@hadoop16 elasticsearch-head]# npm install grunt-contrib-watch 
[root@hadoop16 elasticsearch-head]# npm install grunt-contrib-connect 
[root@hadoop16 elasticsearch-head]# npm install grunt-contrib-copy 
[root@hadoop16 elasticsearch-head]# npm install grunt-contrib-jasmine 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;奇怪的是最后一个没有安装成功，是因为该模块依赖了phantomjs。但是配置之后，依然无法安装。直接启动就可以了：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@hadoop16 elasticsearch-head]# grunt server 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;以上安装head插件的步骤太过复杂，我们可以将下载下来的elasticsearch-head包放入tomcat中，直接启动tomcat就可以访问head插件了。本实验环境下是采用将head插件放入tomcat中运行。&lt;/p&gt;
&lt;h4 id=&quot;配置&quot;&gt;&lt;a href=&quot;#配置&quot; class=&quot;headerlink&quot; title=&quot;配置&quot;&gt;&lt;/a&gt;配置&lt;/h4&gt;&lt;p&gt;1.修改Elasticsearch的配置文件elasticsearch.yml，增加跨域的配置(需要重启es才能生效)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;http.cors.enabled: true
http.cors.allow-origin: &amp;quot;*&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.编辑head/Gruntfile.js，修改服务器监听地址，增加hostname属性，将其值设置为*。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@hadoop16 elasticsearch-head]# pwd
/opt/apache-tomcat-8.5.23/webapps/elasticsearch-head
[root@hadoop16 elasticsearch-head]# vim Gruntfile.js
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;以下两种配置都是可以的：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Type1
connect: {
        hostname: &amp;apos;*&amp;apos;,
        server: {
                options: {
                        port: 9100,
                        base: &amp;apos;.&amp;apos;,
                        keepalive: true
                }
        }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;　　&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Type 2
connect: {
        server: {
                options: {
                        hostname: &amp;apos;*&amp;apos;,
                        port: 9100,
                        base: &amp;apos;.&amp;apos;,
                        keepalive: true
                }
        }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3.编辑head/_site/app.js，修改head连接es的地址，将localhost修改为es的IP地址&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@hadoop16 elasticsearch-head]# pwd
/opt/apache-tomcat-8.5.23/webapps/elasticsearch-head
[root@hadoop16 elasticsearch-head]# vim _site/app.js
# 原配置
this.base_uri = this.config.base_uri || this.prefs.get(&amp;quot;app-base_uri&amp;quot;) || &amp;quot;http://localhost:9200&amp;quot;;
# 将localhost修改为ES的IP地址
this.base_uri = this.config.base_uri || this.prefs.get(&amp;quot;app-base_uri&amp;quot;) || &amp;quot;http://YOUR-ES-IP:9200&amp;quot;;
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;启动&quot;&gt;&lt;a href=&quot;#启动&quot; class=&quot;headerlink&quot; title=&quot;启动&quot;&gt;&lt;/a&gt;启动&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@hadoop16 elasticsearch-head]# cd /opt/apache-tomcat-8.5.23/
[root@hadoop16 apache-tomcat-8.5.23]# bin/startup.sh 
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;Logstash-1&quot;&gt;&lt;a href=&quot;#Logstash-1&quot; class=&quot;headerlink&quot; title=&quot;Logstash&quot;&gt;&lt;/a&gt;Logstash&lt;/h2&gt;&lt;h3 id=&quot;安装JDK8-1&quot;&gt;&lt;a href=&quot;#安装JDK8-1&quot; class=&quot;headerlink&quot; title=&quot;安装JDK8&quot;&gt;&lt;/a&gt;安装JDK8&lt;/h3&gt;&lt;p&gt;Logstash是jruby研发的，需要跑在JVM上，所以需要安装JDK。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# tar zxf jdk-8u73-linux-x64.gz -C /usr/java/
# vim /etc/profile
export JAVA_HOME=/usr/java/jdk1.8.0_73
export PATH=$JAVA_HOME/bin:$PATH
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
# source /etc/profile
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;安装配置Logstash&quot;&gt;&lt;a href=&quot;#安装配置Logstash&quot; class=&quot;headerlink&quot; title=&quot;安装配置Logstash&quot;&gt;&lt;/a&gt;安装配置Logstash&lt;/h3&gt;&lt;p&gt;我这里选择二进制包安装。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 opt]# tar zxf logstash-5.6.3.tar.gz 
[root@spark32 opt]# cd logstash-5.6.3/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;由于我这里选择是二进制安装，pattern文件位置在：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 logstash-5.6.3]# ls /opt/logstash-5.6.3/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/grok-patterns
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;收集Nginx-access日志&quot;&gt;&lt;a href=&quot;#收集Nginx-access日志&quot; class=&quot;headerlink&quot; title=&quot;收集Nginx access日志&quot;&gt;&lt;/a&gt;收集Nginx access日志&lt;/h3&gt;&lt;p&gt;Logstash收集Nginx日志，输出到Elasticsearch中。&lt;/p&gt;
&lt;h4 id=&quot;创建Nginx-pattern&quot;&gt;&lt;a href=&quot;#创建Nginx-pattern&quot; class=&quot;headerlink&quot; title=&quot;创建Nginx pattern&quot;&gt;&lt;/a&gt;创建Nginx pattern&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# vim /opt/logstash-5.6.3/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/grok-patterns
# Nginx log
NGUSERNAME [a-zA-Z\.\@\-\+_%]+
NGUSER %{NGUSERNAME}
NGINXACCESS %{IPORHOST:clientip} - %{NOTSPACE:remote_user} \[%{HTTPDATE:timestamp}\] \&amp;quot;(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\&amp;quot; %{NUMBER:response} (?:%{NUMBER:bytes}|-) %{QS:referrer} %{QS:agent} %{NOTSPACE:http_x_forwarded_for}
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;创建Logstash配置文件&quot;&gt;&lt;a href=&quot;#创建Logstash配置文件&quot; class=&quot;headerlink&quot; title=&quot;创建Logstash配置文件&quot;&gt;&lt;/a&gt;创建Logstash配置文件&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@spark32 logstash-5.6.3]# cd /opt/logstash-5.6.3
[root@spark32 logstash-5.6.3]# mkdir conf
[root@spark32 logstash-5.6.3]# cd conf
[root@spark32 conf]# vim logstash_nginx.conf
input {
  file {
    path  =&amp;gt;  [&amp;quot;/usr/local/openresty/nginx/logs/host.access.log&amp;quot;]
    type  =&amp;gt;  &amp;quot;nginxlog&amp;quot;
    start_position  =&amp;gt;  &amp;quot;beginning&amp;quot;
  }
}

filter {
  grok {
    match =&amp;gt; { &amp;quot;message&amp;quot; =&amp;gt; &amp;quot;%{NGINXACCESS}&amp;quot; }
  }
}

output {
  elasticsearch {
    hosts    =&amp;gt;  [&amp;quot;172.16.206.16:9200&amp;quot;]
    action   =&amp;gt;  &amp;quot;index&amp;quot;
    index    =&amp;gt;  &amp;quot;logstash-%{+YYYY.MM.dd}&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;启动Logstash&quot;&gt;&lt;a href=&quot;#启动Logstash&quot; class=&quot;headerlink&quot; title=&quot;启动Logstash&quot;&gt;&lt;/a&gt;启动Logstash&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@spark32 logstash-5.6.3]# bin/logstash -f /opt/logstash-5.6.3/conf/logstash_nginx.conf -t
Sending Logstash&amp;apos;s logs to /opt/logstash-5.6.3/logs which is now configured via log4j2.properties
[2017-10-20T17:07:14,793][INFO ][logstash.modules.scaffold] Initializing module {:module_name=&amp;gt;&amp;quot;fb_apache&amp;quot;, :directory=&amp;gt;&amp;quot;/opt/logstash-5.6.3/modules/fb_apache/configuration&amp;quot;}
[2017-10-20T17:07:14,797][INFO ][logstash.modules.scaffold] Initializing module {:module_name=&amp;gt;&amp;quot;netflow&amp;quot;, :directory=&amp;gt;&amp;quot;/opt/logstash-5.6.3/modules/netflow/configuration&amp;quot;}
[2017-10-20T17:07:14,803][INFO ][logstash.setting.writabledirectory] Creating directory {:setting=&amp;gt;&amp;quot;path.queue&amp;quot;, :path=&amp;gt;&amp;quot;/opt/logstash-5.6.3/data/queue&amp;quot;}
[2017-10-20T17:07:14,804][INFO ][logstash.setting.writabledirectory] Creating directory {:setting=&amp;gt;&amp;quot;path.dead_letter_queue&amp;quot;, :path=&amp;gt;&amp;quot;/opt/logstash-5.6.3/data/dead_letter_queue&amp;quot;}
Configuration OK
[2017-10-20T17:07:14,999][INFO ][logstash.runner          ] Using config.test_and_exit mode. Config Validation Result: OK. Exiting Logstash
[root@spark32 logstash-5.6.3]# bin/logstash -f /opt/logstash-5.6.3/conf/logstash_nginx.conf &amp;amp; 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;记录收集到日志的文件位置在:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 file]# ls -a
.  ..  .sincedb_650663ba19529187a32a8b9dc99049f8
[root@spark32 file]# pwd
/opt/logstash-5.6.3/data/plugins/inputs/file
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查看Elasticsearch索引：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[es@hadoop16 elasticsearch-5.6.3]$ curl -XGET &amp;apos;172.16.206.16:9200/_cat/indices&amp;apos;
yellow open logstash-2017.10.20 DVARGYZ2R9CfT-xyLrhyAQ 5 1 7 0 49.9kb 49.9kb
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;Kibana&quot;&gt;&lt;a href=&quot;#Kibana&quot; class=&quot;headerlink&quot; title=&quot;Kibana&quot;&gt;&lt;/a&gt;Kibana&lt;/h2&gt;&lt;h3 id=&quot;安装配置kibana&quot;&gt;&lt;a href=&quot;#安装配置kibana&quot; class=&quot;headerlink&quot; title=&quot;安装配置kibana&quot;&gt;&lt;/a&gt;安装配置kibana&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@hadoop16 opt]# tar zxf kibana-5.6.3-linux-x86_64.tar.gz 
[root@hadoop16 opt]# ln -sv kibana-5.6.3-linux-x86_64 kibana
‘kibana’ -&amp;gt; ‘kibana-5.6.3-linux-x86_64’
[root@hadoop16 opt]# cd kibana
[root@hadoop16 kibana]# cd config/
[root@hadoop16 config]# vim kibana.yml 
server.host: &amp;quot;172.16.206.16&amp;quot;
elasticsearch.url: &amp;quot;http://172.16.206.16:9200&amp;quot;
[root@hadoop16 config]# cd ..
[root@hadoop16 kibana]# bin/kibana &amp;amp;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;访问&quot;&gt;&lt;a href=&quot;#访问&quot; class=&quot;headerlink&quot; title=&quot;访问&quot;&gt;&lt;/a&gt;访问&lt;/h3&gt;&lt;p&gt;浏览器输入：&lt;a href=&quot;http://172.16.206.16:5601/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.206.16:5601/&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/1.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/2.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;点击左侧菜单的Discover：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/3.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;比如我用curl访问下nginx，然后去kibana中搜索。要稍微等一下，等日志进到Elasticsearch中。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@hadoop16 kibana]# curl http://172.16.206.32:808
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/4.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/5.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;总结：&lt;/strong&gt;&lt;br&gt;1.Logstash 主要的特点就是它的灵活性，因为它有很多插件。然后它清楚的文档已经直白的配置格式让它可以再多种场景下应用。这样的良性循环让我们可以在网上找到很多资源，几乎可以处理任何问题。&lt;br&gt;2.Logstash不支持缓存，当然我们可以使用redis或者kafka作为中心缓冲池，架构如下：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/6.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;3.Logstash是在jvm跑的，资源消耗比较大，太重量级了。后来作者又用golang写了一个功能较少但是资源消耗也小的轻量级的logstash-forwarder。后来这个人加入了elastic公司。因为elastic公司本身还收购了另一个开源项目packetbeat，而这个项目专门就是用golang写的，有整个团队，所以elastic公司干脆把logstash-forwarder的开发工作也合并到同一个golang团队来搞，于是新的项目就叫filebeat了。当然也可以自己写agent，用go、python都可以写。这样我们就可以使用轻量的日志传输工具，将数据从服务器端经由一个或多个 Logstash 中心服务器传输到 Elasticsearch。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/7.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;ELK介绍&quot;&gt;&lt;a href=&quot;#ELK介绍&quot; class=&quot;headerlink&quot; title=&quot;ELK介绍&quot;&gt;&lt;/a&gt;ELK介绍&lt;/h2&gt;&lt;p&gt;ELK由Elasticsearch、Logstash和Kibana三部分组件组成。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Elasticsearch是个开源分布式搜索引擎，它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制，restful风格接口，多数据源，自动搜索负载等。&lt;/li&gt;
&lt;li&gt;Logstash是一个完全开源的工具，它可以对你的日志进行收集、分析，并将其存储供以后使用。
    
    </summary>
    
      <category term="日志" scheme="http://yoursite.com/categories/%E6%97%A5%E5%BF%97/"/>
    
    
      <category term="ELK" scheme="http://yoursite.com/tags/ELK/"/>
    
  </entry>
  
  <entry>
    <title>EFK收集Kubernetes应用日志</title>
    <link href="http://yoursite.com/2017/10/12/EFK%E6%94%B6%E9%9B%86Kubernetes%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/"/>
    <id>http://yoursite.com/2017/10/12/EFK收集Kubernetes应用日志/</id>
    <published>2017-10-12T06:53:08.000Z</published>
    <updated>2017-12-04T02:36:46.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;EFK介绍&quot;&gt;&lt;a href=&quot;#EFK介绍&quot; class=&quot;headerlink&quot; title=&quot;EFK介绍&quot;&gt;&lt;/a&gt;EFK介绍&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Logstash(或者Fluentd)负责收集日志 &lt;/li&gt;
&lt;li&gt;Elasticsearch存储日志并提供搜索 &lt;/li&gt;
&lt;li&gt;Kibana负责日志查询和展示&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;官方地址：&lt;a href=&quot;https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch&lt;/a&gt;&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;通过在每台node上部署一个以DaemonSet方式运行的fluentd来收集每台node上的日志。Fluentd将docker日志目录/var/lib/docker/containers和/var/log目录挂载到Pod中，然后Pod会在node节点的/var/log/pods目录中创建新的目录，可以区别不同的容器日志输出，该目录下有一个日志文件链接到/var/lib/docker/contianers目录下的容器日志输出。&lt;/p&gt;
&lt;h2 id=&quot;配置efk-rbac-yaml文件&quot;&gt;&lt;a href=&quot;#配置efk-rbac-yaml文件&quot; class=&quot;headerlink&quot; title=&quot;配置efk-rbac.yaml文件&quot;&gt;&lt;/a&gt;配置efk-rbac.yaml文件&lt;/h2&gt;&lt;p&gt;EFK服务也需要一个efk-rbac.yaml文件，配置serviceaccount为efk。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 opt]# mkdir efk
[root@node1 opt]# cd efk
[root@node1 efk]# vim efk-rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: efk
  namespace: kube-system

---

kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: efk
subjects:
  - kind: ServiceAccount
    name: efk
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;配置-es-controller-yaml&quot;&gt;&lt;a href=&quot;#配置-es-controller-yaml&quot; class=&quot;headerlink&quot; title=&quot;配置 es-controller.yaml&quot;&gt;&lt;/a&gt;配置 es-controller.yaml&lt;/h2&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;19&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;20&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;21&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;22&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;23&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;24&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;25&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;26&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;27&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;28&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;29&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;30&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;31&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;32&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;33&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;34&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;35&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;36&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;37&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;38&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;39&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;40&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;41&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;42&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;43&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;44&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;45&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;46&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;47&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;48&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;49&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;50&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;51&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;[root@node1 efk]# vim es-controller.yaml&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;apiVersion: v1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;kind: ReplicationController&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  name: elasticsearch-logging-v1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  namespace: kube-system&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  labels:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    k8s-app: elasticsearch-logging&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    version: v1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    kubernetes.io/cluster-service: &amp;quot;true&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    addonmanager.kubernetes.io/mode: Reconcile&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  replicas: 2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  selector:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    k8s-app: elasticsearch-logging&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    version: v1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  template:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      labels:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        k8s-app: elasticsearch-logging&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        version: v1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        kubernetes.io/cluster-service: &amp;quot;true&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      serviceAccountName: efk&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      containers:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      - image: index.tenxcloud.com/jimmy/elasticsearch:v2.4.1-2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        name: elasticsearch-logging&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        resources:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          # need more cpu upon initialization, therefore burstable class&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          limits:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            cpu: 1000m&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          requests:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            cpu: 100m&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        ports:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        - containerPort: 9200&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          name: db&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          protocol: TCP&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        - containerPort: 9300&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          name: transport&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          protocol: TCP&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        volumeMounts:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        - name: es-persistent-storage&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          mountPath: /data&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        env:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        - name: &amp;quot;NAMESPACE&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          valueFrom:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            fieldRef:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;              fieldPath: metadata.namespace&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      volumes:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      - name: es-persistent-storage&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        emptyDir: &amp;#123;&amp;#125;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;配置-es-service-yaml&quot;&gt;&lt;a href=&quot;#配置-es-service-yaml&quot; class=&quot;headerlink&quot; title=&quot;配置 es-service.yaml&quot;&gt;&lt;/a&gt;配置 es-service.yaml&lt;/h2&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;[root@node1 efk]# vim es-service.yaml&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;apiVersion: v1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;kind: Service&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  name: elasticsearch-logging&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  namespace: kube-system&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  labels:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    k8s-app: elasticsearch-logging&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    kubernetes.io/cluster-service: &amp;quot;true&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    addonmanager.kubernetes.io/mode: Reconcile&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    kubernetes.io/name: &amp;quot;Elasticsearch&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  ports:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  - port: 9200&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    protocol: TCP&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    targetPort: db&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  selector:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    k8s-app: elasticsearch-logging&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;配置-fluentd-es-ds-yaml&quot;&gt;&lt;a href=&quot;#配置-fluentd-es-ds-yaml&quot; class=&quot;headerlink&quot; title=&quot;配置 fluentd-es-ds.yaml&quot;&gt;&lt;/a&gt;配置 fluentd-es-ds.yaml&lt;/h2&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;19&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;20&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;21&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;22&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;23&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;24&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;25&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;26&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;27&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;28&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;29&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;30&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;31&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;32&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;33&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;34&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;35&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;36&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;37&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;38&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;39&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;40&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;41&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;42&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;43&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;44&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;45&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;46&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;47&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;48&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;49&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;50&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;51&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;52&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;53&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;54&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;55&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;56&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;apiVersion: extensions/v1beta1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;kind: DaemonSet&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  name: fluentd-es-v1.22&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  namespace: kube-system&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  labels:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    k8s-app: fluentd-es&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    kubernetes.io/cluster-service: &amp;quot;true&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    addonmanager.kubernetes.io/mode: Reconcile&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    version: v1.22&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  template:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      labels:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        k8s-app: fluentd-es&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        kubernetes.io/cluster-service: &amp;quot;true&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        version: v1.22&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      # This annotation ensures that fluentd does not get evicted if the node&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      # supports critical pod annotation based priority scheme.&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      # Note that this does not guarantee admission on the nodes (#40573).&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      annotations:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        scheduler.alpha.kubernetes.io/critical-pod: &amp;apos;&amp;apos;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    spec:  &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      serviceAccountName: efk&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      containers:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      - name: fluentd-es&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        image: index.tenxcloud.com/jimmy/fluentd-elasticsearch:1.22&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        command:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          - &amp;apos;/bin/sh&amp;apos;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          - &amp;apos;-c&amp;apos;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          - &amp;apos;/usr/sbin/td-agent 2&amp;gt;&amp;amp;1 &amp;gt;&amp;gt; /var/log/fluentd.log&amp;apos;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        resources:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          limits:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            memory: 200Mi&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          requests:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            cpu: 100m&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            memory: 200Mi&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        volumeMounts:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        - name: varlog&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          mountPath: /var/log&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        - name: varlibdockercontainers&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          mountPath: /var/lib/docker/containers&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          readOnly: true&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      nodeSelector:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        beta.kubernetes.io/fluentd-ds-ready: &amp;quot;true&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      tolerations:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      - key : &amp;quot;node.alpha.kubernetes.io/ismaster&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        effect: &amp;quot;NoSchedule&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      terminationGracePeriodSeconds: 30&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      volumes:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      - name: varlog&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        hostPath:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          path: /var/log&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      - name: varlibdockercontainers&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        hostPath:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          path: /var/lib/docker/containers&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;配置-kibana-controller-yaml&quot;&gt;&lt;a href=&quot;#配置-kibana-controller-yaml&quot; class=&quot;headerlink&quot; title=&quot;配置 kibana-controller.yaml&quot;&gt;&lt;/a&gt;配置 kibana-controller.yaml&lt;/h2&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;19&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;20&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;21&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;22&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;23&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;24&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;25&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;26&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;27&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;28&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;29&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;30&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;31&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;32&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;33&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;34&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;35&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;36&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;37&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;38&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;apiVersion: extensions/v1beta1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;kind: Deployment&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  name: kibana-logging&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  namespace: kube-system&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  labels:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    k8s-app: kibana-logging&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    kubernetes.io/cluster-service: &amp;quot;true&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    addonmanager.kubernetes.io/mode: Reconcile&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  replicas: 1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  selector:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    matchLabels:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      k8s-app: kibana-logging&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  template:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      labels:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        k8s-app: kibana-logging&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      serviceAccountName: efk&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      containers:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      - name: kibana-logging&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        image: index.tenxcloud.com/jimmy/kibana:v4.6.1-1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        resources:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          # keep request = limit to keep this container in guaranteed class&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          limits:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            cpu: 100m&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          requests:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            cpu: 100m&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        env:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          - name: &amp;quot;ELASTICSEARCH_URL&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            value: &amp;quot;http://elasticsearch-logging:9200&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          - name: &amp;quot;KIBANA_BASE_URL&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            value: &amp;quot;/api/v1/proxy/namespaces/kube-system/services/kibana-logging&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        ports:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        - containerPort: 5601&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          name: ui&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          protocol: TCP&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;配置-kibana-service-yaml&quot;&gt;&lt;a href=&quot;#配置-kibana-service-yaml&quot; class=&quot;headerlink&quot; title=&quot;配置 kibana-service.yaml&quot;&gt;&lt;/a&gt;配置 kibana-service.yaml&lt;/h2&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;apiVersion: v1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;kind: Service&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  name: kibana-logging&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  namespace: kube-system&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  labels:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    k8s-app: kibana-logging&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    kubernetes.io/cluster-service: &amp;quot;true&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    addonmanager.kubernetes.io/mode: Reconcile&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    kubernetes.io/name: &amp;quot;Kibana&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  ports:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  - port: 5601&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    protocol: TCP&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    targetPort: ui&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  selector:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    k8s-app: kibana-logging&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 efk]# ls
efk-rbac.yaml  es-controller.yaml  es-service.yaml  fluentd-es-ds.yaml  kibana-controller.yaml  kibana-service.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;给-Node-设置标签&quot;&gt;&lt;a href=&quot;#给-Node-设置标签&quot; class=&quot;headerlink&quot; title=&quot;给 Node 设置标签&quot;&gt;&lt;/a&gt;给 Node 设置标签&lt;/h2&gt;&lt;p&gt;定义 DaemonSet fluentd-es-v1.22 时设置了 nodeSelector beta.kubernetes.io/fluentd-ds-ready=true ，所以需要在期望运行 fluentd 的 Node 上设置该标签；&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 efk]# kubectl label nodes 172.16.7.151 beta.kubernetes.io/fluentd-ds-ready=true
node &amp;quot;172.16.7.151&amp;quot; labeled
[root@node1 efk]# kubectl label nodes 172.16.7.152 beta.kubernetes.io/fluentd-ds-ready=true
node &amp;quot;172.16.7.152&amp;quot; labeled
[root@node1 efk]# kubectl label nodes 172.16.7.153 beta.kubernetes.io/fluentd-ds-ready=true
node &amp;quot;172.16.7.153&amp;quot; labeled
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;执行定义文件&quot;&gt;&lt;a href=&quot;#执行定义文件&quot; class=&quot;headerlink&quot; title=&quot;执行定义文件&quot;&gt;&lt;/a&gt;执行定义文件&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;[root@node1 efk]# kubectl create -f .
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;检查执行结果&quot;&gt;&lt;a href=&quot;#检查执行结果&quot; class=&quot;headerlink&quot; title=&quot;检查执行结果&quot;&gt;&lt;/a&gt;检查执行结果&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;[root@node1 efk]# kubectl get deployment -n kube-system|grep kibana
kibana-logging         1         1         1            1           1h

[root@node1 efk]# kubectl get pods -n kube-system|grep -E &amp;apos;elasticsearch|fluentd|kibana&amp;apos;
elasticsearch-logging-v1-nw3p3          1/1       Running   0          43m
elasticsearch-logging-v1-pp89h          1/1       Running   0          43m
fluentd-es-v1.22-cqd1s                  1/1       Running   0          15m
fluentd-es-v1.22-f5ljr                  0/1       Error     6          15m
fluentd-es-v1.22-x24jx                  1/1       Running   0          15m
kibana-logging-4293390753-kg8kx         1/1       Running   0          1h

[root@node1 efk]# kubectl get service  -n kube-system|grep -E &amp;apos;elasticsearch|kibana&amp;apos;
elasticsearch-logging   10.254.50.63     &amp;lt;none&amp;gt;        9200/TCP                        1h
kibana-logging          10.254.169.159   &amp;lt;none&amp;gt;        5601/TCP                        1h
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;kibana Pod 第一次启动时会用&lt;strong&gt;较长时间(10-20分钟)&lt;/strong&gt;来优化和 Cache 状态页面，可以 tailf 该 Pod 的日志观察进度。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 efk]# kubectl logs kibana-logging-4293390753-86h5d -n kube-system -f
ELASTICSEARCH_URL=http://elasticsearch-logging:9200
server.basePath: /api/v1/proxy/namespaces/kube-system/services/kibana-logging
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-10-13T00:51:31Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;info&amp;quot;,&amp;quot;optimize&amp;quot;],&amp;quot;pid&amp;quot;:5,&amp;quot;message&amp;quot;:&amp;quot;Optimizing and caching bundles for kibana and statusPage. This may take a few minutes&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-10-13T01:13:36Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;info&amp;quot;,&amp;quot;optimize&amp;quot;],&amp;quot;pid&amp;quot;:5,&amp;quot;message&amp;quot;:&amp;quot;Optimization of bundles for kibana and statusPage complete in 1324.64 seconds&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-10-13T01:13:37Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;status&amp;quot;,&amp;quot;plugin:kibana@1.0.0&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:5,&amp;quot;state&amp;quot;:&amp;quot;green&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Status changed from uninitialized to green - Ready&amp;quot;,&amp;quot;prevState&amp;quot;:&amp;quot;uninitialized&amp;quot;,&amp;quot;prevMsg&amp;quot;:&amp;quot;uninitialized&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-10-13T01:13:38Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;status&amp;quot;,&amp;quot;plugin:elasticsearch@1.0.0&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:5,&amp;quot;state&amp;quot;:&amp;quot;yellow&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Status changed from uninitialized to yellow - Waiting for Elasticsearch&amp;quot;,&amp;quot;prevState&amp;quot;:&amp;quot;uninitialized&amp;quot;,&amp;quot;prevMsg&amp;quot;:&amp;quot;uninitialized&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-10-13T01:13:39Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;status&amp;quot;,&amp;quot;plugin:kbn_vislib_vis_types@1.0.0&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:5,&amp;quot;state&amp;quot;:&amp;quot;green&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Status changed from uninitialized to green - Ready&amp;quot;,&amp;quot;prevState&amp;quot;:&amp;quot;uninitialized&amp;quot;,&amp;quot;prevMsg&amp;quot;:&amp;quot;uninitialized&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-10-13T01:13:39Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;status&amp;quot;,&amp;quot;plugin:markdown_vis@1.0.0&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:5,&amp;quot;state&amp;quot;:&amp;quot;green&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Status changed from uninitialized to green - Ready&amp;quot;,&amp;quot;prevState&amp;quot;:&amp;quot;uninitialized&amp;quot;,&amp;quot;prevMsg&amp;quot;:&amp;quot;uninitialized&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-10-13T01:13:39Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;status&amp;quot;,&amp;quot;plugin:metric_vis@1.0.0&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:5,&amp;quot;state&amp;quot;:&amp;quot;green&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Status changed from uninitialized to green - Ready&amp;quot;,&amp;quot;prevState&amp;quot;:&amp;quot;uninitialized&amp;quot;,&amp;quot;prevMsg&amp;quot;:&amp;quot;uninitialized&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-10-13T01:13:39Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;status&amp;quot;,&amp;quot;plugin:spyModes@1.0.0&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:5,&amp;quot;state&amp;quot;:&amp;quot;green&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Status changed from uninitialized to green - Ready&amp;quot;,&amp;quot;prevState&amp;quot;:&amp;quot;uninitialized&amp;quot;,&amp;quot;prevMsg&amp;quot;:&amp;quot;uninitialized&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-10-13T01:13:40Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;status&amp;quot;,&amp;quot;plugin:statusPage@1.0.0&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:5,&amp;quot;state&amp;quot;:&amp;quot;green&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Status changed from uninitialized to green - Ready&amp;quot;,&amp;quot;prevState&amp;quot;:&amp;quot;uninitialized&amp;quot;,&amp;quot;prevMsg&amp;quot;:&amp;quot;uninitialized&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-10-13T01:13:40Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;status&amp;quot;,&amp;quot;plugin:table_vis@1.0.0&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:5,&amp;quot;state&amp;quot;:&amp;quot;green&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Status changed from uninitialized to green - Ready&amp;quot;,&amp;quot;prevState&amp;quot;:&amp;quot;uninitialized&amp;quot;,&amp;quot;prevMsg&amp;quot;:&amp;quot;uninitialized&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-10-13T01:13:40Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;listening&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:5,&amp;quot;message&amp;quot;:&amp;quot;Server running at http://0.0.0.0:5601&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-10-13T01:13:45Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;status&amp;quot;,&amp;quot;plugin:elasticsearch@1.0.0&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:5,&amp;quot;state&amp;quot;:&amp;quot;yellow&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Status changed from yellow to yellow - No existing Kibana index found&amp;quot;,&amp;quot;prevState&amp;quot;:&amp;quot;yellow&amp;quot;,&amp;quot;prevMsg&amp;quot;:&amp;quot;Waiting for Elasticsearch&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-10-13T01:13:49Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;status&amp;quot;,&amp;quot;plugin:elasticsearch@1.0.0&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:5,&amp;quot;state&amp;quot;:&amp;quot;green&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Status changed from yellow to green - Kibana index ready&amp;quot;,&amp;quot;prevState&amp;quot;:&amp;quot;yellow&amp;quot;,&amp;quot;prevMsg&amp;quot;:&amp;quot;No existing Kibana index found&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;访问-kibana&quot;&gt;&lt;a href=&quot;#访问-kibana&quot; class=&quot;headerlink&quot; title=&quot;访问 kibana&quot;&gt;&lt;/a&gt;访问 kibana&lt;/h2&gt;&lt;h3 id=&quot;通过-kube-apiserver-访问：获取-kibana-服务-URL&quot;&gt;&lt;a href=&quot;#通过-kube-apiserver-访问：获取-kibana-服务-URL&quot; class=&quot;headerlink&quot; title=&quot;通过 kube-apiserver 访问：获取 kibana 服务 URL&quot;&gt;&lt;/a&gt;通过 kube-apiserver 访问：获取 kibana 服务 URL&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@node1 efk]# kubectl cluster-info
Kubernetes master is running at https://172.16.7.151:6443
Elasticsearch is running at https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/elasticsearch-logging
Heapster is running at https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/heapster
Kibana is running at https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/kibana-logging
KubeDNS is running at https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/kube-dns
kubernetes-dashboard is running at https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard
monitoring-grafana is running at https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana
monitoring-influxdb is running at https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb

To further debug and diagnose cluster problems, use &amp;apos;kubectl cluster-info dump&amp;apos;.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;浏览器访问 URL： &lt;a href=&quot;https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/kibana-logging/app/kibana&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/kibana-logging/app/kibana&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;通过-kubectl-proxy-访问：创建代理&quot;&gt;&lt;a href=&quot;#通过-kubectl-proxy-访问：创建代理&quot; class=&quot;headerlink&quot; title=&quot;通过 kubectl proxy 访问：创建代理&quot;&gt;&lt;/a&gt;通过 kubectl proxy 访问：创建代理&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@node1 efk]# kubectl proxy --address=&amp;apos;172.16.7.151&amp;apos; --port=8086 --accept-hosts=&amp;apos;^*$&amp;apos; &amp;amp;        
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;浏览器访问 URL：&lt;a href=&quot;http://172.16.7.151:8086/api/v1/proxy/namespaces/kube-system/services/kibana-logging&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.151:8086/api/v1/proxy/namespaces/kube-system/services/kibana-logging&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/16.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;如果你在这里发现Create按钮是灰色的无法点击，且Time-filed name中没有选项，fluentd要读取/var/log/containers/目录下的log日志，这些日志是从/var/lib/docker/containers/${CONTAINER_ID}/${CONTAINER_ID}-json.log链接过来的，查看你的docker配置，—-log-driver需要设置为json-file格式，默认的可能是journald。&lt;/p&gt;
&lt;p&gt;查看当前的–log-driver:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker version
Client:
 Version:         1.12.6
 API version:     1.24
 Package version: docker-1.12.6-32.git88a4867.el7.centos.x86_64
 Go version:      go1.7.4
 Git commit:      88a4867/1.12.6
 Built:           Mon Jul  3 16:02:02 2017
 OS/Arch:         linux/amd64

Server:
 Version:         1.12.6
 API version:     1.24
 Package version: docker-1.12.6-32.git88a4867.el7.centos.x86_64
 Go version:      go1.7.4
 Git commit:      88a4867/1.12.6
 Built:           Mon Jul  3 16:02:02 2017
 OS/Arch:         linux/amd64   
[root@node1 efk]# docker info |grep &amp;apos;Logging Driver&amp;apos;
 WARNING: Usage of loopback devices is strongly discouraged for production use. Use `--storage-opt dm.thinpooldev` to specify a custom block storage device.
WARNING: bridge-nf-call-ip6tables is disabled
Logging Driver: journald
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;修改当前版本docker的–log-driver:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# vim /etc/sysconfig/docker
OPTIONS=&amp;apos;--selinux-enabled --log-driver=json-file --signature-verification=false&amp;apos;
[root@node1 efk]# systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【注意】：本来修改这个参数应该在在/etc/docker/daemon.json文件中添加&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
  &amp;quot;log-driver&amp;quot;: &amp;quot;json-file&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;但是在该版本中，–log-driver是在文件/etc/sysconfig/docker中定义的。&lt;br&gt;在docker-ce版本中，默认的–log-driver是json-file。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;遇到的问题：&lt;/strong&gt;&lt;br&gt;由于之前在/etc/docker/daemon.json中配置–log-driver，重启导致docker程序启动失败，等到后来在/etc/sysconfig/docker配置文件中配置好后，启动docker却发现当前node变成NotReady状态，所有的Pod也变为Unknown状态。查看kubelet状态，发现kubelet程序已经挂掉了。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# kubectl get nodes
NAME           STATUS     AGE       VERSION
172.16.7.151   NotReady   28d       v1.6.0
172.16.7.152   Ready      28d       v1.6.0
172.16.7.153   Ready      28d       v1.6.0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;启动kubelet：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# systemctl start kubelet
[root@node1 ~]# kubectl get nodes
NAME           STATUS    AGE       VERSION
172.16.7.151   Ready     28d       v1.6.0
172.16.7.152   Ready     28d       v1.6.0
172.16.7.153   Ready     28d       v1.6.0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;浏览器再次访问 kibana URL：&lt;a href=&quot;http://172.16.7.151:8086/api/v1/proxy/namespaces/kube-system/services/kibana-logging&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.151:8086/api/v1/proxy/namespaces/kube-system/services/kibana-logging&lt;/a&gt;&lt;br&gt;此时就会发现有Create按钮了。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/17.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;在 Settings -&amp;gt; Indices 页面创建一个 index（相当于 mysql 中的一个 database），去掉已经勾选的 &lt;strong&gt;Index contains time-based events&lt;/strong&gt;，使用默认的 &lt;strong&gt;logstash-*&lt;/strong&gt; pattern，点击 Create ;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/18.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;创建Index后，可以在 Discover 下看到 ElasticSearch logging 中汇聚的日志。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/19.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;EFK介绍&quot;&gt;&lt;a href=&quot;#EFK介绍&quot; class=&quot;headerlink&quot; title=&quot;EFK介绍&quot;&gt;&lt;/a&gt;EFK介绍&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Logstash(或者Fluentd)负责收集日志 &lt;/li&gt;
&lt;li&gt;Elasticsearch存储日志并提供搜索 &lt;/li&gt;
&lt;li&gt;Kibana负责日志查询和展示&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;官方地址：&lt;a href=&quot;https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="容器编排" scheme="http://yoursite.com/categories/%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92/"/>
    
    
      <category term="Kubernetes" scheme="http://yoursite.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes架构</title>
    <link href="http://yoursite.com/2017/10/09/Kubernetes%E6%9E%B6%E6%9E%84/"/>
    <id>http://yoursite.com/2017/10/09/Kubernetes架构/</id>
    <published>2017-10-09T06:49:31.000Z</published>
    <updated>2017-11-13T07:39:34.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;Kubernetes整体架构&quot;&gt;&lt;a href=&quot;#Kubernetes整体架构&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes整体架构&quot;&gt;&lt;/a&gt;Kubernetes整体架构&lt;/h2&gt;&lt;p&gt;Kubernetes的整体架构如下图：&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/14.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;Kubernetes主要由以下几个核心组件组成:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;etcd保存了整个集群的状态; &lt;/li&gt;
&lt;li&gt;apiserver提供了资源操作的唯一入口,并提供认证、授权、访问控制、API注册和 发现等机制;&lt;/li&gt;
&lt;li&gt;controller manager负责维护集群的状态,比如故障检测、自动扩展、滚动更新等; - scheduler负责资源的调度,按照预定的调度策略将Pod调度到相应的机器上;&lt;/li&gt;
&lt;li&gt;kubelet负责维护容器的生命周期,同时也负责Volume(CVI)和网络(CNI)的管 理;&lt;/li&gt;
&lt;li&gt;Container runtime负责镜像管理以及Pod和容器的真正运行(CRI); &lt;/li&gt;
&lt;li&gt;kube-proxy负责为Service提供cluster内部的服务发现和负载均衡;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;除了核心组件,还有一些推荐的Add-ons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kube-dns负责为整个集群提供DNS服务 &lt;/li&gt;
&lt;li&gt;Ingress Controller为服务提供外网入口 &lt;/li&gt;
&lt;li&gt;Heapster提供资源监控 &lt;/li&gt;
&lt;li&gt;Dashboard提供GUI&lt;/li&gt;
&lt;li&gt;Federation提供跨可用区的集群 &lt;/li&gt;
&lt;li&gt;Fluentd-elasticsearch提供集群日志采集、存储与查询&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Kubernetes总体包含两种角色，一个是Master节点，负责集群调度、对外接口、访问控制、对象的生命周期维护等工作；另一个是Node节点，负责维护容器的生命周期，例如创建、删除、停止Docker容器，负责容器的服务抽象和负载均衡等工作。其中Master节点上，运行着三个核心组件：API Server, Scheduler, Controller Mananger。Node节点上运行两个核心组件：Kubelet， Kube-Proxy。API Server提供Kubernetes集群访问的统一接口，Scheduler, Controller Manager, Kubelet, Kube-Proxy等组件都通过API Server进行通信，API Server将Pod, Service, Replication Controller, Daemonset等对象存储在ETCD集群中。ETCD是CoreOS开发的高效、稳定的强一致性Key-Value数据库，ETCD本身可以搭建成集群对外服务，它负责存储Kubernetes所有对象的生命周期，是Kubernetes的最核心的组件。&lt;/p&gt;
&lt;h2 id=&quot;核心组件介绍&quot;&gt;&lt;a href=&quot;#核心组件介绍&quot; class=&quot;headerlink&quot; title=&quot;核心组件介绍&quot;&gt;&lt;/a&gt;核心组件介绍&lt;/h2&gt;&lt;p&gt;下面先大概介绍一下Kubernetes的核心组件的功能：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;API Server: 提供了资源对象的唯一操作入口，其他所有的组件都必须通过它提供的API来操作资源对象。它以RESTful风格的API对外提供接口。所有Kubernetes资源对象的生命周期维护都是通过调用API Server的接口来完成，例如，用户通过kubectl创建一个Pod，即是通过调用API Server的接口创建一个Pod对象，并储存在ETCD集群中。&lt;/li&gt;
&lt;li&gt;Controller Manager: 集群内部的管理控制中心，主要目的是实现Kubernetes集群的故障检测和自动恢复等工作。它包含两个核心组件：Node Controller和Replication Controller。其中Node Controller负责计算节点的加入和退出，可以通过Node Controller实现计算节点的扩容和缩容。Replication Controller用于Kubernetes资源对象RC的管理，应用的扩容、缩容以及滚动升级都是有Replication Controller来实现。&lt;/li&gt;
&lt;li&gt;Scheduler: 集群中的调度器，负责Pod在集群的中的调度和分配。&lt;/li&gt;
&lt;li&gt;Kubelet: 负责本Node节点上的Pod的创建、修改、监控、删除等Pod的全生命周期管理，Kubelet实时向API Server发送所在计算节点（Node）的信息。&lt;/li&gt;
&lt;li&gt;Kube-Proxy: 实现Service的抽象，为一组Pod抽象的服务（Service）提供统一接口并提供负载均衡功能。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;核心原理介绍&quot;&gt;&lt;a href=&quot;#核心原理介绍&quot; class=&quot;headerlink&quot; title=&quot;核心原理介绍&quot;&gt;&lt;/a&gt;核心原理介绍&lt;/h2&gt;&lt;h3 id=&quot;API-Server&quot;&gt;&lt;a href=&quot;#API-Server&quot; class=&quot;headerlink&quot; title=&quot;API Server&quot;&gt;&lt;/a&gt;API Server&lt;/h3&gt;&lt;p&gt;1.如何访问Kubernetes API，Kubernetes API通过一个kube-apiserver的进程提供服务，该进程运行在Kubernetes Master节点上，默认的情况下，监听两个端口：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本地端口: 默认值为8080，用于接收HTTP请求，非认证授权的HTTP请求通过该端口访问API Server。&lt;/li&gt;
&lt;li&gt;安全端口：默认值为6443，用于接收HTTPS请求，用于基于Token文件或者客户端证书及HTTP Base的认证，用于基于策略的授权，Kubernetes默认情况下不启动HTTPS安全访问机制。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;用户可以通过编程方式访问API接口，也可以通过curl命令来直接访问它，例如，我们在Master节点上访问API Server：&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;19&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;20&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;21&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;22&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;23&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;24&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;25&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;26&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;27&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;28&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;29&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;30&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;31&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;32&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;33&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;34&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;35&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;36&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;37&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;38&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;39&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;40&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;41&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;42&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;43&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;44&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;45&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;46&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;47&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;48&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;[root@node1 ~]# curl http://172.16.7.151:8080/ap/&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  &amp;quot;paths&amp;quot;: [&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/api&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/api/v1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/apps&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/apps/v1beta1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/authentication.k8s.io&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/authentication.k8s.io/v1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/authentication.k8s.io/v1beta1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/authorization.k8s.io&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/authorization.k8s.io/v1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/authorization.k8s.io/v1beta1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/autoscaling&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/autoscaling/v1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/autoscaling/v2alpha1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/batch&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/batch/v1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/batch/v2alpha1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/certificates.k8s.io&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/certificates.k8s.io/v1beta1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/extensions&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/extensions/v1beta1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/policy&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/policy/v1beta1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/rbac.authorization.k8s.io&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/rbac.authorization.k8s.io/v1alpha1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/rbac.authorization.k8s.io/v1beta1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/settings.k8s.io&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/settings.k8s.io/v1alpha1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/storage.k8s.io&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/storage.k8s.io/v1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/storage.k8s.io/v1beta1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/healthz&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/healthz/ping&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/healthz/poststarthook/bootstrap-controller&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/healthz/poststarthook/ca-registration&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/healthz/poststarthook/extensions/third-party-resources&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/healthz/poststarthook/rbac/bootstrap-roles&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/logs&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/metrics&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/swagger-ui/&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/swaggerapi/&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/ui/&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/version&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  ]&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;Kubernetes还提供了一个代理程序——Kubectl Proxy，它既能作为API Server的反向代理，也能作为普通客户端访问API Server，使用方法如下：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;# kubectl proxy --port=9090 &amp;amp;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;# curl http://172.16.7.151:9090/api&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  &amp;quot;kind&amp;quot;: &amp;quot;APIVersions&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  &amp;quot;versions&amp;quot;: [&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;v1&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  ],&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  &amp;quot;serverAddressByClientCIDRs&amp;quot;: [&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      &amp;quot;clientCIDR&amp;quot;: &amp;quot;0.0.0.0/0&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      &amp;quot;serverAddress&amp;quot;: &amp;quot;172.16.7.151:6443&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  ]&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;2.集群功能模块之间的通信&lt;br&gt;API Server是整个集群的核心，负责集群各个模块之间的通信。集群内部的功能模块通过API Server将信息存入ETCD，其他模块通过API Server读取这些信息，从而实现各模块之间的信息交互。比如，Node节点上的Kubelet每个一个时间周期，通过API Server报告自身状态，API Server接收这些信息后，将节点状态信息保存到ETCd中。Controller Manager中的Node Controller通过API Server定期读取这些节点状态信息，并做相应处理。Scheduler监听到某个Pod创建的信息后，检索所有符合该Pod要求的节点列表，并将Pod绑定到节点李彪中最符合要求的节点上：如果Scheduler监听到某个Pod被删除，则删除本节点上的相应Pod实例。&lt;br&gt;从上面的通信过程可以看出，API Server的访问压力很大，这也是限制（制约）Kubernetes集群规模的关键，缓解API Server的压力可以通过缓存来实现，通过watch/list操作，将资源对象的信息缓存到本地，这种方法在一定程度上缓解了API Server的压力，但是不是最好的解决办法。&lt;/p&gt;
&lt;h3 id=&quot;Controller-Manager&quot;&gt;&lt;a href=&quot;#Controller-Manager&quot; class=&quot;headerlink&quot; title=&quot;Controller Manager&quot;&gt;&lt;/a&gt;Controller Manager&lt;/h3&gt;&lt;p&gt;Controller Manager作为集群的内部管理控制中心，负责集群内的Node，Pod，RC，服务端点（Endpoint），命名空间（Namespace），服务账号（ServiceAccount）、资源配额（ResourceQuota）等的管理并执行自动化修复流程，确保集群出处于预期的工作状态，比如，RC实现自动控制Pod活跃副本数，如果Pod出错退出，RC自动创建一个新的Pod，来保持活跃的Pod的个数。&lt;br&gt;Controller Manager包含Replication Controller、Node Controller、ResourceQuota Controller、Namespace Controller、ServiceAccount Controller、Token Controller、Server Controller以及Endpoint Controller等多个控制器，Controller Manager是这些Controller的管理者。&lt;/p&gt;
&lt;h3 id=&quot;Scheduler&quot;&gt;&lt;a href=&quot;#Scheduler&quot; class=&quot;headerlink&quot; title=&quot;Scheduler&quot;&gt;&lt;/a&gt;Scheduler&lt;/h3&gt;&lt;p&gt;Kubernetes Scheduler负责Pod的调度管理，它负责将要创建的Pod按照一定的规则分配在某个适合的Node上。&lt;br&gt;Scheduler的默认调度流程分为以下两步：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;预选调度过程，即遍历所有目标Node，筛选出符合要求的候选节点。为此，Kubernetes内置了多种预选策略供用户选择。&lt;/li&gt;
&lt;li&gt;确定最优节点，在第一步的基础上，采用优选策略为每个候选节点打分，分值最高的胜出。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Scheduler的调度流程是通过插件方式加载“调度算法提供者”具体实现的，一个调度算法提供者其实就是包括了一组预选策略与一组有限选择策略的结构体，注册算法插件的函数如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;func RegisterAlgorithmProvider(name string, predicateKeys, priorityKeys util.StringSet)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;它包含3个参数：“name string”参数为算法名，“predicateKeys”为为算法用到的预选策略集合，”priorityKeys”为算法用到的优选策略集合。&lt;br&gt;Scheduler中可用的预算策略包含：NoDiskConflict,  PodFitResources, PodSelectorMatches,  PodFitHost,  CheckNodeLabelPresence,  CheckServiceAffinity和PodFitsPorts策略等。其默认的AlgorithmProvider加载的预选策略Predicates包括：PodFitsPorts,  PodFitsResources,  NoDiskConflict,  MatchNodeSelector和HostName，即每个节点只有通过前面的五个默认预选策略后，才能初步被选中，进入下一个流程。&lt;/p&gt;
&lt;h3 id=&quot;kubelet&quot;&gt;&lt;a href=&quot;#kubelet&quot; class=&quot;headerlink&quot; title=&quot;kubelet&quot;&gt;&lt;/a&gt;kubelet&lt;/h3&gt;&lt;p&gt;在Kubernetes集群中，每个计算节点（Node）上会运行一个守护进程：Kubelet。它用于处理Master节点下发到本节点的任务，管理Pod以及Pod中的容器。每个Kubelet进程会在API Server上注册自身节点的信息，定期向API Server汇报节点资源的使用情况，并通过cAdvise监控容器和节点资源。&lt;br&gt;Kubelet主要功能：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;节点管理：kubelet可以自动向API Server注册自己，它可以采集所在计算节点的资源信息和使用情况并提交给API Server，通过启动/停止kubelet进程来实现计算节点的扩容、缩容。&lt;/li&gt;
&lt;li&gt;Pod管理：kubelet通过API Server监听ETCD目录，同步Pod清单，当发现有新的Pod绑定到所在的节点，则按照Pod清单的要求创建改清单。如果发现本地的Pod被删除，则kubelet通过docker client删除该容器。&lt;/li&gt;
&lt;li&gt;健康检查：Pod通过两类探针来检查容器的健康状态。一个是LivenessProbe探针，用于判断容器是否健康，如果LivenessProbe探针探测到容器不健康，则kubelet将删除该容器，并根据容器的重启策略做相应的处理。另一类是ReadnessProbe探针，用于判断容器是否启动完成，且准备接受请求，如果ReadnessProbe探针检测到失败，则Pod的状态被修改。Enpoint Controller将从Service的Endpoint中删除包含该容器的IP地址的Endpoint条目。kubelet定期调用LivenessProbe探针来诊断容器的健康状况，它目前支持三种探测：HTTP的方式发送GET请求; TCP方式执行Connect目的端口; Exec的方式，执行一个脚本。&lt;/li&gt;
&lt;li&gt;cAdvisor资源监控: 在Kubernetes集群中，应用程序的执行情况可以在不同的级别上检测到，这些级别包含Container，Pod，Service和整个集群。作为Kubernetes集群的一部分，Kubernetes希望提供给用户各个级别的资源使用信息，这将使用户能够更加深入地了解应用的执行情况，并找到可能的瓶颈。Heapster项目为Kubernetes提供了一个基本的监控平台，他是集群级别的监控和事件数据集成器。Heapster通过收集所有节点的资源使用情况，将监控信息实时推送至一个可配置的后端，用于存储和可视化展示。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;kube-proxy&quot;&gt;&lt;a href=&quot;#kube-proxy&quot; class=&quot;headerlink&quot; title=&quot;kube-proxy&quot;&gt;&lt;/a&gt;kube-proxy&lt;/h3&gt;&lt;p&gt;每台机器上都运行一个kube-proxy服务,它监听API server中service和endpoint的变化 情况,并通过iptables等来为服务配置负载均衡(仅支持TCP和UDP)。&lt;br&gt;kube-proxy可以直接运行在物理机上,也可以以static pod或者daemonset的方式运行。 kube-proxy当前支持以下几种实现：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;userspace:最早的负载均衡方案,它在用户空间监听一个端口,所有服务通过 iptables转发到这个端口,然后在其内部负载均衡到实际的Pod。该方式最主要的问 题是效率低,有明显的性能瓶颈。 &lt;/li&gt;
&lt;li&gt;iptables:目前推荐的方案,完全以iptables规则的方式来实现service负载均衡。该 方式最主要的问题是在服务多的时候产生太多的iptables规则(社区有人提到过几万 条),大规模下也有性能问题 &lt;/li&gt;
&lt;li&gt;winuserspace:同userspace,但仅工作在windows上&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;另外,基于ipvs的方案正在讨论中,大规模情况下可以大幅提升性 能,比如slide里面提供的示例将服务延迟从小时缩短到毫秒级。&lt;br&gt;kube-proxy目前仅支持TCP和UDP,不支持HTTP路由,并且也没有健康检查机制。这 些可以通过自定义Ingress Controller的方法来解决。&lt;/p&gt;
&lt;h3 id=&quot;kube-dns&quot;&gt;&lt;a href=&quot;#kube-dns&quot; class=&quot;headerlink&quot; title=&quot;kube-dns&quot;&gt;&lt;/a&gt;kube-dns&lt;/h3&gt;&lt;p&gt;kube-dns为Kubernetes集群提供命名服务,一般通过addon的方式部署,从v1.3版本开 始,成为了一个内建的自启动服务。&lt;/p&gt;
&lt;h2 id=&quot;Kubernetes应用部署模型&quot;&gt;&lt;a href=&quot;#Kubernetes应用部署模型&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes应用部署模型&quot;&gt;&lt;/a&gt;Kubernetes应用部署模型&lt;/h2&gt;&lt;p&gt;主要包括Pod、Replication controller、Label和Service。&lt;/p&gt;
&lt;h3 id=&quot;Pod&quot;&gt;&lt;a href=&quot;#Pod&quot; class=&quot;headerlink&quot; title=&quot;Pod&quot;&gt;&lt;/a&gt;Pod&lt;/h3&gt;&lt;p&gt;Kubernetes的最小部署单元是Pod而不是容器。作为First class API公民，Pods能被创建，调度和管理。简单地来说，像一个豌豆荚中的豌豆一样，一个Pod中的应用容器同享同一个上下文：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;PID 名字空间。但是在docker中不支持&lt;/li&gt;
&lt;li&gt;网络名字空间，在同一Pod中的多个容器访问同一个IP和端口空间。&lt;/li&gt;
&lt;li&gt;IPC名字空间，同一个Pod中的应用能够使用SystemV IPC和POSIX消息队列进行通信。&lt;/li&gt;
&lt;li&gt;UTS名字空间，同一个Pod中的应用共享一个主机名。&lt;/li&gt;
&lt;li&gt;Pod中的各个容器应用还可以访问Pod级别定义的共享卷。&lt;br&gt;从生命周期来说，Pod应该是短暂的而不是长久的应用。 Pods被调度到节点，保持在这个节点上直到被销毁。当节点死亡时，分配到这个节点的Pods将会被删掉。将来可能会实现Pod的迁移特性。在实际使用时，我们一般不直接创建Pods, 我们通过replication controller来负责Pods的创建，复制，监控和销毁。一个Pod可以包括多个容器，他们直接往往相互协作完成一个应用功能。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;Replication-controller和ReplicaSet&quot;&gt;&lt;a href=&quot;#Replication-controller和ReplicaSet&quot; class=&quot;headerlink&quot; title=&quot;Replication controller和ReplicaSet&quot;&gt;&lt;/a&gt;Replication controller和ReplicaSet&lt;/h3&gt;&lt;p&gt;复制控制器确保Pod的一定数量的份数(replica)在运行。如果超过这个数量，控制器会杀死一些，如果少了，控制器会启动一些。控制器也会在节点失效、维护的时候来保证这个数量。所以强烈建议即使我们的份数是1，也要使用复制控制器，而不是直接创建Pod。&lt;br&gt;在生命周期上讲，复制控制器自己不会终止，但是跨度不会比Service强。Service能够横跨多个复制控制器管理的Pods。而且在一个Service的生命周期内，复制控制器能被删除和创建。Service和客户端程序是不知道复制控制器的存在的。&lt;br&gt;复制控制器创建的Pods应该是可以互相替换的和语义上相同的，这个对无状态服务特别合适。&lt;br&gt;在新版本的Kubernetes中建议使用ReplicaSet(也简称为rs)来取代 ReplicationController。ReplicaSet跟ReplicationController没有本质的不同,只是名字不 一样,并且ReplicaSet支持集合式的selector(ReplicationController仅支持等式)。&lt;br&gt;虽然也ReplicaSet可以独立使用,但建议使用 Deployment 来自动管理ReplicaSet,这 样就无需担心跟其他机制的不兼容问题(比如ReplicaSet不支持rolling-update但 Deployment支持),并且还支持版本记录、回滚、暂停升级等高级特性。&lt;br&gt;Pod是临时性的对象，被创建和销毁，而且不会恢复。复制器动态地创建和销毁Pod。虽然Pod会分配到IP地址，但是这个IP地址都不是持久的。这样就产生了一个疑问：外部如何消费Pod提供的服务呢？&lt;/p&gt;
&lt;h3 id=&quot;Service&quot;&gt;&lt;a href=&quot;#Service&quot; class=&quot;headerlink&quot; title=&quot;Service&quot;&gt;&lt;/a&gt;Service&lt;/h3&gt;&lt;p&gt;Service定义了一个Pod的逻辑集合和访问这个集合的策略。集合是通过定义Service时提供的Label选择器完成的。举个例子，我们假定有3个Pod的备份来完成一个图像处理的后端。这些后端备份逻辑上是相同的，前端不关心哪个后端在给它提供服务。虽然组成这个后端的实际Pod可能变化，前端客户端不会意识到这个变化，也不会跟踪后端。Service就是用来实现这种分离的抽象。&lt;br&gt;对于Service，我们还可以定义Endpoint，Endpoint把Service和Pod动态地连接起来。&lt;/p&gt;
&lt;h4 id=&quot;Service-Cluster-IP和-kuber-proxy&quot;&gt;&lt;a href=&quot;#Service-Cluster-IP和-kuber-proxy&quot; class=&quot;headerlink&quot; title=&quot;Service Cluster IP和 kuber proxy&quot;&gt;&lt;/a&gt;Service Cluster IP和 kuber proxy&lt;/h4&gt;&lt;p&gt;每个代理节点都运行了一个kube-proxy进程。这个进程从服务进程那边拿到Service和Endpoint对象的变化。 对每一个Service, 它在本地打开一个端口。 到这个端口的任意连接都会代理到后端Pod集合中的一个Pod IP和端口。在创建了服务后，服务Endpoint模型会体现后端Pod的 IP和端口列表，kube-proxy就是从这个endpoint维护的列表中选择服务后端的。另外Service对象的sessionAffinity属性也会帮助kube-proxy来选择哪个具体的后端。缺省情况下，后端Pod的选择是随机的。可以设置service.spec.sessionAffinity 成”ClientIP”来指定同一个ClientIP的流量代理到同一个后端。在实现上，kube-proxy会用IPtables规则把访问Service的Cluster IP和端口的流量重定向到这个本地端口。下面的部分会讲什么是service的Cluster IP。&lt;br&gt;注意：在0.18以前的版本中Cluster IP叫PortalNet IP。&lt;/p&gt;
&lt;h4 id=&quot;Pod-IP-and-Service-Cluster-IP&quot;&gt;&lt;a href=&quot;#Pod-IP-and-Service-Cluster-IP&quot; class=&quot;headerlink&quot; title=&quot;Pod IP and Service Cluster IP&quot;&gt;&lt;/a&gt;Pod IP and Service Cluster IP&lt;/h4&gt;&lt;p&gt;Pod IP 地址是实际存在于某个网卡(可以是虚拟设备)上的，但Service Cluster IP就不一样了，没有网络设备为这个地址负责。它是由kube-proxy使用Iptables规则重新定向到其本地端口，再均衡到后端Pod的。我们前面说的Service环境变量和DNS都使用Service的Cluster IP和端口。&lt;br&gt;就拿上面我们提到的图像处理程序为例。当我们的Service被创建时，Kubernetes给它分配一个地址10.0.0.1。这个地址从我们启动API的service-cluster-ip-range参数(旧版本为portal_net参数)指定的地址池中分配，比如–service-cluster-ip-range=10.0.0.0/16。假设这个Service的端口是1234。集群内的所有kube-proxy都会注意到这个Service。当proxy发现一个新的service后，它会在本地节点打开一个任意端口，建相应的iptables规则，重定向服务的IP和port到这个新建的端口，开始接受到达这个服务的连接。&lt;br&gt;当一个客户端访问这个service时，这些iptable规则就开始起作用，客户端的流量被重定向到kube-proxy为这个service打开的端口上，kube-proxy随机选择一个后端pod来服务客户。这个流程如下图所示：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/15.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;根据Kubernetes的网络模型，使用Service Cluster IP和Port访问Service的客户端可以坐落在任意代理节点上。外部要访问Service，我们就需要给Service外部访问IP。&lt;/p&gt;
&lt;h4 id=&quot;外部访问Service&quot;&gt;&lt;a href=&quot;#外部访问Service&quot; class=&quot;headerlink&quot; title=&quot;外部访问Service&quot;&gt;&lt;/a&gt;外部访问Service&lt;/h4&gt;&lt;p&gt;Service对象在Cluster IP range池中分配到的IP只能在内部访问，如果服务作为一个应用程序内部的层次，还是很合适的。如果这个Service作为前端服务，准备为集群外的客户提供业务，我们就需要给这个服务提供公共IP了。&lt;br&gt;外部访问者是访问集群代理节点的访问者。为这些访问者提供服务，我们可以在定义Service时指定其spec.publicIPs，一般情况下publicIP 是代理节点的物理IP地址。和先前的Cluster IP range上分配到的虚拟的IP一样，kube-proxy同样会为这些publicIP提供Iptables 重定向规则，把流量转发到后端的Pod上。有了publicIP，我们就可以使用load balancer等常用的互联网技术来组织外部对服务的访问了。&lt;br&gt;spec.publicIPs在新的版本中标记为过时了，代替它的是spec.type=NodePort，这个类型的service，系统会给它在集群的各个代理节点上分配一个节点级别的端口，能访问到代理节点的客户端都能访问这个端口，从而访问到服务。&lt;/p&gt;
&lt;h3 id=&quot;Label和Label-selector&quot;&gt;&lt;a href=&quot;#Label和Label-selector&quot; class=&quot;headerlink&quot; title=&quot;Label和Label selector&quot;&gt;&lt;/a&gt;Label和Label selector&lt;/h3&gt;&lt;p&gt;Label标签在Kubernetes模型中占着非常重要的作用。Label表现为key/value对，附加到Kubernetes管理的对象上，典型的就是Pods。它们定义了这些对象的识别属性，用来组织和选择这些对象。Label可以在对象创建时附加在对象上，也可以对象存在时通过API管理对象的Label。&lt;br&gt;在定义了对象的Label后，其它模型可以用Label 选择器（selector)来定义其作用的对象。&lt;br&gt;Label选择器有两种，分别是Equality-based和Set-based。&lt;br&gt;比如如下Equality-based选择器样例：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;environment = production
tier != frontend
environment = production，tier != frontend
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;对于上面的选择器，第一条匹配Label具有environment key且等于production的对象，第二条匹配具有tier key，但是值不等于frontend的对象。由于kubernetes使用AND逻辑，第三条匹配production但不是frontend的对象。&lt;br&gt;Set-based选择器样例：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;environment in (production, qa)
tier notin (frontend, backend)
partition
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;第一条选择具有environment key，而且值是production或者qa的label附加的对象。第二条选择具有tier key，但是其值不是frontend和backend。第三条选则具有partition key的对象，不对value进行校验。&lt;br&gt;replication controller复制控制器和Service都用label和label selctor来动态地配备作用对象。复制控制器在定义的时候就指定了其要创建Pod的Label和自己要匹配这个Pod的selector， API服务器应该校验这个定义。我们可以动态地修改replication controller创建的Pod的Label用于调式，数据恢复等。一旦某个Pod由于Label改变从replication controller移出来后，replication controller会马上启动一个新的Pod来确保复制池子中的份数。对于Service，Label selector可以用来选择一个Service的后端Pods。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Kubernetes整体架构&quot;&gt;&lt;a href=&quot;#Kubernetes整体架构&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes整体架构&quot;&gt;&lt;/a&gt;Kubernetes整体架构&lt;/h2&gt;&lt;p&gt;Kubernetes的整体架构如下图：&lt;br&gt;
    
    </summary>
    
      <category term="容器编排" scheme="http://yoursite.com/categories/%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92/"/>
    
    
      <category term="Kubernetes" scheme="http://yoursite.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Jenkins + Docker 持续集成</title>
    <link href="http://yoursite.com/2017/09/30/Jenkins-Docker-%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/"/>
    <id>http://yoursite.com/2017/09/30/Jenkins-Docker-持续集成/</id>
    <published>2017-09-30T08:56:51.000Z</published>
    <updated>2017-11-14T02:17:53.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;Jenkins介绍&quot;&gt;&lt;a href=&quot;#Jenkins介绍&quot; class=&quot;headerlink&quot; title=&quot;Jenkins介绍&quot;&gt;&lt;/a&gt;Jenkins介绍&lt;/h2&gt;&lt;p&gt;Jenkins是一个开源软件项目，是基于Java开发的一种持续集成工具，用于监控持续重复的工作，旨在提供一个开放易用的软件平台，使软件的持续集成变成可能。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;安装部署Jenkins&quot;&gt;&lt;a href=&quot;#安装部署Jenkins&quot; class=&quot;headerlink&quot; title=&quot;安装部署Jenkins&quot;&gt;&lt;/a&gt;安装部署Jenkins&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://jenkins.io/download/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://jenkins.io/download/&lt;/a&gt;&lt;br&gt;我这里下载war包安装，版本：1.642.3 LTS .war&lt;/p&gt;
&lt;h3 id=&quot;环境信息&quot;&gt;&lt;a href=&quot;#环境信息&quot; class=&quot;headerlink&quot; title=&quot;环境信息&quot;&gt;&lt;/a&gt;环境信息&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;操作系统版本&lt;/th&gt;
&lt;th&gt;IP地址&lt;/th&gt;
&lt;th&gt;安装软件&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;osb30&lt;/td&gt;
&lt;td&gt;Redhat 6.5&lt;/td&gt;
&lt;td&gt;172.16.206.30&lt;/td&gt;
&lt;td&gt;jenkins&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&quot;新建Jenkins用户&quot;&gt;&lt;a href=&quot;#新建Jenkins用户&quot; class=&quot;headerlink&quot; title=&quot;新建Jenkins用户&quot;&gt;&lt;/a&gt;新建Jenkins用户&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@osb30 ~]# groupadd jenkins
[root@osb30 ~]# useradd -g jenkins jenkins
[root@osb30 ~]# id jenkins
uid=501(jenkins) gid=501(jenkins) groups=501(jenkins)
[root@osb30 ~]# echo &amp;quot;wisedu&amp;quot; | passwd --stdin jenkins &amp;amp;&amp;gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;Jenkins安装方式&quot;&gt;&lt;a href=&quot;#Jenkins安装方式&quot; class=&quot;headerlink&quot; title=&quot;Jenkins安装方式&quot;&gt;&lt;/a&gt;Jenkins安装方式&lt;/h3&gt;&lt;p&gt;安装jenkins有两种方式，tomcat方式部署和java部署启动。本次实验我以tomcat下部署启动为例。&lt;/p&gt;
&lt;h4 id=&quot;tomcat方式部署&quot;&gt;&lt;a href=&quot;#tomcat方式部署&quot; class=&quot;headerlink&quot; title=&quot;tomcat方式部署&quot;&gt;&lt;/a&gt;tomcat方式部署&lt;/h4&gt;&lt;p&gt;1.首先安装tomcat和JAVA，配置环境变量（此步骤不再讲述，java配置不可缺少）&lt;br&gt;我这里安装的是jdk 1.8.0_65。&lt;/p&gt;
&lt;p&gt;2.将从官网下载下来的jenkins.war文件放入tomcat下的webapps目录下，进入tomcat的/bin目录下，启动tomcat即启动jenkins。&lt;br&gt;我这里用的是tomcat8。&lt;/p&gt;
&lt;p&gt;3.启动jenkins时，会自动在webapps目录下建立jenkins目录，访问地址为：&lt;a href=&quot;http://localhost:8080/jenkins&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://localhost:8080/jenkins&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[jenkins@osb30 ~]$ tar zxf apache-tomcat-8.0.30.tar.gz
[jenkins@osb30 ~]$ mv jenkins.war apache-tomcat-8.0.30/webapps/
[jenkins@osb30 ~]$ cd apache-tomcat-8.0.30
[jenkins@osb30 apache-tomcat-8.0.30]$ bin/startup.sh
Jenkins home directory: /home/jenkins/.jenkins found at: $user.home/.jenkins
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果启动时报错：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Caused by:java.awt.AWTError: Can&amp;apos;t connect to X11 window server using &amp;apos;:0&amp;apos; as the value of the DISPLAY varible...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;解决：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[jenkins@osb30 ~]$ cd apache-tomcat-8.0.30/bin/
[jenkins@osb30 bin]$ vim catalina.sh 
JAVA_OPTS=&amp;quot;-Xms1024m -Xmx1024m -Djava.awt.headless=true&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;4.访问jenkins&lt;br&gt;&lt;a href=&quot;http://172.16.206.30:8080/jenkins&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.206.30:8080/jenkins&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;java部署启动jenkins&quot;&gt;&lt;a href=&quot;#java部署启动jenkins&quot; class=&quot;headerlink&quot; title=&quot;java部署启动jenkins&quot;&gt;&lt;/a&gt;java部署启动jenkins&lt;/h4&gt;&lt;p&gt;切换到jenkins.war存放的目录，输入如下命令：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ java -jar jenkins.war   
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可以修改启动端口&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ java -jar jenkins.war --httpPort=8000
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后在浏览器中（推荐用火狐、chrome）输入&lt;a href=&quot;http://localhost:8080，localhost可以是本机的ip，也可以是计算机名。就可以打开jenkins；修改端口后，访问地址的端口需同步变更。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://localhost:8080，localhost可以是本机的ip，也可以是计算机名。就可以打开jenkins；修改端口后，访问地址的端口需同步变更。&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;Jenkins授权和访问控制&quot;&gt;&lt;a href=&quot;#Jenkins授权和访问控制&quot; class=&quot;headerlink&quot; title=&quot;Jenkins授权和访问控制&quot;&gt;&lt;/a&gt;Jenkins授权和访问控制&lt;/h3&gt;&lt;p&gt;默认地Jenkins不包含任何的安全检查，任何人可以修改Jenkins设置，job和启动build等。显然地在大规模的公司需要多个部门一起协调工作的时候，没有任何安全检查会带来很多的问题。 我们可以通过下面的方式来增强Jenkins的安全：&lt;br&gt;访问jenkins：&lt;a href=&quot;http://172.16.206.30:8080/jenkins，&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.206.30:8080/jenkins，&lt;/a&gt;&lt;br&gt;点击系统管理—&amp;gt; Configure Global Security，点击”启用安全”，可以看到可以使用多种方式来增强Jenkins的授权和访问控制：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/44.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/45.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;如上图所示，默认是”任何用户可以做任何事情(没有任何限制)”。&lt;br&gt;我们在”安全域”选择”Jenkins专有用户数据库”，”允许用户注册”；并先在“授权策略”点击“任何用户可以做任何事情(没有任何限制)”， 防止注册之后无法再管理jenkins。此时就可以刷新一下jenkins的页面看到右上角有登录、注册的按钮。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/46.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h4 id=&quot;注册管理员账号&quot;&gt;&lt;a href=&quot;#注册管理员账号&quot; class=&quot;headerlink&quot; title=&quot;注册管理员账号&quot;&gt;&lt;/a&gt;注册管理员账号&lt;/h4&gt;&lt;p&gt;1.点击注册，首先注册一个管理员账号。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/47.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;2.点击系统管理—&amp;gt; Configure Global Security，在“授权策略”选择”安全矩阵”，添加用户/组——添加admin账户——为admin账户添加所有权限，为匿名用户勾选你希望对方了解的功能。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/48.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;【注意】：匿名用户一定要开启此处的可读权限，若不开启，后面github或者bitbucket的webhook自动构建会没有权限。&lt;br&gt;并且勾选上该项，点击保存。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/49.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;做完此部操作之后，即可用admin帐号登录，取消登录用户可以做任何事的权限。&lt;br&gt;以上操作，即可完成jenkins的授权和访问控制。&lt;/p&gt;
&lt;h3 id=&quot;Jenkins系统配置&quot;&gt;&lt;a href=&quot;#Jenkins系统配置&quot; class=&quot;headerlink&quot; title=&quot;Jenkins系统配置&quot;&gt;&lt;/a&gt;Jenkins系统配置&lt;/h3&gt;&lt;p&gt;登录jenkins——系统管理——系统设置，为jenkins添加上需要的功能配置，有如下几个方面：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/50.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h4 id=&quot;jdk版本&quot;&gt;&lt;a href=&quot;#jdk版本&quot; class=&quot;headerlink&quot; title=&quot;jdk版本&quot;&gt;&lt;/a&gt;jdk版本&lt;/h4&gt;&lt;p&gt;在jdk的选项，点击”新增JDK”，取消自动安装，输入jdk别名（名称随意），JAVA_HOME大家应该都很了解，在此处填写jenkins所在服务器安装的java程序的HOME位置即可，根据不同操作系统填写不同路径，如win7 D:\Java\jdk1.8   linux /usr/lib/jvm/jdk1.7.0_51。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/51.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;设置完了请记得保存。&lt;/p&gt;
&lt;h4 id=&quot;git-svn版本控制添加&quot;&gt;&lt;a href=&quot;#git-svn版本控制添加&quot; class=&quot;headerlink&quot; title=&quot;git/svn版本控制添加&quot;&gt;&lt;/a&gt;git/svn版本控制添加&lt;/h4&gt;&lt;p&gt;根据使用的版本选择控制版本的应用程序的路径，如jdk配置即可。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/52.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;【注意】:如果使用Git作为版本控制库，Jenkins默认情况下是没有安装Git的。我们需要到插件管理界面中选中Git，然后点击直接安装。&lt;br&gt;点击系统管理—&amp;gt;管理插件—&amp;gt;可选插件，在右上角”过滤”处输入git进行搜索：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/53.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;找到Git client plugin和Git plugin，在前面打上√，点击直接安装。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/54.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/55.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;安装成功后，重启jenkins。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[jenkins@osb30 ~]$ cd apache-tomcat-8.0.30
[jenkins@osb30 apache-tomcat-8.0.30]$ bin/shutdown.sh
[jenkins@osb30 apache-tomcat-8.0.30]$ bin/startup.sh ;tail -f logs/catalina.out
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;Jenkins添加maven配置&quot;&gt;&lt;a href=&quot;#Jenkins添加maven配置&quot; class=&quot;headerlink&quot; title=&quot;Jenkins添加maven配置&quot;&gt;&lt;/a&gt;Jenkins添加maven配置&lt;/h4&gt;&lt;p&gt;先判断jenkins所在主机是否安装了maven：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# mvn –version
-bash: mvn: command not found
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果没有安装，请先安装maven。&lt;/p&gt;
&lt;h5 id=&quot;CentOS-安装maven&quot;&gt;&lt;a href=&quot;#CentOS-安装maven&quot; class=&quot;headerlink&quot; title=&quot;CentOS 安装maven&quot;&gt;&lt;/a&gt;CentOS 安装maven&lt;/h5&gt;&lt;pre&gt;&lt;code&gt;[root@osb30 ~]# cd /usr/local/
[root@osb30 local]# wget http://apache.opencas.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz

[root@osb30 local]# tar zxf apache-maven-3.3.9-bin.tar.gz
[root@osb30 local]# ln -s apache-maven-3.3.9 maven
[root@osb30 local]# vim /etc/profile
# 添加如下配置：
# Maven configuration.
MAVEN_HOME=/usr/local/maven
export PATH=$MAVEN_HOME/bin:$PATH
[root@osb30 local]# source /etc/profile

[root@osb30 local]# mvn -version
Apache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-11T00:41:47+08:00)
Maven home: /usr/local/maven
Java version: 1.8.0_65, vendor: Oracle Corporation
Java home: /usr/java/jdk1.8.0_65/jre
Default locale: en_US, platform encoding: UTF-8
OS name: &amp;quot;linux&amp;quot;, version: &amp;quot;2.6.32-431.el6.x86_64&amp;quot;, arch: &amp;quot;amd64&amp;quot;, family: &amp;quot;unix&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;h5 id=&quot;Jenkins配置maven&quot;&gt;&lt;a href=&quot;#Jenkins配置maven&quot; class=&quot;headerlink&quot; title=&quot;Jenkins配置maven&quot;&gt;&lt;/a&gt;Jenkins配置maven&lt;/h5&gt;&lt;p&gt;安装完成后，登录jenkins。点击系统管理—&amp;gt;系统设置。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/56.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;Jenkins构建maven风格的job&quot;&gt;&lt;a href=&quot;#Jenkins构建maven风格的job&quot; class=&quot;headerlink&quot; title=&quot;Jenkins构建maven风格的job&quot;&gt;&lt;/a&gt;Jenkins构建maven风格的job&lt;/h2&gt;&lt;h3 id=&quot;新建maven任务&quot;&gt;&lt;a href=&quot;#新建maven任务&quot; class=&quot;headerlink&quot; title=&quot;新建maven任务&quot;&gt;&lt;/a&gt;新建maven任务&lt;/h3&gt;&lt;p&gt;登录jenkins，点击新建。输入Item名称，选择“构建一个maven项目”，点击OK。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/57.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;构建任务配置&quot;&gt;&lt;a href=&quot;#构建任务配置&quot; class=&quot;headerlink&quot; title=&quot;构建任务配置&quot;&gt;&lt;/a&gt;构建任务配置&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/58.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;源码管理配置&quot;&gt;&lt;a href=&quot;#源码管理配置&quot; class=&quot;headerlink&quot; title=&quot;源码管理配置&quot;&gt;&lt;/a&gt;源码管理配置&lt;/h3&gt;&lt;p&gt;进入配置页面，找到”源码管理”。我这里是svn，输入项目所在版本库的地址。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/59.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;构建触发器配置&quot;&gt;&lt;a href=&quot;#构建触发器配置&quot; class=&quot;headerlink&quot; title=&quot;构建触发器配置&quot;&gt;&lt;/a&gt;构建触发器配置&lt;/h3&gt;&lt;p&gt;在”源码管理”下面是”构建触发器”。&lt;br&gt;”构建触发器”是一个持续集成的触发器插件，可以根据已经完成构建的结果，触发新Job或者传递参数。默认的选项是Build whenever a SNAPSHOT dependency is built，意思是依赖于快照的构建，意思是依赖于快照的构建，当代码有更新时就构建项目。&lt;br&gt;Build periodically和Poll SCM可以设置定时自动构建。两者区别如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Poll SCM：定时检查源码变更（根据SCM软件的版本号），如果有更新就checkout最新code下来，然后执行构建动作。&lt;/li&gt;
&lt;li&gt;Build periodically：定时进行项目构建（它不care源码是否发生变化）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我这里设置为每12小时构建一次。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/60.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;Maven构建设置&quot;&gt;&lt;a href=&quot;#Maven构建设置&quot; class=&quot;headerlink&quot; title=&quot;Maven构建设置&quot;&gt;&lt;/a&gt;Maven构建设置&lt;/h3&gt;&lt;h4 id=&quot;Pre-Step&quot;&gt;&lt;a href=&quot;#Pre-Step&quot; class=&quot;headerlink&quot; title=&quot;Pre Step&quot;&gt;&lt;/a&gt;Pre Step&lt;/h4&gt;&lt;p&gt;Pre Steps选项用来配置构建前的工作，这里不作更改。&lt;/p&gt;
&lt;h4 id=&quot;配置Root-POM和Goals-and-options&quot;&gt;&lt;a href=&quot;#配置Root-POM和Goals-and-options&quot; class=&quot;headerlink&quot; title=&quot;配置Root POM和Goals and options&quot;&gt;&lt;/a&gt;配置Root POM和Goals and options&lt;/h4&gt;&lt;p&gt;因为是Maven项目，所以Build选项有Root POM和Goals and options的设置。Root POM:填写你项目的pom.xml文件的位置，注意：是相对位置，如果该文件不存在，会有红色字提示。&lt;br&gt;比如我这里是：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/61.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h4 id=&quot;Post-Steps&quot;&gt;&lt;a href=&quot;#Post-Steps&quot; class=&quot;headerlink&quot; title=&quot;Post Steps&quot;&gt;&lt;/a&gt;Post Steps&lt;/h4&gt;&lt;p&gt;在maven项目创建完成后，我们还需要实现每次构建完成，将war发布到阿里云主机上，以实现自动发布。我们通过添加shell实现自动发布。&lt;/p&gt;
&lt;p&gt;找到Post steps下有个Execute shell：&lt;br&gt;【注意】:Jenkins在执行该shell脚本的时候是以jenkins这个用户身份去执行。某些场景下请注意环境变量PATH。&lt;br&gt;将构建完成后，所要采取的动作，shell脚本脚本内容如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#!/bin/bash

# Stop tomcat.
ssh root@114.55.29.246 &amp;apos;/usr/local/apache-tomcat-7.0.65/bin/shutdown.sh&amp;apos; &amp;amp;&amp;gt;/dev/null
sleep 10

# Check the stop is successful or not.
if ssh root@114.55.29.246 &amp;apos;ps -ef|grep tomcat |grep -v &amp;quot;grep&amp;quot;&amp;apos; &amp;amp;&amp;gt;/dev/null; then
  echo &amp;quot;Tomcat stop failed.Please check the problem.&amp;quot;
  exit 5
fi

# Backup previous version and delete the war in the path /usr/local/apache-tomcat-7.0.65/webapps/.
ssh root@114.55.29.246 &amp;apos;/usr/bin/cp -f /usr/local/apache-tomcat-7.0.65/webapps/*.war /backups/*war&amp;apos;
ssh root@114.55.29.246 &amp;apos;rm -rf /usr/local/apache-tomcat-7.0.65/webapps/*&amp;apos;

# Copy the newest war to aliyun ECS.
scp /home/jenkins/.jenkins/workspace/godseye/godseye-parent/godseye-container/target/godseye-container-aliyun.war root@114.55.29.246:/usr/local/apache-tomcat-7.0.65/webapps/godseye.war &amp;amp;&amp;gt;/dev/null

# Start the tomcat.
ssh root@114.55.29.246 &amp;apos;/usr/local/apache-tomcat-7.0.65/bin/startup.sh&amp;apos; &amp;amp;&amp;gt;/dev/null
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;配置阿里云主机信任内网的这台jenkins主机：&lt;br&gt;由于是war包在内网服务器上，发布的环境是在阿里云主机上，所以要配置主机互信，防止scp war包时还需要输入密码。我这里内网服务器ip是172.16.206.30，外网是114.55.29.246。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[jenkins@osb30 ~]$ ssh-keygen -t rsa -f .ssh/id_rsa
[jenkins@osb30 ~]$ ssh-copy-id -i .ssh/id_rsa.pub root@114.55.29.246
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;Jenkins邮件通知设置&quot;&gt;&lt;a href=&quot;#Jenkins邮件通知设置&quot; class=&quot;headerlink&quot; title=&quot;Jenkins邮件通知设置&quot;&gt;&lt;/a&gt;Jenkins邮件通知设置&lt;/h2&gt;&lt;h3 id=&quot;配置jenkins自带的邮件功能&quot;&gt;&lt;a href=&quot;#配置jenkins自带的邮件功能&quot; class=&quot;headerlink&quot; title=&quot;配置jenkins自带的邮件功能&quot;&gt;&lt;/a&gt;配置jenkins自带的邮件功能&lt;/h3&gt;&lt;p&gt;1.找到系统设置&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/62.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;2.填写系统管理员邮箱&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/63.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;3.找到邮件通知，输入SMTP服务器地址，点击高级，输入发件人帐号和密码&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/64.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;4.勾选上”通过发送测试邮件测试配置”，然后输入收件人帐号&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/65.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;此时我们已经可以发送邮件了。在具体job配置处，找到”构建设置”，输入收件人信箱，但是你会发现只能在构建失败时发邮件。可以安装插件Email Extension Plugin来自定义。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/66.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;安装使用插件Email-Extension-Plugin&quot;&gt;&lt;a href=&quot;#安装使用插件Email-Extension-Plugin&quot; class=&quot;headerlink&quot; title=&quot;安装使用插件Email Extension Plugin&quot;&gt;&lt;/a&gt;安装使用插件Email Extension Plugin&lt;/h3&gt;&lt;p&gt;1.安装插件Email Extension Plugin&lt;br&gt;该插件支持jenkins 1.5以上的版本。&lt;br&gt;在系统管理-插件管理-安装Email Extension Plugin。它可根据构建的结果，发送构建报告。该插件支持jenkins 1.5以上的版本。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/67.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;【注意】:安装完如果使用Email Extension Plugin，就可以弃用自带的那个邮件功能了。&lt;/p&gt;
&lt;p&gt;2.配置使用插件Email Extension Plugin&lt;br&gt;点击”系统配置”—&amp;gt;”系统设置”。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/68.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;找到Extended E-mail Notification处，输入如下的配置：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/69.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/70.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;构建通知：$PROJECT_NAME - Build # $BUILD_NUMBER - $BUILD_STATUS!

&amp;lt;hr/&amp;gt;
(本邮件是程序自动下发的，请勿回复！)&amp;lt;br/&amp;gt;&amp;lt;hr/&amp;gt;
项目名称：$PROJECT_NAME&amp;lt;br/&amp;gt;&amp;lt;hr/&amp;gt;
构建编号：$BUILD_NUMBER&amp;lt;br/&amp;gt;&amp;lt;hr/&amp;gt;
svn版本号：${SVN_REVISION}&amp;lt;br/&amp;gt;&amp;lt;hr/&amp;gt;
构建状态：$BUILD_STATUS&amp;lt;br/&amp;gt;&amp;lt;hr/&amp;gt;
触发原因：${CAUSE}&amp;lt;br/&amp;gt;&amp;lt;hr/&amp;gt;
构建日志地址：&amp;lt;a href=&amp;quot;${BUILD_URL}console&amp;quot;&amp;gt;${BUILD_URL}console&amp;lt;/a&amp;gt;&amp;lt;br/&amp;gt;&amp;lt;hr/&amp;gt;
构建地址：&amp;lt;a href=&amp;quot;$BUILD_URL&amp;quot;&amp;gt;$BUILD_URL&amp;lt;/a&amp;gt;&amp;lt;br/&amp;gt;&amp;lt;hr/&amp;gt;
变更集:${JELLY_SCRIPT,template=&amp;quot;html&amp;quot;}&amp;lt;br/&amp;gt;&amp;lt;hr/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;点击下面的保存。&lt;br&gt;然后去job配置页面激活这个插件。找到需要发邮件的项目，点击进去。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/71.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;点击配置，点击”增加构建后操作步骤”，选择Editable Email Notification。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/72.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;附上构建日志，点击高级设置。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/73.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;配置Triggers：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/74.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;更详细的介绍：&lt;a href=&quot;http://www.cnblogs.com/zz0412/p/jenkins_jj_01.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://www.cnblogs.com/zz0412/p/jenkins_jj_01.html&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;sonar&quot;&gt;&lt;a href=&quot;#sonar&quot; class=&quot;headerlink&quot; title=&quot;sonar&quot;&gt;&lt;/a&gt;sonar&lt;/h2&gt;&lt;p&gt;官方文档：&lt;a href=&quot;http://docs.sonarqube.org/display/SONARQUBE45/Documentation&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://docs.sonarqube.org/display/SONARQUBE45/Documentation&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;sonar简介&quot;&gt;&lt;a href=&quot;#sonar简介&quot; class=&quot;headerlink&quot; title=&quot;sonar简介&quot;&gt;&lt;/a&gt;sonar简介&lt;/h3&gt;&lt;p&gt;Sonar是一个用于代码质量管理的开源平台，用于管理Java源代码的质量。通过插件机制，Sonar 可以集成不同的测试工具，代码分析工具，以及持续集成工具，比如pmd-cpd、checkstyle、findbugs、Jenkins。通过不同的插件对这些结果进行再加工处理，通过量化的方式度量代码质量的变化，从而可以方便地对不同规模和种类的工程进行代码质量管理。&lt;br&gt;与持续集成工具（例如 Hudson/Jenkins 等）不同，Sonar 并不是简单地把不同的代码检查工具结果（例如 FindBugs，PMD 等）直接显示在 Web 页面上，而是通过不同的插件对这些结果进行再加工处理，通过量化的方式度量代码质量的变化，从而可以方便地对不同规模和种类的工程进行代码质量管理。&lt;br&gt;在对其他工具的支持方面，Sonar 不仅提供了对 IDE 的支持，可以在 Eclipse 和 IntelliJ IDEA 这些工具里联机查看结果；同时 Sonar 还对大量的持续集成工具提供了接口支持，可以很方便地在持续集成中使用 Sonar。&lt;br&gt;此外，Sonar 的插件还可以对 Java 以外的其他编程语言提供支持，对国际化以及报告文档化也有良好的支持。&lt;/p&gt;
&lt;h3 id=&quot;环境要求&quot;&gt;&lt;a href=&quot;#环境要求&quot; class=&quot;headerlink&quot; title=&quot;环境要求&quot;&gt;&lt;/a&gt;环境要求&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;http://docs.sonarqube.org/display/SONAR/Requirements&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://docs.sonarqube.org/display/SONAR/Requirements&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;新建用户&quot;&gt;&lt;a href=&quot;#新建用户&quot; class=&quot;headerlink&quot; title=&quot;新建用户&quot;&gt;&lt;/a&gt;新建用户&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@osb30 ~]# groupadd sonar
[root@osb30 ~]# useradd -g sonar sonar
[root@osb30 ~]# id sonar
uid=502(sonar) gid=502(sonar) groups=502(sonar)
[root@osb30 ~]# echo &amp;quot;wisedu&amp;quot; | passwd --stdin sonar &amp;amp;&amp;gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;安装jdk&quot;&gt;&lt;a href=&quot;#安装jdk&quot; class=&quot;headerlink&quot; title=&quot;安装jdk&quot;&gt;&lt;/a&gt;安装jdk&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[sonar@osb30 ~]$ java -version
java version &amp;quot;1.8.0_65&amp;quot;
Java(TM) SE Runtime Environment (build 1.8.0_65-b17)
Java HotSpot(TM) 64-Bit Server VM (build 25.65-b01, mixed mode)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;安装配置数据库&quot;&gt;&lt;a href=&quot;#安装配置数据库&quot; class=&quot;headerlink&quot; title=&quot;安装配置数据库&quot;&gt;&lt;/a&gt;安装配置数据库&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@osb30 ~]# mysql -uroot –p
mysql&amp;gt; CREATE DATABASE sonar CHARACTER SET utf8 COLLATE utf8_general_ci;
mysql&amp;gt; CREATE USER &amp;apos;sonar&amp;apos; IDENTIFIED BY &amp;apos;sonar&amp;apos;;
mysql&amp;gt; GRANT ALL ON sonar.* TO &amp;apos;sonar&amp;apos;@&amp;apos;%&amp;apos; IDENTIFIED BY &amp;apos;wisedu&amp;apos;;
mysql&amp;gt; GRANT ALL ON sonar.* TO &amp;apos;sonar&amp;apos;@&amp;apos;localhost&amp;apos; IDENTIFIED BY &amp;apos;wisedu&amp;apos;;
mysql&amp;gt; FLUSH PRIVILEGES;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;安装sonar&quot;&gt;&lt;a href=&quot;#安装sonar&quot; class=&quot;headerlink&quot; title=&quot;安装sonar&quot;&gt;&lt;/a&gt;安装sonar&lt;/h3&gt;&lt;p&gt;我这里用的版本是SonarQube 4.5.7 (LTS *)，上传该软件到sonar用户的家目录下。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[sonar@osb30 ~]$ unzip -oq sonarqube-4.5.7.zip
[sonar@osb30 ~]$ vim sonarqube-4.5.7/conf/sonar.properties
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;修改如下字段(就是配置数据库信息，其他不用动)：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sonar.jdbc.username:                       sonar
sonar.jdbc.password:                       wisedu
sonar.jdbc.url:                            jdbc:mysql://localhost:3306/sonar?useUnicode=true&amp;amp;characterEncoding=utf8&amp;amp;rewriteBatchedStatements=true

# Optional properties
sonar.jdbc.driverClassName:                com.mysql.jdbc.Driver
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;启动sonar&quot;&gt;&lt;a href=&quot;#启动sonar&quot; class=&quot;headerlink&quot; title=&quot;启动sonar&quot;&gt;&lt;/a&gt;启动sonar&lt;/h3&gt;&lt;p&gt;Sonar默认集成了jetty容器，可以直接启动提供服务，也可以通过脚本构建为war包，部署在tomcat容器中。&lt;br&gt;Sonar默认的端口是”9000”、默认的上下文路径是”/”、默认的网络接口是”0.0.0.0”，默认的管理员帐号和密码为:admin/admin，这些参数都可以在配置文件sonar.properties中修改。我这里修改下port，因为本机的9000端口被其他程序占用了。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[sonar@osb30 ~]$ vim sonarqube-4.5.7/conf/sonar.properties
sonar.web.port=9003
[sonar@osb30 ~]$ sonarqube-4.5.7/bin/linux-x86-64/sonar.sh start
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查看日志：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[sonar@osb30 ~]$ tail -f sonarqube-4.5.7/logs/sonar.log
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可以看到第一次启动时，初始化语句：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/75.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;关闭sonar&quot;&gt;&lt;a href=&quot;#关闭sonar&quot; class=&quot;headerlink&quot; title=&quot;关闭sonar&quot;&gt;&lt;/a&gt;关闭sonar&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[sonar@osb30 ~]$ sonarqube-4.5.7/bin/linux-x86-64/sonar.sh stop
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;访问sonar&quot;&gt;&lt;a href=&quot;#访问sonar&quot; class=&quot;headerlink&quot; title=&quot;访问sonar&quot;&gt;&lt;/a&gt;访问sonar&lt;/h3&gt;&lt;p&gt;浏览器输入&lt;a href=&quot;http://172.16.206.30:9003/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.206.30:9003/&lt;/a&gt;&lt;br&gt;默认的管理员帐号和密码为:admin/admin。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/76.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/77.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/78.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;sonar插件&quot;&gt;&lt;a href=&quot;#sonar插件&quot; class=&quot;headerlink&quot; title=&quot;sonar插件&quot;&gt;&lt;/a&gt;sonar插件&lt;/h3&gt;&lt;p&gt;Sonar支持多种插件，插件的下载地址为：&lt;a href=&quot;http://docs.codehaus.org/display/SONAR/Plugin+Library&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://docs.codehaus.org/display/SONAR/Plugin+Library&lt;/a&gt;&lt;br&gt;将下载后的插件上传到${SONAR_HOME}extensions\plugins目录下，重新启动sonar。&lt;/p&gt;
&lt;p&gt;sonar默认集成了Java Ecosystem插件，该插件是一组插件的合集:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Java [sonar-java-plugin]：java源代码解析，计算指标等&lt;/li&gt;
&lt;li&gt;Squid [sonar-squid-java-plugin]：检查违反Sonar定义规则的代码&lt;/li&gt;
&lt;li&gt;Checkstyle [sonar-checkstyle-plugin]：使用CheckStyle检查违反统一代码编写风格的代码&lt;/li&gt;
&lt;li&gt;FindBugs [sonar-findbugs-plugin]：使用FindBugs检查违反规则的缺陷代码&lt;/li&gt;
&lt;li&gt;PMD [sonar-pmd-plugin]：使用pmd检查违反规则的代码&lt;/li&gt;
&lt;li&gt;Surefire [sonar-surefire-plugin]：使用Surefire执行单元测试&lt;/li&gt;
&lt;li&gt;Cobertura [sonar-cobertura-plugin]：使用Cobertura获取代码覆盖率&lt;/li&gt;
&lt;li&gt;JaCoCo [sonar-jacoco-plugin]：使用JaCOCO获取代码覆盖率&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;与jenkins集成&quot;&gt;&lt;a href=&quot;#与jenkins集成&quot; class=&quot;headerlink&quot; title=&quot;与jenkins集成&quot;&gt;&lt;/a&gt;与jenkins集成&lt;/h3&gt;&lt;p&gt;可以通过maven集成，也可以直接与jenkins集成。我这里选择直接与jenkins集成。&lt;/p&gt;
&lt;h4 id=&quot;通过maven集成&quot;&gt;&lt;a href=&quot;#通过maven集成&quot; class=&quot;headerlink&quot; title=&quot;通过maven集成&quot;&gt;&lt;/a&gt;通过maven集成&lt;/h4&gt;&lt;p&gt;修改maven的主配置文件（${MAVEN_HOME}/conf/settings.xml文件或者 ~/.m2/settings.xml文件），在其中增加访问Sonar数据库及Sonar服务地址，添加如下配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;profile&amp;gt;
&amp;lt;id&amp;gt;sonar&amp;lt;/id&amp;gt;
&amp;lt;properties&amp;gt;
    &amp;lt;sonar.jdbc.url&amp;gt;jdbc:mysql://localhost:3306/sonar&amp;lt;/sonar.jdbc.url&amp;gt;
    &amp;lt;sonar.jdbc.driver&amp;gt;com.mysql.jdbc.Driver&amp;lt;/sonar.jdbc.driver&amp;gt;
    &amp;lt;sonar.jdbc.username&amp;gt;sonar&amp;lt;/sonar.jdbc.username&amp;gt;
    &amp;lt;sonar.jdbc.password&amp;gt;sonar&amp;lt;/sonar.jdbc.password&amp;gt;
    &amp;lt;sonar.host.url&amp;gt;http://localhost:9003&amp;lt;/sonar.host.url&amp;gt; &amp;lt;!-- Sonar服务器访问地址 --&amp;gt;
&amp;lt;/properties&amp;gt;
&amp;lt;/profile&amp;gt;

&amp;lt;activeProfiles&amp;gt;
  &amp;lt;activeProfile&amp;gt;sonar&amp;lt;/activeProfile&amp;gt;
&amp;lt;/activeProfiles&amp;gt;

...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这部分内容具体可参照网上&lt;a href=&quot;http://www.cnblogs.com/gao241/p/3190701.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://www.cnblogs.com/gao241/p/3190701.html&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;直接与Jenkins集成&quot;&gt;&lt;a href=&quot;#直接与Jenkins集成&quot; class=&quot;headerlink&quot; title=&quot;直接与Jenkins集成&quot;&gt;&lt;/a&gt;直接与Jenkins集成&lt;/h4&gt;&lt;p&gt;在jenkins的插件管理中选择安装sonar jenkins plugin，该插件可以使项目每次构建都调用sonar进行代码度量。&lt;/p&gt;
&lt;p&gt;1.安装插件&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/79.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;2.系统配置添加sonar的配置&lt;br&gt;进入系统配置页面对sonar插件进行配置，如下图：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/80.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/81.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;然后点击下面的保存。&lt;/p&gt;
&lt;p&gt;3.配置构建项目，增加Post Build Action&lt;br&gt;点击要构建的项目，在点击左侧的配置。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/82.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;在页面的最下面找到”构建后操作”，选择SonarQube。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/83.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/84.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;It is no longer recommended to use SonarQube maven builder. It is preferable to set up SonarQube in the build environment and use a standard Jenkins maven target.&lt;br&gt;【解决】：&lt;br&gt;&lt;a href=&quot;http://docs.sonarqube.org/display/SCAN/Analyzing+with+SonarQube+Scanner+for+Jenkins&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://docs.sonarqube.org/display/SCAN/Analyzing+with+SonarQube+Scanner+for+Jenkins&lt;/a&gt;&lt;br&gt;修改Build处：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/85.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;最后去jenkins构建项目，构建完查看sonar控制台：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/86.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;常见问题&quot;&gt;&lt;a href=&quot;#常见问题&quot; class=&quot;headerlink&quot; title=&quot;常见问题&quot;&gt;&lt;/a&gt;常见问题&lt;/h3&gt;&lt;p&gt;Jenkins构建完成后，sonar扫描代码报错：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/87.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;解决：&lt;br&gt;卸载sonar的JavaScript插件。&lt;/p&gt;
&lt;h2 id=&quot;Jenkins与Docker结合&quot;&gt;&lt;a href=&quot;#Jenkins与Docker结合&quot; class=&quot;headerlink&quot; title=&quot;Jenkins与Docker结合&quot;&gt;&lt;/a&gt;Jenkins与Docker结合&lt;/h2&gt;&lt;p&gt;我这里没有使用Docker Pipeline，直接在构建完成后，执行shell脚本，这样更灵活。&lt;/p&gt;
&lt;h3 id=&quot;部署流程&quot;&gt;&lt;a href=&quot;#部署流程&quot; class=&quot;headerlink&quot; title=&quot;部署流程&quot;&gt;&lt;/a&gt;部署流程&lt;/h3&gt;&lt;p&gt;1.研发push到svn代码库&lt;br&gt;2.Jenkins 构建，pull svn代码 使用maven进行编译打包&lt;br&gt;3.打包生成的代码，生成一个新版本的镜像，push到本地docker仓库harbor&lt;br&gt;4.发布，测试机器 pull 新版本的镜像，并删除原来的容器，重新运行新版本镜像。&lt;/p&gt;
&lt;h3 id=&quot;环境说明&quot;&gt;&lt;a href=&quot;#环境说明&quot; class=&quot;headerlink&quot; title=&quot;环境说明&quot;&gt;&lt;/a&gt;环境说明&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;操作系统版本&lt;/th&gt;
&lt;th&gt;IP地址&lt;/th&gt;
&lt;th&gt;用途&lt;/th&gt;
&lt;th&gt;安装软件&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;osb30&lt;/td&gt;
&lt;td&gt;Redhat 6.5&lt;/td&gt;
&lt;td&gt;172.16.206.30&lt;/td&gt;
&lt;td&gt;svn代码库、Jenkins、Docker&lt;/td&gt;
&lt;td&gt;jenkins、svn、Docker 1.7.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;spark32&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.206.32&lt;/td&gt;
&lt;td&gt;本地docker仓库、业务部署测试环境&lt;/td&gt;
&lt;td&gt;harbor、Docker 17.06.1-ce&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&quot;配置&quot;&gt;&lt;a href=&quot;#配置&quot; class=&quot;headerlink&quot; title=&quot;配置&quot;&gt;&lt;/a&gt;配置&lt;/h3&gt;&lt;p&gt;由于在Jenkins机器上docker是使用root用户运行的，而Jenkins是使用普通用户jenkins运行的，所以要先配置下jenkins用户可以使用docker命令。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@osb30 ~]# visudo
jenkins ALL=(root)      NOPASSWD: /usr/bin/docker
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;另外在Jenkins机器上配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Disable &amp;quot;ssh hostname sudo &amp;lt;cmd&amp;gt;&amp;quot;, because it will show the password in clear.
#         You have to run &amp;quot;ssh -t hostname sudo &amp;lt;cmd&amp;gt;&amp;quot;.
#
#Defaults    requiretty
Defaults:jenkins !requiretty
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果不配置这个，在执行下面脚本时，会报错误：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+ cp -f /home/jenkins/.jenkins/workspace/godseyeBranchForNov/godseye-container/target/godseye-container-wisedu.war /home/jenkins/docker-file/godseye_war/godseye.war
+ sudo docker login -u jkzhao -p Wisedu123 -e 01115004@wisedu.com 172.16.206.32
sudo: sorry, you must have a tty to run sudo
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在172.16.206.32机器上配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# visudo
#
#Defaults    requiretty
Defaults:root !requiretty
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;否则在机器172.16.206.32机器上执行脚本时会报错：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[SSH] executing...
sudo: sorry, you must have a tty to run sudo
docker: invalid reference format.
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;安装插件&quot;&gt;&lt;a href=&quot;#安装插件&quot; class=&quot;headerlink&quot; title=&quot;安装插件&quot;&gt;&lt;/a&gt;安装插件&lt;/h3&gt;&lt;p&gt;登录Jenkins，点击“系统管理”，点击“管理插件”，搜索插件“SSH plugin”，进行安装。&lt;br&gt;登录Jenkins，点击“Credentials”，点击“Add domain”。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/88.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/89.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/90.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;点击“系统管理”，“系统配置”，找到“SSH remote hosts”。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/91.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;配置Post-Steps&quot;&gt;&lt;a href=&quot;#配置Post-Steps&quot; class=&quot;headerlink&quot; title=&quot;配置Post Steps&quot;&gt;&lt;/a&gt;配置Post Steps&lt;/h3&gt;&lt;p&gt;项目其他的配置不变，见上面的章节。&lt;br&gt;【注意】：脚本中用到的仓库和认证的账号需要先在harbor新建好。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/92.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Jenkins机器：编译完成后，build生成一个新版本的镜像，push到远程docker仓库

# Variables
JENKINS_WAR_HOME=&amp;apos;/home/jenkins/.jenkins/workspace/godseyeBranchForNov/godseye-container/target&amp;apos;
DOCKERFILE_HOME=&amp;apos;/home/jenkins/docker-file/godseye_war&amp;apos;
HARBOR_IP=&amp;apos;172.16.206.32&amp;apos;
REPOSITORIES=&amp;apos;godseye_war/godseye&amp;apos;
HARBOR_USER=&amp;apos;jkzhao&amp;apos;
HARBOR_USER_PASSWD=&amp;apos;Wisedu123&amp;apos;
HARBOR_USER_EMAIL=&amp;apos;01115004@wisedu.com&amp;apos;

# Copy the newest war to docker-file directory.
\cp -f ${JENKINS_WAR_HOME}/godseye-container-wisedu.war ${DOCKERFILE_HOME}/godseye.war 

# Delete image early version.
sudo docker login -u ${HARBOR_USER} -p ${HARBOR_USER_PASSWD} -e ${HARBOR_USER_EMAIL} ${HARBOR_IP}  
IMAGE_ID=`sudo docker images | grep ${REPOSITORIES} | awk &amp;apos;{print $3}&amp;apos;`
if [ -n &amp;quot;${IMAGE_ID}&amp;quot; ];then
    sudo docker rmi ${IMAGE_ID}
fi

# Build image.
cd ${DOCKERFILE_HOME}
TAG=`date +%Y%m%d-%H%M%S`
sudo docker build -t ${HARBOR_IP}/${REPOSITORIES}:${TAG} . &amp;amp;&amp;gt;/dev/null

# Push to the harbor registry.
sudo docker push ${HARBOR_IP}/${REPOSITORIES}:${TAG} &amp;amp;&amp;gt;/dev/null
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/93.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 拉取镜像，发布
HARBOR_IP=&amp;apos;172.16.206.32&amp;apos;
REPOSITORIES=&amp;apos;godseye_war/godseye&amp;apos;
HARBOR_USER=&amp;apos;jkzhao&amp;apos;
HARBOR_USER_PASSWD=&amp;apos;Wisedu123&amp;apos;

# 登录harbor
#docker login -u ${HARBOR_USER} -p ${HARBOR_USER_PASSWD} ${HARBOR_IP}

# Stop container, and delete the container.
CONTAINER_ID=`docker ps | grep &amp;quot;godseye_web&amp;quot; | awk &amp;apos;{print $1}&amp;apos;`
if [ -n &amp;quot;$CONTAINER_ID&amp;quot; ]; then
    docker stop $CONTAINER_ID
    docker rm $CONTAINER_ID
else #如果容器启动时失败了，就需要docker ps -a才能找到那个容器
    CONTAINER_ID=`docker ps -a | grep &amp;quot;godseye_web&amp;quot; | awk &amp;apos;{print $1}&amp;apos;`
    if [ -n &amp;quot;$CONTAINER_ID&amp;quot; ]; then  # 如果是第一次在这台机器上拉取运行容器，那么docker ps -a也是找不到这个容器的
        docker rm $CONTAINER_ID
    fi
fi

# Delete godseye_web image early version.
IMAGE_ID=`sudo docker images | grep ${REPOSITORIES} | awk &amp;apos;{print $3}&amp;apos;`
if [ -n &amp;quot;${IMAGE_ID}&amp;quot; ];then
    docker rmi ${IMAGE_ID}
fi

# Pull image.
TAG=`curl -s http://${HARBOR_IP}/api/repositories/${REPOSITORIES}/tags | jq &amp;apos;.[-1]&amp;apos; | sed &amp;apos;s/\&amp;quot;//g&amp;apos;` #最后的sed是为了去掉tag前后的双引号
docker pull ${HARBOR_IP}/${REPOSITORIES}:${TAG} &amp;amp;&amp;gt;/dev/null

# Run.
docker run -d --name godseye_web -p 8080:8080 ${HARBOR_IP}/${REPOSITORIES}:${TAG}
&lt;/code&gt;&lt;/pre&gt;</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Jenkins介绍&quot;&gt;&lt;a href=&quot;#Jenkins介绍&quot; class=&quot;headerlink&quot; title=&quot;Jenkins介绍&quot;&gt;&lt;/a&gt;Jenkins介绍&lt;/h2&gt;&lt;p&gt;Jenkins是一个开源软件项目，是基于Java开发的一种持续集成工具，用于监控持续重复的工作，旨在提供一个开放易用的软件平台，使软件的持续集成变成可能。&lt;br&gt;
    
    </summary>
    
      <category term="持续集成" scheme="http://yoursite.com/categories/%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/"/>
    
    
      <category term="Docker" scheme="http://yoursite.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>在Kubernetes上使用Traefik</title>
    <link href="http://yoursite.com/2017/09/25/%E5%9C%A8Kubernetes%E4%B8%8A%E4%BD%BF%E7%94%A8Traefik/"/>
    <id>http://yoursite.com/2017/09/25/在Kubernetes上使用Traefik/</id>
    <published>2017-09-25T03:30:01.000Z</published>
    <updated>2017-12-01T13:22:25.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;Traefik介绍&quot;&gt;&lt;a href=&quot;#Traefik介绍&quot; class=&quot;headerlink&quot; title=&quot;Traefik介绍&quot;&gt;&lt;/a&gt;Traefik介绍&lt;/h2&gt;&lt;p&gt;traefik 是一个前端负载均衡器，对于微服务架构尤其是 kubernetes 等编排工具具有良好的支持；同 nginx 等相比，traefik 能够自动感知后端容器变化，从而实现自动服务发现。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;br&gt;由于微服务架构以及 Docker 技术和 kubernetes 编排工具最近几年才开始逐渐流行，所以一开始的反向代理服务器比如 nginx、apache 并未提供其支持，毕竟他们也不是先知；所以才会出现 Ingress Controller 这种东西来做 kubernetes 和前端负载均衡器如 nginx 之间做衔接；即 Ingress Controller 的存在就是为了能跟 kubernetes 交互，又能写 nginx 配置，还能 reload 它，这是一种折中方案；而 traefik 天生就是提供了对 kubernetes 的支持，也就是说 traefik 本身就能跟 kubernetes API 交互，感知后端变化，因此可以得知: 在使用 traefik 时，Ingress Controller 已经没什么用了，整体架构如下：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/12.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;部署测试用的两个服务&quot;&gt;&lt;a href=&quot;#部署测试用的两个服务&quot; class=&quot;headerlink&quot; title=&quot;部署测试用的两个服务&quot;&gt;&lt;/a&gt;部署测试用的两个服务&lt;/h2&gt;&lt;p&gt;部署两个服务nginx1-7和nginx1-8，后面用Traefik去负载这两个服务：&lt;br&gt;nginx1-7.yaml：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: frontend
spec:
  ports:
    - port: 80
      targetPort: 80
  selector:
    app: nginx1-7
---
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: nginx1-7-deployment
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx1-7
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;nginx1-8.yaml：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: my-nginx
spec:
  ports:
    - port: 80
      targetPort: 80
  selector:
    app: nginx1-8
---
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: nginx1-8-deployment
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx1-8
    spec:
      containers:
      - name: nginx
        image: nginx:1.8
        ports:
        - containerPort: 80
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;运行两个服务：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 nginx_ingress]# kubectl create -f nginx1-7.yaml
service &amp;quot;frontend&amp;quot; created
deployment &amp;quot;nginx1-7-deployment&amp;quot; created
[root@node1 nginx_ingress]# kubectl create -f nginx1-8.yaml
service &amp;quot;my-nginx&amp;quot; created
deployment &amp;quot;nginx1-8-deployment&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;Role-Based-Access-Control-configuration-Kubernetes-1-6-only&quot;&gt;&lt;a href=&quot;#Role-Based-Access-Control-configuration-Kubernetes-1-6-only&quot; class=&quot;headerlink&quot; title=&quot;Role Based Access Control configuration (Kubernetes 1.6+ only)&quot;&gt;&lt;/a&gt;Role Based Access Control configuration (Kubernetes 1.6+ only)&lt;/h2&gt;&lt;p&gt;我这里部署的是1.6.0集群，开启了RBAC，授权需要使用角色和绑定角色。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 traefik]# pwd
/opt/traefik
[root@node1 traefik]# vim ingress-rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ingress
  namespace: kube-system

---

kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: ingress
subjects:
  - kind: ServiceAccount
    name: ingress
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;部署Traefik&quot;&gt;&lt;a href=&quot;#部署Traefik&quot; class=&quot;headerlink&quot; title=&quot;部署Traefik&quot;&gt;&lt;/a&gt;部署Traefik&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;[root@node1 traefik]# pwd
/opt/traefik
[root@node1 traefik]# vim traefik-deploy.yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: traefik-ingress-lb
  namespace: kube-system
  labels:
    k8s-app: traefik-ingress-lb
spec:
  template:
    metadata:
      labels:
        k8s-app: traefik-ingress-lb
        name: traefik-ingress-lb
    spec:
      terminationGracePeriodSeconds: 60
      hostNetwork: true
      restartPolicy: Always
      serviceAccountName: ingress
      containers:
      - image: traefik
        name: traefik-ingress-lb
        resources:
          limits:
            cpu: 200m
            memory: 30Mi
          requests:
            cpu: 100m
            memory: 20Mi
        ports:
        - name: http
          containerPort: 80
          hostPort: 80
        - name: admin
          containerPort: 8580
          hostPort: 8580
        args:
        - --web
        - --web.address=:8580
        - --kubernetes  
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其中 traefik 监听 node 的 80 和 8580 端口，80 提供正常服务，8580 是其自带的 UI 界面，原本默认是 8080，因为环境里端口冲突了，所以这里临时改一下。&lt;br&gt;【注意】：这里用的是Deploy类型，没有限定该pod运行在哪个主机上。&lt;/p&gt;
&lt;h2 id=&quot;部署-Ingress&quot;&gt;&lt;a href=&quot;#部署-Ingress&quot; class=&quot;headerlink&quot; title=&quot;部署 Ingress&quot;&gt;&lt;/a&gt;部署 Ingress&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;[root@node1 traefik]# cat traefik.yaml 
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: traefik-ingress
  namespace: default
spec:
  rules:
  - host: traefik.nginx.io
    http:
      paths:
      - path: /
        backend:
          serviceName: my-nginx
          servicePort: 80
  - host: traefik.frontend.io
    http:
      paths:
      - path: /
        backend:
          serviceName: frontend
          servicePort: 80
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其中的backend中要配置default namespace中启动的service名字，如果你没有配置namespace名字，默认使用default namespace，如果你在其他namespace中创建服务想要暴露到kubernetes集群外部，可以创建新的ingress.yaml文件，同时在文件中指定该namespace，其他配置与上面的文件格式相同。path就是URL地址后的路径，如traefik.frontend.io/path，service将会接受path这个路径，host最好使用service-name.filed1.filed2.domain-name这种类似主机名称的命名方式，方便区分服务。&lt;br&gt;根据实际环境中部署的service的名字和端口自行修改，有新service增加时，修改该文件后可以使用kubectl replace -f traefik.yaml来更新。&lt;/p&gt;
&lt;h2 id=&quot;部署Traefik-UI&quot;&gt;&lt;a href=&quot;#部署Traefik-UI&quot; class=&quot;headerlink&quot; title=&quot;部署Traefik UI&quot;&gt;&lt;/a&gt;部署Traefik UI&lt;/h2&gt;&lt;p&gt;traefik 本身还提供了一套 UI 供我们使用，其同样以 Ingress 方式暴露，只需要创建一下即可。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 traefik]# cat traefik-ui-service.yaml 
apiVersion: v1
kind: Service
metadata:
  name: traefik-web-ui
  namespace: kube-system
spec:
  selector:
    k8s-app: traefik-ingress-lb
  ports:
  - name: web
    port: 80
    targetPort: 8580
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: traefik-web-ui
  namespace: kube-system
spec:
  rules:
  - host: traefik-ui.local
    http:
      paths:
      - path: /
        backend:
          serviceName: traefik-web-ui
          servicePort: web
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;最后一起创建：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 traefik]# kubectl create -f .
serviceaccount &amp;quot;ingress&amp;quot; created
clusterrolebinding &amp;quot;ingress&amp;quot; created
deployment &amp;quot;traefik-ingress-lb&amp;quot; created
service &amp;quot;traefik-web-ui&amp;quot; created
ingress &amp;quot;traefik-web-ui&amp;quot; created
ingress &amp;quot;traefik-ingress&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;访问测试&quot;&gt;&lt;a href=&quot;#访问测试&quot; class=&quot;headerlink&quot; title=&quot;访问测试&quot;&gt;&lt;/a&gt;访问测试&lt;/h2&gt;&lt;p&gt;查看traefik pod被分配到了哪台主机上：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 traefik]# kubectl get pods -n kube-system -l k8s-app=traefik-ingress-lb -o wide                       
NAME                                  READY     STATUS    RESTARTS   AGE       IP             NODE
traefik-ingress-lb-4237248072-1dg9n   1/1       Running   0          2m        172.16.7.152   172.16.7.152
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;浏览器输入&lt;a href=&quot;http://172.16.7.152:8580/，将可以看到dashboard。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.152:8580/，将可以看到dashboard。&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/13.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;左侧黄色部分部分列出的是所有的rule，右侧绿色部分是所有的backend。&lt;/p&gt;
&lt;p&gt;在Kubernetes集群的任意一个节点上执行。假如现在我要访问nginx的”/“路径。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -H Host:traefik.nginx.io http://172.16.7.152/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果需要在kubernetes集群以外访问就需要设置DNS，或者修改本机的hosts文件。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;172.16.7.152 traefik.nginx.io
172.16.7.152 traefik.frontend.io
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;所有访问这些地址的流量都会发送给172.16.7.152这台主机，就是我们启动traefik的主机。&lt;br&gt;Traefik会解析http请求header里的Host参数将流量转发给Ingress配置里的相应service。&lt;br&gt;修改hosts后就就可以在kubernetes集群外访问以上两个service。&lt;/p&gt;
&lt;h2 id=&quot;健康检查&quot;&gt;&lt;a href=&quot;#健康检查&quot; class=&quot;headerlink&quot; title=&quot;健康检查&quot;&gt;&lt;/a&gt;健康检查&lt;/h2&gt;&lt;p&gt;关于健康检查，测试可以使用 kubernetes 的 Liveness Probe 实现，如果 Liveness Probe检查失败，则 traefik 会自动移除该 pod。&lt;br&gt;【示例】：我们定义一个 test-health 的 deployment，健康检查方式是 cat /tmp/health，容器启动 2 分钟后会删掉这个文件，模拟健康检查失败。&lt;br&gt;test-health的deployment：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 traefik]# vim test-health-deploy.yaml
apiVersion: v1
kind: Deployment
apiVersion: extensions/v1beta1
metadata:
  name: test
  namespace: default
  labels:
    test: alpine
spec:
  replicas: 1
  selector:
    matchLabels:
      test: alpine
  template:
    metadata:
      labels:
        test: alpine
        name: test
    spec:
      containers:
      - image: mritd/alpine:3.4
        name: alpine
        resources:
          limits:
            cpu: 200m
            memory: 30Mi
          requests:
            cpu: 100m
            memory: 20Mi
        ports:
        - name: http
          containerPort: 80
        args:
        command:
        - &amp;quot;bash&amp;quot;
        - &amp;quot;-c&amp;quot;
        - &amp;quot;echo ok &amp;gt; /tmp/health;sleep 120;rm -f /tmp/health&amp;quot;
        livenessProbe:
          exec:
            command:
            - cat
            - /tmp/health
          initialDelaySeconds: 20
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;test-health 的 service：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 traefik]# vim test-health-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: test 
  labels:
    name: test
spec:
  ports:
  - port: 8123
    targetPort: 80
  selector:
    name: test
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;test-health的 Ingress：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 traefik]# vim test-health-ingress.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test
spec:
  rules:
  - host: test.com
    http:
      paths:
      - path: /
        backend:
          serviceName: test
          servicePort: 8123
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;全部创建好以后，进入 traefik ui 界面，可以观察到每隔 2 分钟健康检查失败后，kubernetes 重建 pod，同时 traefik 会从后端列表中移除这个 pod。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Traefik介绍&quot;&gt;&lt;a href=&quot;#Traefik介绍&quot; class=&quot;headerlink&quot; title=&quot;Traefik介绍&quot;&gt;&lt;/a&gt;Traefik介绍&lt;/h2&gt;&lt;p&gt;traefik 是一个前端负载均衡器，对于微服务架构尤其是 kubernetes 等编排工具具有良好的支持；同 nginx 等相比，traefik 能够自动感知后端容器变化，从而实现自动服务发现。&lt;br&gt;
    
    </summary>
    
      <category term="容器编排" scheme="http://yoursite.com/categories/%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92/"/>
    
    
      <category term="Kubernetes" scheme="http://yoursite.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes Ingress实战</title>
    <link href="http://yoursite.com/2017/09/24/Kubernetes-Ingress%E5%AE%9E%E6%88%98/"/>
    <id>http://yoursite.com/2017/09/24/Kubernetes-Ingress实战/</id>
    <published>2017-09-24T08:57:22.000Z</published>
    <updated>2017-11-25T12:50:03.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;服务发现与负载均衡&quot;&gt;&lt;a href=&quot;#服务发现与负载均衡&quot; class=&quot;headerlink&quot; title=&quot;服务发现与负载均衡&quot;&gt;&lt;/a&gt;服务发现与负载均衡&lt;/h2&gt;&lt;p&gt;在前面的安装部署kubernetes集群中已经简单用示例来演示了Pod和Service，Kubernetes通过Service资源在Kubernetes集群内针对容器实现了服务发现和负载均衡。而Service就是kubernetes服务发现与负载均衡中的一种。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;br&gt;目前，kubernetes中的负载均衡大致可以分为以下几种机制，每种机制都有其特定的应用场景：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Service：直接用Service提供cluster内部的负载均衡，并借助cloud provider提供的LB提供外部访问&lt;/li&gt;
&lt;li&gt;Ingress Controller：还是用Service提供cluster内部的负载均衡，但是通过自定义LB提供外部访问&lt;/li&gt;
&lt;li&gt;Service Load Balancer：把load balancer直接跑在容器中，实现Bare Metal的Service Load Balancer&lt;/li&gt;
&lt;li&gt;Custom Load Balancer：自定义负载均衡，并替代kube-proxy，一般在物理部署Kubernetes时使用，方便接入公司已有的外部服务&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;Service&quot;&gt;&lt;a href=&quot;#Service&quot; class=&quot;headerlink&quot; title=&quot;Service&quot;&gt;&lt;/a&gt;Service&lt;/h3&gt;&lt;p&gt;Service是对一组提供相同功能的Pods的抽象，并为它们提供一个统一的入口。借助Service，应用可以方便的实现服务发现与负载均衡，并实现应用的零宕机升级。Service通过标签来选取服务后端，一般配合Replication Controller或者Deployment来保证后端容器的正常运行。这些匹配标签的Pod IP和端口列表组成endpoints,由kube-proxy负责将服务IP负载均衡到这些endpoints上。&lt;br&gt;Service有四种类型:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ClusterIP:默认类型,自动分配一个仅cluster内部可以访问的虚拟IP &lt;/li&gt;
&lt;li&gt;NodePort:在ClusterIP基础上为Service在每台机器上绑定一个端口,这样就可以 通过 &lt;nodeip&gt;:NodePort 来访问该服务 &lt;/nodeip&gt;&lt;/li&gt;
&lt;li&gt;LoadBalancer:在NodePort的基础上,借助cloud provider创建一个外部的负载均 衡器,并将请求转发到 &lt;nodeip&gt;:NodePort&lt;/nodeip&gt;&lt;/li&gt;
&lt;li&gt;ExternalName:将服务通过DNS CNAME记录方式转发到指定的域名(通&lt;br&gt;过 spec.externlName 设定)。需要kube-dns版本在1.7以上。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;另外，也可以将已有的服务以Service的形式加入到Kubernetes集群中来，只需要在创建 Service的时候不指定Label selector，而是在Service创建好后手动为其添加endpoint。&lt;/p&gt;
&lt;h3 id=&quot;Ingress和Ingress-Controller简介&quot;&gt;&lt;a href=&quot;#Ingress和Ingress-Controller简介&quot; class=&quot;headerlink&quot; title=&quot;Ingress和Ingress Controller简介&quot;&gt;&lt;/a&gt;Ingress和Ingress Controller简介&lt;/h3&gt;&lt;p&gt;Service虽然解决了服务发现和负载均衡的问题，但它在使用上还是有一些限制，比如&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;只支持4层负载均衡，没有7层功能 &lt;/li&gt;
&lt;li&gt;对外访问的时候，NodePort类型需要在外部搭建额外的负载均衡，而LoadBalancer要求kubernetes必须跑在支持的cloud provider上面。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;Ingress&quot;&gt;&lt;a href=&quot;#Ingress&quot; class=&quot;headerlink&quot; title=&quot;Ingress&quot;&gt;&lt;/a&gt;Ingress&lt;/h4&gt;&lt;p&gt;Ingress就是为了解决这些限制而引入的新资源，主要用来将服务暴露到cluster外面，并 且可以自定义服务的访问策略。比如想要通过负载均衡器实现不同子域名到不同服务的访问:&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;foo.bar.com --|                 |-&amp;gt; foo.bar.com s1:80&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;              | 178.91.123.132  |&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;bar.foo.com --|                 |-&amp;gt; bar.foo.com s2:80&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;可以这样来定义Ingress:&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;apiVersion: extensions/v1beta1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;kind: Ingress&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  name: test&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  rules:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  - host: foo.bar.com&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    http:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      paths:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      - backend:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          serviceName: s1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          servicePort: 80&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  - host: bar.foo.com&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    http:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      paths:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      - backend:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          serviceName: s2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          servicePort: 80&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;【注意】：Ingress本身并不会自动创建负载均衡器，cluster中需要运行一个ingress controller来根据Ingress的定义来管理负载均衡器。目前社区提供了&lt;a href=&quot;https://github.com/kubernetes/ingress/tree/master/controllers&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;nginx和gce的参考实现&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;简单的说，ingress就是从kubernetes集群外访问集群的入口，将用户的URL请求转发到 不同的service上。Ingress相当于nginx、apache等负载均衡方向代理服务器，其中还包括规则定义，即URL的路由信息，路由信息得的刷新由Ingress controller来提供。&lt;/p&gt;
&lt;h4 id=&quot;Ingress-Controller&quot;&gt;&lt;a href=&quot;#Ingress-Controller&quot; class=&quot;headerlink&quot; title=&quot;Ingress Controller&quot;&gt;&lt;/a&gt;Ingress Controller&lt;/h4&gt;&lt;p&gt;Ingress Controller 实质上可以理解为是个监视器，Ingress Controller 通过不断地跟 kubernetes API 打交道，实时的感知后端 service、pod 等变化，比如新增和减少 pod，service 增加与减少等;当得到这些变化信息后，Ingress Controller 再结合下文的 Ingress 生成配置，然后更新反向代理负载均衡器，并刷新其配置，达到服务发现的作用。&lt;/p&gt;
&lt;h2 id=&quot;Ingress实战&quot;&gt;&lt;a href=&quot;#Ingress实战&quot; class=&quot;headerlink&quot; title=&quot;Ingress实战&quot;&gt;&lt;/a&gt;Ingress实战&lt;/h2&gt;&lt;p&gt;在使用Ingress resource之前，有必要先了解下面几件事情。Ingress是beta版本的resource，在kubernetes1.1之前还没有。你需要一个Ingress Controller来实现Ingress，单纯的创建一个Ingress没有任何意义。目前社区提供了&lt;a href=&quot;https://github.com/kubernetes/ingress/tree/master/controllers&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;nginx和gce的参考实现&lt;/a&gt;。当然还有其他实现，&lt;a href=&quot;https://github.com/nginxinc/kubernetes-ingress&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;开源的 NGINX 和 NGINX Plus 开发了相应的 Ingress controller&lt;/a&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GCE/GKE会在master节点上部署一个ingress controller。你可以在一个pod中部署任意个自定义的ingress controller。你必须正确地annotate每个ingress，比如 &lt;a href=&quot;https://github.com/kubernetes/ingress/tree/master/controllers/nginx#running-multiple-ingress-controllers&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;运行多个ingress controller&lt;/a&gt; 和 &lt;a href=&quot;https://github.com/kubernetes/ingress/blob/master/controllers/gce/BETA_LIMITATIONS.md#disabling-glbc&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;关闭glbc&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;在非GCE/GKE的环境中，你需要在pod中部署一个controller。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;使用-NGINX-和-NGINX-Plus-的-Ingress-Controller-进行-Kubernetes-的负载均衡&quot;&gt;&lt;a href=&quot;#使用-NGINX-和-NGINX-Plus-的-Ingress-Controller-进行-Kubernetes-的负载均衡&quot; class=&quot;headerlink&quot; title=&quot;使用 NGINX 和 NGINX Plus 的 Ingress Controller 进行 Kubernetes 的负载均衡&quot;&gt;&lt;/a&gt;使用 NGINX 和 NGINX Plus 的 Ingress Controller 进行 Kubernetes 的负载均衡&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/nginxinc/kubernetes-ingress/tree/master/examples/complete-example&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/nginxinc/kubernetes-ingress/tree/master/examples/complete-example&lt;/a&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;19&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;20&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;21&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;22&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;[root@node1 nginx_ingress]# kubectl create -f nginx-ingress-rbac.yaml&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;[root@node1 nginx_ingress]# kubectl create -f default-server-secret.yaml &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;secret &amp;quot;default-server-secret&amp;quot; created&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;[root@node1 nginx_ingress]# kubectl create -f nginx-ingress-rc.yaml&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;replicationcontroller &amp;quot;nginx-ingress-rc&amp;quot; created&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;[root@node1 nginx_ingress]# kubectl get pods -l app=nginx-ingress -o wide &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;NAME                     READY     STATUS    RESTARTS   AGE       IP            NODE&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;nginx-ingress-rc-rs1vh   1/1       Running   0          37s       172.30.87.4   172.16.7.151&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;# 查看pod日志&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;[root@node1 nginx_ingress]# kubectl logs nginx-ingress-rc-rs1vh&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;I0924 07:27:37.663514       1 main.go:58] Starting NGINX Ingress controller Version 1.0.0&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2017/09/24 07:27:37 [notice] 20#20: signal process started&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;I0924 07:27:37.975349       1 event.go:218] Event(v1.ObjectReference&amp;#123;Kind:&amp;quot;Secret&amp;quot;, Namespace:&amp;quot;default&amp;quot;, Name:&amp;quot;default-server-secret&amp;quot;, UID:&amp;quot;4e2d9567-9f5a-11e7-9acc-005056b7609a&amp;quot;, APIVersion:&amp;quot;v1&amp;quot;, ResourceVersion:&amp;quot;1019701&amp;quot;, FieldPath:&amp;quot;&amp;quot;&amp;#125;): type: &amp;apos;Normal&amp;apos; reason: &amp;apos;Updated&amp;apos; the default server Secret default/default-server-secret was updated&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2017/09/24 07:27:37 [notice] 26#26: signal process started&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2017/09/24 07:27:38 [notice] 30#30: signal process started&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2017/09/24 07:27:38 [notice] 34#34: signal process started&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2017/09/24 07:27:38 [notice] 38#38: signal process started&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;I0924 07:27:38.073475       1 event.go:218] Event(v1.ObjectReference&amp;#123;Kind:&amp;quot;Ingress&amp;quot;, Namespace:&amp;quot;kube-system&amp;quot;, Name:&amp;quot;traefik-web-ui&amp;quot;, UID:&amp;quot;5d604da9-9f61-11e7-9acc-005056b7609a&amp;quot;, APIVersion:&amp;quot;extensions&amp;quot;, ResourceVersion:&amp;quot;1024008&amp;quot;, FieldPath:&amp;quot;&amp;quot;&amp;#125;): type: &amp;apos;Normal&amp;apos; reason: &amp;apos;AddedOrUpdated&amp;apos; Configuration for kube-system/traefik-web-ui was added or updated&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2017/09/24 07:27:38 [notice] 44#44: signal process started&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;I0924 07:27:38.100887       1 event.go:218] Event(v1.ObjectReference&amp;#123;Kind:&amp;quot;Ingress&amp;quot;, Namespace:&amp;quot;default&amp;quot;, Name:&amp;quot;traefik-ingress&amp;quot;, UID:&amp;quot;5d693739-9f61-11e7-9acc-005056b7609a&amp;quot;, APIVersion:&amp;quot;extensions&amp;quot;, ResourceVersion:&amp;quot;1024009&amp;quot;, FieldPath:&amp;quot;&amp;quot;&amp;#125;): type: &amp;apos;Normal&amp;apos; reason: &amp;apos;AddedOrUpdated&amp;apos; Configuration for default/traefik-ingress was added or updated&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;配置需要测试的service&quot;&gt;&lt;a href=&quot;#配置需要测试的service&quot; class=&quot;headerlink&quot; title=&quot;配置需要测试的service&quot;&gt;&lt;/a&gt;配置需要测试的service&lt;/h3&gt;&lt;p&gt;部署两个服务nginx 1.7和nginx 1.8：&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;19&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;20&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;21&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;22&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;23&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;24&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;25&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;26&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;27&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;28&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;nginx1-7.yaml&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;apiVersion: v1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;kind: Service&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  name: frontend&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  ports:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    - port: 80&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      targetPort: 80&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  selector:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    app: nginx1-7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;---&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;apiVersion: apps/v1beta1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;kind: Deployment&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  name: nginx1-7-deployment&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  replicas: 2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  template:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      labels:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        app: nginx1-7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      containers:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      - name: nginx&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        image: nginx:1.7.9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        ports:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        - containerPort: 80&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;19&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;20&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;21&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;22&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;23&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;24&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;25&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;26&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;27&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;28&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;nginx1-8.yaml&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;apiVersion: v1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;kind: Service&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  name: my-nginx&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  ports:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    - port: 80&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      targetPort: 80&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  selector:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    app: nginx1-8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;---&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;apiVersion: apps/v1beta1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;kind: Deployment&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  name: nginx1-8-deployment&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  replicas: 2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  template:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      labels:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        app: nginx1-8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      containers:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      - name: nginx&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        image: nginx:1.8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        ports:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        - containerPort: 80&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;[root@node1 nginx_ingress]# kubectl create -f nginx1-7.yaml&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;service &amp;quot;frontend&amp;quot; created&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;deployment &amp;quot;nginx1-7-deployment&amp;quot; created&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;[root@node1 nginx_ingress]# kubectl create -f nginx1-8.yaml&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;service &amp;quot;my-nginx&amp;quot; created&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;deployment &amp;quot;nginx1-8-deployment&amp;quot; created&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;创建Ingress&quot;&gt;&lt;a href=&quot;#创建Ingress&quot; class=&quot;headerlink&quot; title=&quot;创建Ingress&quot;&gt;&lt;/a&gt;创建Ingress&lt;/h3&gt;&lt;p&gt;假设这两个服务要暴露到集群外部。要创建一个ingress：&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;19&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;[root@node1 nginx_ingress]# vim test-ingress.yaml&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;apiVersion: extensions/v1beta1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;kind: Ingress&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  name: test&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  rules:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  - host: n17.my.com&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    http:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      paths:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      - backend:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          serviceName: nginx1-7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          servicePort: 80&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  - host: n18.my.com&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    http:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      paths:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      - backend:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          serviceName: nginx1-8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          servicePort: 80&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;[root@node1 nginx_ingress]# kubectl create -f test-ingress.yaml&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;ingress &amp;quot;test&amp;quot; created&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;[root@node1 nginx_ingress]# kubectl get ing&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;NAME              HOSTS                                  ADDRESS   PORTS     AGE&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;test              n17.my.com,n18.my.com                            80        52s&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;打开客户机的/etc/hosts，配置172.16.7.151和n17.my.com,n18.my.com的对应关系，然后在浏览器访问n17.my.com或n18.my.com就可以访问到对应的服务。&lt;br&gt;如果想修改访问规则，修改test-ingress.yaml，使用kubectl replace -f更新就可以了。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;服务发现与负载均衡&quot;&gt;&lt;a href=&quot;#服务发现与负载均衡&quot; class=&quot;headerlink&quot; title=&quot;服务发现与负载均衡&quot;&gt;&lt;/a&gt;服务发现与负载均衡&lt;/h2&gt;&lt;p&gt;在前面的安装部署kubernetes集群中已经简单用示例来演示了Pod和Service，Kubernetes通过Service资源在Kubernetes集群内针对容器实现了服务发现和负载均衡。而Service就是kubernetes服务发现与负载均衡中的一种。&lt;br&gt;
    
    </summary>
    
      <category term="容器编排" scheme="http://yoursite.com/categories/%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92/"/>
    
    
      <category term="Kubernetes" scheme="http://yoursite.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes监控:部署Heapster、InfluxDB和Grafana</title>
    <link href="http://yoursite.com/2017/09/21/Kubernetes%E7%9B%91%E6%8E%A7-%E9%83%A8%E7%BD%B2Heapster%E3%80%81InfluxDB%E5%92%8CGrafana/"/>
    <id>http://yoursite.com/2017/09/21/Kubernetes监控-部署Heapster、InfluxDB和Grafana/</id>
    <published>2017-09-21T05:40:01.000Z</published>
    <updated>2017-11-24T07:23:49.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;Kubernetes-监控方案&quot;&gt;&lt;a href=&quot;#Kubernetes-监控方案&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes 监控方案&quot;&gt;&lt;/a&gt;Kubernetes 监控方案&lt;/h2&gt;&lt;p&gt;可选的方案：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Heapster + InfluxDB + Grafana&lt;/li&gt;
&lt;li&gt;Prometheus + Grafana&lt;/li&gt;
&lt;li&gt;Cadvisor + InfluxDB + Grafana&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本篇文章介绍的是Heapster + InfluxDB + Grafana，kubernetes集群（1.6.0）搭建见前面的文章。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Heapster、InfluxDB和Grafana介绍&quot;&gt;&lt;a href=&quot;#Heapster、InfluxDB和Grafana介绍&quot; class=&quot;headerlink&quot; title=&quot;Heapster、InfluxDB和Grafana介绍&quot;&gt;&lt;/a&gt;Heapster、InfluxDB和Grafana介绍&lt;/h2&gt;&lt;p&gt;开源软件cAdvisor（Container cAdvisor）是用于监控容器运行状态的利器之一（cAdvisor项目的主页为&lt;a href=&quot;https://github.com/cAdvisor），它被用于多个与Docker相关的开源项目中。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/cAdvisor），它被用于多个与Docker相关的开源项目中。&lt;/a&gt;&lt;br&gt;在kubernetes系统中，cAdvisor已经被默认集成到了kubelet组件内，当kubelet服务启动时，它会自动启动cAdvisor服务，然后cAdvisor会实时采集所在节点的性能指标及节点上运行的容器的性能指标。kubelet的启动参数–cadvisor-port可自定义cAdvisor对外提供服务的端口号，默认是4194。&lt;br&gt;cAdvisor提供了web页面可供浏览器访问，例如本kubernetes集群中的一个Node的ip是172.16.7.151，那么浏览器输入&lt;a href=&quot;http://172.16.7.151:4194可以访问cAdvisor的监控页面。cAdvisor主页显示了主机的实时运行状态，包括CPU使用情况、内存使用情况、网络吞吐量及文件系统使用情况等信息。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.151:4194可以访问cAdvisor的监控页面。cAdvisor主页显示了主机的实时运行状态，包括CPU使用情况、内存使用情况、网络吞吐量及文件系统使用情况等信息。&lt;/a&gt;&lt;br&gt;但是cAdvisor只提供了单机的容器资源占用情况，而在大规模容器集群中，需要对所有的Node和全部容器进行性能监控。这就需要一套工具来实现集群性能数据的采集、存储和展示：Heapster、InfluxDB和Grafana。&lt;br&gt;Heapster提供了整个集群的资源监控，并支持持久化数据存储到InfluxDB、Google Cloud Monitoring或者其他的存储后端。Heapster从kubelet提供的API采集节点和容器的资源占用。另外，Heapster的 /metrics API提供了Prometheus格式的数据。&lt;br&gt;InfluxDB是一个开源分布式时序、事件和指标数据库；而Grafana则是InfluxDB的 dashboard，提供了强大的图表展示功能。它们常被组合使用展示图表化的监控数据。&lt;br&gt;Heapster、InfluxDB和Grafana均以Pod的形式启动和运行，其中Heapster需要与Kubernetes Master进行安全连接。&lt;/p&gt;
&lt;h2 id=&quot;安装配置Heapster、InfluxDB和Grafana&quot;&gt;&lt;a href=&quot;#安装配置Heapster、InfluxDB和Grafana&quot; class=&quot;headerlink&quot; title=&quot;安装配置Heapster、InfluxDB和Grafana&quot;&gt;&lt;/a&gt;安装配置Heapster、InfluxDB和Grafana&lt;/h2&gt;&lt;p&gt;到&lt;a href=&quot;https://github.com/kubernetes/heapster/releases&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;heapster release 页面&lt;/a&gt;下载heapster。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 opt]# wget https://github.com/kubernetes/heapster/archive/v1.3.0.zip
[root@node1 opt]# unzip v1.3.0.zip
[root@node1 opt]# cd heapster-1.3.0/deploy/kube-config/influxdb
[root@node1 influxdb]# ls *.yaml
grafana-deployment.yaml  heapster-deployment.yaml  influxdb-deployment.yaml
grafana-service.yaml     heapster-service.yaml     influxdb-service.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;创建heapster的rbac配置heapster-rbac.yaml。已经修改好的 yaml 文件见：&lt;a href=&quot;https://github.com/rootsongjc/follow-me-install-kubernetes-cluster/tree/master/manifests/heapster&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;heapster&lt;/a&gt;&lt;br&gt;下面比对的是我自己修改的和源文件。或者直接点直接去上面的地址下载这几个配置文件替换原有的配置文件。&lt;/p&gt;
&lt;h3 id=&quot;修改-grafana-deployment-yaml&quot;&gt;&lt;a href=&quot;#修改-grafana-deployment-yaml&quot; class=&quot;headerlink&quot; title=&quot;修改 grafana-deployment.yaml&quot;&gt;&lt;/a&gt;修改 grafana-deployment.yaml&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;diff grafana-deployment.yaml.orig grafana-deployment.yaml
16c16
&amp;lt;         image: gcr.io/google_containers/heapster-grafana-amd64:v4.0.2
---
&amp;gt;         image: sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-grafana-amd64:v4.0.2
40,41c40,41
&amp;lt;           # value: /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/
&amp;lt;           value: /
---
&amp;gt;           value: /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/
&amp;gt;           #value: /
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;根据上面的差异修改源文件grafana-deployment.yaml，并将image地址改为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;index.tenxcloud.com/jimmy/heapster-grafana-amd64:v4.0.2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果后续使用 kube-apiserver 或者 kubectl proxy 访问 grafana dashboard，则必须将 GF_SERVER_ROOT_URL 设置为/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/，否则后续访问grafana时访问时提示找不到&lt;a href=&quot;http://10.64.3.7:8086/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/api/dashboards/home&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://10.64.3.7:8086/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/api/dashboards/home&lt;/a&gt; 页面。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;修改-heapster-deployment-yaml&quot;&gt;&lt;a href=&quot;#修改-heapster-deployment-yaml&quot; class=&quot;headerlink&quot; title=&quot;修改 heapster-deployment.yaml&quot;&gt;&lt;/a&gt;修改 heapster-deployment.yaml&lt;/h3&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;19&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;20&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;21&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;22&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;23&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;24&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;25&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;[root@node1 influxdb]# pwd&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;/opt/heapster-1.3.0/deploy/kube-config/influxdb&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;[root@node1 influxdb]# vim heapster-deployment.yaml &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;apiVersion: extensions/v1beta1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;kind: Deployment&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  name: heapster&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  namespace: kube-system&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  replicas: 1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  template:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      labels:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        task: monitoring&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        k8s-app: heapster&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      serviceAccountName: heapster&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      containers:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      - name: heapster&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        image: index.tenxcloud.com/jimmy/heapster-amd64:v1.3.0-beta.1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        imagePullPolicy: IfNotPresent&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        command:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        - /heapster&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        - --source=kubernetes:https://kubernetes.default&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        - --sink=influxdb:http://monitoring-influxdb:8086&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;将image地址改为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;index.tenxcloud.com/jimmy/heapster-amd64:v1.3.0-beta.1 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】:Heapster需要设置的启动参数如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;source：配置采集源，为Master URL地址：–source=kubernetes:&lt;a href=&quot;https://kubernetes.default&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://kubernetes.default&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;sink：配置后端存储系统，使用InfluxDB系统：–sink=influxdb:&lt;a href=&quot;http://monitoring-influxdb:8086&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://monitoring-influxdb:8086&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其他参数可以通过进入heapster容器执行 # heapster –help 命令查看和设置。&lt;/p&gt;
&lt;p&gt;【注意】：URL中的主机名地址使用的是InfluxDB的Service名字，这需要DNS服务正常工作，如果没有配置DNS服务，则也可以使用Service的ClusterIP地址。&lt;br&gt;另外，InfluxDB服务的名称没有加上命名空间，是因为Heapster服务与InfluxDB服务属于相同的命名空间kube-system。也可以使用上命名空间的全服务名，例如：&lt;a href=&quot;http://monitoring-influxdb.kube-system:8086&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://monitoring-influxdb.kube-system:8086&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;修改-influxdb-deployment-yaml&quot;&gt;&lt;a href=&quot;#修改-influxdb-deployment-yaml&quot; class=&quot;headerlink&quot; title=&quot;修改 influxdb-deployment.yaml&quot;&gt;&lt;/a&gt;修改 influxdb-deployment.yaml&lt;/h3&gt;&lt;p&gt;influxdb 官方建议使用命令行或 HTTP API 接口来查询数据库，从 v1.1.0 版本开始默认关闭 admin UI，将在后续版本中移除 admin UI 插件。&lt;br&gt;开启镜像中 admin UI的办法如下：先导出镜像中的 influxdb 配置文件，开启 admin 插件后，再将配置文件内容写入 ConfigMap，最后挂载到镜像中，达到覆盖原始配置的目的。&lt;br&gt;【注意】：manifests 目录已经提供了 &lt;a href=&quot;https://github.com/opsnull/follow-me-install-kubernetes-cluster/blob/master/manifests/heapster/influxdb-cm.yaml&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;修改后的 ConfigMap 定义文件&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 导出镜像中的 influxdb 配置文件
[root@node1 influxdb]# docker run --rm --entrypoint &amp;apos;cat&amp;apos;  -ti lvanneo/heapster-influxdb-amd64:v1.1.1 /etc/config.toml &amp;gt;config.toml.orig
[root@node1 influxdb]# cp config.toml.orig config.toml 
# 修改配置：启用 admin 接口
[root@node1 influxdb]# vim config.toml
[admin]
  enabled = true
# 将修改后的配置写入到 ConfigMap 对象中(kubectl 可以通过 --namespace 或者 -n 选项指定namespace。如果不指定, 默认为default)
[root@node1 influxdb]# kubectl create configmap influxdb-config --from-file=config.toml -n kube-system
configmap &amp;quot;influxdb-config&amp;quot; created
# 将 ConfigMap 中的配置文件挂载到 Pod 中，达到覆盖原始配置的目的
diff influxdb-deployment.yaml.orig influxdb-deployment.yaml
16c16
&amp;lt;         image: grc.io/google_containers/heapster-influxdb-amd64:v1.1.1
---
&amp;gt;         image: sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-influxdb-amd64:v1.1.1
19a20,21
&amp;gt;         - mountPath: /etc/
&amp;gt;           name: influxdb-config
22a25,27
&amp;gt;       - name: influxdb-config
&amp;gt;         configMap:
&amp;gt;           name: influxdb-config
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;根据上面的差异修改源文件influxdb-deployment.yaml，并将image地址改为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;index.tenxcloud.com/jimmy/heapster-influxdb-amd64:v1.1.1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后删除这两个文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 influxdb]# rm -f config.toml config.toml.orig 
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;修改-influxdb-service-yaml&quot;&gt;&lt;a href=&quot;#修改-influxdb-service-yaml&quot; class=&quot;headerlink&quot; title=&quot;修改 influxdb-service.yaml&quot;&gt;&lt;/a&gt;修改 influxdb-service.yaml&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;diff influxdb-service.yaml.orig influxdb-service.yaml
12a13
&amp;gt;   type: NodePort
15a17,20
&amp;gt;     name: http
&amp;gt;   - port: 8083
&amp;gt;     targetPort: 8083
&amp;gt;     name: admin
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;定义端口类型为 NodePort，将InfluxDB暴露在宿主机Node的端口上，以便后续浏览器访问 influxdb 的 admin UI 界面。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;执行所有定义文件进行安装&quot;&gt;&lt;a href=&quot;#执行所有定义文件进行安装&quot; class=&quot;headerlink&quot; title=&quot;执行所有定义文件进行安装&quot;&gt;&lt;/a&gt;执行所有定义文件进行安装&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@node1 influxdb]# pwd
/opt/heapster-1.3.0/deploy/kube-config/influxdb
[root@node1 influxdb]# ls
grafana-deployment.yaml  heapster-deployment.yaml  heapster-service.yaml  influxdb-deployment.yaml
grafana-service.yaml     heapster-rbac.yaml        influxdb-cm.yaml       influxdb-service.yaml
[root@node1 influxdb]# kubectl create -f . 
deployment &amp;quot;monitoring-grafana&amp;quot; created
service &amp;quot;monitoring-grafana&amp;quot; created
deployment &amp;quot;heapster&amp;quot; created
serviceaccount &amp;quot;heapster&amp;quot; created
clusterrolebinding &amp;quot;heapster&amp;quot; created
service &amp;quot;heapster&amp;quot; created
deployment &amp;quot;monitoring-influxdb&amp;quot; created
service &amp;quot;monitoring-influxdb&amp;quot; created
Error from server (AlreadyExists): error when creating &amp;quot;influxdb-cm.yaml&amp;quot;: configmaps &amp;quot;influxdb-config&amp;quot; already exists
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;检查执行结果&quot;&gt;&lt;a href=&quot;#检查执行结果&quot; class=&quot;headerlink&quot; title=&quot;检查执行结果&quot;&gt;&lt;/a&gt;检查执行结果&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;1.检查 Deployment&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get deployments -n kube-system | grep -E &amp;apos;heapster|monitoring&amp;apos;
heapster               1         1         1            1           12m
monitoring-grafana     1         1         1            1           12m
monitoring-influxdb    1         1         1            1           12m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;2.检查 Pods&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get pods -n kube-system | grep -E &amp;apos;heapster|monitoring&amp;apos;
heapster-2291216627-6hv9s               1/1       Running   0          10m
monitoring-grafana-2490289118-n54fk     1/1       Running   0          10m
monitoring-influxdb-1450237832-029q8    1/1       Running   0          10m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;3.检查 kubernets dashboard 界面，看是显示各 Nodes、Pods 的 CPU、内存、负载等利用率曲线图&lt;/strong&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/4.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;访问-grafana&quot;&gt;&lt;a href=&quot;#访问-grafana&quot; class=&quot;headerlink&quot; title=&quot;访问 grafana&quot;&gt;&lt;/a&gt;访问 grafana&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;1.通过 kube-apiserver 访问&lt;/strong&gt;&lt;br&gt;获取 monitoring-grafana 服务 URL：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 influxdb]# kubectl cluster-info
Kubernetes master is running at https://172.16.7.151:6443
Heapster is running at https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/heapster
KubeDNS is running at https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/kube-dns
kubernetes-dashboard is running at https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard
monitoring-grafana is running at https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana
monitoring-influxdb is running at https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb

To further debug and diagnose cluster problems, use &amp;apos;kubectl cluster-info dump&amp;apos;.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;浏览器访问 URL： &lt;a href=&quot;http://172.16.7.151:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.151:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/5.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.通过 kubectl proxy 访问&lt;/strong&gt;&lt;br&gt;创建代理:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl proxy --address=&amp;apos;172.16.7.151&amp;apos; --port=8086 --accept-hosts=&amp;apos;^*$&amp;apos;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;浏览器访问 URL：&lt;a href=&quot;http://172.16.7.151:8086/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.151:8086/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/5.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3.Grafana页面查看和操作&lt;/strong&gt;&lt;br&gt;浏览器访问 URL： &lt;a href=&quot;http://172.16.7.151:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.151:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana&lt;/a&gt;&lt;br&gt;点击“Home”下拉列表，选择cluster，如下图。图中显示了Cluster集群的整体信息，以折线图的形式展示了集群范围内各Node的CPU使用率、内存使用情况等信息。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/10.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;点击“Home”下拉列表，选择Pods，如下图。图中展示了Pod的信息，以折线图的形式展示了集群范围内各Pod的CPU使用率、内存使用情况、网络流量、文件系统使用情况等信息。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/11.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;访问-influxdb-admin-UI&quot;&gt;&lt;a href=&quot;#访问-influxdb-admin-UI&quot; class=&quot;headerlink&quot; title=&quot;访问 influxdb admin UI&quot;&gt;&lt;/a&gt;访问 influxdb admin UI&lt;/h2&gt;&lt;p&gt;获取 influxdb http 8086 映射的 NodePort：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 influxdb]# kubectl get svc -n kube-system|grep influxdb
monitoring-influxdb    10.254.66.133    &amp;lt;nodes&amp;gt;       8086:32570/TCP,8083:31601/TCP   17m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;通过 kube-apiserver 的非安全端口访问 influxdb 的 admin UI 界面：&lt;a href=&quot;http://172.16.7.151:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb:8083/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.151:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb:8083/&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/6.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;在页面的 “Connection Settings” 的 Host 中输入 node IP， Port 中输入 8086 映射的 nodePort 如上面的 32570，点击 “Save” 即可（我的集群中的地址是172.16.7.151:32570）。&lt;br&gt;通过右上角齿轮按钮可以修改连接属性。单击右上角的Database下拉列表可以选择数据库，heapster创建的数据库名为k8s。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/7.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/8.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;heapster采集的metric&quot;&gt;&lt;a href=&quot;#heapster采集的metric&quot; class=&quot;headerlink&quot; title=&quot;heapster采集的metric&quot;&gt;&lt;/a&gt;heapster采集的metric&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;metric名称&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;cpu/limit&lt;/td&gt;
&lt;td&gt;CPU hard limit，单位为毫秒&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;cpu/usage&lt;/td&gt;
&lt;td&gt;全部Core的CPU累计使用时间&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;cpu/usage_rate&lt;/td&gt;
&lt;td&gt;全部Core的CPU累计使用率，单位为毫秒&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;filesystem/limit&lt;/td&gt;
&lt;td&gt;文件系统总空间限制，单位为字节&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;filesystem/usage&lt;/td&gt;
&lt;td&gt;文件系统已用的空间，单位为字节&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory/limit&lt;/td&gt;
&lt;td&gt;Memory hard limit，单位为字节&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory/major_page_faults&lt;/td&gt;
&lt;td&gt;major page faults数量&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory/major_page_faults_rate&lt;/td&gt;
&lt;td&gt;每秒的major page faults数量&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory/node_allocatable&lt;/td&gt;
&lt;td&gt;Node可分配的内存容量&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory/node_capacity&lt;/td&gt;
&lt;td&gt;Node的内存容量&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory/node_reservation&lt;/td&gt;
&lt;td&gt;Node保留的内存share&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory/node_utilization&lt;/td&gt;
&lt;td&gt;Node的内存使用值&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory/page_faults&lt;/td&gt;
&lt;td&gt;page faults数量&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory/page_faults_rate&lt;/td&gt;
&lt;td&gt;每秒的page faults数量&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory/request&lt;/td&gt;
&lt;td&gt;Memory request，单位为字节&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory/usage&lt;/td&gt;
&lt;td&gt;总内存使用量&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory/working_set&lt;/td&gt;
&lt;td&gt;总的Working set usage，Working set是指不会被kernel移除的内存&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;network/rx&lt;/td&gt;
&lt;td&gt;累计接收的网络流量字节数&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;network/rx_errors&lt;/td&gt;
&lt;td&gt;累计接收的网络流量错误数&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;network/rx_errors_rate&lt;/td&gt;
&lt;td&gt;每秒接收的网络流量错误数&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;network/rx_rate&lt;/td&gt;
&lt;td&gt;每秒接收的网络流量字节数&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;network/tx&lt;/td&gt;
&lt;td&gt;累计发送的网络流量字节数&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;network/tx_errors&lt;/td&gt;
&lt;td&gt;累计发送的网络流量错误数&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;network/tx_errors_rate&lt;/td&gt;
&lt;td&gt;每秒发送的网络流量错误数&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;network/tx_rate&lt;/td&gt;
&lt;td&gt;每秒发送的网络流量字节数&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;uptime&lt;/td&gt;
&lt;td&gt;容器启动总时长&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;每个metric可以看作一张数据库表，表中每条记录由一组label组成，可以看成字段。如下表所示：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Label名称&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;pod_id&lt;/td&gt;
&lt;td&gt;系统生成的Pod唯一名称&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;pod_name&lt;/td&gt;
&lt;td&gt;用户指定的Pod名称&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;pod_namespace&lt;/td&gt;
&lt;td&gt;Pod所属的namespace&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;container_base_image&lt;/td&gt;
&lt;td&gt;容器的镜像名称&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;container_name&lt;/td&gt;
&lt;td&gt;用户指定的容器名称&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;host_id&lt;/td&gt;
&lt;td&gt;用户指定的Node主机名&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;hostname&lt;/td&gt;
&lt;td&gt;容器运行所在主机名&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;labels&lt;/td&gt;
&lt;td&gt;逗号分隔的Label列表&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;namespace_id&lt;/td&gt;
&lt;td&gt;Pod所属的namespace的UID&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;resource_id&lt;/td&gt;
&lt;td&gt;资源ID&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;可以使用SQL SELECT语句对每个metric进行查询，例如查询CPU的使用时间：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;select * from &amp;quot;cpu/usage&amp;quot; limit 10
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;结果如下图所示：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/9.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Kubernetes-监控方案&quot;&gt;&lt;a href=&quot;#Kubernetes-监控方案&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes 监控方案&quot;&gt;&lt;/a&gt;Kubernetes 监控方案&lt;/h2&gt;&lt;p&gt;可选的方案：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Heapster + InfluxDB + Grafana&lt;/li&gt;
&lt;li&gt;Prometheus + Grafana&lt;/li&gt;
&lt;li&gt;Cadvisor + InfluxDB + Grafana&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本篇文章介绍的是Heapster + InfluxDB + Grafana，kubernetes集群（1.6.0）搭建见前面的文章。&lt;br&gt;
    
    </summary>
    
      <category term="容器编排" scheme="http://yoursite.com/categories/%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92/"/>
    
    
      <category term="Kubernetes" scheme="http://yoursite.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes1.6集群上(开启了TLS)安装Dashboard</title>
    <link href="http://yoursite.com/2017/09/19/Kubernetes1-6%E9%9B%86%E7%BE%A4%E4%B8%8A-%E5%BC%80%E5%90%AF%E4%BA%86TLS-%E5%AE%89%E8%A3%85Dashboard/"/>
    <id>http://yoursite.com/2017/09/19/Kubernetes1-6集群上-开启了TLS-安装Dashboard/</id>
    <published>2017-09-19T01:14:24.000Z</published>
    <updated>2017-11-22T07:38:50.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;配置和安装-dashboard&quot;&gt;&lt;a href=&quot;#配置和安装-dashboard&quot; class=&quot;headerlink&quot; title=&quot;配置和安装 dashboard&quot;&gt;&lt;/a&gt;配置和安装 dashboard&lt;/h2&gt;&lt;p&gt;这是接着上一篇《二进制方式部署Kubernetes 1.6.0集群(开启TLS)》写的。&lt;br&gt;Kubernetes Dashboard is a general purpose, web-based UI for Kubernetes clusters. It allows users to manage applications running in the cluster and troubleshoot them, as well as manage the cluster itself.&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;配置dashboard&quot;&gt;&lt;a href=&quot;#配置dashboard&quot; class=&quot;headerlink&quot; title=&quot;配置dashboard&quot;&gt;&lt;/a&gt;配置dashboard&lt;/h3&gt;&lt;p&gt;官方文件目录：&lt;a href=&quot;https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dashboard&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dashboard&lt;/a&gt;&lt;br&gt;我使用的文件:&lt;br&gt;从 &lt;a href=&quot;https://github.com/rootsongjc/follow-me-install-kubernetes-cluster/tree/master/manifests/dashboard&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/rootsongjc/follow-me-install-kubernetes-cluster/tree/master/manifests/dashboard&lt;/a&gt; 下载3个文件下来，并上传到/opt/kube-dashboard/目录下。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 opt]# mkdir kube-dashboard
[root@node1 opt]# cd kube-dashboard/
[root@node1 kube-dashboard]# ls
dashboard-controller.yaml  dashboard-rbac.yaml  dashboard-service.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;修改dashboard-controller.yaml文件，将里面的image改为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;index.tenxcloud.com/jimmy/kubernetes-dashboard-amd64:v1.6.0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;由于 kube-apiserver 启用了 RBAC 授权，而官方源码目录的 dashboard-controller.yaml 没有定义授权的 ServiceAccount，所以后续访问 kube-apiserver 的 API 时会被拒绝，web中提示：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Forbidden (403)

User &amp;quot;system:serviceaccount:kube-system:default&amp;quot; cannot list jobs.batch in the namespace &amp;quot;default&amp;quot;. (get jobs.batch)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;因此，增加了一个dashboard-rbac.yaml文件，定义一个名为 dashboard 的 ServiceAccount，然后将它和 Cluster Role view 绑定。&lt;/p&gt;
&lt;h3 id=&quot;执行所有定义的文件&quot;&gt;&lt;a href=&quot;#执行所有定义的文件&quot; class=&quot;headerlink&quot; title=&quot;执行所有定义的文件&quot;&gt;&lt;/a&gt;执行所有定义的文件&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;# pwd
/opt/kube-dashboard
# ls
dashboard-controller.yaml  dashboard-rbac.yaml  dashboard-service.yaml
# kubectl create -f .
deployment &amp;quot;kubernetes-dashboard&amp;quot; created
serviceaccount &amp;quot;dashboard&amp;quot; created
clusterrolebinding &amp;quot;dashboard&amp;quot; created
service &amp;quot;kubernetes-dashboard&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;检查执行结果&quot;&gt;&lt;a href=&quot;#检查执行结果&quot; class=&quot;headerlink&quot; title=&quot;检查执行结果&quot;&gt;&lt;/a&gt;检查执行结果&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;1.查看分配的 NodePort&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get services kubernetes-dashboard -n kube-system
NAME                   CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubernetes-dashboard   10.254.207.77   &amp;lt;nodes&amp;gt;       80:32281/TCP   41s
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;NodePort 32281映射到 dashboard pod 80端口。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;2.检查 controller&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get deployment kubernetes-dashboard  -n kube-system
NAME                   DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kubernetes-dashboard   1         1         1            1           13m
# kubectl get pods  -n kube-system | grep dashboard
kubernetes-dashboard-2888692679-tv54g   1/1       Running   0          13m
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;访问dashboard&quot;&gt;&lt;a href=&quot;#访问dashboard&quot; class=&quot;headerlink&quot; title=&quot;访问dashboard&quot;&gt;&lt;/a&gt;访问dashboard&lt;/h3&gt;&lt;p&gt;有以下三种方式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kubernetes-dashboard 服务暴露了 NodePort，可以使用 &lt;a href=&quot;http://NodeIP:nodePort&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://NodeIP:nodePort&lt;/a&gt; 地址访问 dashboard；&lt;/li&gt;
&lt;li&gt;通过 kube-apiserver 访问 dashboard（https 6443端口和http 8080端口方式）；&lt;/li&gt;
&lt;li&gt;通过 kubectl proxy 访问 dashboard&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;使用-http-NodeIP-nodePort-地址访问-dashboard&quot;&gt;&lt;a href=&quot;#使用-http-NodeIP-nodePort-地址访问-dashboard&quot; class=&quot;headerlink&quot; title=&quot;使用 http://NodeIP:nodePort 地址访问 dashboard&quot;&gt;&lt;/a&gt;使用 &lt;a href=&quot;http://NodeIP:nodePort&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://NodeIP:nodePort&lt;/a&gt; 地址访问 dashboard&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# kubectl get services kubernetes-dashboard -n kube-system
NAME                   CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubernetes-dashboard   10.254.207.77   &amp;lt;nodes&amp;gt;       80:32281/TCP   41s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后检查出这个pod是运行在集群中哪个服务器上的，我这里是检查是运行在node1节点上的，所以浏览器输入&lt;a href=&quot;http://172.16.7.151:32281/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.151:32281/&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/3.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h4 id=&quot;通过-kubectl-proxy-访问-dashboard&quot;&gt;&lt;a href=&quot;#通过-kubectl-proxy-访问-dashboard&quot; class=&quot;headerlink&quot; title=&quot;通过 kubectl proxy 访问 dashboard&quot;&gt;&lt;/a&gt;通过 kubectl proxy 访问 dashboard&lt;/h4&gt;&lt;p&gt;1.启动代理&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 kube-dashboard]# kubectl proxy --address=&amp;apos;172.16.7.151&amp;apos; --port=8086 --accept-hosts=&amp;apos;^*$&amp;apos;         
Starting to serve on 172.16.7.151:8086
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;需要指定 –accept-hosts 选项，否则浏览器访问 dashboard 页面时提示 “Unauthorized”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2.访问&lt;br&gt;浏览器访问 URL：&lt;a href=&quot;http://172.16.7.151:8086/ui&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.151:8086/ui&lt;/a&gt; 自动跳转到：&lt;a href=&quot;http://172.16.7.151:8086/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard/#/workload?namespace=default&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.151:8086/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard/#/workload?namespace=default&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/3.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h4 id=&quot;通过-kube-apiserver-访问dashboard&quot;&gt;&lt;a href=&quot;#通过-kube-apiserver-访问dashboard&quot; class=&quot;headerlink&quot; title=&quot;通过 kube-apiserver 访问dashboard&quot;&gt;&lt;/a&gt;通过 kube-apiserver 访问dashboard&lt;/h4&gt;&lt;p&gt;1.获取集群服务地址列表&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# kubectl cluster-info
Kubernetes master is running at https://172.16.7.151:6443
KubeDNS is running at https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/kube-dns
kubernetes-dashboard is running at https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard

To further debug and diagnose cluster problems, use &amp;apos;kubectl cluster-info dump&amp;apos;.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.导入证书&lt;br&gt;将生成的admin.pem证书转换格式。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# cd /etc/kubernetes/ssl/
[root@node1 ~]# openssl pkcs12 -export -in admin.pem  -out admin.p12 -inkey admin-key.pem
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;将生成的admin.p12证书导入的你的电脑，导出的时候记住你设置的密码，导入的时候还要用到。&lt;br&gt;如果你不想使用https的话，可以直接访问insecure port 8080端口:&lt;a href=&quot;http://172.16.7.151:8080/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.151:8080/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/3.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;由于缺少 Heapster 插件，当前 dashboard 不能展示 Pod、Nodes 的 CPU、内存等 metric 图形。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;配置和安装-dashboard&quot;&gt;&lt;a href=&quot;#配置和安装-dashboard&quot; class=&quot;headerlink&quot; title=&quot;配置和安装 dashboard&quot;&gt;&lt;/a&gt;配置和安装 dashboard&lt;/h2&gt;&lt;p&gt;这是接着上一篇《二进制方式部署Kubernetes 1.6.0集群(开启TLS)》写的。&lt;br&gt;Kubernetes Dashboard is a general purpose, web-based UI for Kubernetes clusters. It allows users to manage applications running in the cluster and troubleshoot them, as well as manage the cluster itself.&lt;br&gt;
    
    </summary>
    
      <category term="容器编排" scheme="http://yoursite.com/categories/%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92/"/>
    
    
      <category term="Kubernetes" scheme="http://yoursite.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>二进制方式部署Kubernetes 1.6.0集群(开启TLS)</title>
    <link href="http://yoursite.com/2017/09/15/%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2Kubernetes-1-6-0%E9%9B%86%E7%BE%A4-%E5%BC%80%E5%90%AFTLS/"/>
    <id>http://yoursite.com/2017/09/15/二进制方式部署Kubernetes-1-6-0集群-开启TLS/</id>
    <published>2017-09-15T06:06:09.000Z</published>
    <updated>2017-11-19T06:01:35.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;Kubernetes简介&quot;&gt;&lt;a href=&quot;#Kubernetes简介&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes简介&quot;&gt;&lt;/a&gt;Kubernetes简介&lt;/h2&gt;&lt;p&gt;Kubernetes是谷歌开源的容器集群管理系统，是Google多年大规模容器管理技术Borg的 开源版本，主要功能包括:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于容器的应用部署、维护和滚动升级&lt;/li&gt;
&lt;li&gt;负载均衡和服务发现&lt;/li&gt;
&lt;li&gt;跨机器和跨地区的集群调度&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;自动伸缩&lt;/li&gt;
&lt;li&gt;无状态服务和有状态服务 &lt;/li&gt;
&lt;li&gt;广泛的Volume支持 &lt;/li&gt;
&lt;li&gt;插件机制保证扩展性&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;之前尝试使用kubeadm自动化部署集群，使用yum去安装kubeadm等工具，但是不翻墙的情况下，这种方式在国内几乎是不可能安装成功的。于是改为采用二进制文件部署Kubernetes集群，同时开启了集群的TLS安全认证。本篇实践是参照opsnull的文章&lt;a href=&quot;https://mp.weixin.qq.com/s/bvCZUl6LQhlqDVv_TNeDFg&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;创建 kubernetes 各组件 TLS 加密通信的证书和秘钥&lt;/a&gt;，结合公司的实际情况进行部署的。&lt;/p&gt;
&lt;h2 id=&quot;环境信息&quot;&gt;&lt;a href=&quot;#环境信息&quot; class=&quot;headerlink&quot; title=&quot;环境信息&quot;&gt;&lt;/a&gt;环境信息&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;操作系统版本&lt;/th&gt;
&lt;th&gt;IP地址&lt;/th&gt;
&lt;th&gt;角色&lt;/th&gt;
&lt;th&gt;安装软件&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;node1&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.7.151&lt;/td&gt;
&lt;td&gt;Kubernetes Master、Node&lt;/td&gt;
&lt;td&gt;etcd 3.2.7、kube-apiserver、kube-scheduler、kube-controller-manager、kubelet、kube-proxy、etcd 3.2.7、flannel 0.7.1、docker 1.12.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;node2&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.7.152&lt;/td&gt;
&lt;td&gt;Kubernetes Node&lt;/td&gt;
&lt;td&gt;kubelet、kube-proxy、flannel 0.7.1、etcd 3.2.7、docker 1.12.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;node3&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.7.153&lt;/td&gt;
&lt;td&gt;Kubernetes Node&lt;/td&gt;
&lt;td&gt;kubelet、kube-proxy、flannel 0.7.1、etcd 3.2.7、docker 1.12.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;spark32&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.206.32&lt;/td&gt;
&lt;td&gt;Harbor&lt;/td&gt;
&lt;td&gt;docker-ce 17.06.1、docker-compose 1.15.0、harbor-online-installer-v1.1.2.tar&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;spark32主机是harbor私有镜像仓库，关于harbor的安装部署见之前的博客《企业级Docker Registry——Harbor搭建和使用》。&lt;/p&gt;
&lt;h2 id=&quot;创建TLS加密通信的证书和密钥&quot;&gt;&lt;a href=&quot;#创建TLS加密通信的证书和密钥&quot; class=&quot;headerlink&quot; title=&quot;创建TLS加密通信的证书和密钥&quot;&gt;&lt;/a&gt;创建TLS加密通信的证书和密钥&lt;/h2&gt;&lt;p&gt;kubernetes各组件需要使用TLS证书对通信进行加密，这里我使用CloudFlare的PKI工具集&lt;a href=&quot;https://github.com/cloudflare/cfssl&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;cfssl&lt;/a&gt;来生成CA和其它证书。&lt;br&gt;生成的CA证书和密钥文件如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ca-key.pem&lt;/li&gt;
&lt;li&gt;ca.pem&lt;/li&gt;
&lt;li&gt;kubernetes-key.pem&lt;/li&gt;
&lt;li&gt;kubernetes.pem&lt;/li&gt;
&lt;li&gt;kube-proxy.pem&lt;/li&gt;
&lt;li&gt;kube-proxy-key.pem&lt;/li&gt;
&lt;li&gt;admin.pem&lt;/li&gt;
&lt;li&gt;admin-key.pem&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;各组件使用证书的情况如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;etcd：使用ca.pem、kubernetes-key.pem、kubernetes.pem；&lt;/li&gt;
&lt;li&gt;kube-apiserver：使用ca.pem、kubernetes-key.pem、kubernetes.pem；&lt;/li&gt;
&lt;li&gt;kubelet：使用ca.pem；&lt;/li&gt;
&lt;li&gt;kube-proxy：使用ca.pem、kube-proxy-key.pem、kube-proxy.pem；&lt;/li&gt;
&lt;li&gt;kubectl：使用ca.pem、admin-key.pem、admin.pem&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;kube-controller、kube-scheduler当前需要和kube-apiserver部署在同一台机器上且使用非安全端口通信，故不需要证书。&lt;/p&gt;
&lt;h3 id=&quot;安装CFSSL&quot;&gt;&lt;a href=&quot;#安装CFSSL&quot; class=&quot;headerlink&quot; title=&quot;安装CFSSL&quot;&gt;&lt;/a&gt;安装CFSSL&lt;/h3&gt;&lt;p&gt;有两种方式安装，一是二进制源码包安装，二是使用go命令安装。&lt;br&gt;1.方式一：二进制源码包安装&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
# chmod +x cfssl_linux-amd64
# mv cfssl_linux-amd64 /root/local/bin/cfssl

# wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
# chmod +x cfssljson_linux-amd64
# mv cfssljson_linux-amd64 /root/local/bin/cfssljson

# wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64
# chmod +x cfssl-certinfo_linux-amd64
# mv cfssl-certinfo_linux-amd64 /root/local/bin/cfssl-certinfo

# export PATH=/root/local/bin:$PATH
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.方式二：使用go命令安装&lt;br&gt;安装go(需要go 1.6+)：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 下载地址：https://golang.org/dl/
[root@node1 ~]# cd /usr/local/
[root@node1 local]# wget https://storage.googleapis.com/golang/go1.9.linux-amd64.tar.gz
[root@node1 local]# tar zxf go1.9.linux-amd64.tar.gz
[root@node1 local]# vim /etc/profile
# Go
export GO_HOME=/usr/local/go
export PATH=$GO_HOME/bin:$PATH 
[root@node1 local]# source /etc/profile
# 查看版本信息
[root@node1 local]# go version
go version go1.9 linux/amd64
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;安装cfssl:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 local]# go get -u github.com/cloudflare/cfssl/cmd/...
[root@node1 local]# ls /root/go/bin/
cfssl  cfssl-bundle  cfssl-certinfo  cfssljson  cfssl-newkey  cfssl-scan  mkbundle  multirootca
[root@node1 local]# mv /root/go/bin/* /usr/local/bin/
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;创建CA&quot;&gt;&lt;a href=&quot;#创建CA&quot; class=&quot;headerlink&quot; title=&quot;创建CA&quot;&gt;&lt;/a&gt;创建CA&lt;/h3&gt;&lt;p&gt;1.创建 CA 配置文件&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 local]# mkdir /opt/ssl
[root@node1 local]# cd /opt/ssl/
[root@node1 ssl]# cfssl print-defaults config &amp;gt; config.json
[root@node1 ssl]# cfssl print-defaults csr &amp;gt; csr.json
# 创建CA配置文件
[root@node1 ssl]# vim ca-config.json
{
  &amp;quot;signing&amp;quot;: {
    &amp;quot;default&amp;quot;: {
      &amp;quot;expiry&amp;quot;: &amp;quot;8760h&amp;quot;
    },
    &amp;quot;profiles&amp;quot;: {
      &amp;quot;kubernetes&amp;quot;: {
        &amp;quot;usages&amp;quot;: [
            &amp;quot;signing&amp;quot;,
            &amp;quot;key encipherment&amp;quot;,
            &amp;quot;server auth&amp;quot;,
            &amp;quot;client auth&amp;quot;
        ],
        &amp;quot;expiry&amp;quot;: &amp;quot;8760h&amp;quot;
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;部分字段说明：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ca-config.json：&lt;/strong&gt;可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个 profile；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;signing：&lt;/strong&gt;表示该证书可用于签名其它证书；生成的 ca.pem 证书中 CA=TRUE；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;server auth：&lt;/strong&gt;表示client可以用该 CA 对server提供的证书进行验证；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;client auth：&lt;/strong&gt;表示server可以用该 CA 对client提供的证书进行验证。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2.创建 CA 证书签名请求&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# vim ca-csr.json
{
  &amp;quot;CN&amp;quot;: &amp;quot;kubernetes&amp;quot;,
  &amp;quot;key&amp;quot;: {
    &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;,
    &amp;quot;size&amp;quot;: 2048
  },
  &amp;quot;names&amp;quot;: [
    {
      &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;,
      &amp;quot;ST&amp;quot;: &amp;quot;BeiJing&amp;quot;,
      &amp;quot;L&amp;quot;: &amp;quot;BeiJing&amp;quot;,
      &amp;quot;O&amp;quot;: &amp;quot;k8s&amp;quot;,
      &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;部分字段说明：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;“CN”：&lt;/strong&gt;Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;“O”：&lt;/strong&gt;Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;3.生成 CA 证书和私钥&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca
2017/09/10 04:22:13 [INFO] generating a new CA key and certificate from CSR
2017/09/10 04:22:13 [INFO] generate received request
2017/09/10 04:22:13 [INFO] received CSR
2017/09/10 04:22:13 [INFO] generating key: rsa-2048
2017/09/10 04:22:13 [INFO] encoded CSR
2017/09/10 04:22:13 [INFO] signed certificate with serial number 348968532213237181927470194452366329323573808966
[root@node1 ssl]# ls ca*
ca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;创建-Kubernetes-证书&quot;&gt;&lt;a href=&quot;#创建-Kubernetes-证书&quot; class=&quot;headerlink&quot; title=&quot;创建 Kubernetes 证书&quot;&gt;&lt;/a&gt;创建 Kubernetes 证书&lt;/h3&gt;&lt;p&gt;1.创建 kubernetes 证书签名请求&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# vim kubernetes-csr.json
{
    &amp;quot;CN&amp;quot;: &amp;quot;kubernetes&amp;quot;,
    &amp;quot;hosts&amp;quot;: [
      &amp;quot;127.0.0.1&amp;quot;,
      &amp;quot;172.16.7.151&amp;quot;,
      &amp;quot;172.16.7.152&amp;quot;,
      &amp;quot;172.16.7.153&amp;quot;,
      &amp;quot;172.16.206.32&amp;quot;,
      &amp;quot;10.254.0.1&amp;quot;,
      &amp;quot;kubernetes&amp;quot;,
      &amp;quot;kubernetes.default&amp;quot;,
      &amp;quot;kubernetes.default.svc&amp;quot;,
      &amp;quot;kubernetes.default.svc.cluster&amp;quot;,
      &amp;quot;kubernetes.default.svc.cluster.local&amp;quot;
    ],
    &amp;quot;key&amp;quot;: {
        &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;,
        &amp;quot;size&amp;quot;: 2048
    },
    &amp;quot;names&amp;quot;: [
        {
            &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;,
            &amp;quot;ST&amp;quot;: &amp;quot;BeiJing&amp;quot;,
            &amp;quot;L&amp;quot;: &amp;quot;BeiJing&amp;quot;,
            &amp;quot;O&amp;quot;: &amp;quot;k8s&amp;quot;,
            &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot;
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;部分字段说明：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果 hosts 字段不为空则需要指定授权使用该证书的 IP 或域名列表，由于该证书后续被 etcd 集群和 kubernetes master 集群使用，所以上面分别指定了 etcd 集群、kubernetes master 集群的主机 IP；&lt;/li&gt;
&lt;li&gt;还需要添加kube-apiserver注册的名为 kubernetes 服务的 IP（一般是 kue-apiserver 指定的 service-cluster-ip-range 网段的第一个IP，如 10.254.0.1)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2.生成 kubernetes 证书和私钥&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes
2017/09/10 07:44:27 [INFO] generate received request
2017/09/10 07:44:27 [INFO] received CSR
2017/09/10 07:44:27 [INFO] generating key: rsa-2048
2017/09/10 07:44:27 [INFO] encoded CSR
2017/09/10 07:44:27 [INFO] signed certificate with serial number 695308968867503306176219705194671734841389082714
[root@node1 ssl]# ls kubernetes*
kubernetes.csr  kubernetes-csr.json  kubernetes-key.pem  kubernetes.pem
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;或者直接在命令行上指定相关参数：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# echo &amp;apos;{&amp;quot;CN&amp;quot;:&amp;quot;kubernetes&amp;quot;,&amp;quot;hosts&amp;quot;:[&amp;quot;&amp;quot;],&amp;quot;key&amp;quot;:{&amp;quot;algo&amp;quot;:&amp;quot;rsa&amp;quot;,&amp;quot;size&amp;quot;:2048}}&amp;apos; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes -hostname=&amp;quot;127.0.0.1,172.16.7.151,172.16.7.152,172.16.7.153,172.16.206.32,10.254.0.1,kubernetes,kubernetes.default&amp;quot; - | cfssljson -bare kubernetes
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;创建-Admin-证书&quot;&gt;&lt;a href=&quot;#创建-Admin-证书&quot; class=&quot;headerlink&quot; title=&quot;创建 Admin 证书&quot;&gt;&lt;/a&gt;创建 Admin 证书&lt;/h3&gt;&lt;p&gt;1.创建 admin 证书签名请求&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# vim admin-csr.json
{
  &amp;quot;CN&amp;quot;: &amp;quot;admin&amp;quot;,
  &amp;quot;hosts&amp;quot;: [],
  &amp;quot;key&amp;quot;: {
    &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;,
    &amp;quot;size&amp;quot;: 2048
  },
  &amp;quot;names&amp;quot;: [
    {
      &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;,
      &amp;quot;ST&amp;quot;: &amp;quot;BeiJing&amp;quot;,
      &amp;quot;L&amp;quot;: &amp;quot;BeiJing&amp;quot;,
      &amp;quot;O&amp;quot;: &amp;quot;system:masters&amp;quot;,
      &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;说明：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;后续 kube-apiserver 使用 RBAC 对客户端(如 kubelet、kube-proxy、Pod)请求进行授权；&lt;/li&gt;
&lt;li&gt;kube-apiserver 预定义了一些 RBAC 使用的 RoleBindings，如 cluster-admin 将 Groupsystem:masters 与 Role cluster-admin 绑定，该 Role 授予了调用kube-apiserver 的所有 API的权限；&lt;/li&gt;
&lt;li&gt;OU 指定该证书的 Group 为 system:masters，kubelet 使用该证书访问 kube-apiserver时 ，由于证书被 CA 签名，所以认证通过，同时由于证书用户组为经过预授权的system:masters，所以被授予访问所有 API 的权限。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2.生成 admin 证书和私钥&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin
2017/09/10 20:01:05 [INFO] generate received request
2017/09/10 20:01:05 [INFO] received CSR
2017/09/10 20:01:05 [INFO] generating key: rsa-2048
2017/09/10 20:01:05 [INFO] encoded CSR
2017/09/10 20:01:05 [INFO] signed certificate with serial number 580169825175224945071583937498159721917720511011
2017/09/10 20:01:05 [WARNING] This certificate lacks a &amp;quot;hosts&amp;quot; field. This makes it unsuitable for websites. For more information see the Baseline Requirements for the Issuance and Management of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 (&amp;quot;Information Requirements&amp;quot;).
[root@node1 ssl]# ls admin*
admin.csr  admin-csr.json  admin-key.pem  admin.pem
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;创建-Kube-Proxy-证书&quot;&gt;&lt;a href=&quot;#创建-Kube-Proxy-证书&quot; class=&quot;headerlink&quot; title=&quot;创建 Kube-Proxy 证书&quot;&gt;&lt;/a&gt;创建 Kube-Proxy 证书&lt;/h3&gt;&lt;p&gt;1.创建 kube-proxy 证书签名请求&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# vim kube-proxy-csr.json
{
  &amp;quot;CN&amp;quot;: &amp;quot;system:kube-proxy&amp;quot;,
  &amp;quot;hosts&amp;quot;: [],
  &amp;quot;key&amp;quot;: {
    &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;,
    &amp;quot;size&amp;quot;: 2048
  },
  &amp;quot;names&amp;quot;: [
    {
      &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;,
      &amp;quot;ST&amp;quot;: &amp;quot;BeiJing&amp;quot;,
      &amp;quot;L&amp;quot;: &amp;quot;BeiJing&amp;quot;,
      &amp;quot;O&amp;quot;: &amp;quot;k8s&amp;quot;,
      &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;说明：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CN 指定该证书的 User 为 system:kube-proxy；&lt;/li&gt;
&lt;li&gt;kube-apiserver 预定义的 RoleBinding cluster-admin 将User system:kube-proxy 与 Rolesystem:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2.生成 kube-proxy 客户端证书和私钥&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes  kube-proxy-csr.json | cfssljson -bare kube-proxy
2017/09/10 20:07:55 [INFO] generate received request
2017/09/10 20:07:55 [INFO] received CSR
2017/09/10 20:07:55 [INFO] generating key: rsa-2048
2017/09/10 20:07:55 [INFO] encoded CSR
2017/09/10 20:07:55 [INFO] signed certificate with serial number 655306618453852718922516297333812428130766975244
2017/09/10 20:07:55 [WARNING] This certificate lacks a &amp;quot;hosts&amp;quot; field. This makes it unsuitable for websites. For more information see the Baseline Requirements for the Issuance and Management of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 (&amp;quot;Information Requirements&amp;quot;).
[root@node1 ssl]# ls kube-proxy*
kube-proxy.csr  kube-proxy-csr.json  kube-proxy-key.pem  kube-proxy.pem
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;校验证书&quot;&gt;&lt;a href=&quot;#校验证书&quot; class=&quot;headerlink&quot; title=&quot;校验证书&quot;&gt;&lt;/a&gt;校验证书&lt;/h3&gt;&lt;p&gt;以校验Kubernetes证书为例。&lt;/p&gt;
&lt;h4 id=&quot;使用openssl命令校验证书&quot;&gt;&lt;a href=&quot;#使用openssl命令校验证书&quot; class=&quot;headerlink&quot; title=&quot;使用openssl命令校验证书&quot;&gt;&lt;/a&gt;使用openssl命令校验证书&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# openssl x509 -noout -text -in kubernetes.pem 
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number:
            79:ca:bb:84:73:15:b1:db:aa:24:d7:a3:60:65:b0:55:27:a7:e8:5a
    Signature Algorithm: sha256WithRSAEncryption
        Issuer: C=CN, ST=BeiJing, L=BeiJing, O=k8s, OU=System, CN=kubernetes
        Validity
            Not Before: Sep 10 11:39:00 2017 GMT
            Not After : Sep 10 11:39:00 2018 GMT
        Subject: C=CN, ST=BeiJing, L=BeiJing, O=k8s, OU=System, CN=kubernetes
...
        X509v3 extensions:
            X509v3 Key Usage: critical
                Digital Signature, Key Encipherment
            X509v3 Extended Key Usage: 
                TLS Web Server Authentication, TLS Web Client Authentication
            X509v3 Basic Constraints: critical
                CA:FALSE
            X509v3 Subject Key Identifier: 
                79:48:C1:1B:81:DD:9C:75:04:EC:B6:35:26:5E:82:AA:2E:45:F6:C5
            X509v3 Subject Alternative Name: 
                DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster, DNS:kubernetes.default.svc.cluster.local, IP Address:127.0.0.1, IP Address:172.16.7.151, IP Address:172.16.7.152, IP Address:172.16.7.153, IP Address:172.16.206.32, IP Address:10.254.0.1
...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;确认 Issuer 字段的内容和 ca-csr.json 一致；&lt;/li&gt;
&lt;li&gt;确认 Subject 字段的内容和 kubernetes-csr.json 一致；&lt;/li&gt;
&lt;li&gt;确认 X509v3 Subject Alternative Name 字段的内容和 kubernetes-csr.json 一致；&lt;/li&gt;
&lt;li&gt;确认 X509v3 Key Usage、Extended Key Usage 字段的内容和 ca-config.json 中 kubernetesprofile 一致。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;使用-Cfssl-Certinfo-命令校验&quot;&gt;&lt;a href=&quot;#使用-Cfssl-Certinfo-命令校验&quot; class=&quot;headerlink&quot; title=&quot;使用 Cfssl-Certinfo 命令校验&quot;&gt;&lt;/a&gt;使用 Cfssl-Certinfo 命令校验&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# cfssl-certinfo -cert kubernetes.pem
{
  &amp;quot;subject&amp;quot;: {
    &amp;quot;common_name&amp;quot;: &amp;quot;kubernetes&amp;quot;,
    &amp;quot;country&amp;quot;: &amp;quot;CN&amp;quot;,
    &amp;quot;organization&amp;quot;: &amp;quot;k8s&amp;quot;,
    &amp;quot;organizational_unit&amp;quot;: &amp;quot;System&amp;quot;,
    &amp;quot;locality&amp;quot;: &amp;quot;BeiJing&amp;quot;,
    &amp;quot;province&amp;quot;: &amp;quot;BeiJing&amp;quot;,
    &amp;quot;names&amp;quot;: [
      &amp;quot;CN&amp;quot;,
      &amp;quot;BeiJing&amp;quot;,
      &amp;quot;BeiJing&amp;quot;,
      &amp;quot;k8s&amp;quot;,
      &amp;quot;System&amp;quot;,
      &amp;quot;kubernetes&amp;quot;
    ]
  },
  &amp;quot;issuer&amp;quot;: {
    &amp;quot;common_name&amp;quot;: &amp;quot;kubernetes&amp;quot;,
    &amp;quot;country&amp;quot;: &amp;quot;CN&amp;quot;,
    &amp;quot;organization&amp;quot;: &amp;quot;k8s&amp;quot;,
    &amp;quot;organizational_unit&amp;quot;: &amp;quot;System&amp;quot;,
    &amp;quot;locality&amp;quot;: &amp;quot;BeiJing&amp;quot;,
    &amp;quot;province&amp;quot;: &amp;quot;BeiJing&amp;quot;,
    &amp;quot;names&amp;quot;: [
      &amp;quot;CN&amp;quot;,
      &amp;quot;BeiJing&amp;quot;,
      &amp;quot;BeiJing&amp;quot;,
      &amp;quot;k8s&amp;quot;,
      &amp;quot;System&amp;quot;,
      &amp;quot;kubernetes&amp;quot;
    ]
  },
  &amp;quot;serial_number&amp;quot;: &amp;quot;695308968867503306176219705194671734841389082714&amp;quot;,
  &amp;quot;sans&amp;quot;: [
    &amp;quot;kubernetes&amp;quot;,
    &amp;quot;kubernetes.default&amp;quot;,
    &amp;quot;kubernetes.default.svc&amp;quot;,
    &amp;quot;kubernetes.default.svc.cluster&amp;quot;,
    &amp;quot;kubernetes.default.svc.cluster.local&amp;quot;,
    &amp;quot;127.0.0.1&amp;quot;,
    &amp;quot;172.16.7.151&amp;quot;,
    &amp;quot;172.16.7.152&amp;quot;,
    &amp;quot;172.16.7.153&amp;quot;,
    &amp;quot;172.16.206.32&amp;quot;,
    &amp;quot;10.254.0.1&amp;quot;
  ],
  &amp;quot;not_before&amp;quot;: &amp;quot;2017-09-10T11:39:00Z&amp;quot;,
  &amp;quot;not_after&amp;quot;: &amp;quot;2018-09-10T11:39:00Z&amp;quot;,
  &amp;quot;sigalg&amp;quot;: &amp;quot;SHA256WithRSA&amp;quot;,
  &amp;quot;authority_key_id&amp;quot;: &amp;quot;&amp;quot;,
  &amp;quot;subject_key_id&amp;quot;: &amp;quot;79:48:C1:1B:81:DD:9C:75:4:EC:B6:35:26:5E:82:AA:2E:45:F6:C5&amp;quot;,
...
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;分发证书&quot;&gt;&lt;a href=&quot;#分发证书&quot; class=&quot;headerlink&quot; title=&quot;分发证书&quot;&gt;&lt;/a&gt;分发证书&lt;/h3&gt;&lt;p&gt;将生成的证书和秘钥文件（后缀名为.pem）拷贝到所有机器的 /etc/kubernetes/ssl 目录下备用:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# mkdir -p /etc/kubernetes/ssl
[root@node1 ssl]# cp *.pem /etc/kubernetes/ssl
[root@node1 ssl]# scp -p *.pem root@172.16.7.152:/etc/kubernetes/ssl/
[root@node1 ssl]# scp -p *.pem root@172.16.7.153:/etc/kubernetes/ssl/
[root@node1 ssl]# scp -p *.pem root@172.16.206.32:/etc/kubernetes/ssl/
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;下载和配置-kubectl-kubecontrol-命令行工具&quot;&gt;&lt;a href=&quot;#下载和配置-kubectl-kubecontrol-命令行工具&quot; class=&quot;headerlink&quot; title=&quot;下载和配置 kubectl(kubecontrol) 命令行工具&quot;&gt;&lt;/a&gt;下载和配置 kubectl(kubecontrol) 命令行工具&lt;/h2&gt;&lt;h3 id=&quot;下载kubectl&quot;&gt;&lt;a href=&quot;#下载kubectl&quot; class=&quot;headerlink&quot; title=&quot;下载kubectl&quot;&gt;&lt;/a&gt;下载kubectl&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@node1 local]# wget https://dl.k8s.io/v1.6.0/kubernetes-client-linux-amd64.tar.gz
[root@node1 local]# tar zxf kubernetes-client-linux-amd64.tar.gz
[root@node1 local]# cp kubernetes/client/bin/kube* /usr/bin/
[root@node1 local]# chmod +x /usr/bin/kube*
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;创建-kubectl-kubeconfig-文件&quot;&gt;&lt;a href=&quot;#创建-kubectl-kubeconfig-文件&quot; class=&quot;headerlink&quot; title=&quot;创建 kubectl kubeconfig 文件&quot;&gt;&lt;/a&gt;创建 kubectl kubeconfig 文件&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@node1 local]# cd /etc/kubernetes/
[root@node1 kubernetes]# export KUBE_APISERVER=&amp;quot;https://172.16.7.151:6443&amp;quot;
# 设置集群参数
[root@node1 kubernetes]# kubectl config set-cluster kubernetes \
&amp;gt; --certificate-authority=/etc/kubernetes/ssl/ca.pem \
&amp;gt; --embed-certs=true \
&amp;gt; --server=${KUBE_APISERVER}
Cluster &amp;quot;kubernetes&amp;quot; set.
# 设置客户端认证参数
[root@node1 kubernetes]# kubectl config set-credentials admin \
&amp;gt; --client-certificate=/etc/kubernetes/ssl/admin.pem \
&amp;gt; --embed-certs=true \
&amp;gt; --client-key=/etc/kubernetes/ssl/admin-key.pem
User &amp;quot;admin&amp;quot; set.
# 设置上下文参数
[root@node1 kubernetes]# kubectl config set-context kubernetes \
&amp;gt; --cluster=kubernetes \
&amp;gt; --user=admin
Context &amp;quot;kubernetes&amp;quot; set
# 设置默认上下文
[root@node1 kubernetes]# kubectl config use-context kubernetes
Switched to context &amp;quot;kubernetes&amp;quot;.
[root@node1 kubernetes]# ls ~/.kube/config 
/root/.kube/config
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;admin.pem 证书 OU 字段值为 system:masters，kube-apiserver 预定义的 RoleBinding cluster-admin 将 Groupsystem:masters 与 Role cluster admin 绑定，该 Role 授予了调用kube-apiserver 相关 API 的权限；&lt;/li&gt;
&lt;li&gt;生成的 kubeconfig 被保存到 ~/.kube/config 文件。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;创建-kubeconfig-文件&quot;&gt;&lt;a href=&quot;#创建-kubeconfig-文件&quot; class=&quot;headerlink&quot; title=&quot;创建 kubeconfig 文件&quot;&gt;&lt;/a&gt;创建 kubeconfig 文件&lt;/h2&gt;&lt;p&gt;kubelet、kube-proxy 等 Node 机器上的进程与 Master 机器的 kube-apiserver 进程通信时需要认证和授权.&lt;br&gt;kubernetes 1.4 开始支持由 kube-apiserver 为客户端生成 TLS 证书的 TLS Bootstrapping 功能，这样就不需要为每个客户端生成证书了；该功能当前仅支持为 kubelet 生成证书。&lt;/p&gt;
&lt;h3 id=&quot;创建-TLS-Bootstrapping-Token&quot;&gt;&lt;a href=&quot;#创建-TLS-Bootstrapping-Token&quot; class=&quot;headerlink&quot; title=&quot;创建 TLS Bootstrapping Token&quot;&gt;&lt;/a&gt;创建 TLS Bootstrapping Token&lt;/h3&gt;&lt;h4 id=&quot;Token-auth-file&quot;&gt;&lt;a href=&quot;#Token-auth-file&quot; class=&quot;headerlink&quot; title=&quot;Token auth file&quot;&gt;&lt;/a&gt;Token auth file&lt;/h4&gt;&lt;p&gt;Token可以是任意的包涵128 bit的字符串，可以使用安全的随机数发生器生成。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d &amp;apos; &amp;apos;)
[root@node1 ssl]# cat &amp;gt; token.csv &amp;lt;&amp;lt;EOF
&amp;gt; ${BOOTSTRAP_TOKEN},kubelet-bootstrap,10001,&amp;quot;system:kubelet-bootstrap&amp;quot;
&amp;gt; EOF
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;将token.csv发到所有机器（Master 和 Node）的 /etc/kubernetes/ 目录。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# cp token.csv /etc/kubernetes/
[root@node1 ssl]# scp -p token.csv root@172.16.7.152:/etc/kubernetes/
[root@node1 ssl]# scp -p token.csv root@172.16.7.153:/etc/kubernetes/
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;创建-kubelet-bootstrapping-kubeconfig-文件&quot;&gt;&lt;a href=&quot;#创建-kubelet-bootstrapping-kubeconfig-文件&quot; class=&quot;headerlink&quot; title=&quot;创建 kubelet bootstrapping kubeconfig 文件&quot;&gt;&lt;/a&gt;创建 kubelet bootstrapping kubeconfig 文件&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# cd /etc/kubernetes
[root@node1 kubernetes]# export KUBE_APISERVER=&amp;quot;https://172.16.7.151:6443&amp;quot;
# 设置集群参数
[root@node1 kubernetes]# kubectl config set-cluster kubernetes \
&amp;gt; --certificate-authority=/etc/kubernetes/ssl/ca.pem \
&amp;gt; --embed-certs=true \
&amp;gt; --server=${KUBE_APISERVER} \
&amp;gt; --kubeconfig=bootstrap.kubeconfig
Cluster &amp;quot;kubernetes&amp;quot; set.
# 设置客户端认证参数
[root@node1 kubernetes]# kubectl config set-credentials kubelet-bootstrap \
&amp;gt; --token=${BOOTSTRAP_TOKEN} \
&amp;gt; --kubeconfig=bootstrap.kubeconfig
User &amp;quot;kubelet-bootstrap&amp;quot; set.
# 设置上下文参数
[root@node1 kubernetes]# kubectl config set-context default \
&amp;gt; --cluster=kubernetes \
&amp;gt; --user=kubelet-bootstrap \
&amp;gt; --kubeconfig=bootstrap.kubeconfig
Context &amp;quot;default&amp;quot; created.
# 设置默认上下文
[root@node1 kubernetes]# kubectl config use-context default --kubeconfig=bootstrap.kubeconfig
Switched to context &amp;quot;default&amp;quot;.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;–embed-certs 为 true 时表示将 certificate-authority 证书写入到生成的 bootstrap.kubeconfig 文件中；&lt;/li&gt;
&lt;li&gt;设置客户端认证参数时没有指定秘钥和证书，后续由 kube-apiserver 自动生成。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;创建-kube-proxy-kubeconfig-文件&quot;&gt;&lt;a href=&quot;#创建-kube-proxy-kubeconfig-文件&quot; class=&quot;headerlink&quot; title=&quot;创建 kube-proxy kubeconfig 文件&quot;&gt;&lt;/a&gt;创建 kube-proxy kubeconfig 文件&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@node1 kubernetes]# export KUBE_APISERVER=&amp;quot;https://172.16.7.151:6443&amp;quot;
# 设置集群参数
[root@node1 kubernetes]# kubectl config set-cluster kubernetes \
&amp;gt; --certificate-authority=/etc/kubernetes/ssl/ca.pem \
&amp;gt; --embed-certs=true \
&amp;gt; --server=${KUBE_APISERVER} \
&amp;gt; --kubeconfig=kube-proxy.kubeconfig
Cluster &amp;quot;kubernetes&amp;quot; set.
# 设置客户端认证参数
[root@node1 kubernetes]# kubectl config set-credentials kube-proxy \
&amp;gt; --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \
&amp;gt; --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \
&amp;gt; --embed-certs=true \
&amp;gt; --kubeconfig=kube-proxy.kubeconfig
User &amp;quot;kube-proxy&amp;quot; set.
# 设置上下文参数
[root@node1 kubernetes]# kubectl config set-context default \
&amp;gt; --cluster=kubernetes \
&amp;gt; --user=kube-proxy \
&amp;gt; --kubeconfig=kube-proxy.kubeconfig
Context &amp;quot;default&amp;quot; created.
# 设置默认上下文
[root@node1 kubernetes]# kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
Switched to context &amp;quot;default&amp;quot;.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;设置集群参数和客户端认证参数时 –embed-certs 都为 true，这会将 certificate-authority、client-certificate 和client-key 指向的证书文件内容写入到生成的 kube-proxy.kubeconfig 文件中；&lt;/li&gt;
&lt;li&gt;kube-proxy.pem 证书中 CN 为 system:kube-proxy，kube-apiserver 预定义的 RoleBinding cluster-admin 将Usersystem:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;分发-kubeconfig-文件&quot;&gt;&lt;a href=&quot;#分发-kubeconfig-文件&quot; class=&quot;headerlink&quot; title=&quot;分发 kubeconfig 文件&quot;&gt;&lt;/a&gt;分发 kubeconfig 文件&lt;/h3&gt;&lt;p&gt;将两个 kubeconfig 文件分发到所有 Node 机器的 /etc/kubernetes/ 目录。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 kubernetes]# scp -p bootstrap.kubeconfig root@172.16.7.152:/etc/kubernetes/
[root@node1 kubernetes]# scp -p kube-proxy.kubeconfig root@172.16.7.152:/etc/kubernetes/
[root@node1 kubernetes]# scp -p bootstrap.kubeconfig root@172.16.7.153:/etc/kubernetes/
[root@node1 kubernetes]# scp -p kube-proxy.kubeconfig root@172.16.7.153:/etc/kubernetes/
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;创建高可用-etcd-集群&quot;&gt;&lt;a href=&quot;#创建高可用-etcd-集群&quot; class=&quot;headerlink&quot; title=&quot;创建高可用 etcd 集群&quot;&gt;&lt;/a&gt;创建高可用 etcd 集群&lt;/h2&gt;&lt;p&gt;etcd 是 CoreOS 团队发起的开源项目，基于 Go 语言实现，做为一个分布式键值对存储，通过分布式锁，leader选举和写屏障(write barriers)来实现可靠的分布式协作。&lt;br&gt;kubernetes系统使用etcd存储所有数据。&lt;br&gt;CoreOS官方推荐集群规模5个为宜，我这里使用了3个节点。&lt;/p&gt;
&lt;h3 id=&quot;安装配置etcd集群&quot;&gt;&lt;a href=&quot;#安装配置etcd集群&quot; class=&quot;headerlink&quot; title=&quot;安装配置etcd集群&quot;&gt;&lt;/a&gt;安装配置etcd集群&lt;/h3&gt;&lt;p&gt;搭建etcd集群有3种方式，分别为Static, etcd Discovery, DNS Discovery。Discovery请参见&lt;a href=&quot;https://coreos.com/etcd/docs/latest/op-guide/clustering.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;官网&lt;/a&gt;。这里仅以Static方式展示一次集群搭建过程。&lt;/p&gt;
&lt;p&gt;首先请做好3个节点的时间同步。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.TLS 认证文件&lt;/strong&gt;&lt;br&gt;需要为 etcd 集群创建加密通信的 TLS 证书，这里复用以前创建的 kubernetes 证书。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# cp ca.pem kubernetes-key.pem kubernetes.pem /etc/kubernetes/ssl
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这步在之前做过，可以忽略。【注意】:kubernetes 证书的 hosts 字段列表中包含上面三台机器的 IP，否则后续证书校验会失败。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.下载二进制文件&lt;/strong&gt;&lt;br&gt;到 &lt;a href=&quot;https://github.com/coreos/etcd/releases&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/coreos/etcd/releases&lt;/a&gt; 页面下载最新版本的二进制文件，并上传到/usr/local/目录下。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 local]# tar xf etcd-v3.2.7-linux-amd64.tar
[root@node1 local]# mv etcd-v3.2.7-linux-amd64/etcd* /usr/local/bin/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;etcd集群中另外两台机器也需要如上操作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3.创建 etcd 的 systemd unit 文件&lt;/strong&gt;&lt;br&gt;配置文件模板如下，注意替换 ETCD_NAME 和 INTERNAL_IP 变量的值。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# cat etcd.service
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
EnvironmentFile=-/etc/etcd/etcd.conf
ExecStart=/usr/local/bin/etcd \
  --name ${ETCD_NAME} \
  --cert-file=/etc/kubernetes/ssl/kubernetes.pem \
  --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
  --peer-cert-file=/etc/kubernetes/ssl/kubernetes.pem \
  --peer-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
  --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \
  --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \
  --initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \
  --listen-peer-urls https://${INTERNAL_IP}:2380 \
  --listen-client-urls https://${INTERNAL_IP}:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://${INTERNAL_IP}:2379 \
  --initial-cluster-token etcd-cluster-0 \
  --initial-cluster node1=https://172.16.7.151:2380,node2=https://172.16.7.152:2380,node3=https://172.16.7.153:2380 \
  --initial-cluster-state new \
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;针对上面几个配置参数做下简单的解释：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;–name：方便理解的节点名称，默认为default，在集群中应该保持唯一，可以使用 hostname&lt;/li&gt;
&lt;li&gt;–data-dir：服务运行数据保存的路径，默认为 ${name}.etcd&lt;/li&gt;
&lt;li&gt;–snapshot-count：指定有多少事务（transaction）被提交时，触发截取快照保存到磁盘&lt;/li&gt;
&lt;li&gt;–heartbeat-interval：leader 多久发送一次心跳到 followers。默认值是 100ms&lt;/li&gt;
&lt;li&gt;–eletion-timeout：重新投票的超时时间，如果 follow 在该时间间隔没有收到心跳包，会触发重新投票，默认为 1000 ms&lt;/li&gt;
&lt;li&gt;–listen-peer-urls：和同伴通信的地址，比如 &lt;a href=&quot;http://ip:2380&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://ip:2380&lt;/a&gt;&lt;br&gt;如果有多个，使用逗号分隔。需要所有节点都能够访问，所以不要使用 localhost！&lt;/li&gt;
&lt;li&gt;–listen-client-urls：对外提供服务的地址：比如 &lt;a href=&quot;http://ip:2379,http://127.0.0.1:2379，客户端会连接到这里和&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://ip:2379,http://127.0.0.1:2379，客户端会连接到这里和&lt;/a&gt; etcd 交互&lt;/li&gt;
&lt;li&gt;–advertise-client-urls：对外公告的该节点客户端监听地址，这个值会告诉集群中其他节点&lt;/li&gt;
&lt;li&gt;–initial-advertise-peer-urls：该节点同伴监听地址，这个值会告诉集群中其他节点&lt;/li&gt;
&lt;li&gt;–initial-cluster：集群中所有节点的信息，格式为 node1=&lt;a href=&quot;http://ip1:2380,node2=http://ip2:2380,…。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://ip1:2380,node2=http://ip2:2380,…。&lt;/a&gt;&lt;br&gt;注意：这里的 node1 是节点的 –name 指定的名字；后面的 ip1:2380 是 –initial-advertise-peer-urls 指定的值&lt;/li&gt;
&lt;li&gt;–initial-cluster-state：新建集群的时候，这个值为new；假如已经存在的集群，这个值为 existing&lt;/li&gt;
&lt;li&gt;–initial-cluster-token：创建集群的token，这个值每个集群保持唯一。这样的话，如果你要重新创建集群，即使配置和之前一样，也会再次生成新的集群和节点 uuid；否则会导致多个集群之间的冲突，造成未知的错误&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所有以–init开头的配置都是在bootstrap集群的时候才会用到，后续节点的重启会被忽略。&lt;/p&gt;
&lt;p&gt;node1主机：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 local]# mkdir -p /var/lib/etcd
[root@node1 local]# cd /etc/systemd/system/
[root@node1 system]# vim etcd.service
...
ExecStart=/usr/local/bin/etcd \
  --name node1 \
...
  --initial-advertise-peer-urls https://172.16.7.151:2380 \
  --listen-peer-urls https://172.16.7.151:2380 \
  --listen-client-urls https://172.16.7.151:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://172.16.7.151:2379 \
  --initial-cluster-token etcd-cluster-0 \
...

[root@node1 system]# scp -p etcd.service root@172.16.7.152:/etc/systemd/system/
[root@node1 system]# scp -p etcd.service root@172.16.7.153:/etc/systemd/system/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;node2主机：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node2 ~]# mkdir -p /var/lib/etcd /var/lib/etcd
[root@node2 ~]# vim /etc/systemd/system/etcd.service
...
ExecStart=/usr/local/bin/etcd \
  --name node2 \
...
  --initial-advertise-peer-urls https://172.16.7.152:2380 \
  --listen-peer-urls https://172.16.7.152:2380 \
  --listen-client-urls https://172.16.7.152:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://172.16.7.152:2379 \
  --initial-cluster-token etcd-cluster-0 \
...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;node3主机：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node3 ~]# mkdir -p /var/lib/etcd /var/lib/etcd
[root@node3 ~]# vim /etc/systemd/system/etcd.service
...
ExecStart=/usr/local/bin/etcd \
  --name node2 \
...
  --initial-advertise-peer-urls https://172.16.7.153:2380 \
  --listen-peer-urls https://172.16.7.153:2380 \
  --listen-client-urls https://172.16.7.153:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://172.16.7.153:2379 \
  --initial-cluster-token etcd-cluster-0 \
...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;指定 etcd 的工作目录为 /var/lib/etcd，数据目录为 /var/lib/etcd，需在启动服务前创建这两个目录；&lt;/li&gt;
&lt;li&gt;为了保证通信安全，需要指定 etcd 的公私钥(cert-file和key-file)、Peers 通信的公私钥和 CA 证书(peer-cert-file、peer-key-file、peer-trusted-ca-file)、客户端的CA证书（trusted-ca-file）；&lt;/li&gt;
&lt;li&gt;创建 kubernetes.pem 证书时使用的 kubernetes-csr.json 文件的 hosts 字段包含所有 etcd 节点的 INTERNAL_IP，否则证书校验会出错；&lt;/li&gt;
&lt;li&gt;–initial-cluster-state 值为 new 时，–name 的参数值必须位于 –initial-cluster 列表中。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;启动-etcd-服务&quot;&gt;&lt;a href=&quot;#启动-etcd-服务&quot; class=&quot;headerlink&quot; title=&quot;启动 etcd 服务&quot;&gt;&lt;/a&gt;启动 etcd 服务&lt;/h3&gt;&lt;p&gt;集群中的节点都执行以下命令：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable etcd
# systemctl start etcd
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;验证服务&quot;&gt;&lt;a href=&quot;#验证服务&quot; class=&quot;headerlink&quot; title=&quot;验证服务&quot;&gt;&lt;/a&gt;验证服务&lt;/h3&gt;&lt;p&gt;etcdctl 是一个命令行客户端，它能提供一些简洁的命令，供用户直接跟 etcd 服务打交道，而无需基于 HTTP API 方式。这在某些情况下将很方便，例如用户对服务进行测试或者手动修改数据库内容。我们也推荐在刚接触 etcd 时通过 etcdctl 命令来熟悉相关的操作，这些操作跟 HTTP API 实际上是对应的。&lt;br&gt;在etcd集群任意一台机器上执行如下命令：&lt;/p&gt;
&lt;p&gt;1.查看集群健康状态&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 system]# etcdctl --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/kubernetes/ssl/kubernetes.pem --key-file=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints &amp;quot;https://172.16.7.151:2379&amp;quot; cluster-health            
member 31800ab6b566b2b is healthy: got healthy result from https://172.16.7.151:2379
member 9a0745d96695eec6 is healthy: got healthy result from https://172.16.7.153:2379
member e64edc68e5e81b55 is healthy: got healthy result from https://172.16.7.152:2379
cluster is healthy
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;结果最后一行为 cluster is healthy 时表示集群服务正常。&lt;/p&gt;
&lt;p&gt;2.查看集群成员，并能看出哪个是leader节点&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 system]# etcdctl --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/kubernetes/ssl/kubernetes.pem --key-file=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints &amp;quot;https://172.16.7.151:2379&amp;quot; member list
31800ab6b566b2b: name=node1 peerURLs=https://172.16.7.151:2380 clientURLs=https://172.16.7.151:2379 isLeader=false
9a0745d96695eec6: name=node3 peerURLs=https://172.16.7.153:2380 clientURLs=https://172.16.7.153:2379 isLeader=false
e64edc68e5e81b55: name=node2 peerURLs=https://172.16.7.152:2380 clientURLs=https://172.16.7.152:2379 isLeader=true
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3.删除一个节点&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 如果你想更新一个节点的IP(peerURLS)，首先你需要知道那个节点的ID
# etcdctl --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/kubernetes/ssl/kubernetes.pem --key-file=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints &amp;quot;https://172.16.7.151:2379&amp;quot; member list
31800ab6b566b2b: name=node1 peerURLs=https://172.16.7.151:2380 clientURLs=https://172.16.7.151:2379 isLeader=false
9a0745d96695eec6: name=node3 peerURLs=https://172.16.7.153:2380 clientURLs=https://172.16.7.153:2379 isLeader=false
e64edc68e5e81b55: name=node2 peerURLs=https://172.16.7.152:2380 clientURLs=https://172.16.7.152:2379 isLeader=true
# 删除一个节点
# etcdctl --endpoints &amp;quot;http://192.168.2.210:2379&amp;quot; member remove 9a0745d96695eec6
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;部署-kubernetes-master&quot;&gt;&lt;a href=&quot;#部署-kubernetes-master&quot; class=&quot;headerlink&quot; title=&quot;部署 kubernetes master&quot;&gt;&lt;/a&gt;部署 kubernetes master&lt;/h2&gt;&lt;p&gt;kubernetes master 节点包含的组件：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kube-apiserver&lt;/li&gt;
&lt;li&gt;kube-scheduler&lt;/li&gt;
&lt;li&gt;kube-controller-manager&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;目前这三个组件需要部署在同一台机器上。&lt;br&gt;kube-scheduler、kube-controller-manager 和 kube-apiserver 三者的功能紧密相关；&lt;br&gt;同时只能有一个 kube-scheduler、kube-controller-manager 进程处于工作状态，如果运行多个，则需要通过选举产生一个 leader。&lt;/p&gt;
&lt;h3 id=&quot;TLS-证书文件&quot;&gt;&lt;a href=&quot;#TLS-证书文件&quot; class=&quot;headerlink&quot; title=&quot;TLS 证书文件&quot;&gt;&lt;/a&gt;TLS 证书文件&lt;/h3&gt;&lt;p&gt;检查之前生成的证书。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 kubernetes]# ls /etc/kubernetes/ssl
admin-key.pem  admin.pem  ca-key.pem  ca.pem  kube-proxy-key.pem  kube-proxy.pem  kubernetes-key.pem  kubernetes.pem
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;下载二进制文件&quot;&gt;&lt;a href=&quot;#下载二进制文件&quot; class=&quot;headerlink&quot; title=&quot;下载二进制文件&quot;&gt;&lt;/a&gt;下载二进制文件&lt;/h3&gt;&lt;p&gt;有两种下载方式：&lt;br&gt;方式一：从 &lt;a href=&quot;https://github.com/kubernetes/kubernetes/releases&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;github release&lt;/a&gt; 页面下载发布版 tarball，解压后再执行下载脚本。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 local]# cd /opt/
[root@node1 opt]# wget https://github.com/kubernetes/kubernetes/releases/download/v1.6.0/kubernetes.tar.gz
[root@node1 opt]# tar zxf kubernetes.tar.gz 
[root@node1 opt]# cd kubernetes/
[root@node1 kubernetes]# ./cluster/get-kube-binaries.sh
Kubernetes release: v1.6.0
Server: linux/amd64  (to override, set KUBERNETES_SERVER_ARCH)
Client: linux/amd64  (autodetected)

Will download kubernetes-server-linux-amd64.tar.gz from https://storage.googleapis.com/kubernetes-release/release/v1.6.0
Will download and extract kubernetes-client-linux-amd64.tar.gz from https://storage.googleapis.com/kubernetes-release/release/v1.6.0
Is this ok? [Y]/n
y
...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;方式二：从 CHANGELOG页面 下载 client 或 server tarball 文件&lt;br&gt;server 的 tarball kubernetes-server-linux-amd64.tar.gz 已经包含了 client(kubectl) 二进制文件，所以不用单独下载kubernetes-client-linux-amd64.tar.gz文件&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;wget https://dl.k8s.io/v1.6.0/kubernetes-server-linux-amd64.tar.gz
tar -xzvf kubernetes-server-linux-amd64.tar.gz
...
cd kubernetes
tar -xzvf  kubernetes-src.tar.gz
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;将二进制文件拷贝到指定路径：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 kubernetes]# pwd
/opt/kubernetes
[root@node1 kubernetes]# cd server/
[root@node1 server]# tar zxf kubernetes-server-linux-amd64.tar.gz
[root@node1 server]# cp -r kubernetes/server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet} /usr/local/bin/
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;配置和启动-kube-apiserver&quot;&gt;&lt;a href=&quot;#配置和启动-kube-apiserver&quot; class=&quot;headerlink&quot; title=&quot;配置和启动 kube-apiserver&quot;&gt;&lt;/a&gt;配置和启动 kube-apiserver&lt;/h3&gt;&lt;h4 id=&quot;创建-kube-apiserver的service配置文件&quot;&gt;&lt;a href=&quot;#创建-kube-apiserver的service配置文件&quot; class=&quot;headerlink&quot; title=&quot;创建 kube-apiserver的service配置文件&quot;&gt;&lt;/a&gt;创建 kube-apiserver的service配置文件&lt;/h4&gt;&lt;p&gt;在/usr/lib/systemd/system/下创建kube-apiserver.service，内容如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cd /usr/lib/systemd/system/
# vim kube-apiserver.service
[Unit]
Description=Kubernetes API Service
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
After=etcd.service

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/apiserver
ExecStart=/usr/local/bin/kube-apiserver \
        $KUBE_LOGTOSTDERR \
        $KUBE_LOG_LEVEL \
        $KUBE_ETCD_SERVERS \
        $KUBE_API_ADDRESS \
        $KUBE_API_PORT \
        $KUBELET_PORT \
        $KUBE_ALLOW_PRIV \
        $KUBE_SERVICE_ADDRESSES \
        $KUBE_ADMISSION_CONTROL \
        $KUBE_API_ARGS
Restart=on-failure
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;/etc/kubernetes/config文件的内容为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/kubernetes/config
###
# kubernetes system config
#
# The following values are used to configure various aspects of all
# kubernetes services, including
#
#   kube-apiserver.service
#   kube-controller-manager.service
#   kube-scheduler.service
#   kubelet.service
#   kube-proxy.service
# logging to stderr means we get it in the systemd journal
KUBE_LOGTOSTDERR=&amp;quot;--logtostderr=true&amp;quot;

# journal message level, 0 is debug
KUBE_LOG_LEVEL=&amp;quot;--v=0&amp;quot;

# Should this cluster be allowed to run privileged docker containers
KUBE_ALLOW_PRIV=&amp;quot;--allow-privileged=true&amp;quot;

# How the controller-manager, scheduler, and proxy find the apiserver
#KUBE_MASTER=&amp;quot;--master=http://domainName:8080&amp;quot;                             
KUBE_MASTER=&amp;quot;--master=http://172.16.7.151:8080&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;该配置文件同时被kube-apiserver、kube-controller-manager、kube-scheduler、kubelet、kube-proxy使用。&lt;/p&gt;
&lt;p&gt;创建apiserver配置文件/etc/kubernetes/apiserver：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/kubernetes/apiserver
###
## kubernetes system config
##
## The following values are used to configure the kube-apiserver
##
#
## The address on the local server to listen to.
#KUBE_API_ADDRESS=&amp;quot;--insecure-bind-address=sz-pg-oam-docker-test-001.tendcloud.com&amp;quot;
KUBE_API_ADDRESS=&amp;quot;--advertise-address=172.16.7.151 --bind-address=172.16.7.151 --insecure-bind-address=172.16.7.151&amp;quot;
#
## The port on the local server to listen on.
#KUBE_API_PORT=&amp;quot;--port=8080&amp;quot;
#
## Port minions listen on
#KUBELET_PORT=&amp;quot;--kubelet-port=10250&amp;quot;
#
## Comma separated list of nodes in the etcd cluster
KUBE_ETCD_SERVERS=&amp;quot;--etcd-servers=https://172.16.7.151:2379,https://172.16.7.152:2379,https://172.16.7.153:2379&amp;quot;
#
## Address range to use for services
KUBE_SERVICE_ADDRESSES=&amp;quot;--service-cluster-ip-range=10.254.0.0/16&amp;quot;
#
## default admission control policies
KUBE_ADMISSION_CONTROL=&amp;quot;--admission-control=ServiceAccount,NamespaceLifecycle,NamespaceExists,LimitRanger,ResourceQuota&amp;quot;
#
## Add your own!
KUBE_API_ARGS=&amp;quot;--authorization-mode=RBAC --runtime-config=rbac.authorization.k8s.io/v1beta1 --kubelet-https=true --experimental-bootstrap-token-auth --token-auth-file=/etc/kubernetes/token.csv --service-node-port-range=30000-32767 --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem --client-ca-file=/etc/kubernetes/ssl/ca.pem --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem --etcd-cafile=/etc/kubernetes/ssl/ca.pem --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem --enable-swagger-ui=true --apiserver-count=3 --audit-log-maxage=30 --audit-log-maxbackup=3 --audit-log-maxsize=100 --audit-log-path=/var/lib/audit.log --event-ttl=1h&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;–authorization-mode=RBAC 指定在安全端口使用 RBAC 授权模式，拒绝未通过授权的请求；&lt;/li&gt;
&lt;li&gt;kube-scheduler、kube-controller-manager 一般和 kube-apiserver 部署在同一台机器上，它们使用非安全端口和 kube-apiserver通信;&lt;/li&gt;
&lt;li&gt;kubelet、kube-proxy、kubectl 部署在其它 Node 节点上，如果通过安全端口访问 kube-apiserver，则必须先通过 TLS 证书认证，再通过 RBAC 授权；&lt;/li&gt;
&lt;li&gt;kube-proxy、kubectl 通过在使用的证书里指定相关的 User、Group 来达到通过 RBAC 授权的目的；&lt;/li&gt;
&lt;li&gt;如果使用了 kubelet TLS Boostrap 机制，则不能再指定 –kubelet-certificate-authority、–kubelet-client-certificate 和 –kubelet-client-key 选项，否则后续 kube-apiserver 校验 kubelet 证书时出现 ”x509: certificate signed by unknown authority“ 错误；&lt;/li&gt;
&lt;li&gt;–admission-control 值必须包含 ServiceAccount；&lt;/li&gt;
&lt;li&gt;–bind-address 不能为 127.0.0.1；&lt;/li&gt;
&lt;li&gt;runtime-config配置为rbac.authorization.k8s.io/v1beta1，表示运行时的apiVersion；&lt;/li&gt;
&lt;li&gt;–service-cluster-ip-range 指定 Service Cluster IP 地址段，该地址段不能路由可达；&lt;/li&gt;
&lt;li&gt;缺省情况下 kubernetes 对象保存在 etcd /registry 路径下，可以通过 –etcd-prefix 参数进行调整。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;启动kube-apiserver&quot;&gt;&lt;a href=&quot;#启动kube-apiserver&quot; class=&quot;headerlink&quot; title=&quot;启动kube-apiserver&quot;&gt;&lt;/a&gt;启动kube-apiserver&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable kube-apiserver
# systemctl start kube-apiserver
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;启动过程中可以观察日志：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# tail -f /var/log/message
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;配置和启动-kube-controller-manager&quot;&gt;&lt;a href=&quot;#配置和启动-kube-controller-manager&quot; class=&quot;headerlink&quot; title=&quot;配置和启动 kube-controller-manager&quot;&gt;&lt;/a&gt;配置和启动 kube-controller-manager&lt;/h3&gt;&lt;h4 id=&quot;创建-kube-controller-manager-的service配置文件&quot;&gt;&lt;a href=&quot;#创建-kube-controller-manager-的service配置文件&quot; class=&quot;headerlink&quot; title=&quot;创建 kube-controller-manager 的service配置文件&quot;&gt;&lt;/a&gt;创建 kube-controller-manager 的service配置文件&lt;/h4&gt;&lt;p&gt;在/usr/lib/systemd/system/下创建kube-controller-manager.service，内容如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# pwd
/usr/lib/systemd/system
# vim kube-controller-manager.service
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/controller-manager
ExecStart=/usr/local/bin/kube-controller-manager \
            $KUBE_LOGTOSTDERR \
            $KUBE_LOG_LEVEL \
            $KUBE_MASTER \
            $KUBE_CONTROLLER_MANAGER_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;创建kube-controller-manager配置文件/etc/kubernetes/controller-manager：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/kubernetes/controller-manager
###
# The following values are used to configure the kubernetes controller-manager

# defaults from config and apiserver should be adequate

# Add your own!
KUBE_CONTROLLER_MANAGER_ARGS=&amp;quot;--address=127.0.0.1 --service-cluster-ip-range=10.254.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem  --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem --root-ca-file=/etc/kubernetes/ssl/ca.pem --leader-elect=true&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;–service-cluster-ip-range 参数指定 Cluster 中 Service 的CIDR范围，该网络在各 Node 间必须路由不可达，必须和 kube-apiserver 中的参数一致；&lt;/li&gt;
&lt;li&gt;–cluster-signing-* 指定的证书和私钥文件用来签名为 TLS BootStrap 创建的证书和私钥；&lt;/li&gt;
&lt;li&gt;–root-ca-file 用来对 kube-apiserver 证书进行校验，指定该参数后，才会在Pod 容器的 ServiceAccount 中放置该 CA 证书文件；&lt;/li&gt;
&lt;li&gt;&lt;p&gt;–address 值必须为 127.0.0.1，因为当前 kube-apiserver 期望 scheduler 和 controller-manager 在同一台机器，否则：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get componentstatuses
NAME                 STATUS      MESSAGE                                                                                        ERROR
scheduler            Unhealthy   Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: getsockopt: connection refused   
controller-manager   Healthy     ok                                                                                             
etcd-2               Unhealthy   Get http://172.20.0.113:2379/health: malformed HTTP response &amp;quot;\x15\x03\x01\x00\x02\x02&amp;quot;        
etcd-0               Healthy     {&amp;quot;health&amp;quot;: &amp;quot;true&amp;quot;}                                                                             
etcd-1               Healthy     {&amp;quot;health&amp;quot;: &amp;quot;true&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;参考：&lt;a href=&quot;https://github.com/kubernetes-incubator/bootkube/issues/64&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/kubernetes-incubator/bootkube/issues/64&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;启动-kube-controller-manager&quot;&gt;&lt;a href=&quot;#启动-kube-controller-manager&quot; class=&quot;headerlink&quot; title=&quot;启动 kube-controller-manager&quot;&gt;&lt;/a&gt;启动 kube-controller-manager&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable kube-controller-manager
# systemctl start kube-controller-manager
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;配置和启动-kube-scheduler&quot;&gt;&lt;a href=&quot;#配置和启动-kube-scheduler&quot; class=&quot;headerlink&quot; title=&quot;配置和启动 kube-scheduler&quot;&gt;&lt;/a&gt;配置和启动 kube-scheduler&lt;/h3&gt;&lt;h4 id=&quot;创建-kube-scheduler的serivce配置文件&quot;&gt;&lt;a href=&quot;#创建-kube-scheduler的serivce配置文件&quot; class=&quot;headerlink&quot; title=&quot;创建 kube-scheduler的serivce配置文件&quot;&gt;&lt;/a&gt;创建 kube-scheduler的serivce配置文件&lt;/h4&gt;&lt;p&gt;在/usr/lib/systemd/system/下创建kube-scheduler.serivce，内容如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# pwd
/usr/lib/systemd/system
# vim kube-scheduler.service
[Unit]
Description=Kubernetes Scheduler Plugin
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/scheduler
ExecStart=/usr/local/bin/kube-scheduler \
            $KUBE_LOGTOSTDERR \
            $KUBE_LOG_LEVEL \
            $KUBE_MASTER \
            $KUBE_SCHEDULER_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;创建kube-scheduler配置文件/etc/kubernetes/scheduler：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/kubernetes/scheduler
###
# kubernetes scheduler config

# default config should be adequate

# Add your own!
KUBE_SCHEDULER_ARGS=&amp;quot;--leader-elect=true --address=127.0.0.1&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;–address 值必须为 127.0.0.1，因为当前 kube-apiserver 期望 scheduler 和 controller-manager 在同一台机器。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;启动-kube-scheduler&quot;&gt;&lt;a href=&quot;#启动-kube-scheduler&quot; class=&quot;headerlink&quot; title=&quot;启动 kube-scheduler&quot;&gt;&lt;/a&gt;启动 kube-scheduler&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable kube-scheduler
# systemctl start kube-scheduler
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;验证-master-节点功能&quot;&gt;&lt;a href=&quot;#验证-master-节点功能&quot; class=&quot;headerlink&quot; title=&quot;验证 master 节点功能&quot;&gt;&lt;/a&gt;验证 master 节点功能&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;# kubectl get componentstatuses
NAME                 STATUS    MESSAGE              ERROR
scheduler            Healthy   ok                   
controller-manager   Healthy   ok                   
etcd-0               Healthy   {&amp;quot;health&amp;quot;: &amp;quot;true&amp;quot;}   
etcd-1               Healthy   {&amp;quot;health&amp;quot;: &amp;quot;true&amp;quot;}   
etcd-2               Healthy   {&amp;quot;health&amp;quot;: &amp;quot;true&amp;quot;}              
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;部署kubernetes-node节点&quot;&gt;&lt;a href=&quot;#部署kubernetes-node节点&quot; class=&quot;headerlink&quot; title=&quot;部署kubernetes node节点&quot;&gt;&lt;/a&gt;部署kubernetes node节点&lt;/h2&gt;&lt;p&gt;kubernetes node 节点包含如下组件：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Docker 1.12.6&lt;/li&gt;
&lt;li&gt;Flanneld&lt;/li&gt;
&lt;li&gt;kubelet&lt;/li&gt;
&lt;li&gt;kube-proxy&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;安装Docker&quot;&gt;&lt;a href=&quot;#安装Docker&quot; class=&quot;headerlink&quot; title=&quot;安装Docker&quot;&gt;&lt;/a&gt;安装Docker&lt;/h3&gt;&lt;p&gt;参见之前的文章《Docker镜像和容器》。&lt;/p&gt;
&lt;h3 id=&quot;安装配置Flanneld&quot;&gt;&lt;a href=&quot;#安装配置Flanneld&quot; class=&quot;headerlink&quot; title=&quot;安装配置Flanneld&quot;&gt;&lt;/a&gt;安装配置Flanneld&lt;/h3&gt;&lt;h4 id=&quot;Flannel介绍&quot;&gt;&lt;a href=&quot;#Flannel介绍&quot; class=&quot;headerlink&quot; title=&quot;Flannel介绍&quot;&gt;&lt;/a&gt;Flannel介绍&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Flannel是CoreOS团队针对Kubernetes设计的一个网络规划服务，简单来说，它的功能是让集群中的不同节点主机创建的Docker容器都具有全集群唯一的虚拟IP地址。&lt;/li&gt;
&lt;li&gt;在默认的Docker配置中，每个节点上的Docker服务会分别负责所在节点容器的IP分配。这样导致的一个问题是，不同节点上容器可能获得相同的内外IP地址。&lt;/li&gt;
&lt;li&gt;Flannel的设计目的就是为集群中的所有节点重新规划IP地址的使用规则，从而使得不同节点上的容器能够获得“同属一个内网”且”不重复的”IP地址，并让属于不同节点上的容器能够直接通过内网IP通信。&lt;/li&gt;
&lt;li&gt;Flannel实质上是一种“覆盖网络(overlay network)”，也就是将TCP数据包装在另一种网络包里面进行路由转发和通信，目前已经支持udp、vxlan、host-gw、aws-vpc、gce和alloc路由等数据转发方式，默认的节点间数据通信方式是UDP转发。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在Flannel的GitHub页面有如下的一张原理图：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数据从源容器中发出后，经由所在主机的docker0虚拟网卡转发到flannel0虚拟网卡，这是个P2P的虚拟网卡，flanneld服务监听在网卡的另外一端。（Flannel通过ETCD服务维护了一张节点间的路由表）；&lt;/li&gt;
&lt;li&gt;源主机的flanneld服务将原本的数据内容UDP封装后根据自己的路由表投递给目的节点的flanneld服务，数据到达后被解包，然后直接进入目的节点的flannel0虚拟网卡，然后被转发到目的主机的docker0虚拟网卡；&lt;/li&gt;
&lt;li&gt;最后就像本机容器通信一样由docker0路由到目标容器，这样整个数据包的传递就完成了。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;安装配置flannel&quot;&gt;&lt;a href=&quot;#安装配置flannel&quot; class=&quot;headerlink&quot; title=&quot;安装配置flannel&quot;&gt;&lt;/a&gt;安装配置flannel&lt;/h4&gt;&lt;p&gt;我这里使用yum安装，安装的版本是0.7.1。集群中的3台node都需要安装配置flannel。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# yum install -y flannel
# rpm -ql flannel
/etc/sysconfig/flanneld
/run/flannel
/usr/bin/flanneld
/usr/bin/flanneld-start
/usr/lib/systemd/system/docker.service.d/flannel.conf
/usr/lib/systemd/system/flanneld.service
/usr/lib/tmpfiles.d/flannel.conf
/usr/libexec/flannel
/usr/libexec/flannel/mk-docker-opts.sh
...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;修改flannel配置文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/sysconfig/flanneld 
# Flanneld configuration options  

# etcd url location.  Point this to the server where etcd runs
FLANNEL_ETCD_ENDPOINTS=&amp;quot;https://172.16.7.151:2379,https://172.16.7.152:2379,https://172.16.7.153:2379&amp;quot;

# etcd config key.  This is the configuration key that flannel queries
# For address range assignment
FLANNEL_ETCD_PREFIX=&amp;quot;/kube-centos/network&amp;quot;

# Any additional options that you want to pass
#FLANNEL_OPTIONS=&amp;quot;&amp;quot;
FLANNEL_OPTIONS=&amp;quot;-etcd-cafile=/etc/kubernetes/ssl/ca.pem -etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem -etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：        &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;etcd的地址FLANNEL_ETCD_ENDPOINT&lt;/li&gt;
&lt;li&gt;etcd查询的目录，包含docker的IP地址段配置。FLANNEL_ETCD_PREFIX        &lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;在etcd中初始化flannel网络数据&quot;&gt;&lt;a href=&quot;#在etcd中初始化flannel网络数据&quot; class=&quot;headerlink&quot; title=&quot;在etcd中初始化flannel网络数据&quot;&gt;&lt;/a&gt;在etcd中初始化flannel网络数据&lt;/h4&gt;&lt;p&gt;多个node上的Flanneld依赖一个etcd cluster来做集中配置服务，etcd保证了所有node上flanned所看到的配置是一致的。同时每个node上的flanned监听etcd上的数据变化，实时感知集群中node的变化。&lt;/p&gt;
&lt;p&gt;执行下面的命令为docker分配IP地址段：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# etcdctl --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/kubernetes/ssl/kubernetes.pem --key-file=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints &amp;quot;https://172.16.7.151:2379&amp;quot; mkdir /kube-centos/network
# etcdctl --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/kubernetes/ssl/kubernetes.pem --key-file=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints &amp;quot;https://172.16.7.151:2379&amp;quot; mk /kube-centos/network/config &amp;apos;{&amp;quot;Network&amp;quot;: &amp;quot;172.30.0.0/16&amp;quot;, &amp;quot;SubnetLen&amp;quot;: 24, &amp;quot;Backend&amp;quot;: { &amp;quot;Type&amp;quot;: &amp;quot;vxlan&amp;quot; }}&amp;apos;
{&amp;quot;Network&amp;quot;: &amp;quot;172.30.0.0/16&amp;quot;, &amp;quot;SubnetLen&amp;quot;: 24, &amp;quot;Backend&amp;quot;: { &amp;quot;Type&amp;quot;: &amp;quot;vxlan&amp;quot; }}
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;启动flannel&quot;&gt;&lt;a href=&quot;#启动flannel&quot; class=&quot;headerlink&quot; title=&quot;启动flannel&quot;&gt;&lt;/a&gt;启动flannel&lt;/h4&gt;&lt;p&gt;集群中的3台node都启动flannel：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl start flanneld
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;启动完成后，会在/run/flannel/目录下生成两个文件，以node1为例：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# ls /run/flannel/         
docker  subnet.env
# cd /run/flannel/
[root@node1 flannel]# cat docker 
DOCKER_OPT_BIP=&amp;quot;--bip=172.30.51.1/24&amp;quot;
DOCKER_OPT_IPMASQ=&amp;quot;--ip-masq=true&amp;quot;
DOCKER_OPT_MTU=&amp;quot;--mtu=1450&amp;quot;
DOCKER_NETWORK_OPTIONS=&amp;quot; --bip=172.30.51.1/24 --ip-masq=true --mtu=1450&amp;quot;
# cat subnet.env 
FLANNEL_NETWORK=172.30.0.0/16
FLANNEL_SUBNET=172.30.51.1/24
FLANNEL_MTU=1450
FLANNEL_IPMASQ=false
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;现在查询etcd中的内容可以看到：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# etcdctl --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/kubernetes/ssl/kubernetes.pem --key-file=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints &amp;quot;https://172.16.7.151:2379&amp;quot; ls /kube-centos/network/subnets
/kube-centos/network/subnets/172.30.51.0-24
/kube-centos/network/subnets/172.30.29.0-24
/kube-centos/network/subnets/172.30.19.0-24
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;设置docker0网桥的IP地址(集群中node节点都需要设置)：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# source /run/flannel/subnet.env
# ifconfig docker0 $FLANNEL_SUBNET
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这样docker0和flannel网桥会在同一个子网中，查看node1主机网卡：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker0: flags=4099&amp;lt;UP,BROADCAST,MULTICAST&amp;gt;  mtu 1500
        inet 172.30.51.1  netmask 255.255.255.0  broadcast 172.30.51.255
flannel.1: flags=4163&amp;lt;UP,BROADCAST,RUNNING,MULTICAST&amp;gt;  mtu 1450
        inet 172.30.51.0  netmask 255.255.255.255  broadcast 0.0.0.0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;重启docker：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;【注意】:经过测试，docker 17.06.1-ce版本重启后，docker0网桥又会被重置为172.17.0.1，docker 1.12.6版本测试是不会有问题的。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果想重新设置flannel，先停止flanneld，清理etcd里的数据，然后 ifconfig flannel.1 down，然后启动flanneld，会重新生成子网，并up flannel.1网桥设备。&lt;/p&gt;
&lt;h4 id=&quot;测试跨主机容器通信&quot;&gt;&lt;a href=&quot;#测试跨主机容器通信&quot; class=&quot;headerlink&quot; title=&quot;测试跨主机容器通信&quot;&gt;&lt;/a&gt;测试跨主机容器通信&lt;/h4&gt;&lt;p&gt;分别在node1和node2上启动一个容器，然后ping对方容器的地址：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 flannel]# docker run -i -t centos /bin/bash
[root@38be151deb71 /]# yum install net-tools -y
[root@38be151deb71 /]# ifconfig
eth0: flags=4163&amp;lt;UP,BROADCAST,RUNNING,MULTICAST&amp;gt;  mtu 1450
        inet 172.30.51.2  netmask 255.255.255.0  broadcast 0.0.0.0

[root@node2 flannel]# docker run -i -t centos /bin/bash
[root@90e85c215fda /]# yum install net-tools -y
[root@90e85c215fda /]# ifconfig
eth0: flags=4163&amp;lt;UP,BROADCAST,RUNNING,MULTICAST&amp;gt;  mtu 1450
        inet 172.30.29.2  netmask 255.255.255.0  broadcast 0.0.0.0
[root@90e85c215fda /]# ping 172.16.51.2  
PING 172.16.51.2 (172.16.51.2) 56(84) bytes of data.
64 bytes from 172.16.51.2: icmp_seq=1 ttl=254 time=1.00 ms
64 bytes from 172.16.51.2: icmp_seq=2 ttl=254 time=1.29 ms
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;补充：下载二进制包安装flannel&quot;&gt;&lt;a href=&quot;#补充：下载二进制包安装flannel&quot; class=&quot;headerlink&quot; title=&quot;补充：下载二进制包安装flannel&quot;&gt;&lt;/a&gt;补充：下载二进制包安装flannel&lt;/h4&gt;&lt;p&gt;从官网 &lt;a href=&quot;https://github.com/coreos/flannel/releases&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/coreos/flannel/releases&lt;/a&gt; 下载的flannel release 0.7.1，并将下载的文件上传到服务器的/opt/flannel/目录下。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# mkdir flannel
# cd flannel/
# tar xf flannel-v0.7.1-linux-amd64.tar  
# ls
flanneld  flannel-v0.7.1-linux-amd64.tar  mk-docker-opts.sh  README.md
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;mk-docker-opts.sh是用来Generate Docker daemon options based on flannel env file。&lt;br&gt;执行 ./mk-docker-opts.sh -i 将会生成如下两个文件环境变量文件。&lt;/p&gt;
&lt;p&gt;Flannel的&lt;a href=&quot;https://github.com/coreos/flannel/blob/master/Documentation/running.md&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;文档&lt;/a&gt;中有写Docker Integration：&lt;br&gt;Docker daemon accepts –bip argument to configure the subnet of the docker0 bridge. It also accepts –mtu to set the MTU for docker0 and veth devices that it will be creating.&lt;br&gt;Because flannel writes out the acquired subnet and MTU values into a file, the script starting Docker can source in the values and pass them to Docker daemon:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;source /run/flannel/subnet.env
docker daemon --bip=${FLANNEL_SUBNET} --mtu=${FLANNEL_MTU} &amp;amp;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Systemd users can use EnvironmentFile directive in the .service file to pull in /run/flannel/subnet.env&lt;/p&gt;
&lt;h3 id=&quot;安装和配置-kubelet&quot;&gt;&lt;a href=&quot;#安装和配置-kubelet&quot; class=&quot;headerlink&quot; title=&quot;安装和配置 kubelet&quot;&gt;&lt;/a&gt;安装和配置 kubelet&lt;/h3&gt;&lt;p&gt;kubelet 启动时向 kube-apiserver 发送 TLS bootstrapping 请求，需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper cluster 角色(role)， 然后 kubelet 才能有权限创建认证请求(certificate signing requests)：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cd /etc/kubernetes
[root@node1 kubernetes]# kubectl create clusterrolebinding kubelet-bootstrap \
&amp;gt; --clusterrole=system:node-bootstrapper \
&amp;gt; --user=kubelet-bootstrap
clusterrolebinding &amp;quot;kubelet-bootstrap&amp;quot; created  
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;【注意】：以上这步只需要在kubernetes node集群中的一台执行一次就可以了。&lt;/strong&gt;&lt;br&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;–user=kubelet-bootstrap 是在 /etc/kubernetes/token.csv 文件中指定的用户名，同时也写入了/etc/kubernetes/bootstrap.kubeconfig 文件。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;下载最新的-kubelet-和-kube-proxy-二进制文件&quot;&gt;&lt;a href=&quot;#下载最新的-kubelet-和-kube-proxy-二进制文件&quot; class=&quot;headerlink&quot; title=&quot;下载最新的 kubelet 和 kube-proxy 二进制文件&quot;&gt;&lt;/a&gt;下载最新的 kubelet 和 kube-proxy 二进制文件&lt;/h4&gt;&lt;p&gt;这个在之前安装kubernetes master时已经下载好了二进制文件，只需要复制到相应目录即可。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 kubernetes]# cd /opt/kubernetes/server/kubernetes/server/bin/
[root@node1 bin]# scp -p kubelet root@172.16.7.152:/usr/local/bin/
[root@node1 bin]# scp -p kube-proxy root@172.16.7.152:/usr/local/bin/
[root@node1 bin]# scp -p kubelet root@172.16.7.153:/usr/local/bin/
[root@node1 bin]# scp -p kube-proxy root@172.16.7.153:/usr/local/bin/
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;配置kubelet&quot;&gt;&lt;a href=&quot;#配置kubelet&quot; class=&quot;headerlink&quot; title=&quot;配置kubelet&quot;&gt;&lt;/a&gt;配置kubelet&lt;/h4&gt;&lt;p&gt;以下操作需要在集群的kubernetes node节点上都要运行，下面以node1服务器为例：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.创建 kubelet 的service配置文件：&lt;/strong&gt;&lt;br&gt;在/usr/lib/systemd/system/下创建文件kubelet.serivce：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /usr/lib/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/kubelet
ExecStart=/usr/local/bin/kubelet \
            $KUBE_LOGTOSTDERR \
            $KUBE_LOG_LEVEL \
            $KUBELET_API_SERVER \
            $KUBELET_ADDRESS \
            $KUBELET_PORT \
            $KUBELET_HOSTNAME \
            $KUBE_ALLOW_PRIV \
            $KUBELET_POD_INFRA_CONTAINER \
            $KUBELET_ARGS
Restart=on-failure

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;2.创建kubelet配置文件&lt;/strong&gt;&lt;br&gt;创建kubelet工作目录（必须创建，否则kubelet启动不了）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# mkdir /var/lib/kubelet
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;创建kubelet配置文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/kubernetes/kubelet
###
## kubernetes kubelet (minion) config
#
## The address for the info server to serve on (set to 0.0.0.0 or &amp;quot;&amp;quot; for all interfaces)
KUBELET_ADDRESS=&amp;quot;--address=172.16.7.151&amp;quot;
#
## The port for the info server to serve on
#KUBELET_PORT=&amp;quot;--port=10250&amp;quot;
#
## You may leave this blank to use the actual hostname
KUBELET_HOSTNAME=&amp;quot;--hostname-override=172.16.7.151&amp;quot;
#
## location of the api-server
KUBELET_API_SERVER=&amp;quot;--api-servers=http://172.16.7.151:8080&amp;quot;
#
## pod infrastructure container
#KUBELET_POD_INFRA_CONTAINER=&amp;quot;--pod-infra-container-image=sz-pg-oam-docker-hub-001.tendcloud.com/library/pod-infrastructure:rhel7&amp;quot;
KUBELET_POD_INFRA_CONTAINER=&amp;quot;--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure&amp;quot;
#
## Add your own!
KUBELET_ARGS=&amp;quot;--cgroup-driver=systemd --cluster-dns=10.254.0.2 --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig --require-kubeconfig --cert-dir=/etc/kubernetes/ssl --cluster-domain=cluster.local. --hairpin-mode promiscuous-bridge --serialize-image-pulls=false&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【注意】：将配置文件中的IP地址更改为你的每台node节点的IP地址（除了–api-servers=&lt;a href=&quot;http://172.16.7.151:8080这个ip地址是不用改的）。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.151:8080这个ip地址是不用改的）。&lt;/a&gt;&lt;br&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;–address 不能设置为 127.0.0.1，否则后续 Pods 访问 kubelet 的 API 接口时会失败，因为 Pods 访问的 127.0.0.1 指向自己而不是 kubelet；&lt;br&gt;如果设置了 –hostname-override 选项，则 kube-proxy 也需要设置该选项，否则会出现找不到 Node 的情况；&lt;/li&gt;
&lt;li&gt;KUBELET_POD_INFRA_CONTAINER=”–pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure”，这个是一个基础容器，每一个Pod启动的时候都会启动一个这样的容器。如果你的本地没有这个镜像，kubelet会连接外网把这个镜像下载下来。最开始的时候是在Google的registry上，因此国内因为GFW都下载不了导致Pod运行不起来。现在每个版本的Kubernetes都把这个镜像打包，你可以提前传到自己的registry上，然后再用这个参数指定。&lt;/li&gt;
&lt;li&gt;–experimental-bootstrap-kubeconfig 指向 bootstrap kubeconfig 文件，kubelet 使用该文件中的用户名和 token 向 kube-apiserver 发送 TLS Bootstrapping 请求；&lt;/li&gt;
&lt;li&gt;管理员通过了 CSR 请求后，kubelet 自动在 –cert-dir 目录创建证书和私钥文件(kubelet-client.crt 和 kubelet-client.key)，然后写入 –kubeconfig 文件；&lt;/li&gt;
&lt;li&gt;建议在 –kubeconfig 配置文件中指定 kube-apiserver 地址，如果未指定 –api-servers 选项，则必须指定 –require-kubeconfig 选项后才从配置文件中读取 kube-apiserver 的地址，否则 kubelet 启动后将找不到 kube-apiserver (日志中提示未找到 API Server），kubectl get nodes 不会返回对应的 Node 信息;&lt;/li&gt;
&lt;li&gt;–cluster-dns 指定 kubedns 的 Service IP(可以先分配，后续创建 kubedns 服务时指定该 IP)，–cluster-domain 指定域名后缀，这两个参数同时指定后才会生效。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;启动kubelet&quot;&gt;&lt;a href=&quot;#启动kubelet&quot; class=&quot;headerlink&quot; title=&quot;启动kubelet&quot;&gt;&lt;/a&gt;启动kubelet&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable kubelet
# systemctl start kubelet
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;通过-kubelet-的-TLS-证书请求&quot;&gt;&lt;a href=&quot;#通过-kubelet-的-TLS-证书请求&quot; class=&quot;headerlink&quot; title=&quot;通过 kubelet 的 TLS 证书请求&quot;&gt;&lt;/a&gt;通过 kubelet 的 TLS 证书请求&lt;/h4&gt;&lt;p&gt;kubelet 首次启动时向 kube-apiserver 发送证书签名请求，必须通过后 kubernetes 系统才会将该 Node 加入到集群。&lt;/p&gt;
&lt;p&gt;1.查看未授权的 CSR 请求&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get csr 
NAME        AGE       REQUESTOR           CONDITION
csr-fv3bj   49s       kubelet-bootstrap   Pending
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.通过 CSR 请求&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl certificate approve csr-fv3bj
certificatesigningrequest &amp;quot;csr-fv3bj&amp;quot; approved
[root@node1 kubernetes]# kubectl get csr
NAME        AGE       REQUESTOR           CONDITION
csr-fv3bj   42m       kubelet-bootstrap   Approved,Issued
# kubectl get nodes
NAME           STATUS    AGE       VERSION
172.16.7.151   Ready     18s       v1.6.0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3.查看自动生成的 kubelet kubeconfig 文件和公私钥&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 kubernetes]# ls -l /etc/kubernetes/kubelet.kubeconfig
-rw-------. 1 root root 2215 Sep 13 09:04 /etc/kubernetes/kubelet.kubeconfig
[root@node1 kubernetes]# ls -l /etc/kubernetes/ssl/kubelet*
-rw-r--r--. 1 root root 1046 Sep 13 09:04 /etc/kubernetes/ssl/kubelet-client.crt
-rw-------. 1 root root  227 Sep 13 09:02 /etc/kubernetes/ssl/kubelet-client.key
-rw-r--r--. 1 root root 1111 Sep 13 09:04 /etc/kubernetes/ssl/kubelet.crt
-rw-------. 1 root root 1675 Sep 13 09:04 /etc/kubernetes/ssl/kubelet.key
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在集群中其它的kubernetes node节点上操作完成后，查看集群kubernetes node情况如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get csr
NAME        AGE       REQUESTOR           CONDITION
csr-5n72m   3m        kubelet-bootstrap   Approved,Issued
csr-clwzj   16m       kubelet-bootstrap   Approved,Issued
csr-fv3bj   4h        kubelet-bootstrap   Approved,Issued
# kubectl get nodes
NAME           STATUS    AGE       VERSION
172.16.7.151   Ready     4h        v1.6.0
172.16.7.152   Ready     6m        v1.6.0
172.16.7.153   Ready     12s       v1.6.0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【问题】：切记每台node节点上的kubelet配置文件/etc/kubernetes/kubelet中的ip地址要改正确，否则会出现加入不了的情况。我在将node1节点的/etc/kubernetes/kubelet远程复制到node2节点上，没有修改ip，直接启动了，配置文件中写的ip地址是node1的ip地址，这就造成了node2节点并没有加入进来。采取的恢复操作是：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node2 ~]# systemctl stop kubelet
[root@node2 ~]# cd /etc/kubernetes
[root@node2 kubernetes]# rm -f kubelet.kubeconfig
[root@node2 kubernetes]# rm -rf ~/.kube/cache
# 修改/etc/kubernetes/kubelet中的ip地址
[root@node2 kubernetes]# vim /etc/kubernetes/kubelet
[root@node2 ~]# systemctl start kubelet
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这样，再次启动kubelet时，kube-apiserver才收到证书签名请求。&lt;/p&gt;
&lt;h3 id=&quot;配置-kube-proxy&quot;&gt;&lt;a href=&quot;#配置-kube-proxy&quot; class=&quot;headerlink&quot; title=&quot;配置 kube-proxy&quot;&gt;&lt;/a&gt;配置 kube-proxy&lt;/h3&gt;&lt;p&gt;上面已经把kube-proxy复制到了kubernetes node节点的/usr/local/bin/目录下了，下面开始做配置。每台kubernetes node节点都需要做如下的操作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.创建 kube-proxy 的service配置文件&lt;/strong&gt;&lt;br&gt;在/usr/lib/systemd/system/目录下创建kube-proxy.service：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /usr/lib/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/proxy
ExecStart=/usr/local/bin/kube-proxy \
            $KUBE_LOGTOSTDERR \
            $KUBE_LOG_LEVEL \
            $KUBE_MASTER \
            $KUBE_PROXY_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;2.创建kube-proxy配置文件/etc/kubernetes/proxy&lt;/strong&gt;&lt;br&gt;【注意】：需要修改每台kubernetes node的ip地址。以下以node1主机为例：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/kubernetes/proxy
###
# kubernetes proxy config

# default config should be adequate

# Add your own!
KUBE_PROXY_ARGS=&amp;quot;--bind-address=172.16.7.151 --hostname-override=172.16.7.151 --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig --cluster-cidr=10.254.0.0/16&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;–hostname-override 参数值必须与 kubelet 的值一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 iptables 规则；&lt;/li&gt;
&lt;li&gt;kube-proxy 根据 –cluster-cidr 判断集群内部和外部流量，指定 –cluster-cidr 或 –masquerade-all 选项后 kube-proxy 才会对访问 Service IP 的请求做 SNAT；&lt;/li&gt;
&lt;li&gt;–kubeconfig 指定的配置文件嵌入了 kube-apiserver 的地址、用户名、证书、秘钥等请求和认证信息；&lt;/li&gt;
&lt;li&gt;预定义的 RoleBinding cluster-admin 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;3.启动 kube-proxy&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable kube-proxy
# systemctl start kube-proxy
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;验证测试&quot;&gt;&lt;a href=&quot;#验证测试&quot; class=&quot;headerlink&quot; title=&quot;验证测试&quot;&gt;&lt;/a&gt;验证测试&lt;/h3&gt;&lt;p&gt;创建一个niginx的service试一下集群是否可用。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl run nginx --replicas=2 --labels=&amp;quot;run=load-balancer-example&amp;quot; --image=docker.io/nginx:latest --port=80
deployment &amp;quot;nginx&amp;quot; created                   
# kubectl expose deployment nginx --type=NodePort --name=example-service     
service &amp;quot;example-service&amp;quot; exposed
# kubectl describe svc example-service
Name:                   example-service
Namespace:              default
Labels:                 run=load-balancer-example
Annotations:            &amp;lt;none&amp;gt;
Selector:               run=load-balancer-example
Type:                   NodePort
IP:                     10.254.67.61
Port:                   &amp;lt;unset&amp;gt; 80/TCP
NodePort:               &amp;lt;unset&amp;gt; 32201/TCP
Endpoints:              172.30.32.2:80,172.30.87.2:80
Session Affinity:       None
Events:                 &amp;lt;none&amp;gt;

# kubectl get all
NAME                        READY     STATUS    RESTARTS   AGE
po/nginx-1931613429-nlsj1   1/1       Running   0          5m
po/nginx-1931613429-xr7zk   1/1       Running   0          5m

NAME                  CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
svc/example-service   10.254.67.61   &amp;lt;nodes&amp;gt;       80:32201/TCP   1m
svc/kubernetes        10.254.0.1     &amp;lt;none&amp;gt;        443/TCP        5h

NAME           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deploy/nginx   2         2         2            2           5m

NAME                  DESIRED   CURRENT   READY     AGE
rs/nginx-1931613429   2         2         2         5m

# curl &amp;quot;10.254.67.61:80&amp;quot; 
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&amp;lt;style&amp;gt;
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
&amp;lt;/style&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;p&amp;gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;For online documentation and support please refer to
&amp;lt;a href=&amp;quot;http://nginx.org/&amp;quot;&amp;gt;nginx.org&amp;lt;/a&amp;gt;.&amp;lt;br/&amp;gt;
Commercial support is available at
&amp;lt;a href=&amp;quot;http://nginx.com/&amp;quot;&amp;gt;nginx.com&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;&amp;lt;em&amp;gt;Thank you for using nginx.&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;浏览器输入172.16.7.151:32201或172.16.7.152:32201或者172.16.7.153:32201都可以得到nginx的页面。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;查看运行的容器（在node1和node2上分别运行了一个pod）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# docker ps
CONTAINER ID        IMAGE                                                                                     COMMAND                  CREATED             STATUS              PORTS               NAMES
7d2ef8e34e43        docker.io/nginx@sha256:fc6d2ef47e674a9ffb718b7ac361ec4e421e3a0ef2c93df79abbe4e9ffb5fa08   &amp;quot;nginx -g &amp;apos;daemon off&amp;quot;   40 minutes ago      Up 40 minutes                           k8s_nginx_nginx-1931613429-xr7zk_default_c628f12f-9912-11e7-9acc-005056b7609a_0
5bbb98fba623        registry.access.redhat.com/rhel7/pod-infrastructure                                       &amp;quot;/usr/bin/pod&amp;quot;           42 minutes ago      Up 42 minutes                           k8s_POD_nginx-1931613429-xr7zk_default_c628f12f-9912-11e7-9acc-005056b7609a_0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果想删除刚才创建的deployment：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get deployments
NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx     2         2         2            0           2m
# kubectl delete deployment nginx
deployment &amp;quot;nginx&amp;quot; deleted
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;安装和配置-kube-dns-插件&quot;&gt;&lt;a href=&quot;#安装和配置-kube-dns-插件&quot; class=&quot;headerlink&quot; title=&quot;安装和配置 kube-dns 插件&quot;&gt;&lt;/a&gt;安装和配置 kube-dns 插件&lt;/h2&gt;&lt;h3 id=&quot;kube-dns是什么&quot;&gt;&lt;a href=&quot;#kube-dns是什么&quot; class=&quot;headerlink&quot; title=&quot;kube-dns是什么&quot;&gt;&lt;/a&gt;kube-dns是什么&lt;/h3&gt;&lt;p&gt;刚才在上一步中创建了个Nginx deployment，得到了两个运行nginx服务的Pod。待Pod运行之后查看一下它们的IP，并在k8s集群内通过podIP和containerPort来访问Nginx服务。&lt;br&gt;获取Pod IP：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get pod -o yaml -l run=load-balancer-example|grep podIP 
    podIP: 172.30.32.2
    podIP: 172.30.87.2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后在Kubernetes集群的任一节点上就可以通过podIP在k8s集群内访问Nginx服务了。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# curl &amp;quot;172.30.32.2:80&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;但是这样存在几个问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每次收到获取podIP太扯了，总不能每次都要手动改程序或者配置才能访问服务吧，要怎么提前知道podIP呢？&lt;/li&gt;
&lt;li&gt;Pod在运行中可能会重建，Pod的IP地址会随着Pod的重启而变化,并 不建议直接拿Pod的IP来交互&lt;/li&gt;
&lt;li&gt;如何在多个Pod中实现负载均衡嘞？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;使用k8s Service就可以解决。Service为一组Pod(通过labels来选择)提供一个统一的入口，并为它们提供负载均衡和自动服务发现。&lt;br&gt;所以紧接着就创建了个service：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl expose deployment nginx --type=NodePort --name=example-service
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;创建之后，仍需要获取Service的Cluster-IP，再结合Port访问Nginx服务。&lt;br&gt;获取IP：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get service example-service
NAME              CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
example-service   10.254.67.61   &amp;lt;nodes&amp;gt;       80:32201/TCP   1h
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在集群内访问Service：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# curl &amp;quot;10.254.67.61:80&amp;quot; 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;而在Kubernetes cluster外面，则只能通过&lt;a href=&quot;http://node-ip:32201来访问。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://node-ip:32201来访问。&lt;/a&gt;&lt;br&gt;虽然Service解决了Pod的服务发现和负载均衡问题，但存在着类似的问题：不提前知道Service的IP，还是需要改程序或配置啊。kube-dns就是用来解决上面这个问题的。&lt;br&gt;kube-dns可以解决Service的发现问题，k8s将Service的名称当做域名注册到kube-dns中，通过Service的名称就可以访问其提供的服务。也就是说其他应用能够直接使用服务的名字，不需要关心它实际的 ip 地址，中间的转换能够自动完成。名字和 ip 之间的转换就是 DNS 系统的功能。&lt;br&gt;kubu-dns 服务不是独立的系统服务，而是一种 addon ，作为插件来安装的，不是 kubernetes 集群必须的（但是非常推荐安装）。可以把它看做运行在集群上的应用，只不过这个应用比较特殊而已。&lt;/p&gt;
&lt;h3 id=&quot;安装配置kube-dns&quot;&gt;&lt;a href=&quot;#安装配置kube-dns&quot; class=&quot;headerlink&quot; title=&quot;安装配置kube-dns&quot;&gt;&lt;/a&gt;安装配置kube-dns&lt;/h3&gt;&lt;p&gt;官方的yaml文件目录：&lt;a href=&quot;https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns。&lt;/a&gt;&lt;br&gt;kube-dns 有两种配置方式，在 1.3 之前使用 etcd + kube2sky + skydns 的方式，在 1.3 之后可以使用 kubedns + dnsmasq 的方式。&lt;br&gt;该插件直接使用kubernetes部署，实际上kube-dns插件只是运行在kube-system命名空间下的Pod，完全可以手动创建它。官方的配置文件中包含以下镜像：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.1
gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.1
gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.1
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;下载yaml文件&quot;&gt;&lt;a href=&quot;#下载yaml文件&quot; class=&quot;headerlink&quot; title=&quot;下载yaml文件&quot;&gt;&lt;/a&gt;下载yaml文件&lt;/h4&gt;&lt;p&gt;从 &lt;a href=&quot;https://github.com/rootsongjc/follow-me-install-kubernetes-cluster/tree/master/manifests/kubedns&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/rootsongjc/follow-me-install-kubernetes-cluster/tree/master/manifests/kubedns&lt;/a&gt; 下载 kubedns-cm.yaml、kubedns-sa.yaml、kubedns-controller.yaml和kubedns-svc.yaml这4个文件下来，并上传到/opt/kube-dns/目录下。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# mkdir /opt/kube-dns
# cd /opt/kube-dns/
# ls kubedns-*
kubedns-cm.yaml  kubedns-controller.yaml  kubedns-sa.yaml  kubedns-svc.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;修改kubedns-controller.yaml文件，将其中的镜像地址改为时速云的地址：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;index.tenxcloud.com/jimmy/k8s-dns-dnsmasq-nanny-amd64:1.14.1
index.tenxcloud.com/jimmy/k8s-dns-kube-dns-amd64:1.14.1
index.tenxcloud.com/jimmy/k8s-dns-sidecar-amd64:1.14.1
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;kubeDNS：提供了原来 kube2sky + etcd + skyDNS 的功能，可以单独对外提供 DNS 查询服务&lt;/li&gt;
&lt;li&gt;dnsmasq： 一个轻量级的 DNS 服务软件，可以提供 DNS 缓存功能。kubeDNS 模式下，dnsmasq 在内存中预留一块大小（默认是 1G）的地方，保存当前最常用的 DNS 查询记录，如果缓存中没有要查找的记录，它会到 kubeDNS 中查询，并把结果缓存起来。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;系统预定义的-RoleBinding&quot;&gt;&lt;a href=&quot;#系统预定义的-RoleBinding&quot; class=&quot;headerlink&quot; title=&quot;系统预定义的 RoleBinding&quot;&gt;&lt;/a&gt;系统预定义的 RoleBinding&lt;/h4&gt;&lt;p&gt;预定义的 RoleBinding system:kube-dns 将 kube-system 命名空间的 kube-dns ServiceAccount 与 system:kube-dns Role 绑定， 该 Role 具有访问 kube-apiserver DNS 相关 API 的权限。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get clusterrolebindings system:kube-dns -o yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: &amp;quot;true&amp;quot;
  creationTimestamp: 2017-09-14T00:46:08Z
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kube-dns
  resourceVersion: &amp;quot;56&amp;quot;
  selfLink: /apis/rbac.authorization.k8s.io/v1beta1/clusterrolebindingssystem%3Akube-dns
  uid: 18fa2aff-98e6-11e7-a153-005056b7609a
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kube-dns
subjects:
- kind: ServiceAccount
  name: kube-dns
  namespace: kube-system
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;kubedns-controller.yaml 中定义的 Pods 时使用了 kubedns-sa.yaml 文件定义的 kube-dns ServiceAccount，所以具有访问 kube-apiserver DNS 相关 API 的权限。&lt;/p&gt;
&lt;h4 id=&quot;配置-kube-dns-ServiceAccount&quot;&gt;&lt;a href=&quot;#配置-kube-dns-ServiceAccount&quot; class=&quot;headerlink&quot; title=&quot;配置 kube-dns ServiceAccount&quot;&gt;&lt;/a&gt;配置 kube-dns ServiceAccount&lt;/h4&gt;&lt;p&gt;无需修改。&lt;/p&gt;
&lt;h4 id=&quot;配置-kube-dns-服务&quot;&gt;&lt;a href=&quot;#配置-kube-dns-服务&quot; class=&quot;headerlink&quot; title=&quot;配置 kube-dns 服务&quot;&gt;&lt;/a&gt;配置 kube-dns 服务&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# diff kubedns-svc.yaml.base kubedns-svc.yaml
30c30
&amp;lt;   clusterIP: __PILLAR__DNS__SERVER__
---
&amp;gt;   clusterIP: 10.254.0.2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;spec.clusterIP = 10.254.0.2，即明确指定了 kube-dns Service IP，这个 IP 需要和 kubelet 的 –cluster-dns 参数值一致。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;配置-kube-dns-Deployment&quot;&gt;&lt;a href=&quot;#配置-kube-dns-Deployment&quot; class=&quot;headerlink&quot; title=&quot;配置 kube-dns Deployment&quot;&gt;&lt;/a&gt;配置 kube-dns Deployment&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# diff kubedns-controller.yaml.base kubedns-controller.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用系统已经做了 RoleBinding 的 kube-dns ServiceAccount，该账户具有访问 kube-apiserver DNS 相关 API 的权限。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;执行所有定义文件&quot;&gt;&lt;a href=&quot;#执行所有定义文件&quot; class=&quot;headerlink&quot; title=&quot;执行所有定义文件&quot;&gt;&lt;/a&gt;执行所有定义文件&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# pwd
/opt/kube-dns
# ls
kubedns-cm.yaml  kubedns-controller.yaml  kubedns-sa.yaml  kubedns-svc.yaml
# kubectl create -f .
configmap &amp;quot;kube-dns&amp;quot; created
deployment &amp;quot;kube-dns&amp;quot; created
serviceaccount &amp;quot;kube-dns&amp;quot; created
service &amp;quot;kube-dns&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在3台node节点上查看生成的kube-dns相关pod和container：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node2 ~]# docker ps
CONTAINER ID        IMAGE                                                                                                                           COMMAND                  CREATED             STATUS              PORTS               NAMES
9b1dbfde7eac        index.tenxcloud.com/jimmy/k8s-dns-sidecar-amd64@sha256:947271f3e08b1fd61c4b26478f08d3a8f10bbca90d4dec067e3b33be08066970         &amp;quot;/sidecar --v=2 --log&amp;quot;   4 hours ago         Up 4 hours                              k8s_sidecar_kube-dns-351402727-6vnsj_kube-system_efb96c05-9928-11e7-9acc-005056b7609a_0
a455dc0a9b55        index.tenxcloud.com/jimmy/k8s-dns-dnsmasq-nanny-amd64@sha256:b253876345427dbd626b145897be51d87bfd535e2cd5d7d166deb97ea37701f8   &amp;quot;/dnsmasq-nanny -v=2 &amp;quot;   4 hours ago         Up 4 hours                              k8s_dnsmasq_kube-dns-351402727-6vnsj_kube-system_efb96c05-9928-11e7-9acc-005056b7609a_0
7f18c10c8d60        index.tenxcloud.com/jimmy/k8s-dns-kube-dns-amd64@sha256:94426e872d1a4a0cf88e6c5cd928a1acbe1687871ae5fe91ed751593aa6052d3        &amp;quot;/kube-dns --domain=c&amp;quot;   4 hours ago         Up 4 hours                              k8s_kubedns_kube-dns-351402727-6vnsj_kube-system_efb96c05-9928-11e7-9acc-005056b7609a_0
a6feb213296b        registry.access.redhat.com/rhel7/pod-infrastructure                                                                             &amp;quot;/usr/bin/pod&amp;quot;           4 hours ago         Up 4 hours                              k8s_POD_kube-dns-351402727-6vnsj_kube-system_efb96c05-9928-11e7-9acc-005056b7609a_0
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;检查-kube-dns-功能&quot;&gt;&lt;a href=&quot;#检查-kube-dns-功能&quot; class=&quot;headerlink&quot; title=&quot;检查 kube-dns 功能&quot;&gt;&lt;/a&gt;检查 kube-dns 功能&lt;/h3&gt;&lt;p&gt;上面是通过 kubectl run 来启动了第一个Pod，但是并不支持所有的功能。使用kubectl run在设定很复杂的时候需要非常长的一条语句，敲半天也很容易出错，也没法保存，在碰到转义字符的时候也经常会很抓狂，所以更多场景下会使用yaml或者json文件，而使用kubectl create或者delete就可以利用这些yaml文件。通过 kubectl create -f file.yaml 来创建资源。kubectl run 并不是直接创建一个Pod，而是先创建一个Deployment资源 (replicas=1)，再由Deployment来自动创建Pod。&lt;/p&gt;
&lt;p&gt;新建一个 Deployment：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 kube-dns]# vim my-nginx.yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: my-nginx
spec:
  replicas: 2
  template:
    metadata:
      labels:
        run: my-nginx
    spec:
      containers:
      - name: my-nginx
        image: docker.io/nginx:latest                                  
        ports:
        - containerPort: 80
# kubectl create -f my-nginx.yaml
deployment &amp;quot;my-nginx&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Export 该 Deployment，生成 my-nginx 服务:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl expose deploy my-nginx 
service &amp;quot;my-nginx&amp;quot; exposed
# kubectl get services --all-namespaces |grep my-nginx
default       my-nginx          10.254.34.181   &amp;lt;none&amp;gt;        80/TCP          26s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;创建另一个 Pod，查看 /etc/resolv.conf 是否包含 kubelet 配置的 –cluster-dns 和 –cluster-domain，是否能够将服务my-nginx 解析到 Cluster IP 10.254.34.181。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 kube-dns]# vim dns-test-busybox.yaml
apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: default
spec:
  containers:
  - image: busybox
    command:
      - sleep
      - &amp;quot;3600&amp;quot;
    imagePullPolicy: IfNotPresent
    name: busybox
  restartPolicy: Always
[root@node1 kube-dns]# kubectl create -f dns-test-busybox.yaml
pod &amp;quot;busybox&amp;quot; created
[root@node1 kube-dns]# kubectl exec -ti busybox -- nslookup kubernetes.default
Server:    10.254.0.2
Address 1: 10.254.0.2 kube-dns.kube-system.svc.cluster.local

Name:      kubernetes.default
Address 1: 10.254.0.1 kubernetes.default.svc.cluster.local

kubectl exec -ti busybox -- ping my-nginx
PING my-nginx (10.254.34.181): 56 data bytes

kubectl exec -ti busybox -- ping kubernetes
PING kubernetes (10.254.0.1): 56 data bytes

kubectl exec -ti busybox -- ping kube-dns.kube-system.svc.cluster.local
PING kube-dns.kube-system.svc.cluster.local (10.254.0.2): 56 data bytes
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;从结果来看，service名称可以正常解析。&lt;br&gt;另外，使用kubernetes的时候建议不要再用docker命令操作。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Kubernetes简介&quot;&gt;&lt;a href=&quot;#Kubernetes简介&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes简介&quot;&gt;&lt;/a&gt;Kubernetes简介&lt;/h2&gt;&lt;p&gt;Kubernetes是谷歌开源的容器集群管理系统，是Google多年大规模容器管理技术Borg的 开源版本，主要功能包括:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于容器的应用部署、维护和滚动升级&lt;/li&gt;
&lt;li&gt;负载均衡和服务发现&lt;/li&gt;
&lt;li&gt;跨机器和跨地区的集群调度
    
    </summary>
    
      <category term="容器编排" scheme="http://yoursite.com/categories/%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92/"/>
    
    
      <category term="Kubernetes" scheme="http://yoursite.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>企业级Docker Registry —— Harbor搭建和使用</title>
    <link href="http://yoursite.com/2017/09/08/%E4%BC%81%E4%B8%9A%E7%BA%A7Docker-Registry-%E2%80%94%E2%80%94-Harbor%E6%90%AD%E5%BB%BA%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    <id>http://yoursite.com/2017/09/08/企业级Docker-Registry-——-Harbor搭建和使用/</id>
    <published>2017-09-08T08:38:52.000Z</published>
    <updated>2017-11-12T09:12:33.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;Harbor介绍&quot;&gt;&lt;a href=&quot;#Harbor介绍&quot; class=&quot;headerlink&quot; title=&quot;Harbor介绍&quot;&gt;&lt;/a&gt;Harbor介绍&lt;/h2&gt;&lt;p&gt;Harbor是由VMWare公司开源的容器镜像仓库。事实上，Habor是在Docker Registry上进行了相应的企业级扩展，从而获得了更加广泛的应用，这些新的企业级特性包括：管理用户界面，基于角色的访问控制 ，AD/LDAP集成以及审计日志等。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;安装部署Harbor&quot;&gt;&lt;a href=&quot;#安装部署Harbor&quot; class=&quot;headerlink&quot; title=&quot;安装部署Harbor&quot;&gt;&lt;/a&gt;安装部署Harbor&lt;/h2&gt;&lt;p&gt;官方安装文档：&lt;br&gt;&lt;a href=&quot;https://github.com/vmware/harbor/blob/master/docs/installation_guide.md&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/vmware/harbor/blob/master/docs/installation_guide.md&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;环境要求&quot;&gt;&lt;a href=&quot;#环境要求&quot; class=&quot;headerlink&quot; title=&quot;环境要求&quot;&gt;&lt;/a&gt;环境要求&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;python 2.7+&lt;/li&gt;
&lt;li&gt;docker 1.10+&lt;/li&gt;
&lt;li&gt;docker-compose 1.6.0+&lt;br&gt;Docker的安装见文章《Docker镜像和容器》和docker-compose的安装见文章《Registry私有仓库搭建及认证》。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;环境信息&quot;&gt;&lt;a href=&quot;#环境信息&quot; class=&quot;headerlink&quot; title=&quot;环境信息&quot;&gt;&lt;/a&gt;环境信息&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;操作系统版本&lt;/th&gt;
&lt;th&gt;IP地址&lt;/th&gt;
&lt;th&gt;安装软件&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;spark32&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.206.32&lt;/td&gt;
&lt;td&gt;docker-ce 17.06.1、docker-compose 1.15.0、harbor-online-installer-v1.1.2.tar&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&quot;安装部署harbor&quot;&gt;&lt;a href=&quot;#安装部署harbor&quot; class=&quot;headerlink&quot; title=&quot;安装部署harbor&quot;&gt;&lt;/a&gt;安装部署harbor&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# cd /opt/
[root@spark32 opt]# mkdir harbor
[root@spark32 opt]# cd harbor/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;下载地址：&lt;a href=&quot;https://github.com/vmware/harbor/releases&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/vmware/harbor/releases&lt;/a&gt;&lt;br&gt;由于我这里服务器可以联网，离线版本又很大，所以下载的在线安装版本的1.1.2版本，将软件上传到/opt/harbor/目录下。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 harbor]# tar xvf harbor-online-installer-v1.1.2.tar 
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;配置harbor&quot;&gt;&lt;a href=&quot;#配置harbor&quot; class=&quot;headerlink&quot; title=&quot;配置harbor&quot;&gt;&lt;/a&gt;配置harbor&lt;/h3&gt;&lt;p&gt;可配置的参数在文件harbor.cfg中。在该文件中，有两种参数，必须配置的和可选的。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;必须的：这些参数会在用户更新配置文件和执行install.sh脚本重新安装harbor起作用。你至少得设置 hostname 参数。&lt;/li&gt;
&lt;li&gt;可选的：用户可以就使用默认值，或者在启动后通过web ui进行修改。如果这些可选参数被设置在harbor.cfg文件中，它们只会在第一次启动时起作用，以后在harbor.cfg文件中修改会被忽略。也就是说在 Harbor 初次启动时，Admin Server 从 harbor.cfg 文件读取配置并记录下来。之后重新启动Harbor的过程中，只有必需的配置会从 harbor.cfg 文件读取；其他可选的配置将不再生效，需要通过 Admin Server 的管理界面来修改。&lt;br&gt;&lt;strong&gt;【注意】:如果你通过web ui设置这些参数，请在harbor启动后立即设置。尤其是，必须先设置 auth_mode 在你注册或创建任何用户之前。&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些具体参数的名字和意义详见&lt;a href=&quot;https://github.com/vmware/harbor/blob/master/docs/installation_guide.md&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/vmware/harbor/blob/master/docs/installation_guide.md&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 harbor]# cd harbor/
[root@spark32 harbor]# vim harbor.cfg
# 指定 hostname，为IP或者域名，用于登录 Web UI 界面
hostname = 172.16.206.32
# mysql 数据库 root 账户密码
db_password = wisedu123
# 邮件相关信息配置，如忘记密码发送邮件
email_server = smtp.exmail.qq.com
email_server_port = 465
email_username = 01115004@wisedu.com
email_password = zjk230640
email_from = admin &amp;lt;01115004@wisedu.com&amp;gt;
email_ssl = on
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;配置存储&quot;&gt;&lt;a href=&quot;#配置存储&quot; class=&quot;headerlink&quot; title=&quot;配置存储&quot;&gt;&lt;/a&gt;配置存储&lt;/h3&gt;&lt;p&gt;默认情况下，harbor把镜像存储在本地文件系统，在生产环境中，你可能考虑用其他的存储替代本地存储，比如S3, Openstack Swift, Ceph等等。你需要修改 common/templates/registry/config.yml 中的 storage 段。比如使用Openstack Swift，storage段的配置类似如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;storage:
  swift:
    username: admin
    password: ADMIN_PASS
    authurl: http://keystone_addr:35357/v3/auth
    tenant: admin
    domain: default
    region: regionOne
    container: docker_images
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;更详细的存储配置，见 &lt;a href=&quot;https://docs.docker.com/registry/configuration/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Registry Configuration Reference&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;完成安装和启动harbor&quot;&gt;&lt;a href=&quot;#完成安装和启动harbor&quot; class=&quot;headerlink&quot; title=&quot;完成安装和启动harbor&quot;&gt;&lt;/a&gt;完成安装和启动harbor&lt;/h3&gt;&lt;p&gt;一旦harbor.cfg和后端存储(可选的)配置完成，就可以使用install.sh脚本安装和启动harbor了。注意在线安装可能需要等待一些时间从dockerhubs下载harbor镜像。&lt;br&gt;&lt;strong&gt;【注意】：请确保主机上80和443端口没被占用。如果想修改端口，需要去修改docker-compose.yml文件&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 harbor]# pwd
/opt/harbor/harbor
[root@spark32 harbor]# ./install.sh 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我这里配置了阿里云容器加速，所以在线版安装没有问题，如果有网络问题可以选择离线包安装。&lt;/p&gt;
&lt;h2 id=&quot;访问Harbor&quot;&gt;&lt;a href=&quot;#访问Harbor&quot; class=&quot;headerlink&quot; title=&quot;访问Harbor&quot;&gt;&lt;/a&gt;访问Harbor&lt;/h2&gt;&lt;p&gt;浏览器输入&lt;a href=&quot;http://172.16.206.32&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.206.32&lt;/a&gt;&lt;br&gt;默认账号密码是admin/Harbor12345。默认是80端口，如果端口占用，我们可以去修改docker-compose.yml文件中，对应服务的端口映射。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/36.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/37.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;我们可以看到系统各个模块如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;项目：新增/删除项目，查看镜像仓库，给项目添加成员、查看操作日志、复制项目等&lt;/li&gt;
&lt;li&gt;日志：仓库各个镜像create、push、pull等操作日志&lt;/li&gt;
&lt;li&gt;系统管理 &lt;ul&gt;
&lt;li&gt;用户管理：新增/删除用户、设置管理员等&lt;/li&gt;
&lt;li&gt;复制管理：新增/删除从库目标、新建/删除/启停复制规则等&lt;/li&gt;
&lt;li&gt;配置管理：认证模式、复制、邮箱设置、系统设置等&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;其他设置 &lt;ul&gt;
&lt;li&gt;用户设置：修改用户名、邮箱、名称信息&lt;/li&gt;
&lt;li&gt;修改密码：修改用户密码&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;注意：非系统管理员用户登录，只能看到有权限的项目和日志，其他模块不可见。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;修改管理员密码&quot;&gt;&lt;a href=&quot;#修改管理员密码&quot; class=&quot;headerlink&quot; title=&quot;修改管理员密码&quot;&gt;&lt;/a&gt;修改管理员密码&lt;/h3&gt;&lt;p&gt;点击右上角的admin，点击修改密码。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/38.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;启动后相关容器&quot;&gt;&lt;a href=&quot;#启动后相关容器&quot; class=&quot;headerlink&quot; title=&quot;启动后相关容器&quot;&gt;&lt;/a&gt;启动后相关容器&lt;/h3&gt;&lt;p&gt;Harbor的所有服务组件都是在Docker中部署的。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 harbor]# ls
common  docker-compose.notary.yml  docker-compose.yml  harbor_1_1_0_template  harbor.cfg  install.sh  LICENSE  NOTICE  prepare  upgrade
[root@spark32 harbor]# docker-compose ps
       Name                     Command               State                                Ports                               
------------------------------------------------------------------------------------------------------------------------------
harbor-adminserver   /harbor/harbor_adminserver       Up                                                                       
harbor-db            docker-entrypoint.sh mysqld      Up      3306/tcp                                                         
harbor-jobservice    /harbor/harbor_jobservice        Up                                                                       
harbor-log           /bin/sh -c crond &amp;amp;&amp;amp; rm -f  ...   Up      127.0.0.1:1514-&amp;gt;514/tcp                                          
harbor-ui            /harbor/harbor_ui                Up                                                                       
nginx                nginx -g daemon off;             Up      0.0.0.0:443-&amp;gt;443/tcp, 0.0.0.0:4443-&amp;gt;4443/tcp, 0.0.0.0:80-&amp;gt;80/tcp 
registry             /entrypoint.sh serve /etc/ ...   Up      5000/tcp 
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;harbor-adminserver：用来管理系统配置，并提供了相应的 WEB 页面和 API 来供用户操作，改进了之前需用户手动修改配置文件并手动重启系统的用户体验。&lt;/li&gt;
&lt;li&gt;harbor-db : 由官方MySql镜像构成的数据库容器&lt;/li&gt;
&lt;li&gt;harbor-jobservice：是harbor的job管理模块，job在harbor里面主要是为了镜像仓库同步使用的。&lt;/li&gt;
&lt;li&gt;harbor-log : 运行着rsyslogd的容器，通过log-driver的形式收集其他容器的日志&lt;/li&gt;
&lt;li&gt;harbor-ui : 即架构中的core services, 构成此容器的代码是Harbor项目的主体&lt;/li&gt;
&lt;li&gt;nginx : 由 nginx 服务器构成的反向代理&lt;/li&gt;
&lt;li&gt;registry : 由Docker官方的开源 registry 镜像构成的容器实例&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这几个 Contianer 通过 Docker link 的形式连接在一起，在容器之间通过容器名字互相访问。对终端用户而言，只需要暴露 proxy（即Nginx）的服务端口。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;【注意】:之前的版本更新配置，需要修改harbor.cfg，然后停止并删除现有Harbor实例，再重新运行Harbor，比较繁琐。新版本的adminconsole可以使用户很方便地通过WEB界面配置认证、同步、邮件和系统等信息，修改立即生效，无需重启整个系统。&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&quot;Harbor持久化数据和日志&quot;&gt;&lt;a href=&quot;#Harbor持久化数据和日志&quot; class=&quot;headerlink&quot; title=&quot;Harbor持久化数据和日志&quot;&gt;&lt;/a&gt;Harbor持久化数据和日志&lt;/h2&gt;&lt;p&gt;默认情况下，registrys数据被持久化在宿主机的/data/目录下，甚至你删除harbor容器或者重新被创建，这部分数据也不会改变。&lt;br&gt;另外，harbor使用rsyslog来收集每一个容器日志，默认情况下，这些日志存放在宿主机的/var/log/harbor/目录下。&lt;/p&gt;
&lt;h2 id=&quot;管理Harbor的生命&quot;&gt;&lt;a href=&quot;#管理Harbor的生命&quot; class=&quot;headerlink&quot; title=&quot;管理Harbor的生命&quot;&gt;&lt;/a&gt;管理Harbor的生命&lt;/h2&gt;&lt;p&gt;可以使用docker-compose来管理harbor的启动、停止和销毁。但是注意必须切换到docker-compose.yml同级目录运行以下的命令。&lt;/p&gt;
&lt;h3 id=&quot;停止harbor&quot;&gt;&lt;a href=&quot;#停止harbor&quot; class=&quot;headerlink&quot; title=&quot;停止harbor&quot;&gt;&lt;/a&gt;停止harbor&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;# docker-compose stop
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;启动harbor&quot;&gt;&lt;a href=&quot;#启动harbor&quot; class=&quot;headerlink&quot; title=&quot;启动harbor&quot;&gt;&lt;/a&gt;启动harbor&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;# docker-compose start
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;修改配置后启动&quot;&gt;&lt;a href=&quot;#修改配置后启动&quot; class=&quot;headerlink&quot; title=&quot;修改配置后启动&quot;&gt;&lt;/a&gt;修改配置后启动&lt;/h3&gt;&lt;p&gt;先停止harbor，在修改配置文件harbor.cfg，然后运行prepare脚本应用配置，最后重新创建harbor并运行它。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# docker-compose down -v
# vim harbor.cfg
# prepare
# docker-compose up -d
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;清除harbor容器，保留镜像和数据&quot;&gt;&lt;a href=&quot;#清除harbor容器，保留镜像和数据&quot; class=&quot;headerlink&quot; title=&quot;清除harbor容器，保留镜像和数据&quot;&gt;&lt;/a&gt;清除harbor容器，保留镜像和数据&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;# docker-compose down -v
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;删除harbors数据库和镜像-用于干净的重新安装&quot;&gt;&lt;a href=&quot;#删除harbors数据库和镜像-用于干净的重新安装&quot; class=&quot;headerlink&quot; title=&quot;删除harbors数据库和镜像(用于干净的重新安装)&quot;&gt;&lt;/a&gt;删除harbors数据库和镜像(用于干净的重新安装)&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;# rm -r /data/database
# rm -r /data/registry
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;Harbor的安全机制&quot;&gt;&lt;a href=&quot;#Harbor的安全机制&quot; class=&quot;headerlink&quot; title=&quot;Harbor的安全机制&quot;&gt;&lt;/a&gt;Harbor的安全机制&lt;/h2&gt;&lt;p&gt;企业中的软件研发团队往往划分为诸多角色，如项目经理、产品经理、测试、运维等。在实际的软件开发和运维过程中，这些角色对于镜像的使用需求是不一样的。比如：开发人员需要拥有对镜像的读写(PULL/PUSH)权限以更新和改正代码；测试人员中需要读取(PULL)权限；而项目经理需要对上述的角色进行管理。&lt;br&gt;Harbor为这种需求提供了用户和成员两种管理概念。&lt;/p&gt;
&lt;h3 id=&quot;用户&quot;&gt;&lt;a href=&quot;#用户&quot; class=&quot;headerlink&quot; title=&quot;用户&quot;&gt;&lt;/a&gt;用户&lt;/h3&gt;&lt;p&gt;用户主要分两类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;管理员&lt;/li&gt;
&lt;li&gt;普通用户&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;两类用户都可以成为项目的成员。而管理员可以对用户进行管理。&lt;/p&gt;
&lt;h3 id=&quot;成员&quot;&gt;&lt;a href=&quot;#成员&quot; class=&quot;headerlink&quot; title=&quot;成员&quot;&gt;&lt;/a&gt;成员&lt;/h3&gt;&lt;p&gt;成员是对应于项目的概念，分为三类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;管理员&lt;/li&gt;
&lt;li&gt;开发者&lt;/li&gt;
&lt;li&gt;访客&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;管理员可以对开发者和访客作权限的配置和管理。测试和运维人员可以访客身份读取项目镜像，或者公共镜像库中的文件。&lt;br&gt;从项目的角度出发，项目管理员拥有最大的项目权限，如果要对用户进行禁用或限权等，可以通过修改用户在项目中的成员角色来实现，甚至将用户移除出这个项目。&lt;br&gt;下面以实际操作来演示。&lt;/p&gt;
&lt;h2 id=&quot;Harbor使用&quot;&gt;&lt;a href=&quot;#Harbor使用&quot; class=&quot;headerlink&quot; title=&quot;Harbor使用&quot;&gt;&lt;/a&gt;Harbor使用&lt;/h2&gt;&lt;p&gt;官方使用文档：&lt;a href=&quot;https://github.com/vmware/harbor/blob/master/docs/user_guide.md&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/vmware/harbor/blob/master/docs/user_guide.md&lt;/a&gt;&lt;br&gt;注意：当项目设为公开后，任何人都有此项目下镜像的读权限。命令行用户不需要“docker login”就可以拉取此项目下的镜像。所以一般需要建立私有项目。&lt;br&gt;1.登录harbor，点击“+项目”&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/39.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;2.点击左侧菜单“用户管理”，点击“+用户”&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/40.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;3.点击左侧菜单项目，选择刚才创建的项目“godseye”，在点击右侧正文中的选项卡“成员”，点击“+成员”，输入刚才创建的用户，并设置其为管理员。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/41.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;对于权限(角色)，项目管理员和开发人员可以有 push 的权限，而访客只能查看和 pull&lt;/p&gt;
&lt;p&gt;4.测试&lt;br&gt;我这里找了另外一台机器，安装了docker 1.12.6。由于这里harbor采用了默认的 http 方式连接，而 Docker 认为这是不安全的，所以在 push 之前需要调整一下 docker 配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node3 ~]# vim /etc/docker/daemon.json 
{
   &amp;quot;insecure-registries&amp;quot;: [&amp;quot;172.16.206.32&amp;quot;]
}
[root@node3 ~]# systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;登录harbor：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node3 ~]# docker login 172.16.206.32
Username: jkzhao
Password: 
Login Succeeded
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后 tag 一个 image，名称一定要标准( registryAddress[:端口]/项目/imageName[:tag] )，最后将其 push 即可&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node3 ~]# docker tag centos:centos7 172.16.206.32/godseye/centos:latest
[root@node3 ~]# docker push 172.16.206.32/godseye/centos:latest
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后到web ui上查看刚才push的镜像是否成功了：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/42.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;【补充】：如果使用的docker客户端版本比较低，比如在centos6上安装了docker 1.7.1，那么同样需要先调整docker的配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@osb30 ~]# vim /etc/sysconfig/docker
other_args=&amp;quot;--insecure-registry=172.16.206.32&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;镜像删除和空间回收&quot;&gt;&lt;a href=&quot;#镜像删除和空间回收&quot; class=&quot;headerlink&quot; title=&quot;镜像删除和空间回收&quot;&gt;&lt;/a&gt;镜像删除和空间回收&lt;/h2&gt;&lt;p&gt;Docker命令没有提供Registry镜像删除功能，日积月累，将会产生许多无用的镜像，占用大量存储空间。若要删除镜像并回收空间，需要调用docker registry API来完成，比较麻烦。Harbor提供了可视化的镜像删除界面，可以逻辑删除镜像。在维护状态下可以回收垃圾镜像的空间。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node3 ~]# docker images
REPOSITORY                           TAG                    IMAGE ID            CREATED             SIZE
java                                 openjdk-8-jre-alpine   d61ff40a5bf6        16 months ago       108.3 MB
[root@node3 ~]# docker login 172.16.206.32
Username: jkzhao
Password: 
Login Succeeded
[root@node3 ~]# docker tag java:openjdk-8-jre-alpine 172.16.206.32/godseye/jdk:openjdk-8-jre-alpine
[root@node3 ~]# docker push 172.16.206.32/godseye/jdk:openjdk-8-jre-alpine
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们先查看下宿主机上存放镜像的目录大小：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 2017-09-08]# du -sh /data/registry/docker/registry/v2/
110M    /data/registry/docker/registry/v2/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;登录harbor界面，点击godseye项目，删除刚才上传的镜像：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/43.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;但是实际上这只是逻辑删除，我们可以查看此时宿主机上存放镜像的目录大小，仍然是110M：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 2017-09-08]# du -sh /data/registry/docker/registry/v2/
110M    /data/registry/docker/registry/v2/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;此时你完全可以再次上传这个镜像，会显示这些镜像层已经存在了：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node3 ~]# docker push 172.16.206.32/godseye/jdk:openjdk-8-jre-alpine
The push refers to a repository [172.16.206.32/godseye/jdk]
2b4866cc0048: Layer already exists 
5f70bf18a086: Layer already exists 
82a47053c51a: Layer already exists 
8f01a53880b9: Layer already exists 
openjdk-8-jre-alpine: digest:sha256:56b1ffe13af2ee1c5e2c9a3d3cd8c377b5f1bc6130a87648d48ba3fffab0d5eb size: 1977
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;那么如何彻底删除这个镜像呢？&lt;/strong&gt;&lt;br&gt;1.首先去界面删除这个镜像&lt;br&gt;2.在harbor宿主机上执行如下的命令：&lt;br&gt;先找到当前的registry版本：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 harbor]# docker images vmware/registry
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
vmware/registry     2.6.1-photon        0f6c96580032        3 months ago        150MB
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;列出要删除的镜像：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 harbor]# docker run -it --name gc --rm --volumes-from registry vmware/registry:2.6.1-photon garbage-collect --dry-run /etc/registry/config.yml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;选项 –dry-run 只是在最后打印出界面删除了的但是实际上并未删除的镜像层，但是这条命令不会删除这些镜像层。&lt;br&gt;运行下面的命令删除镜像：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 harbor]# docker run -it --name gc --rm --volumes-from registry vmware/registry:2.6.1-photon garbage-collect  /etc/registry/config.yml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;再次查看存放镜像的目录大小：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 2017-09-08]# du -sh /data/registry/docker/registry/v2/                                              
70M     /data/registry/docker/registry/v2/
&lt;/code&gt;&lt;/pre&gt;</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Harbor介绍&quot;&gt;&lt;a href=&quot;#Harbor介绍&quot; class=&quot;headerlink&quot; title=&quot;Harbor介绍&quot;&gt;&lt;/a&gt;Harbor介绍&lt;/h2&gt;&lt;p&gt;Harbor是由VMWare公司开源的容器镜像仓库。事实上，Habor是在Docker Registry上进行了相应的企业级扩展，从而获得了更加广泛的应用，这些新的企业级特性包括：管理用户界面，基于角色的访问控制 ，AD/LDAP集成以及审计日志等。&lt;br&gt;
    
    </summary>
    
      <category term="容器" scheme="http://yoursite.com/categories/%E5%AE%B9%E5%99%A8/"/>
    
    
      <category term="Docker Registry" scheme="http://yoursite.com/tags/Docker-Registry/"/>
    
  </entry>
  
  <entry>
    <title>overlay实现容器跨主机通信</title>
    <link href="http://yoursite.com/2017/09/05/overlay%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B7%A8%E4%B8%BB%E6%9C%BA%E9%80%9A%E4%BF%A1/"/>
    <id>http://yoursite.com/2017/09/05/overlay实现容器跨主机通信/</id>
    <published>2017-09-05T05:42:06.000Z</published>
    <updated>2017-11-07T13:06:15.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;Docker容器跨主机通信方案&quot;&gt;&lt;a href=&quot;#Docker容器跨主机通信方案&quot; class=&quot;headerlink&quot; title=&quot;Docker容器跨主机通信方案&quot;&gt;&lt;/a&gt;Docker容器跨主机通信方案&lt;/h2&gt;&lt;p&gt;实现跨主机的容器通信有很多种方案，需要看实际的网络状况，是云上环境，私有云环境，还是混合云环境；是否有SDN对网络做特殊控制等等。网络状况不一样，适用的方案也会不一样。比如有的环境可以使用路由的方案，有的却不能使用。不考虑网络模型的话，基本是两个派别：overlay和路由方案。&lt;br&gt;Docker 1.12中把swarmkit集成到了docker中，本篇博客使用的版本是docker 1.11版本，这是我以前做的一个方案，现整理出来。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;环境信息&quot;&gt;&lt;a href=&quot;#环境信息&quot; class=&quot;headerlink&quot; title=&quot;环境信息&quot;&gt;&lt;/a&gt;环境信息&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;操作系统版本&lt;/th&gt;
&lt;th&gt;IP地址&lt;/th&gt;
&lt;th&gt;Docker版本&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;node1&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.7.151&lt;/td&gt;
&lt;td&gt;1.11.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;node2&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.7.152&lt;/td&gt;
&lt;td&gt;1.11.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&quot;升级内核&quot;&gt;&lt;a href=&quot;#升级内核&quot; class=&quot;headerlink&quot; title=&quot;升级内核&quot;&gt;&lt;/a&gt;升级内核&lt;/h2&gt;&lt;p&gt;默认内核：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# uname -r
3.10.0-229.el7.x86_64
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;1.升级内核需要使用 elrepo 的yum 源&lt;br&gt;首先我们导入 elrepo 的key&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.安装 elrepo 源&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3.在yum的ELRepo源中，mainline 为最新版本的内核&lt;br&gt;安装 ml 的内核&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# yum --enablerepo=elrepo-kernel install  kernel-ml-devel kernel-ml -y
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;4.修改内核启动顺序，默认启动的顺序应该为1，升级以后内核是往前面插入，为0&lt;br&gt;由于CentOS 7使用grub2作为引导程序 ，所以和CentOS 6有所不同，并不是修改/etc/grub.conf来修改启动项，需要如下操作：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# cat /boot/grub2/grub.cfg |grep menuentry   #查看有哪些内核选项
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/21.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# grub2-editenv list
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/22.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# grub2-set-default 0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;5.重启系统&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# shutdown -r now
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;6.查看内核版本&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# uname -r
4.5.2-1.el7.elrepo.x86_64
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;安装docker&quot;&gt;&lt;a href=&quot;#安装docker&quot; class=&quot;headerlink&quot; title=&quot;安装docker&quot;&gt;&lt;/a&gt;安装docker&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;[root@node1 ~]# vim /etc/yum.repos.d/docker.repo
[dockerrepo]
name=Docker Repository
baseurl=https://yum.dockerproject.org/repo/main/centos/$releasever/
enabled=1
gpgcheck=1
gpgkey=https://yum.dockerproject.org/gpg

[root@node1 ~]# yum install -y docker-engine
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果后面升级：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# yum update docker-engine
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;卸载：yum remove  &lt;/p&gt;
&lt;h2 id=&quot;防火墙设置和开启内核转发&quot;&gt;&lt;a href=&quot;#防火墙设置和开启内核转发&quot; class=&quot;headerlink&quot; title=&quot;防火墙设置和开启内核转发&quot;&gt;&lt;/a&gt;防火墙设置和开启内核转发&lt;/h2&gt;&lt;p&gt;停止firewalld，安装iptables-services&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# systemctl stop firewalld.service
[root@node1 ~]# systemctl disable firewalld.service
[root@node1 ~]# yum install -y iptables-services
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;修改防火墙策略：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# vim /etc/sysconfig/iptables
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/23.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;        [root@node1 ~]# systemctl start iptables.service&lt;br&gt;        [root@node1 ~]# systemctl enable iptables.service&lt;/p&gt;
&lt;p&gt;开启内核转发，在/etc/sysctl.conf中添加一行配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# vim /etc/sysctl.conf 
net.ipv4.ip_forward=1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;运行下面的命令使内核修改生效：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# sysctl -p
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;安装启动consul&quot;&gt;&lt;a href=&quot;#安装启动consul&quot; class=&quot;headerlink&quot; title=&quot;安装启动consul&quot;&gt;&lt;/a&gt;安装启动consul&lt;/h2&gt;&lt;p&gt;overlay一般需要一个全局的KV存储（sdn controller、etcd、consul）来存储各个主机节点在overlay网络中的配置信息。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# wget https://releases.hashicorp.com/consul/0.6.4/consul_0.6.4_linux_amd64.zip
# unzip -oq consul_0.6.4_linux_amd64.zip
# mv consul /usr/local/bin/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;启动consul：&lt;br&gt;host-1 Start Consul as a server in bootstrap mode:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# nohup consul agent -server -bootstrap -data-dir /tmp/consul -bind=172.16.7.151 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;host-2 Start the Consul agent:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node2 ~]# nohup consul agent -data-dir /tmp/consul -bind=172.16.7.152 &amp;amp;
[root@node2 ~]# consul join 172.16.7.151
Successfully joined cluster by contacting 1 nodes.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/24.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;启动Docker&quot;&gt;&lt;a href=&quot;#启动Docker&quot; class=&quot;headerlink&quot; title=&quot;启动Docker&quot;&gt;&lt;/a&gt;启动Docker&lt;/h2&gt;&lt;h3 id=&quot;修改docker-daemon配置&quot;&gt;&lt;a href=&quot;#修改docker-daemon配置&quot; class=&quot;headerlink&quot; title=&quot;修改docker daemon配置&quot;&gt;&lt;/a&gt;修改docker daemon配置&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;# cp /usr/lib/systemd/system/docker.service /etc/systemd/system/
# vim /etc/systemd/system/docker.service
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在ExecStart那行加上如下的选项，其中ens32是网卡名字：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--cluster-store=consul://localhost:8500 --cluster-advertise=ens32:2376
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/25.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;其中–cluster-store是指向key-value存储的地址，我这里就是consul的地址，consul里保存着整个overlay网络配置和节点信息。–cluster-advertise中是Host1和Host2互通的端口。&lt;/p&gt;
&lt;h3 id=&quot;启动Docker-1&quot;&gt;&lt;a href=&quot;#启动Docker-1&quot; class=&quot;headerlink&quot; title=&quot;启动Docker&quot;&gt;&lt;/a&gt;启动Docker&lt;/h3&gt;&lt;p&gt;执行systemctl daemon-reload使配置生效，然后执行systemctl start docker.service启动docker服务。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl start docker.service
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;加入开机自启动：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl enable docker.service
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;创建overlay-network&quot;&gt;&lt;a href=&quot;#创建overlay-network&quot; class=&quot;headerlink&quot; title=&quot;创建overlay network&quot;&gt;&lt;/a&gt;创建overlay network&lt;/h2&gt;&lt;h3 id=&quot;vxlan简介&quot;&gt;&lt;a href=&quot;#vxlan简介&quot; class=&quot;headerlink&quot; title=&quot;vxlan简介&quot;&gt;&lt;/a&gt;vxlan简介&lt;/h3&gt;&lt;p&gt;overlay network这种方式一般也是只需要三层可达，容器就能互通。overlay模式容器有独立IP，不同overlay方案之间的性能差别也是很大的。我这里采用的的vxlan技术。&lt;br&gt;vxlan(virtual Extensible LAN)虚拟可扩展局域网，是一种overlay的网络技术，使用MAC in UDP的方法进 行封装，共50字节的封装报文头。&lt;br&gt;用于对VXLAN报文进行封装/解封装，包括ARP请求报文和正常的VXLAN数据报文，在一段封装报文 后通过隧道向另一端VTEP发送封装报文，另一端VTEP接收到封装的报文解封装后根据封装的MAC地址进行转发。VTEP可由支持VXLAN的硬件设备或软件来实现。&lt;br&gt;从封装的结构上来看，VXLAN提供了将二层网络overlay在三层网络上的能力。&lt;/p&gt;
&lt;h3 id=&quot;创建overlay-network-1&quot;&gt;&lt;a href=&quot;#创建overlay-network-1&quot; class=&quot;headerlink&quot; title=&quot;创建overlay network&quot;&gt;&lt;/a&gt;创建overlay network&lt;/h3&gt;&lt;p&gt;默认情况下，docker启动后初始化3种网络，这3种都是不能删除的。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# docker network ls
NETWORK ID          NAME                DRIVER
5944745e7d6d             bridge                   bridge              
ce5d1ba0be32             host                       host                
244bb9a34016             none                      null  
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在node1主机上创建overlay network:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker network create -d overlay --subnet=10.10.10.0/24 net1
ca0c50dd3a49e028c3323024b9d6e8f837f4b76889b8d5848046ec0a5948ee2d
[root@node1 ~]# docker network ls
NETWORK ID          NAME                DRIVER
5944745e7d6d             bridge                  bridge              
ce5d1ba0be32             host                     host                
ca0c50dd3a49             net1                    overlay             
244bb9a34016             none                    null
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在其他主机上执行docker network ls，也会看到新建的这个名字叫net1的overlay网络。&lt;/p&gt;
&lt;h2 id=&quot;创建容器&quot;&gt;&lt;a href=&quot;#创建容器&quot; class=&quot;headerlink&quot; title=&quot;创建容器&quot;&gt;&lt;/a&gt;创建容器&lt;/h2&gt;&lt;p&gt;node1主机创建容器：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker run -it --net=net1 --name=contain1 --hostname=test1 --ip=10.10.10.3 --add-host test2:10.10.10.4 centos:centos7
[root@test1 /]# yum install -y iproute net-tools
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/26.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/27.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;node2上创建容器：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node2 ~]# docker run -it --net=net1 --name=contain2 --hostname=test2 --ip=10.10.10.4 --add-host test1:10.10.10.3 centos:centos7
[root@test2 /]# yum install -y iproute net-tools
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/28.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/29.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;测试容器跨主机通信&quot;&gt;&lt;a href=&quot;#测试容器跨主机通信&quot; class=&quot;headerlink&quot; title=&quot;测试容器跨主机通信&quot;&gt;&lt;/a&gt;测试容器跨主机通信&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/30.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/31.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;网络拓扑&quot;&gt;&lt;a href=&quot;#网络拓扑&quot; class=&quot;headerlink&quot; title=&quot;网络拓扑&quot;&gt;&lt;/a&gt;网络拓扑&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/32.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;容器内部有两个网络接口eth0、eth1。实际上，eth1连接到docker_gwbridge，这可以从ip就能看出。eth0即为overlay network的接口。&lt;/p&gt;
&lt;h2 id=&quot;抓包分析&quot;&gt;&lt;a href=&quot;#抓包分析&quot; class=&quot;headerlink&quot; title=&quot;抓包分析&quot;&gt;&lt;/a&gt;抓包分析&lt;/h2&gt;&lt;p&gt;在node2主机上使用tcpdump抓包，然后在windows上用wireshark分析。&lt;br&gt;1.container1容器里ping container2的ip地址：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/33.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;2.node2主机上抓包&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node2 ~]# tcpdump -i ens32 -s 0 -X -nnn -vvv -w /tmp/package.pcap
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3.把package.pcap传下来放到wireshark上分析&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/34.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/35.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;strong&gt;以在container1中ping container2，分析数据包流向：&lt;/strong&gt;&lt;br&gt;①container1(10.10.10.3)中ping container2(10.10.10.4)，根据container1的路由表，数据包可通过直连网络到达container2。于是arp请求获取container2的MAC地址(在xvlan上的arp这里不详述)，得到mac地址后，封包，从eth0发出；&lt;br&gt;②eth0桥接在net ns 1-ca0c50dd3a中的br0上，这个br0是个网桥(交换机)虚拟设备，需要将来自eth0的包转发出去，于是包转给了vxlan设备；这个可以通过arp -a看到一些端倪：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ip netns exec 1-ca0c50dd3a arp -a
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;③vxlan是个特殊设备，收到包后，由vxlan设备创建时注册的设备处理程序对包进行处理，即进行VXLAN封包（这期间会查询consul中存储的net1信息），将ICMP包整体作为UDP包的payload封装起来，并将UDP包通过宿主机的eth0发送出去。&lt;br&gt;④152宿主机收到UDP包后，发现是VXLAN包，根据VXLAN包中的相关信息（比如Vxlan Network Identifier，VNI=256)找到vxlan设备，并转给该vxlan设备处理。vxlan设备的处理程序进行解包，并将UDP中的payload取出，整体通过br0转给veth口，net1c2从eth0收到ICMP数据包，回复icmp reply。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;从这个通信过程中来看，跨主机通信过程中的步骤如下：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;容器的网络命名空间与overlay网络的网络命名空间通过一对veth pair连接起来，当容器对外通信时，veth pair起到网线的作用，将流量发送到overlay网络的网络命名空间中。 &lt;/li&gt;
&lt;li&gt;容器的veth pair对端eth2与vxlan设备通过br0这个Linux bridge桥接在一起，br0在同一宿主机上起到虚拟机交换机的作用，如果目标地址在同一宿主机上，则直接通信，如果不再则通过设置在vxlan1这个vxlan设备进行跨主机通信。 &lt;/li&gt;
&lt;li&gt;vxlan1设备上会在创建时，由docker daemon为其分配vxlan隧道ID，起到网络隔离的作用。 &lt;/li&gt;
&lt;li&gt;docker主机集群通过key/value存储共享数据，在7946端口上，相互之间通过gossip协议学习各个宿主机上运行了哪些容器。守护进程根据这些数据来在vxlan1设备上生成静态MAC转发表。 &lt;/li&gt;
&lt;li&gt;根据静态MAC转发表的设置，通过UDP端口4789，将流量转发到对端宿主机的网卡上。&lt;br&gt;根据流量包中的vxlan隧道ID，将流量转发到对端宿主机的overlay网络的网络命名空间中。 &lt;/li&gt;
&lt;li&gt;对端宿主机的overlay网络的网络命名空间中br0网桥，起到虚拟交换机的作用，将流量根据MAC地址转发到对应容器内部。&lt;/li&gt;
&lt;/ul&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Docker容器跨主机通信方案&quot;&gt;&lt;a href=&quot;#Docker容器跨主机通信方案&quot; class=&quot;headerlink&quot; title=&quot;Docker容器跨主机通信方案&quot;&gt;&lt;/a&gt;Docker容器跨主机通信方案&lt;/h2&gt;&lt;p&gt;实现跨主机的容器通信有很多种方案，需要看实际的网络状况，是云上环境，私有云环境，还是混合云环境；是否有SDN对网络做特殊控制等等。网络状况不一样，适用的方案也会不一样。比如有的环境可以使用路由的方案，有的却不能使用。不考虑网络模型的话，基本是两个派别：overlay和路由方案。&lt;br&gt;Docker 1.12中把swarmkit集成到了docker中，本篇博客使用的版本是docker 1.11版本，这是我以前做的一个方案，现整理出来。&lt;br&gt;
    
    </summary>
    
      <category term="容器" scheme="http://yoursite.com/categories/%E5%AE%B9%E5%99%A8/"/>
    
    
      <category term="Docker" scheme="http://yoursite.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Registry私有仓库搭建及认证</title>
    <link href="http://yoursite.com/2017/09/01/Registry%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93%E6%90%AD%E5%BB%BA%E5%8F%8A%E8%AE%A4%E8%AF%81/"/>
    <id>http://yoursite.com/2017/09/01/Registry私有仓库搭建及认证/</id>
    <published>2017-09-01T09:44:19.000Z</published>
    <updated>2017-11-10T04:20:42.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;Registry相关概念&quot;&gt;&lt;a href=&quot;#Registry相关概念&quot; class=&quot;headerlink&quot; title=&quot;Registry相关概念&quot;&gt;&lt;/a&gt;Registry相关概念&lt;/h2&gt;&lt;p&gt;前面的文章讲过Docker的组成部分，我们一般在使用Docker的过程中更为常用的是pull image、run image、build image和push image。主要是围绕image展开的。&lt;br&gt;image和Registry的关系可以想象成自己机器上的源码和远端SVN或者Git服务的关系。Registry是一个几种存放image并对外提供上传下载以及一系列API的服务。可以很容易和本地源代码以及远端Git服务的关系相对应。&lt;br&gt;Docker hub是Docker公司提供的一些存储镜像的空间，这部分空间是有限的。我们一般会自主建设Docker私有仓库Registry。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Registry-V1和V2&quot;&gt;&lt;a href=&quot;#Registry-V1和V2&quot; class=&quot;headerlink&quot; title=&quot;Registry V1和V2&quot;&gt;&lt;/a&gt;Registry V1和V2&lt;/h2&gt;&lt;p&gt;Docker Registry 2.0版本在安全性和性能上做了诸多优化，并重新设计了镜像的存储的格式。Docker目前1.6之后支持V2。&lt;/p&gt;
&lt;h2 id=&quot;安装Docker&quot;&gt;&lt;a href=&quot;#安装Docker&quot; class=&quot;headerlink&quot; title=&quot;安装Docker&quot;&gt;&lt;/a&gt;安装Docker&lt;/h2&gt;&lt;p&gt;见前面发布的文章《CentOS安装Docker CE》。&lt;/p&gt;
&lt;h2 id=&quot;搭建本地registry-v2&quot;&gt;&lt;a href=&quot;#搭建本地registry-v2&quot; class=&quot;headerlink&quot; title=&quot;搭建本地registry v2&quot;&gt;&lt;/a&gt;搭建本地registry v2&lt;/h2&gt;&lt;p&gt;环境：172.16.7.151 CentOS 7.0&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker run -d -p 5000:5000 --name wisedu_registry registry:2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;本地push镜像到仓库:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker pull ubuntu:16.04
[root@node1 ~]# docker tag ubuntu:16.04 localhost:5000/my-ubuntu
[root@node1 ~]# docker push localhost:5000/my-ubuntu
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;删除本地的ubuntu:16.04和localhost:5000/my-ubuntu镜像&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker image remove ubuntu:16.04
[root@node1 ~]# docker image remove localhost:5000/my-ubuntu
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;从本地registry中拉取 localhost:5000/my-ubuntu 镜像：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker pull localhost:5000/my-ubuntu
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;但是这种registry只是本地能使用，我们找另外一台主机172.16.7.152往该registry中push镜像：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node2 ~]# docker pull ubuntu:16.04
[root@node2 docker]# docker tag ubuntu:16.04 172.16.7.151:5000/ubuntu:v1
[root@node2 docker]# docker push 172.16.7.151:5000/ubuntu:v1
The push refers to a repository [172.16.7.151:5000/ubuntu]
Get https://172.16.7.151:5000/v2/: http: server gave HTTP response to HTTPS client
[root@node2 ~]# echo $?
1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这是因为从docker1.13.2版本开始，使用registry时，必须使用TLS保证其安全。&lt;/p&gt;
&lt;p&gt;停止并删除本地registry：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker stop wisedu_registry         
wisedu_registry
[root@node1 ~]# docker rm -v wisedu_registry
wisedu_registry
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;搭建外部可访问的Registry&quot;&gt;&lt;a href=&quot;#搭建外部可访问的Registry&quot; class=&quot;headerlink&quot; title=&quot;搭建外部可访问的Registry&quot;&gt;&lt;/a&gt;搭建外部可访问的Registry&lt;/h2&gt;&lt;p&gt;官方文档：&lt;a href=&quot;https://docs.docker.com/registry/deploying/#run-an-externally-accessible-registry&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://docs.docker.com/registry/deploying/#run-an-externally-accessible-registry&lt;/a&gt;&lt;br&gt;Running a registry only accessible on localhost has limited usefulness. In order to make your registry accessible to external hosts, you must first secure it using TLS.&lt;/p&gt;
&lt;p&gt;使用TLS认证registry容器时，必须有证书。一般情况下，是要去认证机构购买签名证书。这里使用openssl生成自签名的证书。&lt;br&gt;环境信息：172.16.206.32  CentOS 7.0  主机名：spark32&lt;/p&gt;
&lt;p&gt;1.生成自签名证书&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# mkdir -p /opt/docker/registry/certs
[root@spark32 ~]# openssl req -newkey rsa:4096 -nodes -sha256 -keyout /opt/docker/registry/certs/domain.key -x509 -days 365 -out /opt/docker/registry/certs/domain.crt
Generating a 4096 bit RSA private key
...
Country Name (2 letter code) [XX]:CN
State or Province Name (full name) []:JiangSu
Locality Name (eg, city) [Default City]:NanJing
Organization Name (eg, company) [Default Company Ltd]:wisedu
Organizational Unit Name (eg, section) []:edu
Common Name (eg, your name or your server&amp;apos;s hostname) []:registry.docker.com
Email Address []:01115004@wisedu.com
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.创建带有TLS认证的registry容器&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# docker run -d --name registry2 -p 5000:5000 -v /opt/docker/registry/certs:/certs -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key registry:2 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3.在每一个docker客户端宿主机上配置/etc/hosts，以使客户端宿主机可以解析域名”registry.docker.com”。并创建与这个registry服务器域名一致的目录（因为我这里的域名是假的）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/hosts
172.16.206.32 registry.docker.com
[root@node1 ~]# cd /etc/docker/certs.d/
[root@node1 certs.d]# mkdir registry.docker.com:5000
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;4.将证书 domain.crt 复制到每一个docker客户端宿主机/etc/docker/certs.d/registry.docker.com:5000/ca.crt，不需要重启docker&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# scp -p /opt/docker/registry/certs/domain.crt root@172.16.7.151:/etc/docker/certs.d/registry.docker.com\:5000/ca.crt
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;5.push镜像到registry&lt;br&gt;另找一台客户机，node1。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 certs.d]# docker tag ubuntu:16.04 registry.docker.com:5000/my-ubuntu:v1
[root@node1 certs.d]# docker push registry.docker.com:5000/my-ubuntu:v1
The push refers to a repository [registry.docker.com:5000/my-ubuntu]
a09947e71dc0: Pushed 
9c42c2077cde: Pushed 
625c7a2a783b: Pushed 
25e0901a71b8: Pushed 
8aa4fcad5eeb: Pushed 
v1: digest: sha256:634a341aa83f32b48949ef428db8fefcd897dbacfdac26f044b60c14d1b5e972 size: 1357
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;6.列出私有仓库中的所有镜像&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 certs.d]# curl -X GET https://registry.docker.com:5000/v2/_catalog -k
{&amp;quot;repositories&amp;quot;:[&amp;quot;my-ubuntu&amp;quot;]}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;7.查看存储在registry:2宿主机上的镜像&lt;br&gt;在registry:2创建的私有仓库中，上传的镜像保存在容器的/var/lib/registry目录下。创建registry:2的容器时，会自动创建一个数据卷(Data Volumes)，数据卷对应的宿主机下的目录一般为：/var/lib/docker/volumes/XXX/_data。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# ls /var/lib/docker/volumes/91a0091963fa6d107dc988a60b61790bba843a115573e331db967921d5e83372/_data/docker/registry/v2/repositories/my-ubuntu/
_layers  _manifests  _uploads
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可以在创建registry:2的容器时，通过-v参数，修改这种数据卷关系：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;–v /opt/docker/registry/data:/var/lib/registry
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;除了可以将数据保存在当前主机的文件系统上，registry也支持其他基于云的存储系统，比如S3，Microsoft Azure, Ceph Rados, OpenStack Swift and Aliyun OSS等。可以在配置文件中进行配置：&lt;a href=&quot;https://github.com/docker/distribution/blob/master/docs/configuration.md#storage&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/docker/distribution/blob/master/docs/configuration.md#storage&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;【补充】：&lt;br&gt;一般情况下，证书只支持域名访问，要使其支持IP地址访问，需要修改配置文件openssl.cnf。&lt;br&gt;在Redhat7系统中，文件所在位置是/etc/pki/tls/openssl.cnf。在其中的[ v3_ca]部分，添加subjectAltName选项：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[ v3_ca ]  
subjectAltName = IP:192.168.1.104 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;生成证书：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;...  
Country Name (2 letter code) [XX]:  
State or Province Name (full name) []:  
Locality Name (eg, city) [Default City]:  
Organization Name (eg, company) [Default Company Ltd]:  
Organizational Unit Name (eg, section) []:  
Common Name (eg, your name or your server&amp;apos;s hostname) []:172.16.206.32:5000  
Email Address []:  
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;添加认证&quot;&gt;&lt;a href=&quot;#添加认证&quot; class=&quot;headerlink&quot; title=&quot;添加认证&quot;&gt;&lt;/a&gt;添加认证&lt;/h2&gt;&lt;h3 id=&quot;Native-basic-auth&quot;&gt;&lt;a href=&quot;#Native-basic-auth&quot; class=&quot;headerlink&quot; title=&quot;Native basic auth&quot;&gt;&lt;/a&gt;Native basic auth&lt;/h3&gt;&lt;p&gt;The simplest way to achieve access restriction is through basic authentication (this is very similar to other web servers’ basic authentication mechanism). This example uses native basic authentication using htpasswd to store the secrets.&lt;/p&gt;
&lt;p&gt;1.创建用户密码文件，testuser，testpassword&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# mkdir /opt/docker/registry/auth
[root@spark32 ~]# docker run --entrypoint htpasswd registry:2 -Bbn testuser testpassword &amp;gt; /opt/docker/registry/auth/htpasswd
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.运行registry容器&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# docker run -d --name registry_native_auth -p 5000:5000 -v /opt/docker/registry/auth:/auth -e &amp;quot;REGISTRY_AUTH=htpasswd&amp;quot; -e &amp;quot;REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm&amp;quot; -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd -v /opt/docker/registry/certs:/certs -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key registry:2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3.现在尝试拉取镜像&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker pull registry.docker.com:5000/my-ubuntu:v1
Error response from daemon: Get https://registry.docker.com:5000/v2/my-ubuntu/manifests/v1: no basic auth credentials
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;4.登录registry，push镜像&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker login registry.docker.com:5000
Username: testuser
Password: 
Login Succeeded
[root@node1 ~]# docker tag ubuntu:16.04 registry.docker.com:5000/my-ubuntu:v1
[root@node1 ~]# docker push registry.docker.com:5000/my-ubuntu:v1 
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;更高级的认证&quot;&gt;&lt;a href=&quot;#更高级的认证&quot; class=&quot;headerlink&quot; title=&quot;更高级的认证&quot;&gt;&lt;/a&gt;更高级的认证&lt;/h2&gt;&lt;p&gt;更好的方式是在registry前使用代理，利用代理提供https的ssl的认证和basic authentication。&lt;a href=&quot;https://docs.docker.com/registry/recipes/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://docs.docker.com/registry/recipes/&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;配置Nginx作为认证代理&quot;&gt;&lt;a href=&quot;#配置Nginx作为认证代理&quot; class=&quot;headerlink&quot; title=&quot;配置Nginx作为认证代理&quot;&gt;&lt;/a&gt;配置Nginx作为认证代理&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://docs.docker.com/registry/recipes/nginx/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://docs.docker.com/registry/recipes/nginx/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;1.创建需要的目录&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# mkdir -p /opt/nginx_proxy_registry/{auth,data}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.创建Nginx主配置文件&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# vim /opt/nginx_proxy_registry/auth/nginx.conf
events {
    worker_connections  1024;
}

http {

  upstream docker-registry {
    server registry:5000;
  }

  ## Set a variable to help us decide if we need to add the
  ## &amp;apos;Docker-Distribution-Api-Version&amp;apos; header.
  ## The registry always sets this header.
  ## In the case of nginx performing auth, the header will be unset
  ## since nginx is auth-ing before proxying.
  map $upstream_http_docker_distribution_api_version $docker_distribution_api_version {
    &amp;apos;&amp;apos; &amp;apos;registry/2.0&amp;apos;;
  }

  server {
    listen 443 ssl;
    server_name registry.docker.com;

    # SSL
    ssl_certificate /etc/nginx/conf.d/domain.crt;
    ssl_certificate_key /etc/nginx/conf.d/domain.key;

    # Recommendations from https://raymii.org/s/tutorials/Strong_SSL_Security_On_nginx.html
    ssl_protocols TLSv1.1 TLSv1.2;
    ssl_ciphers &amp;apos;EECDH+AESGCM:EDH+AESGCM:AES256+EECDH:AES256+EDH&amp;apos;;
    ssl_prefer_server_ciphers on;
    ssl_session_cache shared:SSL:10m;

    # disable any limits to avoid HTTP 413 for large image uploads
    client_max_body_size 0;

    # required to avoid HTTP 411: see Issue #1486 (https://github.com/moby/moby/issues/1486)
    chunked_transfer_encoding on;

    location /v2/ {
      # Do not allow connections from docker 1.5 and earlier
      # docker pre-1.6.0 did not properly set the user agent on ping, catch &amp;quot;Go *&amp;quot; user agents
      if ($http_user_agent ~ &amp;quot;^(docker\/1\.(3|4|5(?!\.[0-9]-dev))|Go ).*$&amp;quot; ) {
        return 404;
      }

      # To add basic authentication to v2 use auth_basic setting.
      auth_basic &amp;quot;Registry realm&amp;quot;;
      auth_basic_user_file /etc/nginx/conf.d/nginx.htpasswd;

      ## If $docker_distribution_api_version is empty, the header will not be added.
      ## See the map directive above where this variable is defined.
      add_header &amp;apos;Docker-Distribution-Api-Version&amp;apos; $docker_distribution_api_version always;

      proxy_pass                          http://docker-registry;
      proxy_set_header  Host              $http_host;   # required for docker client&amp;apos;s sake
      proxy_set_header  X-Real-IP         $remote_addr; # pass on real client&amp;apos;s IP
      proxy_set_header  X-Forwarded-For   $proxy_add_x_forwarded_for;
      proxy_set_header  X-Forwarded-Proto $scheme;
      proxy_read_timeout                  900;
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3.创建密码认证文件&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# docker run --rm --entrypoint htpasswd registry:2 -bn testuser testpassword &amp;gt; /opt/nginx_proxy_registry/auth/nginx.htpasswd
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;4.拷贝之前生成的证书和key到auth目录下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# cp /opt/docker/registry/certs/domain.crt /opt/nginx_proxy_registry/auth/
[root@spark32 ~]# cp /opt/docker/registry/certs/domain.key /opt/nginx_proxy_registry/auth/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;5.创建compose文件 &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# cd /opt/nginx_proxy_registry/
[root@spark32 nginx_proxy_registry]# vim docker-compose.yml
nginx:
  image: &amp;quot;nginx:1.9&amp;quot;
  ports:
    - 5043:443
  links:
    - registry:registry
  volumes:
    - ./auth:/etc/nginx/conf.d
    - ./auth/nginx.conf:/etc/nginx/nginx.conf:ro

registry:
  image: registry:2
  ports:
    - 127.0.0.1:5000:5000
  volumes:
    - ./data:/var/lib/registry
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;6.启动&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 nginx_proxy_registry]# docker-compose up -d
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;7.验证启动的服务&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 nginx_proxy_registry]# docker-compose ps
[root@spark32 ~]# docker ps 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;找一台docker客户端机器登录：&lt;br&gt;创建需要的目录：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# mkdir /etc/docker/certs.d/registry.docker.com:5043
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;把 domain.crt 传到上一步生成的目录里：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# scp -p /opt/nginx_proxy_registry/auth/domain.crt root@172.16.7.151:/etc/docker/certs.d/registry.docker.com:5043/ca.crt
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;登录：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker login -u=testuser -p=testpassword registry.docker.com:5043
Login Succeeded
[root@node1 ~]# docker tag ubuntu:16.04 registry.docker.com:5043/ubuntu-test:v1
[root@node1 ~]# docker push registry.docker.com:5043/ubuntu-test:v1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;8.停止服务&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# cd /opt/nginx_proxy_registry/
[root@spark32 nginx_proxy_registry]# docker-compose stop
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;9.查看日志&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 nginx_proxy_registry]# docker-compose logs
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;补充Docker-compose&quot;&gt;&lt;a href=&quot;#补充Docker-compose&quot; class=&quot;headerlink&quot; title=&quot;补充Docker compose&quot;&gt;&lt;/a&gt;补充Docker compose&lt;/h3&gt;&lt;h4 id=&quot;Docker-compose是什么&quot;&gt;&lt;a href=&quot;#Docker-compose是什么&quot; class=&quot;headerlink&quot; title=&quot;Docker compose是什么&quot;&gt;&lt;/a&gt;Docker compose是什么&lt;/h4&gt;&lt;p&gt;Docker Compose是一个用来定义和运行复杂应用的Docker工具。使用Compose，你可以在一个文件中定义一个多容器应用，然后使用一条命令来启动你的应用，完成一切准备工作。&lt;br&gt;一个使用Docker容器的应用，通常由多个容器组成。使用Docker Compose，不再需要使用shell脚本来启动容器。&lt;br&gt;Docker Compose将所管理的容器分为三层，工程(project)、服务(service)以及容器(contaienr)。一个工程当中可包含多个服务，每个服务中定义了容器运行的镜像，参数，依赖。一个服务当中可包括多个容器实例，Docker Compose并没有解决负载均衡的问题，因此需要借助其他工具实现服务发现及负载均衡。&lt;/p&gt;
&lt;h4 id=&quot;安装docker-compose&quot;&gt;&lt;a href=&quot;#安装docker-compose&quot; class=&quot;headerlink&quot; title=&quot;安装docker compose&quot;&gt;&lt;/a&gt;安装docker compose&lt;/h4&gt;&lt;p&gt;将变量 $dockerComposeVersion 换成指定的&lt;a href=&quot;https://github.com/docker/compose/releases&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;版本&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# curl -L https://github.com/docker/compose/releases/download/$dockerComposeVersion/docker-compose-`uname -m` -o /usr/local/bin/docker-compose
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;比如下载安装1.15.0版本：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# curl -L https://github.com/docker/compose/releases/download/1.15.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可能会下载失败，多试几次。实在不行就需要翻墙去下载。&lt;/p&gt;
&lt;p&gt;赋予执行权限：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# chmod +x /usr/local/bin/docker-compose
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查看docker compose版本：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# docker-compose --version
docker-compose version 1.15.0, build e12f3b9
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;卸载docker-compose&quot;&gt;&lt;a href=&quot;#卸载docker-compose&quot; class=&quot;headerlink&quot; title=&quot;卸载docker compose&quot;&gt;&lt;/a&gt;卸载docker compose&lt;/h4&gt;&lt;p&gt;如果docker compose是通过curl安装的:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;rm /usr/local/bin/docker-compose
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果docker compose是通过pip安装的:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip uninstall docker-compose
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;registry-web-ui&quot;&gt;&lt;a href=&quot;#registry-web-ui&quot; class=&quot;headerlink&quot; title=&quot;registry web ui&quot;&gt;&lt;/a&gt;registry web ui&lt;/h2&gt;&lt;p&gt;搭建完了docker registry，我们可以使用 docker 命令行工具对我们搭建的 registry 做各种操作了，如 push / pull。但是不够方便，比如不能直观的查看 registry 中的资源情况，如果有一个 ui 工具，能够看到仓库中有哪些镜像、每个镜像的版本是多少。&lt;br&gt;registry web ui主要有3个，一个是 &lt;a href=&quot;https://github.com/kwk/docker-registry-frontend&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;docker-registry-frontend&lt;/a&gt;，一个是 &lt;a href=&quot;https://hub.docker.com/r/hyper/docker-registry-web/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;hyper/docker-registry-web&lt;/a&gt;，还有一个是&lt;a href=&quot;http://port.us.org/documentation.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Portus&lt;/a&gt;。&lt;br&gt;关于registry ui的搭建会在后面的文章中介绍。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Registry相关概念&quot;&gt;&lt;a href=&quot;#Registry相关概念&quot; class=&quot;headerlink&quot; title=&quot;Registry相关概念&quot;&gt;&lt;/a&gt;Registry相关概念&lt;/h2&gt;&lt;p&gt;前面的文章讲过Docker的组成部分，我们一般在使用Docker的过程中更为常用的是pull image、run image、build image和push image。主要是围绕image展开的。&lt;br&gt;image和Registry的关系可以想象成自己机器上的源码和远端SVN或者Git服务的关系。Registry是一个几种存放image并对外提供上传下载以及一系列API的服务。可以很容易和本地源代码以及远端Git服务的关系相对应。&lt;br&gt;Docker hub是Docker公司提供的一些存储镜像的空间，这部分空间是有限的。我们一般会自主建设Docker私有仓库Registry。&lt;br&gt;
    
    </summary>
    
      <category term="容器" scheme="http://yoursite.com/categories/%E5%AE%B9%E5%99%A8/"/>
    
    
      <category term="Docker Registry" scheme="http://yoursite.com/tags/Docker-Registry/"/>
    
  </entry>
  
  <entry>
    <title>CentOS安装Docker CE</title>
    <link href="http://yoursite.com/2017/08/28/CentOS%E5%AE%89%E8%A3%85Docker-CE/"/>
    <id>http://yoursite.com/2017/08/28/CentOS安装Docker-CE/</id>
    <published>2017-08-28T05:29:47.000Z</published>
    <updated>2017-11-07T07:46:34.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;在搭建Registry的过程中，发现使用Docker 1.12版本，在push镜像到Registry时会报错误，几经查询解决之道无果后，去github的docker项目上提问，得到的回答是”Also note you appear running an unsupported version of docker which has changes specifically around how registries are handled.”，并且建议我尝试较新的版本。在2017年4月份的DockerCon会议上，Docker公司直接将 Github 上原隶属于 Docker 组织的 Docker 项目，直接 transfer 到了一个新的、名叫 Moby 的组织下，并将其重命名为 Moby 项目。由此先来了解下这个Moby项目。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Moby项目&quot;&gt;&lt;a href=&quot;#Moby项目&quot; class=&quot;headerlink&quot; title=&quot;Moby项目&quot;&gt;&lt;/a&gt;Moby项目&lt;/h2&gt;&lt;p&gt;为什么会产生Moby这个项目呢？可以看到最近几年里面，在2015年之后的Docker 1.17版本之后，引入了很多的新特性，比如Network、runC等，Docker的组件越来越多，提供支持的场景也越来越复杂，比如微服务、机器学习、物联网等，所以Docker镜像的下载量也呈现指数型上升，最终Docker运行的环境，也是越来越复杂，越来越多，比如在Linux，Windows，还有在嵌入式设备上。&lt;br&gt;所以Docker原本的发行版本已经不能适配于这些越来越复杂的场景了，举个例子，其实在IoT里面，树莓派的性能是比较好的，但是它去运行Docker的话，Docker就会占整个性能的一大部分，这样它上面就已经无法运行其他的一些自己的应用了，那用户要怎么去解决这些事情，难道自己去再写一个Docker容器引擎吗？这个成本是很高的，需要自己去造轮子。所以为了提供更加开放的生态，Docker公司把Docker项目中现在的一些组件抽象成Moby项目，这样系统构建者就可以通过Moby项目把现有的组件去进行组装，然后组装成自己所需要的一个容器引擎。&lt;br&gt;Moby项目现在有80多个组件，通过这些组件，用户可以避免重复地去造轮子。用户可以按照自己的需要去组装组件，做出自己的一个容器系统。这些组件有一个标准化的调用方式，他们之间通过gRPC通信，它的语言也是可以去定制的，不会像之前一样必须用Go语言去写。通过标准化的方式，通过Moby这个项目就可以把这些组件进行组合，成为自己所依赖的容器系统。&lt;br&gt;Docker项目现在改名成Moby项目，但是Docker会逐渐的从Moby项目中去抽象和剥离出来，作为Moby的一种组装方式，比如Docker依赖于这里面的一些库，它就特化成一种组装，组装成自己的Docker CE的版本，也就是Docker的社区版。Docker社区版后面也会继续做开源，所以用户和开发者不需要担心以后用Docker就会收费了，对于Docker用户来说，他也无须感知用户接口的变化，使用的命令还是Docker不是Moby，需要更多运维支持的可以选择Docker EE的版本，让Docker公司的工程师去替你去做运维和更复杂的线上支持，如果自己开发可以继续选择Docker社区版。对于架构师而言，现在就可以不强依赖于Docker项目，而是通过这些组件去拼装出来一个容器引擎去满足自己的需求。&lt;/p&gt;
&lt;h2 id=&quot;安装Docker-CE&quot;&gt;&lt;a href=&quot;#安装Docker-CE&quot; class=&quot;headerlink&quot; title=&quot;安装Docker CE&quot;&gt;&lt;/a&gt;安装Docker CE&lt;/h2&gt;&lt;h3 id=&quot;版本说明&quot;&gt;&lt;a href=&quot;#版本说明&quot; class=&quot;headerlink&quot; title=&quot;版本说明&quot;&gt;&lt;/a&gt;版本说明&lt;/h3&gt;&lt;p&gt;2017年2月份，Docker公司发布了全新的Docker版本：V1.13.0。从2017年3月1号开始，Docker的版本命名发生如下变化：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;项目&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;版本格式&lt;/td&gt;
&lt;td&gt;YY.MM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;stable版本&lt;/td&gt;
&lt;td&gt;每个季度发行&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;edge版本&lt;/td&gt;
&lt;td&gt;每个月发行&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;同时将Docker分成CE和EE 2个版本。CE版本即社区版（免费，支持周期三个月），EE即企业版，强调安全，付费使用。&lt;br&gt;Docker 会每月发布一个 edge 版本(17.03, 17.04, 17.05…)，每三个月发布一个 stable 版本(17.03, 17.06, 17.09…)，企业版(EE) 和 stable 版本号保持一致，但每个版本提供一年维护。&lt;br&gt;Docker 的 Linux 发行版的软件仓库也从以前的&lt;a href=&quot;https://apt.dockerproject.org和https://yum.dockerproject.org&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://apt.dockerproject.org和https://yum.dockerproject.org&lt;/a&gt; 变更为目前的 &lt;a href=&quot;https://download.docker.com&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://download.docker.com&lt;/a&gt; 。软件包名变更为 docker-ce(社区版) 和 docker-ee(企业版)。&lt;br&gt;当前的CE版本为17.03.0，基于V1.13.0。主要修复错误，没有重大功能增加，API亦保持不变。本文以此版本安装。&lt;br&gt;此版本的发行说明，请参考：&lt;a href=&quot;https://github.com/docker/docker/releases&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/docker/docker/releases&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;安装Docker&quot;&gt;&lt;a href=&quot;#安装Docker&quot; class=&quot;headerlink&quot; title=&quot;安装Docker&quot;&gt;&lt;/a&gt;安装Docker&lt;/h3&gt;&lt;p&gt;官方安装文档：&lt;a href=&quot;https://docs.docker.com/engine/installation/linux/docker-ce/centos/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://docs.docker.com/engine/installation/linux/docker-ce/centos/&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;环境要求&quot;&gt;&lt;a href=&quot;#环境要求&quot; class=&quot;headerlink&quot; title=&quot;环境要求&quot;&gt;&lt;/a&gt;环境要求&lt;/h4&gt;&lt;p&gt;To install Docker CE, you need the 64-bit version of CentOS 7.&lt;br&gt;The centos-extras repository must be enabled. This repository is enabled by default, but if you have disabled it, you need to re-enable it.&lt;/p&gt;
&lt;h4 id=&quot;卸载安装的所有Docker组件&quot;&gt;&lt;a href=&quot;#卸载安装的所有Docker组件&quot; class=&quot;headerlink&quot; title=&quot;卸载安装的所有Docker组件&quot;&gt;&lt;/a&gt;卸载安装的所有Docker组件&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@spark32 lib]# systemctl stop docker.service
[root@spark32 lib]# yum remove docker docker-common docker-selinux docker-engine container-selinux
[root@spark32 lib]# rm -rf docker/
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;通过仓库安装Docker-CE&quot;&gt;&lt;a href=&quot;#通过仓库安装Docker-CE&quot; class=&quot;headerlink&quot; title=&quot;通过仓库安装Docker CE&quot;&gt;&lt;/a&gt;通过仓库安装Docker CE&lt;/h4&gt;&lt;p&gt;1.安装依赖包&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# yum install -y yum-utils device-mapper-persistent-data lvm2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.下载docker yum源&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3.更新软件缓存&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# yum makecache fast
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;4.安装Docker CE&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# yum install docker-ce -y
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;5.启动docker&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# systemctl start docker.service
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;6.查看docker版本信息&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# docker info
Containers: 0
 Running: 0
 Paused: 0
 Stopped: 0
Images: 0
Server Version: 17.06.1-ce
Storage Driver: overlay
 Backing Filesystem: extfs
 Supports d_type: true
Logging Driver: json-file
Cgroup Driver: cgroupfs
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【注意】:在生产环境中，可能需要使用某一个特定版本的Docker，而不是使用最新的版本，我们可以先列出版本，然后安装指定版本：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# yum list docker-ce.x86_64  --showduplicates | sort -r
[root@spark32 ~]# yum install docker-ce-&amp;lt;VERSION&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;配置镜像加速&quot;&gt;&lt;a href=&quot;#配置镜像加速&quot; class=&quot;headerlink&quot; title=&quot;配置镜像加速&quot;&gt;&lt;/a&gt;配置镜像加速&lt;/h3&gt;&lt;p&gt;国内访问 Docker Hub 有时会遇到困难，此时可以配置镜像加速器。国内很多云服务商都提供了加速器服务，例如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;阿里云加速器&lt;/li&gt;
&lt;li&gt;DaoCloud 加速器&lt;/li&gt;
&lt;li&gt;灵雀云加速器&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以阿里云加速器为例&lt;br&gt;1.首先进入&lt;a href=&quot;https://dev.aliyun.com/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;阿里云docker库首页&lt;/a&gt;&lt;br&gt;注册用户并登录，点击右上角“管理中心”。如果第一次，会提示“您还没有开通服务”，点击“确定”，然后会出现弹窗“初始化设置”，设置docker登录时使用的密码。&lt;br&gt;2.点击左侧菜单“Docker Hub镜像站点”，可以看到“您的专属加速器地址：&lt;a href=&quot;https://6101v8g5.mirror.aliyuncs.com”，我们需要将其配置到Docker&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://6101v8g5.mirror.aliyuncs.com”，我们需要将其配置到Docker&lt;/a&gt; 引擎。&lt;br&gt;3.修改daemon配置文件/etc/docker/daemon.json来使用加速器：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# vim /etc/docker/daemon.json
{
  &amp;quot;registry-mirrors&amp;quot;: [&amp;quot;https://6101v8g5.mirror.aliyuncs.com&amp;quot;]
}
[root@spark32 ~]# systemctl daemon-reload
[root@spark32 ~]# systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;卸载Docker-CE&quot;&gt;&lt;a href=&quot;#卸载Docker-CE&quot; class=&quot;headerlink&quot; title=&quot;卸载Docker CE&quot;&gt;&lt;/a&gt;卸载Docker CE&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;# yum remove docker-ce
# rm -rf /var/lib/docker
&lt;/code&gt;&lt;/pre&gt;</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;在搭建Registry的过程中，发现使用Docker 1.12版本，在push镜像到Registry时会报错误，几经查询解决之道无果后，去github的docker项目上提问，得到的回答是”Also note you appear running an unsupported version of docker which has changes specifically around how registries are handled.”，并且建议我尝试较新的版本。在2017年4月份的DockerCon会议上，Docker公司直接将 Github 上原隶属于 Docker 组织的 Docker 项目，直接 transfer 到了一个新的、名叫 Moby 的组织下，并将其重命名为 Moby 项目。由此先来了解下这个Moby项目。&lt;br&gt;
    
    </summary>
    
      <category term="容器" scheme="http://yoursite.com/categories/%E5%AE%B9%E5%99%A8/"/>
    
    
      <category term="Docker" scheme="http://yoursite.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Dockerfile构建镜像</title>
    <link href="http://yoursite.com/2017/08/25/Dockerfile%E6%9E%84%E5%BB%BA%E9%95%9C%E5%83%8F/"/>
    <id>http://yoursite.com/2017/08/25/Dockerfile构建镜像/</id>
    <published>2017-08-25T02:55:41.000Z</published>
    <updated>2017-11-07T07:46:49.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;Dockerfile介绍&quot;&gt;&lt;a href=&quot;#Dockerfile介绍&quot; class=&quot;headerlink&quot; title=&quot;Dockerfile介绍&quot;&gt;&lt;/a&gt;Dockerfile介绍&lt;/h2&gt;&lt;p&gt;dockerfile是构建镜像的说明书。dockerfile提供了一种基于DSL语法的指令来构建镜像，通过代码化，镜像构建过程一目了然，我们能看出镜像构建的每一步都在干什么。&lt;br&gt;若要共享镜像，我们只需要共享dockerfile就可以了。共享dockerfile文件，具有以下优点：&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可重现&lt;/li&gt;
&lt;li&gt;dockerfile可以加入版本控制，这样可以追踪文件的变化和回滚错误&lt;/li&gt;
&lt;li&gt;dockerfile很轻量，我们不需要copy几百M甚至上G的docker镜像&lt;br&gt;使用Dockerfile构建的流程：&lt;/li&gt;
&lt;li&gt;编写Dockerfile&lt;/li&gt;
&lt;li&gt;执行docker build命令&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Dockerfile指令简单介绍&quot;&gt;&lt;a href=&quot;#Dockerfile指令简单介绍&quot; class=&quot;headerlink&quot; title=&quot;Dockerfile指令简单介绍&quot;&gt;&lt;/a&gt;Dockerfile指令简单介绍&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;FROM：基础镜像。&lt;/li&gt;
&lt;li&gt;MAINTAINER：维护者信息。&lt;/li&gt;
&lt;li&gt;RUN：在需要执行的命令前加RUN。&lt;/li&gt;
&lt;li&gt;ADD：ADD的文件和Dockerfile必须在同一个路径下。&lt;/li&gt;
&lt;li&gt;ENV：设置环境变量。&lt;/li&gt;
&lt;li&gt;WORKDIR：相当于cd。&lt;/li&gt;
&lt;li&gt;VOLUME：目录挂载。&lt;/li&gt;
&lt;li&gt;EXPOSE：映射端口。&lt;/li&gt;
&lt;li&gt;ENTRYPOINT&lt;/li&gt;
&lt;li&gt;CMD&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;RUN指令&quot;&gt;&lt;a href=&quot;#RUN指令&quot; class=&quot;headerlink&quot; title=&quot;RUN指令&quot;&gt;&lt;/a&gt;RUN指令&lt;/h3&gt;&lt;p&gt;RUN指令会在前一条命令创建的镜像的基础上启动一个容器，并在容器中运行指令的命令。在该命令结束后，会提交容器为新的镜像。新镜像会被dockerfile中的下一条指令所使用。&lt;br&gt;默认情况下，RUN指令会以shell的形式去执行命令。当然我们也可以使用exec格式的RUN指令。在exec方式中，我们使用一个数组来指定要运行的命令和传递给该命令的参数。exec中的参数会当成什么数组被docker解析，因此必须使用双引号，而不能使用单引号。&lt;br&gt;&lt;strong&gt;shell形式：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;RUN yum install -y nginx
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;exec形式：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;RUN [&amp;quot;yum&amp;quot;, &amp;quot;install&amp;quot;, &amp;quot;-y&amp;quot;, &amp;quot;nginx&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;EXPOSE指令&quot;&gt;&lt;a href=&quot;#EXPOSE指令&quot; class=&quot;headerlink&quot; title=&quot;EXPOSE指令&quot;&gt;&lt;/a&gt;EXPOSE指令&lt;/h3&gt;&lt;p&gt;告诉docker该容器内的应用程序将会指定端口，但是这并不意味着我们可以自动访问该容器内服务的端口。出于安全的原因，docker并不会自动打开该端口。而是需要我们在使用docker run的时候通过-p参数指定需要打开哪些端口。我们可以使用多个EXPOSE指令向外部暴露多个端口。注意不可以在dockerfile中指定端口映射关系。比如EXPOSE 80，这条指令是对的，而EXPOSE 8080:80这条指令就是错的。&lt;br&gt;【注意】:这会影响docker的可移植性。我们应该在docker run命令中使用-p参数实现端口映射。&lt;/p&gt;
&lt;h3 id=&quot;CMD指令&quot;&gt;&lt;a href=&quot;#CMD指令&quot; class=&quot;headerlink&quot; title=&quot;CMD指令&quot;&gt;&lt;/a&gt;CMD指令&lt;/h3&gt;&lt;p&gt;CMD 指令允许用户指定容器启动的默认执行的命令。此命令会在容器启动且 docker run 没有指定其他命令时运行。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果 docker run 指定了其他命令，CMD 指定的默认命令将被忽略。&lt;/li&gt;
&lt;li&gt;&lt;p&gt;如果 Dockerfile 中有多个 CMD 指令，只有最后一个 CMD 有效。&lt;br&gt;CMD 有三种格式：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Exec 格式：CMD [&amp;quot;executable&amp;quot;,&amp;quot;param1&amp;quot;,&amp;quot;param2&amp;quot;] 这是 CMD 的推荐格式。
CMD [&amp;quot;param1&amp;quot;,&amp;quot;param2&amp;quot;] 为 ENTRYPOINT 提供额外的参数，此时 ENTRYPOINT 必须使用 Exec 格式。
Shell 格式：CMD command param1 param2
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;第一种格式：运行一个可执行的文件并提供参数。&lt;br&gt;第二种格式 CMD [“param1”,”param2”] 要与 Exec 格式 的 ENTRYPOINT 指令配合使用，其用途是为 ENTRYPOINT 设置默认的参数。&lt;br&gt;第三种格式：是以”/bin/sh -c”的方法执行的命令。&lt;/p&gt;
&lt;p&gt;下面看看 CMD 是如何工作的。Dockerfile 片段如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CMD echo &amp;quot;Hello world&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;运行容器 docker run -it [image] 将输出：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Hello world
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;但当后面加上一个命令，比如 docker run -it [image] /bin/bash，CMD 会被忽略掉，命令 bash 将被执行：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@76a5fd777648 /]# 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【注意】:使用CMD指令时，最好使用数组语法。&lt;/p&gt;
&lt;h3 id=&quot;ENTRYPOIN指令&quot;&gt;&lt;a href=&quot;#ENTRYPOIN指令&quot; class=&quot;headerlink&quot; title=&quot;ENTRYPOIN指令&quot;&gt;&lt;/a&gt;ENTRYPOIN指令&lt;/h3&gt;&lt;p&gt;ENTRYPOINT 看上去与 CMD 很像，它们都可以指定要执行的命令及其参数。不同的地方在于 ENTRYPOINT 不会被忽略，一定会被执行，即使运行 docker run 时指定了其他命令。&lt;br&gt;实际上docker run命令行中指定的任何参数都会被当作参数再次传递给ENTRYPOINT指令中指定的命令。&lt;br&gt;ENTRYPOINT 有两种格式：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Exec 格式：ENTRYPOINT [&amp;quot;executable&amp;quot;, &amp;quot;param1&amp;quot;, &amp;quot;param2&amp;quot;] 这是 ENTRYPOINT 的推荐格式。
Shell 格式：ENTRYPOINT command param1 param2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在为 ENTRYPOINT 选择格式时必须小心，因为这两种格式的效果差别很大。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ENTRYPOINT [&amp;quot;/usr/sbin/nginx&amp;quot;, &amp;quot;-g&amp;quot;, &amp;quot;daemon off;&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;Exec-格式&quot;&gt;&lt;a href=&quot;#Exec-格式&quot; class=&quot;headerlink&quot; title=&quot;Exec 格式&quot;&gt;&lt;/a&gt;Exec 格式&lt;/h4&gt;&lt;p&gt;ENTRYPOINT 的 Exec 格式用于设置要执行的命令及其参数，同时可通过 CMD 提供额外的参数。&lt;br&gt;ENTRYPOINT 中的参数始终会被使用，而 CMD 的额外参数可以在容器启动时动态替换掉。&lt;br&gt;比如下面的 Dockerfile 片段：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ENTRYPOINT [&amp;quot;/bin/echo&amp;quot;, &amp;quot;Hello&amp;quot;]  
CMD [&amp;quot;world&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;当容器通过 docker run -it [image] 启动时，输出为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Hello world
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;而如果通过 docker run -it [image] CloudMan 启动，则输出为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Hello CloudMan
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;Shell-格式&quot;&gt;&lt;a href=&quot;#Shell-格式&quot; class=&quot;headerlink&quot; title=&quot;Shell 格式&quot;&gt;&lt;/a&gt;Shell 格式&lt;/h4&gt;&lt;p&gt;ENTRYPOINT 的 Shell 格式会忽略任何 CMD 或 docker run 提供的参数。&lt;br&gt;&lt;strong&gt;【注意】:使用CMD和ENTRYPOINT时，请务必使用数组语法。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;【总结】：&lt;/strong&gt;&lt;br&gt;1.使用 RUN 指令安装应用和软件包，构建镜像。&lt;br&gt;2.如果想为容器设置默认的启动命令，可使用 CMD 指令。用户可在 docker run 命令行中替换此默认命令。&lt;br&gt;3.如果 Docker 镜像的用途是运行应用程序或服务，比如运行一个 MySQL，应该优先使用 Exec 格式的 ENTRYPOINT 指令。CMD 可为 ENTRYPOINT 提供额外的默认参数，同时可利用 docker run 命令行替换默认参数。&lt;/p&gt;
&lt;h2 id=&quot;示例1：构建Nginx镜像&quot;&gt;&lt;a href=&quot;#示例1：构建Nginx镜像&quot; class=&quot;headerlink&quot; title=&quot;示例1：构建Nginx镜像&quot;&gt;&lt;/a&gt;示例1：构建Nginx镜像&lt;/h2&gt;&lt;p&gt;创建一个目录单独存放各种Dockerfile：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# mkdir /opt/docker-file
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;创建Nginx目录存放Nginx Dockerfile和相关文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# mkdir nginx
[root@node1 ~]# cd nginx
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;上传openresty-1.9.7.3.tar.gz到Nginx目录下，编写Dockerfile文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 nginx]# vim Dockerfile
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;该Dockerfile内容如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# This dockerfile is for openresty
# Version 1.0

# Base images. 
FROM centos:centos7

# Author.
MAINTAINER jkzhao &amp;lt;01115004@wisedu.com&amp;gt;

# Add openresty software.
ADD openresty-1.9.7.3.tar.gz /usr/local

# Define working directory.
WORKDIR /usr/local/openresty-1.9.7.3

# Install epel repo
RUN rpm -ivh http://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpm

# Install openresty.
RUN yum install -y readline-devel pcre-devel openssl-devel gcc perl make 
RUN ./configure &amp;amp;&amp;amp; gmake &amp;amp;&amp;amp; gmake install
RUN sed -i &amp;apos;1 i\daemon off;&amp;apos; /usr/local/openresty/nginx/conf/nginx.conf
RUN sed -i &amp;apos;s@#error_log  logs\/error.log;@error_log logs\/error.log debug;@&amp;apos; /usr/local/openresty/nginx/conf/nginx.conf

# Define environment variables.
ENV PATH /usr/local/openresty/nginx/sbin:$PATH

# Define default command. 
CMD [&amp;quot;nginx&amp;quot;]

# Expose ports.
EXPOSE 80
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;利用该Dockerfile构建镜像：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 nginx]# docker build -t jkzhao/mynginx:v2 /opt/docker-file/nginx/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查看构建的镜像：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 nginx]# docker images
REPOSITORY                              TAG                 IMAGE ID            CREATED             SIZE
jkzhao/mynginx                          v2                  f61afc8ce858        17 minutes ago      581.6 MB
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;基于镜像启动容器：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 nginx]# docker run -d -p 84:80 jkzhao/mynginx:v2   
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;访问Nginx服务：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/20.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;如果dockerfile中某条指令执行失败了，没有正常结束，那么我们也将得到一个可以使用的镜像。这对调试非常有帮助，我们可以基于该镜像，运行一个具有交互功能的容器，使用最后创建的镜像对为什么我们的指令会失败进行调试。&lt;/p&gt;
&lt;h2 id=&quot;示例2：构建一个Ruby运行环境&quot;&gt;&lt;a href=&quot;#示例2：构建一个Ruby运行环境&quot; class=&quot;headerlink&quot; title=&quot;示例2：构建一个Ruby运行环境&quot;&gt;&lt;/a&gt;示例2：构建一个Ruby运行环境&lt;/h2&gt;&lt;p&gt;创建Ruby目录存放Ruby Dockerfile和相关文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# mkdir ruby
[root@node1 ~]# cd ruby
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;该Dockerfile内容如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;FROM centos:7  

MAINTAINER jkzhao &amp;lt;01115004@wisedu.com&amp;gt;

# 为镜像添加备注信息
LABEL version=&amp;quot;2.2.2&amp;quot; lang=&amp;quot;ruby&amp;quot;

ENV RUBY_MAJOR 2.2
ENV RUBY_VERSION 2.2.2

RUN yum install -y wget tar gcc g++ make automake autoconf curl-devel openssl-devel zlib-devel httpd-devel apr-devel apr-util-devel sqlite-devel

RUN cd /tmp \
    &amp;amp;&amp;amp; wget http://cache.ruby-lang.org/pub/ruby/2.2/ruby-2.2.2.tar.gz \
    &amp;amp;&amp;amp; tar zxvf ruby-2.2.2.tar.gz \
    &amp;amp;&amp;amp; cd ruby-2.2.2 \
    &amp;amp;&amp;amp; autoconf \
    &amp;amp;&amp;amp; ./configure --disable-install-doc \
    &amp;amp;&amp;amp; make \
    &amp;amp;&amp;amp; make install \
    &amp;amp;&amp;amp; rm -rf /tmp/ruby-2.2.2*

# skip installing gem documentation
RUN echo -e &amp;apos;install: --no-document\nupdate: --no-document&amp;apos; &amp;gt;&amp;gt; &amp;quot;$HOME/.gemrc&amp;quot;

ENV GEM_HOME /usr/local/bundle
ENV PATH $GEM_HOME/bin:$PATH

ENV BUNDLER_VERSION 1.10.6

RUN gem install bundler --version &amp;quot;$BUNDLER_VERSION&amp;quot; \
 &amp;amp;&amp;amp; bundle config --global path &amp;quot;$GEM_HOME&amp;quot; \
 &amp;amp;&amp;amp; bundle config --global bin &amp;quot;$GEM_HOME/bin&amp;quot;

# don&amp;apos;t create &amp;quot;.bundle&amp;quot; in all our apps
ENV BUNDLE_APP_CONFIG $GEM_HOME

CMD [ &amp;quot;irb&amp;quot; ]
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;构建缓存&quot;&gt;&lt;a href=&quot;#构建缓存&quot; class=&quot;headerlink&quot; title=&quot;构建缓存&quot;&gt;&lt;/a&gt;构建缓存&lt;/h2&gt;&lt;p&gt;dockerfile构建镜像过程非常聪明，它会将每一步的构建结果提交为一个镜像，我们可以将之前指令创建的镜像层看做缓存，比如第5条指令由于命令书写错误而导致构建失败，当我们修改它之后，前面的4条指令不会再执行，而是直接使用之前构建过程中保存的缓存。这样速度会提高很多，大大减少了构建的时间。&lt;br&gt;然而有时候我们要确保构建过程中不会使用缓存，比如对yum update等操作，要想跳过缓存，&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker build no-cache
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;缓存还有个好处，就是调试方便。比如第5条指令失败了，那么我们可以基于第4条指令创建的镜像启动一个新的容器。在容器里手工执行第5条指令所要的操作，调查指令失败的原因。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Dockerfile介绍&quot;&gt;&lt;a href=&quot;#Dockerfile介绍&quot; class=&quot;headerlink&quot; title=&quot;Dockerfile介绍&quot;&gt;&lt;/a&gt;Dockerfile介绍&lt;/h2&gt;&lt;p&gt;dockerfile是构建镜像的说明书。dockerfile提供了一种基于DSL语法的指令来构建镜像，通过代码化，镜像构建过程一目了然，我们能看出镜像构建的每一步都在干什么。&lt;br&gt;若要共享镜像，我们只需要共享dockerfile就可以了。共享dockerfile文件，具有以下优点：&lt;br&gt;
    
    </summary>
    
      <category term="容器" scheme="http://yoursite.com/categories/%E5%AE%B9%E5%99%A8/"/>
    
    
      <category term="Docker" scheme="http://yoursite.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>手动构建镜像</title>
    <link href="http://yoursite.com/2017/08/23/%E6%89%8B%E5%8A%A8%E6%9E%84%E5%BB%BA%E9%95%9C%E5%83%8F/"/>
    <id>http://yoursite.com/2017/08/23/手动构建镜像/</id>
    <published>2017-08-23T03:15:08.000Z</published>
    <updated>2017-11-07T07:46:11.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;创建docker镜像的方法&quot;&gt;&lt;a href=&quot;#创建docker镜像的方法&quot; class=&quot;headerlink&quot; title=&quot;创建docker镜像的方法&quot;&gt;&lt;/a&gt;创建docker镜像的方法&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;使用”docker commit”命令&lt;/li&gt;
&lt;li&gt;使用”docker build”命令+”Dockerfile”文件&lt;br&gt;不推荐使用docker commit命令，而应该使用更灵活、更强大的dockerfile来构建docker镜像。&lt;br&gt;本篇文章先介绍docker commit来构建镜像。&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;手动构建镜像&quot;&gt;&lt;a href=&quot;#手动构建镜像&quot; class=&quot;headerlink&quot; title=&quot;手动构建镜像&quot;&gt;&lt;/a&gt;手动构建镜像&lt;/h2&gt;&lt;p&gt;一般来说，我们并不是真正从0开始构建镜像，而是基于一个已经存在的镜像，比如centos，然后进行一些安装和配置，构建自己新的镜像。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;【示例】:手动构建Nginx镜像。&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker pull centos
[root@node1 ~]# docker run -it --name mynginx centos 
[root@eadfe0c0903d /]# rpm -ivh http://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpm
[root@eadfe0c0903d /]# yum install nginx -y
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;安装完成后，我们需要将nginx程序设置为前台运行模式，这样容器启动后nginx进程会一直在前台运行而不会退出。因为如果启动容器时的进程退出，容器也就结束了。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@eadfe0c0903d /]# vi /etc/nginx/nginx.conf
daemon off;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/17.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@eadfe0c0903d /]# exit
exit
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;基于上面的容器制作一个镜像：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker ps -a
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                     PORTS                NAMES
eadfe0c0903d        centos              &amp;quot;/bin/bash&amp;quot;              12 minutes ago      Exited (0) 2 minutes ago                        mynginx
[root@node1 ~]# docker commit -m &amp;quot;nginx test mirror&amp;quot; eadfe0c0903d jkzhao/mynginx:v1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/18.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;基于这个镜像启动容器：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker images
REPOSITORY                              TAG                 IMAGE ID            CREATED             SIZE
jkzhao/mynginx                          v1                  bdf7d4fda6fb        4 minutes ago       381.6 MB
docker.io/centos                        latest              328edcd84f1b        2 weeks ago         192.5 MB
registry.docker-cn.com/library/centos   latest              328edcd84f1b        2 weeks ago         192.5 MB
docker.io/nginx                         latest              b8efb18f159b        3 weeks ago         107.5 MB
[root@node1 ~]# docker run -d -p 82:80 jkzhao/mynginx:v1 nginx
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【注意】:&lt;br&gt;1.必须加tag v1，否则会去仓库中找latest标签的镜像；&lt;br&gt;2.最后 nginx 是命令，容器启动时运行的命令，我这里是yum安装，不是yum安装的要写绝对路径。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/19.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;创建docker镜像的方法&quot;&gt;&lt;a href=&quot;#创建docker镜像的方法&quot; class=&quot;headerlink&quot; title=&quot;创建docker镜像的方法&quot;&gt;&lt;/a&gt;创建docker镜像的方法&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;使用”docker commit”命令&lt;/li&gt;
&lt;li&gt;使用”docker build”命令+”Dockerfile”文件&lt;br&gt;不推荐使用docker commit命令，而应该使用更灵活、更强大的dockerfile来构建docker镜像。&lt;br&gt;本篇文章先介绍docker commit来构建镜像。
    
    </summary>
    
      <category term="容器" scheme="http://yoursite.com/categories/%E5%AE%B9%E5%99%A8/"/>
    
    
      <category term="Docker" scheme="http://yoursite.com/tags/Docker/"/>
    
  </entry>
  
</feed>
