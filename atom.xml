<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>jkzhao&#39;s blog</title>
  <subtitle>学习 总结 思考</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-01-27T10:45:50.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Zhao Jiankai</name>
    <email>jk.zhaocoder@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Nginx配置详解及优化</title>
    <link href="http://yoursite.com/2018/01/23/Nginx%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3%E5%8F%8A%E4%BC%98%E5%8C%96/"/>
    <id>http://yoursite.com/2018/01/23/Nginx配置详解及优化/</id>
    <published>2018-01-23T07:06:27.000Z</published>
    <updated>2018-01-27T10:45:50.000Z</updated>
    
    <content type="html">&lt;blockquote&gt;
&lt;p&gt;上篇博文中介绍了安装部署OpenResty，这篇博文主要记录下Nginx的配置及优化。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;Nginx&quot;&gt;&lt;a href=&quot;#Nginx&quot; class=&quot;headerlink&quot; title=&quot;Nginx&quot;&gt;&lt;/a&gt;Nginx&lt;/h2&gt;&lt;p&gt;Nginx的代码是由一个核心和一系列的模块组成, 核心主要用于提供Web Server的基本功能，以及Web和Mail反向代理的功能；&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;还用于启用网络协议，创建必要的运行时环境以及确保不同的模块之间平滑地进行交互。不过，大多跟协议相关的功能和某应用特有的功能都是由nginx的模块实现的。这些功能模块大致可以分为事件模块、阶段性处理器、输出过滤器、变量处理器、协议、upstream和负载均衡几个类别，这些共同组成了nginx的http功能。事件模块主要用于提供OS独立的(不同操作系统的事件机制有所不同)事件通知机制如kqueue或epoll等。协议模块则负责实现nginx通过http、tls/ssl、smtp、pop3以及imap与对应的客户端建立会话。&lt;/p&gt;
&lt;p&gt;Nginx的核心模块为Main和Events，此外还包括标准HTTP模块、可选HTTP模块和邮件模块，其还可以支持诸多第三方模块。Main用于配置错误日志、进程及权限等相关的参数，Events用于配置IO模型，如epoll、kqueue、select或poll等，它们是必备模块。&lt;/p&gt;
&lt;p&gt;Nginx的主配置文件由几个段组成，这个段通常也被称为nginx的上下文，每个段的定义格式如下所示。需要注意的是，其每一个指令都必须使用分号(;)结束，否则为语法错误。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;section&amp;gt; {
    &amp;lt;directive&amp;gt; &amp;lt;parameters&amp;gt;;
}
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;配置文件有哪些&quot;&gt;&lt;a href=&quot;#配置文件有哪些&quot; class=&quot;headerlink&quot; title=&quot;配置文件有哪些&quot;&gt;&lt;/a&gt;配置文件有哪些&lt;/h2&gt;&lt;p&gt;1.主配置文件：nginx.conf&lt;br&gt;2.可以使用include指令引入其他地方的配置文件，比如&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;include conf.d/*.conf
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3.fastcgi的配置文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;fastcgi_params、uwsgi_params
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;4.配置指令(必须以分号结尾)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Directive  value1 [value2...];

支持使用变量：
    内置变量：由模块引入；
    自定义变量：
        set  variable  value; 

    引用变量：$variable
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;配置文件组织结构&quot;&gt;&lt;a href=&quot;#配置文件组织结构&quot; class=&quot;headerlink&quot; title=&quot;配置文件组织结构&quot;&gt;&lt;/a&gt;配置文件组织结构&lt;/h2&gt;&lt;p&gt;主配置文件结构：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;main block
event {
    ...
}
http {
    ...
    server{
        location{  
            ...          
        }           
    }
}
mail{
    ...
}
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;配置main段&quot;&gt;&lt;a href=&quot;#配置main段&quot; class=&quot;headerlink&quot; title=&quot;配置main段&quot;&gt;&lt;/a&gt;配置main段&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;1.user USERNAME  [GROUPNAME];&lt;/strong&gt;&lt;br&gt;指定用于运行worker进程的用户和组，如果不设置，默认是nobody。比如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;user  nginx  nginx;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;2.pid  /PATH/TO/PID_FILE;&lt;/strong&gt;&lt;br&gt;指定nginx进程的pid文件路径，也可以使用默认的。比如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pid  /var/run/nginx.pid;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;3.error_log&lt;/strong&gt;&lt;br&gt;用于配置错误日志，可用于main、http、server及location上下文中；语法格式为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;error_log file | stderr [ debug | info | notice | warn | error | crit | alert | emerg ]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;比如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;error_log  logs/error.log debug;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;4.worker_processes&lt;/strong&gt;&lt;br&gt;worker进程是单线程进程。如果Nginx用于CPU密集型的场景中，如SSL或gzip，且主机上的CPU个数至少有2个，那么应该将此参数值设定为与CPU核心数相同；如果负载以IO密集型为主，如响应大量内容给客户端，则worker数应该为CPU个数的1.5或2倍。比如：Nginx所在服务器有2颗CPU，每颗两核，那么可以配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;worker_processes  4; #启动的work线程数
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;此参数与Events上下文中的work_connections变量一起决定了maxclient的值：&lt;br&gt;maxclients = work_processes * work_connections&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5.worker_cpu_affinity&lt;/strong&gt;&lt;br&gt;通过sched_setaffinity()将worker绑定至CPU上，只能用于main上下文。语法格式为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;worker_cpu_affinity cpumask ...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;例如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;worker_processes     4;
worker_cpu_affinity 0001 0010 0100 1000;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;6.worker_priority&lt;/strong&gt;&lt;br&gt;为worker进程设定优先级(指定nice值)，此参数只能用于main上下文中，默认为0；语法格式为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;worker_priority number
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;7.worker_rlimit_nofile&lt;/strong&gt;&lt;br&gt;设定worker进程所能够打开的文件描述符个数的最大值。语法格式：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;worker_rlimit_nofile number
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;配置Events段&quot;&gt;&lt;a href=&quot;#配置Events段&quot; class=&quot;headerlink&quot; title=&quot;配置Events段&quot;&gt;&lt;/a&gt;配置Events段&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;1.worker_connections&lt;/strong&gt;&lt;br&gt;设定每个worker所处理的最大连接数，它与来自main上下文的worker_processes一起决定了maxclients的值。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;max clients = worker_processes * worker_connections
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;2.use&lt;/strong&gt;&lt;br&gt;在有着多于一个的事件模型IO的应用场景中，可以使用此指令设定nginx所使用的IO机制，默认为./configure脚本选定的各机制中最适用当前OS的版本。语法格式：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;use [ kqueue | rtsig | epoll | /dev/poll | select | poll | eventport ]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;一个配置示例&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;user nginx;
# the load is CPU-bound and we have 16 cores
worker_processes 16;
error_log logs/error.log debug;
pid logs/nginx.pid;

events {
    use epoll;
    worker_connections 2048;
}
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;http段配置&quot;&gt;&lt;a href=&quot;#http段配置&quot; class=&quot;headerlink&quot; title=&quot;http段配置&quot;&gt;&lt;/a&gt;http段配置&lt;/h2&gt;&lt;p&gt;http上下文专用于配置用于http的各模块，此类指令非常的多，每个模块都有其专用指定，具体请参数&lt;a href=&quot;http://nginx.org/en/docs/http/ngx_http_core_module.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;nginx官方文档关于模块部分的说明&lt;/a&gt;。大体上来讲，这些模块所提供的配置指令还可以分为如下几个类别。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;客户端类指令：如client_body_buffer_size、client_header_buffer_size、client_header_timeout和keepalive_timeout等；&lt;/li&gt;
&lt;li&gt;文件IO类指令：如aio、directio、open_file_cache、open_file_cache_min_uses、open_file_cache_valid和sendfile等；&lt;/li&gt;
&lt;li&gt;hash类指令：用于定义Nginx为某特定的变量分配多大的内存空间，如types_hash_bucket_size、server_names_hash_bucket_size和variables_hash_bucket_size等；&lt;/li&gt;
&lt;li&gt;套接字类指令：用于定义Nginx如何处理tcp套接字相关的功能，如tcp_nodelay(用于keepalive功能启用时)和tcp_nopush(用于sendfile启用时)等；&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;套接字或主机相关配置&quot;&gt;&lt;a href=&quot;#套接字或主机相关配置&quot; class=&quot;headerlink&quot; title=&quot;套接字或主机相关配置&quot;&gt;&lt;/a&gt;套接字或主机相关配置&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;server {
    &amp;lt;directive&amp;gt; &amp;lt;parameters&amp;gt;;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;用于定义虚拟主机相关的属性。比如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;server {
    listen PORT; #listen指令监听在不同的端口；
    server_name NAME; #server_name指令指向不同的主机名；
    root /PATH/TO/DOCUMENTROOT;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;1.listen&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;listen address[:port] [default_server] [ssl] [http2 | spdy] 
listen port [default_server] [ssl] [http2 | spdy]
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;default_server：设置默认虚拟主机；用于基于IP地址，或使用了任意不能对应于任何一个server的name时所返回站点；&lt;/li&gt;
&lt;li&gt;ssl：用于限制只能通过ssl连接提供服务；&lt;/li&gt;
&lt;li&gt;spdy：SPDY protocol（speedy），在编译了spdy模块的情况下，用于支持SPDY协议；&lt;/li&gt;
&lt;li&gt;http2：http version 2；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;2.server_name NAME […];&lt;/strong&gt;&lt;br&gt;后可跟一个或多个主机名；名称还可以使用通配符和正则表达式(~)；&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;首先做精确匹配；例如：www.wisedu.com&lt;/li&gt;
&lt;li&gt;左侧通配符；例如：*.wisedu.com&lt;/li&gt;
&lt;li&gt;右侧通配符，例如：www.wisedu.*&lt;/li&gt;
&lt;li&gt;正则表达式，例如：~^.*.wisedu.com$&lt;/li&gt;
&lt;li&gt;default_server &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;3.tcp_nodelay on|off;&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Syntax:    tcp_nodelay on | off;
Default:    tcp_nodelay on;
Context:    http, server, location
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;对keepalive模式下的连接是否使用TCP_NODELAY选项；&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4.tcp_nopush on|off;&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Syntax:    tcp_nopush on | off;
Default:    tcp_nopush off;
Context:    http, server, location
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;是否启用TCP_NOPUSH(FREEBSE）或TCP_CORK(Linux)选项；仅在sendfile为on时有用；&lt;br&gt;&lt;strong&gt;tcp_nopush和tcp_nodelay选项：&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;先来了解下Nagle算法：&lt;/strong&gt;&lt;br&gt;在网络拥塞控制领域，有一个非常有名的算法叫做&lt;strong&gt;Nagle算法（Nagle algorithm）&lt;/strong&gt;，这是使用它的发明人John Nagle的名字来命名的，John Nagle在1984年首次用这个算法来尝试解决福特汽车公司的网络拥塞问题（RFC 896），该问题的具体描述是：如果我们的应用程序一次产生1个字节的数据，而这个1个字节数据又以网络数据包的形式发送到远端服务器，那么就很容易导致网络由于太多的数据包而过载。比如，当用户使用Telnet连接到远程服务器时，每一次击键操作就会产生1个字节数据，进而发送出去一个数据包，所以，在典型情况下，传送一个只拥有1个字节有效数据的数据包，却要发费40个字节长包头（即ip头20字节+tcp头20字节）的额外开销，这种有效载荷（payload）利用率极其低下的情况被统称之为愚蠢窗口症候群（Silly Window Syndrome）。可以看到，这种情况对于轻负载的网络来说，可能还可以接受，但是对于重负载的网络而言，就极有可能承载不了而轻易的发生拥塞瘫痪。&lt;br&gt;针对上面提到的这个状况，Nagle算法的改进在于：如果发送端欲多次发送包含少量字符的数据包（一般情况下，后面统一称长度小于MSS的数据包为小包，与此相对，称长度等于MSS的数据包为大包，为了某些对比说明，还有中包，即长度比小包长，但又不足一个MSS的包），则发送端会先将第一个小包发送出去，而将后面到达的少量字符数据都缓存起来而不立即发送，直到收到接收端对前一个数据包报文段的ACK确认、或当前字符属于紧急数据，或者积攒到了一定数量的数据（比如缓存的字符数据已经达到数据包报文段的最大长度）等多种情况才将其组成一个较大的数据包发送出去。&lt;br&gt;TCP中的Nagle算法默认是启用的，但是它并不是适合任何情况，对于telnet或rlogin这样的远程登录应用的确比较适合（原本就是为此而设计），但是在某些应用场景下我们却又需要关闭它。&lt;br&gt;Nagle算法是指发送方发送的数据不会立即发出, 而是先放在缓冲区, 等缓存区满了再发出。发送完一批数据后, 会等待接收方对这批数据的回应, 然后再发送下一批数据。Negale 算法适用于发送方需要发送大批量数据, 并且接收方会及时作出回应的场合, 这种算法通过减少传输数据的次数来提高通信效率。如果发送方持续地发送小批量的数据, 并且接收方不一定会立即发送响应数据, 那么Negale算法会使发送方运行很慢. 对于GUI 程序, 如网络游戏程序(服务器需要实时跟踪客户端鼠标的移动), 这个问题尤其突出。客户端鼠标位置改动的信息需要实时发送到服务器上, 由于Negale 算法采用缓冲, 大大减低了实时响应速度, 导致客户程序运行很慢。这个时候就需要使用TCP_NODELAY选项。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;tcp_nopush&lt;/strong&gt;&lt;br&gt;官方:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tcp_nopush
Syntax: tcp_nopush on | off
Default: off
Context: http, server, location
Reference: tcp_nopush

This directive permits or forbids the use of thesocket options TCP_NOPUSH on FreeBSD or TCP_CORK on Linux. This option is onlyavailable when using sendfile.
Setting this option causes nginx to attempt to sendit’s HTTP response headers in one packet on Linux and FreeBSD 4.x
You can read more about the TCP_NOPUSH and TCP_CORKsocket options here.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;linux 下是tcp_cork，上面的意思就是说，当使用sendfile函数时，tcp_nopush才起作用，它和指令tcp_nodelay是互斥的。tcp_cork是linux下tcp/ip传输的一个标准了，这个标准的大概的意思是，一般情况下，在tcp交互的过程中，当应用程序接收到数据包后马上传送出去，不等待，而tcp_cork选项是数据包不会马上传送出去，等到数据包最大时，一次性的传输出去，这样有助于解决网络堵塞。&lt;br&gt;也就是说tcp_nopush = on 会设置调用tcp_cork方法，这个也是默认的，结果就是数据包不会马上传送出去，等到数据包最大时，一次性的传输出去，这样有助于解决网络堵塞。&lt;br&gt;以快递投递举例说明一下（以下是我的理解，也许是不正确的），当快递东西时，快递员收到一个包裹，马上投递，这样保证了即时性，但是会耗费大量的人力物力，在网络上表现就是会引起网络堵塞，而当快递收到一个包裹，把包裹放到集散地，等一定数量后统一投递，这样就是tcp_cork的选项干的事情，这样的话，会最大化的利用网络资源，虽然有一点点延迟。&lt;br&gt;对于nginx配置文件中的tcp_nopush，tcp_nopush on;这个选项对于www，ftp等大文件很有帮助。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;tcp_nodelay&lt;/strong&gt;&lt;br&gt;TCP_NODELAY和TCP_CORK基本上控制了包的“Nagle化”，Nagle化在这里的含义是采用Nagle算法把较小的包组装为更大的帧。 John Nagle是Nagle算法的发明人，后者就是用他的名字来命名的，他在1984年首次用这种方法来尝试解决福特汽车公司的网络拥塞问题（欲了解详情请参看IETF RFC 896）。他解决的问题就是所谓的silly window syndrome，中文称“愚蠢窗口症候群”，具体含义是，因为普遍终端应用程序每产生一次击键操作就会发送一个包，而典型情况下一个包会拥有一个字节的数据载荷以及40个字节长的包头，于是产生4000%的过载，很轻易地就能令网络发生拥塞,。 Nagle化后来成了一种标准并且立即在因特网上得以实现。它现在已经成为缺省配置了，但在我们看来，有些场合下把这一选项关掉也是合乎需要的。&lt;br&gt;现在让我们假设某个应用程序发出了一个请求，希望发送小块数据。我们可以选择立即发送数据或者等待产生更多的数据然后再一次发送两种策略。如果我们马上发送数据，那么交互性的以及客户/服务器型的应用程序将极大地受益。如果请求立即发出那么响应时间也会快一些。以上操作可以通过设置套接字的TCP_NODELAY = on 选项来完成，这样就禁用了Nagle 算法。&lt;br&gt;另外一种情况则需要我们等到数据量达到最大时才通过网络一次发送全部数据，这种数据传输方式有益于大量数据的通信性能，典型的应用就是文件服务器。应用 Nagle算法在这种情况下就会产生问题。但是，如果你正在发送大量数据，你可以设置TCP_CORK选项禁用Nagle化，其方式正好同 TCP_NODELAY相反（TCP_CORK和 TCP_NODELAY是互相排斥的）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5.sendfile on|off;&lt;/strong&gt;&lt;br&gt;是否启用sendfile功能；&lt;br&gt;先来看下nginx作为web服务器的工作方式：&lt;br&gt;用户请求进来了，先到达网卡，由内核处理下交给了监听在80套接字上的应用程序，即交给worker进程，这个worker进程通过连接建立、通过接入分析发现用户请求的是一个静态页面，下面就是I/O了。首先进程向内核发出系统调用。内核为它准备一个缓冲，然后内核从磁盘中加载这个文件到缓冲中，然后将这个文件复制给worker进程自己的地址空间，然后进程将这个文件封装成响应报文，这个封装过程是，进程封装http请求首部，然后交给内核封装TCP首部、IP首部，然后交给客户端。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Nginx/10.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;然后你会发现这个文件是这么走的：从硬盘到内核空间，从内核到用户空间，从用户空间再到内核空间，白白绕一圈。如果说这个请求直接在内核中就封装好(http请求首部封装其实也是在内核封装的)，这样就避免了两次复制(注意是复制，内核任何时候和进程交互都是复制，除非共享内存)。复制虽然时间短，但是架不住多啊。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Nginx/11.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;这就是sendfile机制，读过来就响应了。send只支持很小的文件，sendfile64支持更大的文件。&lt;br&gt;现在流行的web 服务器里面都提供 sendfile 选项用来提高服务器性能，那到底 sendfile是什么，怎么影响性能的呢？&lt;br&gt;sendfile实际上是 Linux2.0+以后的推出的一个系统调用，web服务器可以通过调整自身的配置来决定是否利用sendfile这个系统调用。先来看一下不用 sendfile的传统网络传输过程：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;read(file,tmp_buf, len);
write(socket,tmp_buf, len);
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;硬盘 &amp;gt;&amp;gt; kernel buffer &amp;gt;&amp;gt; user buffer&amp;gt;&amp;gt; kernel socket buffer &amp;gt;&amp;gt;协议栈&lt;br&gt;一般来说一个网络应用是通过读硬盘数据，然后写数据到socket 来完成网络传输的。上面2行用代码解释了这一点，不过上面2行简单的代码掩盖了底层的很多操作。来看看底层是怎么执行上面2行代码的：&lt;br&gt;①系统调用 read()产生一个上下文切换：从 user mode 切换到 kernel mode，然后 DMA 执行拷贝，把文件数据从硬盘读到一个 kernel buffer 里。&lt;br&gt;②数据从 kernel buffer拷贝到 user buffer，然后系统调用 read() 返回，这时又产生一个上下文切换：从kernel mode 切换到 user mode。&lt;br&gt;③系统调用write()产生一个上下文切换：从 user mode切换到 kernel mode，然后把步骤2读到 user buffer的数据拷贝到 kernel buffer（数据第2次拷贝到 kernel buffer），不过这次是个不同的 kernel buffer，这个 buffer和 socket相关联。&lt;br&gt;④系统调用 write()返回，产生一个上下文切换：从 kernel mode 切换到 user mode（第4次切换了），然后 DMA 从 kernel buffer拷贝数据到协议栈（第4次拷贝了）。&lt;br&gt;上面4个步骤有4次上下文切换，有4次拷贝，我们发现如果能减少切换次数和拷贝次数将会有效提升性能。在kernel2.0+ 版本中，系统调用 sendfile() 就是用来简化上面步骤提升性能的。sendfile() 不但能减少切换次数而且还能减少拷贝次数。&lt;br&gt;再来看一下用 sendfile() 来进行网络传输的过程：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sendfile(socket,file, len);
硬盘 &amp;gt;&amp;gt; kernel buffer (快速拷贝到kernel socket buffer) &amp;gt;&amp;gt;协议栈
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;①系统调用sendfile()通过 DMA把硬盘数据拷贝到 kernel buffer，然后数据被 kernel直接拷贝到另外一个与 socket相关的 kernel buffer。这里没有 user mode和 kernel mode之间的切换，在 kernel中直接完成了从一个 buffer到另一个buffer的拷贝。&lt;br&gt;②DMA 把数据从 kernelbuffer 直接拷贝给协议栈，没有切换，也不需要数据从 user mode 拷贝到 kernel mode，因为数据就在 kernel 里。&lt;br&gt;步骤减少了，切换减少了，拷贝减少了，自然性能就提升了。这就是为什么说在 Nginx 配置文件里打开 sendfile on 选项能提高 web server性能的原因。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Nginx/12.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4.gzip  on;&lt;/strong&gt;&lt;br&gt;对于响应用户的内容是不是先压缩再发送，可以节省带宽。如果网络带宽小，用户访问量大的话可以使用这种方式。&lt;/p&gt;
&lt;h3 id=&quot;路径相关的指令&quot;&gt;&lt;a href=&quot;#路径相关的指令&quot; class=&quot;headerlink&quot; title=&quot;路径相关的指令&quot;&gt;&lt;/a&gt;路径相关的指令&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;1.root&lt;/strong&gt;&lt;br&gt;设置web资源的路径映射；用于指明请求的URL所对应的文档的目录路径；&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;server {
    ...
    root  /data/www/vhost1;
}
http://www.wisedu.com/images/logo.jpg --&amp;gt; /data/www/vhosts/images/logo.jpg

server {
    ...
    server_name  www.wisedu.com;

    location /images/ {
         root  /data/imgs/;
         ...
    }
}
http://www.wisedu.com/images/logo.jpg --&amp;gt; /data/imgs/images/logo.jpg
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;2.location [ = | ~ | ~* | ^~ ] uri { … }&lt;br&gt;  location @name { … }&lt;/strong&gt;&lt;br&gt;功能：允许根据用户请求的URI来匹配定义的各location，匹配到时，此请求将被相应的location块中的配置所处理；简言之，即用于为需要用到专用配置的uri提供特定配置。&lt;br&gt;&lt;strong&gt;先来看下location [ = | ~ | ~* | ^~ ] uri { … } &lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;location URI{}：对当前路径及所有对象都生效。&lt;/li&gt;
&lt;li&gt;location = URI{}：只对当前路径生效，不包括子路径。这是精确匹配。&lt;/li&gt;
&lt;li&gt;location ~ URI{}：&lt;/li&gt;
&lt;li&gt;location ~&lt;em&gt; URI{}：模式匹配URI，此处的URI可使用正则表达式，~区分字符大小写。~&lt;/em&gt;不区分字符大小写。&lt;/li&gt;
&lt;li&gt;location ^~ URI{}：明确说明不使用正则表达式。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;【注意】：如果被两个location匹配到，nginx是有优先级的。=优先级最高，^~优先级第二，模式匹配优先级第三，没加任何符号的优先级最低。&lt;br&gt;官方例子：&lt;a href=&quot;http://nginx.org/en/docs/http/ngx_http_core_module.html#location&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://nginx.org/en/docs/http/ngx_http_core_module.html#location&lt;/a&gt;&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;19&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;20&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;location = / &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    [ configuration A ]&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;location / &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    [ configuration B ]&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;location /documents/ &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    [ configuration C ]&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;location ^~ /images/ &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    [ configuration D ]&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;location ~* \.(gif|jpg|jpeg)$ &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    [ configuration E ]&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;The “/” request will match configuration A, the “/index.html” request will match configuration B, the “/documents/document.html” request will match configuration C, the “/images/1.gif” request will match configuration D, and the “/documents/1.jpg” request will match configuration E.&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;再来看下location @name { … }，命名的location&lt;/strong&gt;&lt;br&gt;The “@” prefix defines a named location. Such a location is not used for a regular request processing, but instead used for request redirection. They cannot be nested, and cannot contain nested locations.&lt;br&gt;@：内部服务跳转&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;server &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  location /img/ &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    set $memcached_key $uri;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    memcached_pass     name:11211;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    default_type       text/html;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    error_page         404 @fallback;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  &amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt; &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  location @fallback &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    proxy_pass http://backend;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  &amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  #以 /img/ 开头的请求，如果连接的状态为 404。则会匹配到 @fallback 这条规则上。&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3.alias&lt;/strong&gt;&lt;br&gt;定义路径别名。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;location  /images/ {
    root /data/imgs/;
}

location  /images/  {
    alias /data/imgs/;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;访问/images/test.jpg，对应的结果如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;root指令：给定的路径对应于location的“/”这个URL；&lt;br&gt;/images/test.jpg –&amp;gt;  /data/imgs/images/test.jpg &lt;/li&gt;
&lt;li&gt;alias指令：给定的路径对应于location的“/uri/“这个URL；&lt;br&gt;/images/test.jpg –&amp;gt;  /data/imgs/test.jpg，注意alias把location后配置的路径images丢弃掉了。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;【注意】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用alias时，目录名后面一定要加”/“。&lt;/li&gt;
&lt;li&gt;alias在使用正则匹配时，必须捕捉要匹配的内容并在指定的内容处使用。&lt;/li&gt;
&lt;li&gt;alias只能位于location块中。（root可以不放在location中）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;4.index&lt;/strong&gt; &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;index file ...;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;设置默认主页面。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5.error_page code … [=[response]] uri;&lt;/strong&gt;&lt;br&gt;根据http的状态码重定向错误页面；&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;error_page  404  /404.html
error_page  404  =200  /404.html  （以指定的响应状态码进行响应）
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;6.try_files file … uri;&lt;br&gt;  try_files file … =code;&lt;/strong&gt;&lt;br&gt;其作用是按顺序检查文件是否存在，尝试查找第1至第N-1个文件，返回第一个找到的文件或文件夹（结尾加斜线表示为文件夹），如果所有的文件或文件夹都找不到，会进行一个内部重定向到最后一个参数。（必须不能匹配至当前location，而应该匹配至其它location，否则会导致死循环）。&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;location / &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    try_files $uri $uri/ @fallback;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    root   /home/data/FS/desgin_style/;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;location @fallback &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    proxy_pass_header Server;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    proxy_set_header Host $http_host;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    proxy_set_header X-Real-IP $remote_addr;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    proxy_set_header X-Scheme $scheme;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt; &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    proxy_pass http://backend;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;$uri为Nginx内置变量，下面会讲到。&lt;/p&gt;
&lt;h3 id=&quot;客户端请求相关的配置&quot;&gt;&lt;a href=&quot;#客户端请求相关的配置&quot; class=&quot;headerlink&quot; title=&quot;客户端请求相关的配置&quot;&gt;&lt;/a&gt;客户端请求相关的配置&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;1.keepalive_timeout timeout [header_timeout];&lt;/strong&gt;&lt;br&gt;设定keepalive连接的超时时长；0表示禁止长连接；默认为75s。&lt;br&gt;KeepAlive，意思为是否长连接。如果设置了超时时间，那么在这个时间内，那么当Nginx完成用户的请求后，那么Nginx进程不会断开用户的请求连接，依然保持连接状态。设置成0s则当Nginx完成用户的请求后，那么Nginx进程会立即断开和用户的请求连接。&lt;br&gt;完成用户的请求后，连接依然存在着，这样的好处是：当该用户的请求在过来时，Nginx会用这个已经建立的连接，不需要重新创建连接。这样会节省CPU的资源。但是却耗费了内存。为什么呢？可以假设这样的场景。假如keepalive 超时时间为10s，而每1s中有100个用户请求访问，每个用户3次连接，每个连接耗费2M内存，那么10s内建立的连接次数为1000次（跟用户每s请求次数无关），消耗内存为1000x2=2000M，相反，如果不保持长连接，同样的环境场景下，每1s内有100x3个连接，下一秒还是100x3个连接，也就是说永远都是100x3个连接，那么1s内甚至10s内消耗的内存为100x3x2=600M。 然而，在这10s内创建的连接次数100x3x10=3000次，这样肯定消耗了更多的cpu资源。毕竟每次tcp连接都是需要cpu去处理的。&lt;br&gt;问题来了，既然知道长连接与否的利与弊，那么如何判定什么时候On，什么时候Off？&lt;br&gt;在上面的举例中，涉及到了一个数，那就是每个用户在1s内请求的次数，如果把3改为1，是不是10s内得到的连接次数总和是一样的。那么这样无论是On还是Off，消耗的CPU资源是一样的。所以，我们考虑3种情况：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;①用户浏览一个网页时，除了网页本身外，还引用了多个 javascript 文件，多个 css 文件，多个图片文件，并且这些文件都在同一个 HTTP 服务器上。&lt;/li&gt;
&lt;li&gt;②用户浏览一个网页时，除了网页本身外，还引用一个 javascript 文件，一个图片文件。&lt;/li&gt;
&lt;li&gt;③用户浏览的是一个动态网页，由程序即时生成内容，并且不引用其他内容。&lt;br&gt;对于上面3中情况，我认为：1 最适合打开 KeepAlive ，2 随意，3 最适合关闭 KeepAlive（连接消耗的内存比较大）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;总结一下：&lt;br&gt;在内存非常充足的服务器上，不管是否关闭 KeepAlive 功能，服务器性能不会有明显变化；&lt;br&gt;如果服务器内存较少，或者服务器有非常大量的文件系统访问时，或者主要处理动态网页服务，关闭 KeepAlive 后可以节省很多内存，而节省出来的内存用于文件系统Cache，可以提高文件系统访问的性能，并且系统会更加稳定。&lt;br&gt;目前的服务器，CPU很强，所以不用考虑频繁的tcp连接对cpu造成的压力，那还让它长连接干什么，故，建议关闭你的长连接吧！！！&lt;br&gt;PS： 如果，你的服务器上请求量很大，那你最好还是关闭这个参数吧。我试过一次，打开长连接，并且设置超时时间为30s，结果仅仅十几s就把所有的Nginx进程跑满。这样很危险的，直接让用户等待，等30s，这不扯淡嘛？即使是你设置成3s，照样会让用户等待3s，这样很不合理的。所以，归根结蒂还是关闭长连接吧，这样效率会更高。&lt;/p&gt;
&lt;p&gt;举个网上的具体例子：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1.项目环境：nginx（前段代理，仅作代理用途）+3个tomcat（都在同一个服务器上），做的web项目&lt;/li&gt;
&lt;li&gt;2.涉及到的业务逻辑：文件上传（可能有大文件，比如说android游戏，100m）；客户端接口请求；网站后台管理&lt;/li&gt;
&lt;li&gt;3.问题重现流程：&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;3.1 配置好tomcat后，直接加上nginx前段代理（仅配置了http代理）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;3.2 问题一：当管理员后台上传文件时，大文件无法上传成功，出现time-out，经重复测试，发现上传时间超过1分钟以后，就会返回超时信息，小文件没有问题&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;3.3 经调研得知nginx默认设置的http连接超时时间为75s，超过75s，会断掉当前的http连接，而大文件上传时经常会超过75s，这就导致大文件无法上传成功，当时的解决方案是，设置nginx http连接超时时间为30分钟，即参数keepalive_timeout=1800；文件上传问题基本解决；&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;3.4项目运行2天后，发现服务器突然宕机了，重启nginx可以解决问题，但是2个小时后又再次宕机，重启nginx又解决了问题，调研了一个中午，并且查看nginx的错误日志（socket() failed (24: Too many open files) while connecting to upstream），发现问题来源与nginx的连接数（设置的默认值为1024）达到上限&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;3.5发现这个问题后，我就想应该把nginx的连接数调大点，于是设置 worker_connections  10240；重启nginx，短时间没有出现问题，但是运行过程中，我再次查看错误日志，发现（socket() failed (24: Too many open files) while connecting to upstream）时不时的出现&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;3.6 此时发现调整nginx的连接数并不能完全解决问题，于是google，百度之，发现问题所在，罪魁祸首是：nginx的keepalive_timeout(参看&lt;a href=&quot;http://fengzheng369.blog.163.com/blog/static/752209792012418103813580/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://fengzheng369.blog.163.com/blog/static/752209792012418103813580/&lt;/a&gt; )设置项时间太长，客户端接口访问其实是一个比较快速的过程，访问完成了已经不需要继续使用http连接了，但是由于对nginx的错误配置，导致接口访问完成后http连接并没有被释放掉，所以导致连接数越来越大，最终nginx崩溃。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;4.那么这个问题应该如何解决呢？&lt;br&gt;将keepalive_timeout时间调小会导致上传操作可能无法完成；调大点的话，许多无效的http连接占据着nginx的连接数。这貌似是一个两难的问题。&lt;br&gt;解决方案一：将接口请求，后台管理，文件上传这三个业务逻辑分开，nginx对这三种业务逻辑分开转发，每个业务逻辑单独设置一个keepalive-timeout(未实验)。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;2.keepalive_requests number;&lt;/strong&gt;&lt;br&gt;在keepalived连接上所允许请求的最大资源数量；默认为100；&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3.keepalive_disable none | browser …;&lt;/strong&gt;&lt;br&gt;指明禁止为何种浏览器使用keepalive功能；&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4.send_timeout #;&lt;/strong&gt;&lt;br&gt;发送响应报文的超时时长，默认为60s; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5.client_body_buffer_size size;&lt;/strong&gt;&lt;br&gt;接收客户请求报文body的缓冲区大小；默认为16k；超出此指定大小时，其将被移存于磁盘上；&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;6.client_body_temp_path path [level1 [level2 [level3]]];&lt;/strong&gt;&lt;br&gt;设定用于存储客户端请求body的临时存储路径及子目录结构和数量；&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;client_body_temp_path  /var/tmp/client_body  2 2;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;7.client_max_body_size 10m;&lt;/strong&gt;&lt;br&gt;允许客户端请求的最大的单个文件字节数，这个参数可以限制body的大小，默认是1m。如果上传的文件较大，那么需要调大这个参数。&lt;/p&gt;
&lt;h3 id=&quot;对客户端请求的进行限制（限流）&quot;&gt;&lt;a href=&quot;#对客户端请求的进行限制（限流）&quot; class=&quot;headerlink&quot; title=&quot;对客户端请求的进行限制（限流）&quot;&gt;&lt;/a&gt;对客户端请求的进行限制（限流）&lt;/h3&gt;&lt;p&gt;nginx的限速功能通过limit_zone、limit_conn和limit_rate指令进行配置。首先需要在http上下文配置一个limit_zone，然后在需要的地方使用limit_conn和limit_rate 进行限速设置。下面是一个简单的例子。&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;http &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  limit_zone  first  $binary_remote_addr  10m;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  server &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    location /downloads/ &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      limit_conn   first  1;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      limit_rate 50k;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  &amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;【说明】:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;limit_zone：语法格式“limit_req_zone $variable zone=name:size rate=rate;”，实现针对每个IP定义一个存储session状态的容器。这个示例中定义了一个名叫first的10m大小的容器，这个名字会在后面的limit_conn中使用。&lt;/li&gt;
&lt;li&gt;limit_conn first 1; 限制在first中记录状态的每个IP只能发起一个并发连接。&lt;/li&gt;
&lt;li&gt;limit_rate 50k; 对每个连接限速50k. 注意，这里是对连接限速，而不是对IP限速。如果一个IP允许三个并发连接，那么这个IP就是限速为limit_rate×3，在设置的时候要根据自己的需要做设置调整，要不然会达不到自己希望的目的。&lt;/li&gt;
&lt;/ul&gt;
</content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;上篇博文中介绍了安装部署OpenResty，这篇博文主要记录下Nginx的配置及优化。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;Nginx&quot;&gt;&lt;a href=&quot;#Nginx&quot; class=&quot;headerlink&quot; title=&quot;Nginx&quot;&gt;&lt;/a&gt;Nginx&lt;/h2&gt;&lt;p&gt;Nginx的代码是由一个核心和一系列的模块组成, 核心主要用于提供Web Server的基本功能，以及Web和Mail反向代理的功能；
    
    </summary>
    
      <category term="Nginx" scheme="http://yoursite.com/categories/Nginx/"/>
    
    
      <category term="Nginx" scheme="http://yoursite.com/tags/Nginx/"/>
    
  </entry>
  
  <entry>
    <title>安装部署Openresty</title>
    <link href="http://yoursite.com/2018/01/21/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2Openresty/"/>
    <id>http://yoursite.com/2018/01/21/安装部署Openresty/</id>
    <published>2018-01-21T11:55:10.000Z</published>
    <updated>2018-01-27T02:28:22.000Z</updated>
    
    <content type="html">&lt;blockquote&gt;
&lt;p&gt;由于公司的一个产品用了Nginx Lua写了认证，所以在选型时选了OpenResty。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;什么是OpenResty&quot;&gt;&lt;a href=&quot;#什么是OpenResty&quot; class=&quot;headerlink&quot; title=&quot;什么是OpenResty&quot;&gt;&lt;/a&gt;什么是OpenResty&lt;/h2&gt;&lt;p&gt;Nginx 是俄罗斯人发明的， Lua 是巴西几个教授发明的，中国人章亦春把 LuaJIT VM 嵌入到 Nginx 中，实现了 OpenResty 这个高性能服务端解决方案。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;br&gt;最先将Nginx，Lua组合到一起的是OpenResty，它有一个ngx_lua模块，将Lua嵌入到了Nginx里面；随后Tengine也包含了ngx_lua模块。至于二者的区别：OpenResty是Nginx的Bundle；而Tengine则是Nginx的Fork。值得一提的是，OpenResty和Tengine均是国人自己创建的项目，前者主要由章亦春和晓哲开发，后者主要由淘宝打理。&lt;/p&gt;
&lt;h2 id=&quot;Nginx特性&quot;&gt;&lt;a href=&quot;#Nginx特性&quot; class=&quot;headerlink&quot; title=&quot;Nginx特性&quot;&gt;&lt;/a&gt;Nginx特性&lt;/h2&gt;&lt;h3 id=&quot;HTTP服务器&quot;&gt;&lt;a href=&quot;#HTTP服务器&quot; class=&quot;headerlink&quot; title=&quot;HTTP服务器&quot;&gt;&lt;/a&gt;HTTP服务器&lt;/h3&gt;&lt;p&gt;即web服务器，具有支持一个基本的web服务器应该具备的绝大多数功能。 (&lt;a href=&quot;http://www.netcraft.com/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://www.netcraft.com/&lt;/a&gt; 在这个站点上随时统计更新的有全球web服务器的占有状况)&lt;br&gt;灵活的配置：nginx和apache差不多，也是一个进程生成许多子进程，而这个子进程本身有worker进程，专门负责响应用户请求的。nginx是一个进程响应多个请求，事实上可以称为一个线程响应多个请求。除此之外还有许多其他进程，比如说管理缓存的进程，管理会话的进程。有了这样的架构设计，将来在重新添加配置之后，可以将新的连接都使用新配置，而老的连接即已经建立的连接使用原有的配置，所以在线升级的时候不需要中断正在处理的请求，这称为nginx热部署(无非就是平滑升级)。&lt;br&gt;重写(rewrite)模块：web服务器httpd的重写功能异常强大，但是也比较复杂。nginx比较简单，只需要使用正则表达式重写URL。对于任何一个web服务器来说，URL重写是个非常重要的功能，尤其是反向代理的服务器。&lt;/p&gt;
&lt;h3 id=&quot;模块化设计&quot;&gt;&lt;a href=&quot;#模块化设计&quot; class=&quot;headerlink&quot; title=&quot;模块化设计&quot;&gt;&lt;/a&gt;模块化设计&lt;/h3&gt;&lt;p&gt;nginx是高度模块化的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;核心模块：core module&lt;/li&gt;
&lt;li&gt;Standard HTTP modules&lt;/li&gt;
&lt;li&gt;Optional HTTP modules&lt;/li&gt;
&lt;li&gt;Mail modules&lt;/li&gt;
&lt;li&gt;3rd party modules&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;支持热部署&quot;&gt;&lt;a href=&quot;#支持热部署&quot; class=&quot;headerlink&quot; title=&quot;支持热部署&quot;&gt;&lt;/a&gt;支持热部署&lt;/h3&gt;&lt;p&gt;不停机更新配置文件、更换日志、更新服务器程序版本。&lt;/p&gt;
&lt;h3 id=&quot;主要用途&quot;&gt;&lt;a href=&quot;#主要用途&quot; class=&quot;headerlink&quot; title=&quot;主要用途&quot;&gt;&lt;/a&gt;主要用途&lt;/h3&gt;&lt;p&gt;nginx有两个作用。第一，web服务器。具有web服务器要求的所有功能。第二，轻量级的反向代理服务器，能够反向代理两种应用：web和mail，只不过反向代理mail的功能很少被提到。我们的主要着眼点也是它的web服务器和web服务器反向代理。&lt;br&gt;Nginx基于File AIO(异步I/O)，在文件级别上磁盘I/O上基于异步I/O来实现的；同时对于异步通信，nginx基于事件驱动加上边缘触发来完成一个线程处理多个请求，这对于c10k问题是十分有效的解决方案。&lt;/p&gt;
&lt;h2 id=&quot;Nginx工作模式、框架、模型&quot;&gt;&lt;a href=&quot;#Nginx工作模式、框架、模型&quot; class=&quot;headerlink&quot; title=&quot;Nginx工作模式、框架、模型&quot;&gt;&lt;/a&gt;Nginx工作模式、框架、模型&lt;/h2&gt;&lt;p&gt;nginx会按需同时运行多个进程：一个主进程(master)和几个工作进程(worker)，配置了缓存时还会有缓存加载器进程(cache loader)和缓存管理器进程(cache manager)等。所有进程均是仅含有一个线程，并主要通过“共享内存”的机制实现进程间通信。主进程以root用户身份运行，而worker、cache loader和cache manager均应以非特权用户身份运行。 &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;master进程(线程)监控worker进程(线程)，包括运行是否正常等。&lt;/li&gt;
&lt;li&gt;worker进程：master的子进程，响应用户请求。&lt;/li&gt;
&lt;li&gt;cache loader进程：在反向代理的时候用于管理缓存的进程。&lt;/li&gt;
&lt;li&gt;·······&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Nginx/8.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;注意：master进程是以管理员启动的，因为nginx要启动为web服务器或反向代理的时候，它应该工作在80端口，只有管理员才有权限启用小于1023的端口，所以启动主进程的时候必须使用管理员身份。然后由master进程启动worker进程，而且master启动worker进程的时候完全以普通用户的身份运行，所以让我们的系统安全性得到提升。&lt;br&gt;nginx是高度模块化的，所以nginx的核心模块，像master进程、worker进程所处理的web应用非常简单，最多就是个web服务器，而额外的更多功能比如SSL、FLV流等都不是worker进程自己提供的，而是由额外的模块提供，所以在每个worker进程内部，它可能调用很多个模块，用到哪个模块再去加载哪个模块，而这些模块以流水线的方式工作，也就是一个模块只负责一个功能。比如说第一个分析头部，第二个帮助去的数据，第三个创建响应等等。每个请求，在内部串联起来的模块还不一样，所以每个请求到来了，worker一分析，到底要使用几个模块，这几个模块组成流水线，随时等待响应。&lt;br&gt;正是由于这种机制的存在，master负责装在主配置文件，如果我们改了nginx配置文件，由master分析一下配置文件中有没有语法错误，就算重新装在配置文件有语法错误，但是并不会影响worker进程，最多master返回错误告诉你配置文件有语法错误，需要重新装载。一装载成功了，并不会让运行中的worker进程都使用这个配置是文件的，让这些已经建立的连接继续使用老的配置文件，当某个worker进程建立的连接都退出了，此时把worker进程挂了，再重新启动个新worker进程，新worker进程响应新请求，而这个新worker进程用的就是新配置。&lt;br&gt;&lt;strong&gt;【nginx进程分工】：&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;主进程master：&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;读取并验正配置信息；&lt;/li&gt;
&lt;li&gt;创建、绑定及关闭套接字；&lt;/li&gt;
&lt;li&gt;启动、终止及维护worker进程的个数；&lt;/li&gt;
&lt;li&gt;无须中止服务而重新配置工作特性；&lt;/li&gt;
&lt;li&gt;控制非中断式程序升级，启用新的二进制程序并在需要时回滚至老版本；   ——nginx自身升级&lt;/li&gt;
&lt;li&gt;重新打开日志文件，实现日志滚动；&lt;/li&gt;
&lt;li&gt;编译嵌入式perl脚本；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;worker进程：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;接收、传入并处理来自客户端的连接；&lt;/li&gt;
&lt;li&gt;提供反向代理及过滤功能；&lt;/li&gt;
&lt;li&gt;nginx任何能完成的其它任务；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;cache loader进程：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;检查缓存存储中的缓存对象；&lt;/li&gt;
&lt;li&gt;使用缓存元数据建立内存数据库；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;cache manager进程：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;缓存的失效及过期检验；&lt;br&gt;【注意】：cache loader和cache manager只是在使用nginx作为反向代理并使用了缓存功能的时候才启动，不是所有时候都运行的。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;安装OpenResty&quot;&gt;&lt;a href=&quot;#安装OpenResty&quot; class=&quot;headerlink&quot; title=&quot;安装OpenResty&quot;&gt;&lt;/a&gt;安装OpenResty&lt;/h2&gt;&lt;p&gt;使用的版本是openresty-1.9.7.3.tar.gz&lt;br&gt;&lt;strong&gt;1.安装依赖包&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# yum install readline-devel pcre-devel openssl-devel gcc -y
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;2.安装openresty&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cd /usr/local
# wget https://openresty.org/download/openresty-1.9.7.3.tar.gz
# tar zxf openresty-1.9.7.3.tar.gz
# cd openresty-1.9.7.3/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查看可配置的选项：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# ./configure --help
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Nginx/9.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;常用的配置参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;–-prefix=path&lt;br&gt;设置安装目录，默认为/usr/local/openresty&lt;/li&gt;
&lt;li&gt;–user=name&lt;br&gt;设置工作进程使用的非特权用户的用户名，默认为nobody。安装完成后可以在nginx.conf中通过user指令修改。&lt;/li&gt;
&lt;li&gt;–group=name&lt;br&gt;设置工作进程使用的非特权用户组的名称，默认组名和–user的名称一致。安装完成后可以在nginx.conf配置文件中通过user指令指定。&lt;/li&gt;
&lt;li&gt;–-with-http_ssl_module&lt;br&gt;启用添加HTTPS协议支持到HTTP服务器的模块，该模块默认不启用。构建和运行该模块需要OpenSSL库。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# ./configure
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;编译和安装：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# gmake &amp;amp;&amp;amp; gmake install
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;安装完成后，在/usr/local/下多了个openresty目录，nginx部署安装在/usr/local/openresty/nginx。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3.将nginx加入系统服务&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;【Redhat7之前的版本】：&lt;/strong&gt;&lt;br&gt;(1)    上传脚本nginx启动脚本到/etc/init.d/目录下&lt;br&gt;(2)    授权脚本执行权限&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# chmod a+x nginx
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;(3)    加入系统服务&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# chkconfig --add nginx
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;(4)    nginx开启自启动&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# chkconfig nginx on
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;(5)    nginx启停重载&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# service nginx start/stop/restart/reload
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;【Redhat7版本】：&lt;/strong&gt;&lt;br&gt;(1)    启动服务单元&lt;br&gt;把写好的nginx.service放到/etc/systemd/system/目录下。&lt;br&gt;(2)    设置开机启动&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl enable nginx.service
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;(3)    启动/停止/重载nginx服务&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl start/stop/reload nginx.service
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;脚本nginx的内容如下：&lt;/strong&gt;&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;19&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;20&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;21&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;22&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;23&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;24&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;25&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;26&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;27&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;28&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;29&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;30&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;31&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;32&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;33&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;34&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;35&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;36&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;37&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;38&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;39&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;40&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;41&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;42&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;43&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;44&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;45&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;46&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;47&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;48&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;49&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;50&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;51&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;52&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;53&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;54&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;55&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;56&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;57&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;58&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;59&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;60&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;61&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;62&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;63&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;64&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;65&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;66&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;67&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;68&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;69&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;70&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;71&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;72&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;73&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;74&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;75&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;76&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;77&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;78&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;79&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;80&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;81&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;82&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;83&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;84&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;85&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;86&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;87&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;88&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;89&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;90&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;91&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;92&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;93&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;94&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;95&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;#!/bin/sh&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;#&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;# nginx - this script starts and stops the nginx daemon&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;#&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;# chkconfig:   - 85 15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;# description: Nginx is an HTTP(S) server, HTTP(S) reverse \&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;#               proxyand IMAP/POP3 proxy server&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;# processname: nginx&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;# config:      /etc/nginx/nginx.conf&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;# config:      /etc/sysconfig/nginx&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;# pidfile:     /var/run/nginx.pid&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;# Source function library.&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;. /etc/rc.d/init.d/functions&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;# Source networking configuration.&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;. /etc/sysconfig/network&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;# Check that networking is up.&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;[ &amp;quot;$NETWORKING&amp;quot; = &amp;quot;no&amp;quot; ] &amp;amp;&amp;amp; exit 0&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;nginx=&amp;quot;/usr/local/openresty/nginx/sbin/nginx&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;prog=$(basename $nginx)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;NGINX_CONF_FILE=&amp;quot;/usr/local/openresty/nginx/conf/nginx.conf&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;[ -f /etc/sysconfig/nginx ] &amp;amp;&amp;amp; . /etc/sysconfig/nginx&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;lockfile=/var/lock/subsys/nginx&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;start() &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    [ -x $nginx ] || exit 5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    [ -f $NGINX_CONF_FILE ] || exit 6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    echo -n $&amp;quot;Starting $prog: &amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    daemon $nginx -c $NGINX_CONF_FILE&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    retval=$?&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    echo&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    [ $retval -eq 0 ] &amp;amp;&amp;amp; touch $lockfile&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    return $retval&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;stop() &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    echo -n $&amp;quot;Stopping $prog: &amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    killproc $prog -QUIT&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    retval=$?&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    echo&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    [ $retval -eq 0 ] &amp;amp;&amp;amp; rm -f $lockfile&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    return $retval&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;killall -9 nginx&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;restart() &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    configtest || return $?&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    stop&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    sleep 1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    start&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;reload() &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    configtest || return $?&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    echo -n $&amp;quot;Reloading $prog: &amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    killproc $nginx -HUP&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;RETVAL=$?&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    echo&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;force_reload() &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    restart&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;configtest() &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;$nginx -t -c $NGINX_CONF_FILE&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;rh_status() &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    status $prog&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;rh_status_q() &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    rh_status &amp;gt;/dev/null 2&amp;gt;&amp;amp;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;case &amp;quot;$1&amp;quot; in&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    start)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        rh_status_q &amp;amp;&amp;amp; exit0&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    $1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        ;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    stop)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        rh_status_q || exit 0&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        $1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        ;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    restart|configtest)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        $1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        ;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    reload)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        rh_status_q || exit 7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        $1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        ;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    force-reload)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        force_reload&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        ;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    status)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        rh_status&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        ;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    condrestart|try-restart)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        rh_status_q || exit 0&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            ;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    *)  &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      echo $&amp;quot;Usage: $0 &amp;#123;start|stop|status|restart|condrestart|try-restart|reload|force-reload|configtest&amp;#125;&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        exit 2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;esac&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;服务单元nginx.service的内容如下：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[Unit]
Description=nginx - high performance web server
Documentation=http://nginx.org/en/docs/
After=network.target remote-fs.target nss-lookup.target

[Service]
Type=forking
PIDFile=/usr/local/openresty/nginx/logs/nginx.pid
ExecStartPre=/usr/local/openresty/nginx/sbin/nginx -t -c /usr/local/openresty/nginx/conf/nginx.conf
ExecStart=/usr/local/openresty/nginx/sbin/nginx -c /usr/local/openresty/nginx/conf/nginx.conf
ExecReload=/bin/kill -s HUP $MAINPID
ExecStop=/bin/kill -s QUIT $MAINPID
PrivateTmp=true

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【补充】:&lt;br&gt;RHEL/CentOS 7.0中一个最主要的改变，就是切换到了systemd。它用于替代红帽企业版Linux前任版本中的SysV和Upstart，对系统和服务进行管理。systemd兼容SysV和Linux标准组的启动脚本。&lt;br&gt;Systemd 是 Linux 系统中最新的初始化系统（init），它主要的设计目标是克服 sysvinit 固有的缺点，提高系统的启动速度。systemd 和 ubuntu 的 upstart 是竞争对手，已经取代了UpStart。&lt;br&gt;Systemd也是一个Linux操作系统下的系统和服务管理器。它被设计成向后兼容SysV启动脚本，并提供了大量的特性，如开机时平行启动系统服务，按需启动守护进程，支持系统状态快照，或者基于依赖的服务控制逻辑。&lt;br&gt;先前的使用SysV初始化或Upstart的红帽企业版Linux版本中，使用位于/etc/rc.d/init.d/目录中的bash初始化脚本进行管理。而在RHEL 7/CentOS 7中，这些启动脚本被服务单元取代了。服务单元以.service文件扩展结束，提供了与初始化脚本同样的用途。要查看、启动、停止、重启、启用或者禁用系统服务，你要使用systemctl来代替旧的service命令。&lt;br&gt;注：为了向后兼容，旧的service命令在CentOS 7中仍然可用，它会重定向所有命令到新的systemctl工具。&lt;/p&gt;
&lt;p&gt;关于Nginx的具体配置将在下篇博文中分享。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;由于公司的一个产品用了Nginx Lua写了认证，所以在选型时选了OpenResty。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;什么是OpenResty&quot;&gt;&lt;a href=&quot;#什么是OpenResty&quot; class=&quot;headerlink&quot; title=&quot;什么是OpenResty&quot;&gt;&lt;/a&gt;什么是OpenResty&lt;/h2&gt;&lt;p&gt;Nginx 是俄罗斯人发明的， Lua 是巴西几个教授发明的，中国人章亦春把 LuaJIT VM 嵌入到 Nginx 中，实现了 OpenResty 这个高性能服务端解决方案。&lt;br&gt;
    
    </summary>
    
      <category term="Nginx" scheme="http://yoursite.com/categories/Nginx/"/>
    
    
      <category term="Nginx" scheme="http://yoursite.com/tags/Nginx/"/>
    
  </entry>
  
  <entry>
    <title>I/O模型</title>
    <link href="http://yoursite.com/2018/01/18/I-O%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2018/01/18/I-O模型/</id>
    <published>2018-01-18T09:08:21.000Z</published>
    <updated>2018-01-21T12:12:42.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;I-O类型&quot;&gt;&lt;a href=&quot;#I-O类型&quot; class=&quot;headerlink&quot; title=&quot;I/O类型&quot;&gt;&lt;/a&gt;I/O类型&lt;/h2&gt;&lt;p&gt;从不同的角度来划分，有两种不同的方式：&lt;br&gt;&lt;strong&gt;同步I/O和异步I/O&lt;/strong&gt;    ——synchronous, asynchronous&lt;br&gt;同步和异步关注的是消息通知机制。说白了就是如何通知调用者的。I/O就是一方能够提供服务，一方需要调用别人的服务，&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;所以I/O请求就是调用方向被调用方请求运行一个应用，通常是一个函数，可以理解成是一个库调用、函数调用或者系统调用。假如是一次系统调用，调用方向被调用方发起一次系统调用请求，被调用方本地要把这个系统调用运行完成，所以要在本地处理处理，把处理的结果响应给调用方。问题是调用方什么时候知道自己的请求结束了呢？自己的请求对方响应了呢？所以这就是同步和异步两种模式。&lt;br&gt;所谓同步是调用发出之后不会立即返回，但一旦返回，则返回是最终结果；(被调用者一直在处理处理，处理到最后返回结果给调用者)&lt;br&gt;所谓异步是调用发出之后，被调用方返回消息，但返回的并非最终结果；被调用者通过状态、通知机制(比如打电话通知你)等来通知调用者，或通过回调函数来处理结果。 (当调用者发出请求以后，被调用者立即就告诉调用者了，比如：请求已收到，等着叫号吧。)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;阻塞I/O和非阻塞I/O&lt;/strong&gt;    ——block, nonblock&lt;br&gt;阻塞和非阻塞关注的是调用者等待被调用返回结果时的状态。&lt;br&gt;阻塞是指调用结果返回之前，调用者(一般是进程或者线程)会被挂起；调用者只有在得到返回结果之后才能继续；&lt;br&gt;非阻塞是指调用者在结果返回之前，不会被挂起，即调用不会阻塞调用者。&lt;/p&gt;
&lt;p&gt;【例子】：你去一个饭馆吃饭去了，点了一碗面，师傅在现做。然后你就在那等着，这期间不能做其他的事情，这就是阻塞。还有一种方案是在等面的时候该干嘛干嘛去，比如出去玩了会游戏等，估摸着面差不多的时候回到饭馆，吃面。这就是非阻塞。&lt;/p&gt;
&lt;h2 id=&quot;I-O模型&quot;&gt;&lt;a href=&quot;#I-O模型&quot; class=&quot;headerlink&quot; title=&quot;I/O模型&quot;&gt;&lt;/a&gt;I/O模型&lt;/h2&gt;&lt;p&gt;同步、异步和阻塞、非阻塞看起来很像，但是关注的点不同，一个关注着调用者如何等待结果，一个关注着被调用者如何通知调用者调用完成的。所以压根不是一回事。站在这个角度来划分的话，I/O可以分成5种模型：&lt;br&gt;  ● 阻塞式I/O(blocking I/O)&lt;br&gt;  ● 非阻塞式I/O(nonblocking I/O)&lt;br&gt;  ● 复用式I/O(I/O multiplexing)&lt;br&gt;  ● 事件驱动式I/O(signal driven I/O)&lt;br&gt;  ● 异步I/O(asynchronous I/O)&lt;/p&gt;
&lt;p&gt;下面以磁盘I/O来解释，这些概念非常关键。例如，用户程序发起一个I/O调用，如从磁盘上做一次read操作(用户空间的进程是没有权限直接访问文件的，进程向用户内核发起I/O调用，请求说我要读取某数据)：&lt;br&gt;(1)内核从磁盘将数据加载到内核内存空间；&lt;br&gt;(2)将内核内存中的数据copy一份到进程内存空间。&lt;br&gt;以上两步中真正被称为I/O的那一步其实只是第(2)步，第(1)步只是内核处理数据的过程。&lt;/p&gt;
&lt;h3 id=&quot;阻塞式I-O-blocking-I-O&quot;&gt;&lt;a href=&quot;#阻塞式I-O-blocking-I-O&quot; class=&quot;headerlink&quot; title=&quot;阻塞式I/O(blocking I/O)&quot;&gt;&lt;/a&gt;阻塞式I/O(blocking I/O)&lt;/h3&gt;&lt;p&gt;下图中，左侧竖线代表调用者，右侧竖线代表被调用者。整个I/O调用在右侧内核来看，分为两步。如上。所谓阻塞式I/O指的是调用方发起调用后将会被挂起，这个进程或线程将转为不可中断式睡眠状态。调用者在得到返回结果之前，什么事都不能做，一直处于等待过程当中。想象一下，假如是个web服务器，第一个用户请求来了，由一个进程响应用户请求，这个进程向内核发起I/O调用请求，加载用户请求的页面资源，在加载这个页面资源的过程当中，假设它工作于阻塞式I/O的话，它就会被挂起，它显然不能响应用户的其他请求。Apache的prefork并不是工作在阻塞式I/O之下。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Nginx/1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;非阻塞式I-O-nonblocking-I-O&quot;&gt;&lt;a href=&quot;#非阻塞式I-O-nonblocking-I-O&quot; class=&quot;headerlink&quot; title=&quot;非阻塞式I/O(nonblocking I/O)&quot;&gt;&lt;/a&gt;非阻塞式I/O(nonblocking I/O)&lt;/h3&gt;&lt;p&gt;左侧是调用者，右侧是被调用者。对于被调用者，依然是两个阶段，数据从磁盘到内核内存，然后数据从内核内存到进程内存。对于非阻塞式I/O，调用者也把这个过程分成两个阶段(左侧也是两个阶段)，第1阶段，当调用者发出请求之后，被调用者立即告诉你：请求已收到，你等着吧。但问题是等到什么时候呢？就像去面馆点了一碗面一样，然后你出去玩了，但是你怎么知道面好了呢？为了及时得到吃面，所以你不得不每隔几秒钟回去看看面好了没有。这种就叫做盲等，效率并不高。这种不会直接转为睡眠状态，直到好了告诉你。一旦老板告诉你面OK了，是指面已经在柜台上了，其实是指数据从磁盘到内核内存了，你想吃还得自己从柜台上端过去。所以第二阶段你去端面，在这个过程你是什么事情都是做不了的，所以虽然是非阻塞，第2阶段依然是阻塞的。&lt;br&gt;所以对于非阻塞式I/O，第一阶段是盲等，第二阶段依然是阻塞的。很显然第二种模式相对第一种模式，性能并没有提升。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Nginx/2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;复用式I-O-I-O-multiplexing&quot;&gt;&lt;a href=&quot;#复用式I-O-I-O-multiplexing&quot; class=&quot;headerlink&quot; title=&quot;复用式I/O(I/O multiplexing)&quot;&gt;&lt;/a&gt;复用式I/O(I/O multiplexing)&lt;/h3&gt;&lt;p&gt;任何一个进程，它只能处理一个I/O，因为它一旦被一个I/O阻塞了，直到被唤醒之前，其他人干的事情它一概不知。但作为web服务器进程，它其实是处理两路I/O的，第一路用户通过网络进来，这是网络I/O，第二路是自己向内核发请求加载数据，这是磁盘I/O。这是两路不同的I/O，因此一旦进程被阻塞在磁盘I/O上，如果这个时候网络I/O发生异动了，这个进程是不会知道的。默认情况下，只能处理完一路I/O，在去处理另一路I/O。&lt;br&gt;再举个例子，比如终端执行一个命令，然后阻塞在磁盘I/O上，你不要了，按了ctrl+c，按道理这个进程处于不可中断睡眠，它应该是不知道的。怎么才能取消。现实中是能取消的，这就是多路I/O(或者说是复用I/O)的工作机制了。&lt;br&gt;默认情况下，调用者向被调用者发起调用请求时，如果阻塞了，调用者就任何事都做不了，任何信号都处理不了。为了避免这种情况，有人就在内核中开发了复用式I/O的程序，当调用者需要发起I/O调用时，而是内核给调用者准备了个代理人，调用者将请求发给这个代理人，代理人在把这个请求转为内核可以理解的请求，这样一来，调用者就被阻塞在代理人上，而不是阻塞在内核中完成任务的那个事情上。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Nginx/3.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;再举个例子，银行很多人在办业务，都在那排队，假如柜台是单进程模型的，在某个时刻只能处理一个业务。这个银行柜台就相当于内核准备的代理人，比如第一个用户想通过这个银行柜台开一张银行卡。向营业员发调用请求，给我开张银行卡，营业员在把请求结果响应给调用者之前，一般而言，这个调用者要一直在柜台坐在那等着。但是也有第二种情况，比如银行业务是两段式的，营业员不直接面对客户，每个营业员配一个助理，这个助理就站在柜台前，负责处理用户请求。所以当任何一个用户请求来了，用户请求扔给助理，助理帮你负责送到内部去。银行柜台有多个，分别办理不同的业务，比如说某个请求既要开张银行卡，又要存款。假设两个业务能同时进行的话，请求者跟助理说我要开张银行卡，于是助理把这个请求扔给一柜台了，自己就闲出来了。然后请求者又扔给助理一个请求，说我要存钱，于是助理又要第二个请求扔给第二个柜台了。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Nginx/4.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;内核的那个代理人就相当于助理，帮我们实现多路I/O。如果没有这个助理，想完成复用是不可能的。这个助理在内核中就是一种特殊的系统调用。早期的内核有个select()调用，就是这种模式。另外一个poll()调用，也是这种模式。select()是由BSD研发的，后来BSD内部吵架，又模仿select()写了个poll()。两者功能是一样的，只不过一个是BSD风格的，一个是SysV风格的。&lt;br&gt;此后，任何调用者在发起系统调用时，它把调用请求不是扔给内核，而是扔给内核中的助理了。或者说是向内核助理注册一个I/O。select()在实现多路I/O时有限制，要求最多不能超出1024路，如果你想超出，拿内核源码改一改。改成2048，没问题，但是1024的限制是由道理的，因为超出后性能会下降的。我们在讲到prefork模型时，说最多能接受1024个请求并发，其实prefork就是基于select()多路复用I/O模型来实现的。&lt;br&gt;对于这种复用式I/O，依然是阻塞型的，只不过它不是阻塞在自己真正的那个调用之上，而是阻塞在那个助理上，也是select()或poll()上。&lt;br&gt;因此两个阶段，第一个阶段数据从磁盘到内核空间，这个阶段调用者是被阻塞的，只不过没阻塞在内核的I/O调用上，而是阻塞在select()或poll()上。select()这个时候还可以接受其他请求。阻塞在select()之上的最大好处就是可以继续接受其他请求，因为它能够接受其他信号进来。但是对于第二阶段来讲，调用者依然是阻塞的。第二阶段是真正的I/O过程，这个时候调用者就不是阻塞在select()之上，而是阻塞在自己真正的调用之上。&lt;br&gt;复用式I/O对性能并没有提升，最多只是能处理额外的事情而已，并不是在性能上有什么提升。&lt;/p&gt;
&lt;h3 id=&quot;事件驱动式I-O-signal-driven-I-O&quot;&gt;&lt;a href=&quot;#事件驱动式I-O-signal-driven-I-O&quot; class=&quot;headerlink&quot; title=&quot;事件驱动式I/O(signal driven I/O)&quot;&gt;&lt;/a&gt;事件驱动式I/O(signal driven I/O)&lt;/h3&gt;&lt;p&gt;调用者发起调用之后，在第一阶段，内核立即返回结果给调用者，告诉调用者：你的请求我已经收到了，你该干嘛干嘛去，一旦我这边完成了，我会通知你。就相当于你去吃面了，老板告诉你面做好了我会打电话通知你的。这不是盲等，你该干嘛干嘛，不需要回来看，过一会老板通知你OK了，你就回来了。回来后，面在柜台上，我们去端面。端面的过程才是真正的I/O过程，这个过程是不能干其他事情的。(也像我们去餐馆，没位置，留了个电话号码，然后出去玩了，等服务员电话。)&lt;br&gt;这个过程中，第一段是非阻塞的，第二段是阻塞的。作为web服务器来讲，这有什么用呢？一个进程，第一个用户请求来了，进程向内核发起系统调用，加载文件，内核说你该干嘛干嘛去。于是这个进程就闲下来了，这样它就可以处理其它请求了。这就是为什么一个进程可以处理多个请求的原因。但是这个并不意味着性能一定好。虽然已经比阻塞和盲等有优势了，但是它第二段依然是阻塞的。面已经OK了，你仍然得自己端，所以在内核空间数据复制到进程空间这段，仍然是阻塞的。&lt;br&gt;为什么叫事件驱动呢？因为一旦数据从磁盘到内核空间后，内核会通知调用者。所以调用者本身使用回调函数来处理。&lt;br&gt;假如第二个请求数据已经准备好了，等着调用者进程将数据从内核空间复制到用户空间，但是假如此时调用者阻塞在第一个请求上，正在复制第一个请求的数据到进程空间，怎么办？内核通知完了在特定时间内你没有接收，这个信号就消失了。怎么办？所以通知与否，怎么通知，这就引入了两种通知机制。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第一种：水平触发。通知一次，你没响应，没来处理，内核就再通知一次。没来处理留再通知一次，直到你处理为止。多次通知看上去更可靠，但是浪费资源。(类似老板一直打电话给你，直到你接电话)&lt;/li&gt;
&lt;li&gt;第二种：边缘触发。只通知一次，但是如果调用者没响应怎么办呢？把通知事件通过回调函数让调用者自行获取，或者把通知信息放置某处，或者过一会儿调用者自己去要。(老板打了一遍电话，你没接，怕面凉了，就端到厨房。过一会你看到未接电话，知道面好了，就回来了，跟老板要。这叫回调。至于什么是回调函数，可以看：&lt;a href=&quot;https://www.zhihu.com/question/19801131&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://www.zhihu.com/question/19801131&lt;/a&gt;)&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Nginx/5.png&quot; alt=&quot;&quot;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;异步I-O-asynchronous-I-O&quot;&gt;&lt;a href=&quot;#异步I-O-asynchronous-I-O&quot; class=&quot;headerlink&quot; title=&quot;异步I/O(asynchronous I/O)&quot;&gt;&lt;/a&gt;异步I/O(asynchronous I/O)&lt;/h3&gt;&lt;p&gt;当调用者向内核发调用，内核说请求已收到，该干嘛干嘛去。于是内核在后台默默地完成第一步，默默地完成第二步。然后才告诉调用者，饭已OK了，过来吃吧。饭都不用你端了，直接由营业员端到你事先定义好的座位上。因此第一个阶段不用阻塞，第二个阶段不用阻塞。&lt;br&gt;作为一个web服务器来讲，当第一个用户请求进来时，用户请求资源，这个进程向内核发起系统调用，内核自己把数据从磁盘到内核空间，在复制到进程空间，所有东西都准备好了，告诉进程OK了，进程就立即打包响应报文响应给客户端。&lt;br&gt;httpd的event模型就是事件驱动型I/O，但是据说新版2.4也支持异步I/O。在异步模型下，生产力大大解放。内核从磁盘加载数据到内核内存中，一般都会缓存下来。如果第二个用户请求的资源和前面的用户请求的资源是一样的，直接响应就可以了。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Nginx/6.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;五种I-O模型对比&quot;&gt;&lt;a href=&quot;#五种I-O模型对比&quot; class=&quot;headerlink&quot; title=&quot;五种I/O模型对比&quot;&gt;&lt;/a&gt;五种I/O模型对比&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Nginx/7.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;有通知机制的我们称为异步I/O，无通知机制的我们称为同步I/O。见上图最下面，前面3个是同步I/O，后面2个是异步I/O。&lt;/p&gt;
&lt;p&gt;Nginx在设计时，用的就是事件驱动型I/O，而且基于边缘触发来实现。Nginx还支持异步I/O。而且还能完成mmap机制，内存映射机制。&lt;/p&gt;
&lt;p&gt;Nginx基于File AIO(异步I/O)，在文件级别上磁盘I/O上基于异步I/O来实现的；同时对于异步通信，nginx基于事件驱动加上边缘触发来完成一个线程处理多个请求，这对于c10k问题是十分有效的解决方案。&lt;/p&gt;
&lt;h2 id=&quot;HTTPD的MPM&quot;&gt;&lt;a href=&quot;#HTTPD的MPM&quot; class=&quot;headerlink&quot; title=&quot;HTTPD的MPM&quot;&gt;&lt;/a&gt;HTTPD的MPM&lt;/h2&gt;&lt;p&gt;httpd的MPM（多道处理模块）&lt;br&gt;  ● prefork：进程模型的。用的是复用型I/O。&lt;br&gt;  ● worker：线程模型的。用的是复用型I/O。并发有限，select()最多1024个。&lt;br&gt;  ● event：线程模型的。用的是事件驱动式I/O。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;prefork工作模型&lt;br&gt;有一个主进程，主进程生成多个子进程，每个子进程处理一个请求。主进程是以管理员的身份启动的，所以它能够监听在80端口上。端口&amp;lt;1024的被称为特权端口，只有管理员才有权限使用的。&lt;/li&gt;
&lt;li&gt;worker工作模型&lt;br&gt;有一个主进程，主进程生成多个子进程，每个子进程在生成多个线程，每个线程响应一个请求。&lt;/li&gt;
&lt;li&gt;event工作模型&lt;br&gt;有一个主进程，生成多个子进程，每个子进程响应多个请求。当然你也可以理解有一个主进程，生成多个子线程，对Linux而言，子进程和线程并没有严格意义上的区分。而event模型中最著名的、最典型的特性是事件驱动机制。&lt;/li&gt;
&lt;/ul&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;I-O类型&quot;&gt;&lt;a href=&quot;#I-O类型&quot; class=&quot;headerlink&quot; title=&quot;I/O类型&quot;&gt;&lt;/a&gt;I/O类型&lt;/h2&gt;&lt;p&gt;从不同的角度来划分，有两种不同的方式：&lt;br&gt;&lt;strong&gt;同步I/O和异步I/O&lt;/strong&gt;    ——synchronous, asynchronous&lt;br&gt;同步和异步关注的是消息通知机制。说白了就是如何通知调用者的。I/O就是一方能够提供服务，一方需要调用别人的服务，
    
    </summary>
    
      <category term="Nginx" scheme="http://yoursite.com/categories/Nginx/"/>
    
    
      <category term="Nginx" scheme="http://yoursite.com/tags/Nginx/"/>
    
  </entry>
  
  <entry>
    <title>Nginx配置HTTP/2.0</title>
    <link href="http://yoursite.com/2018/01/16/Nginx%E9%85%8D%E7%BD%AEHTTP-2-0/"/>
    <id>http://yoursite.com/2018/01/16/Nginx配置HTTP-2-0/</id>
    <published>2018-01-16T05:53:17.000Z</published>
    <updated>2018-01-16T10:49:01.000Z</updated>
    
    <content type="html">&lt;blockquote&gt;
&lt;p&gt;最近将公司前端组件的Nginx配置了http 2.0，特此在此记录一下。突然发现博客里竟然没有什么关于Nginx的文章，毕竟是一直在使用的，后面会写几篇关于自己这几年使用Nginx的一些总结。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;从 2015 年 5 月 14 日 HTTP/2 协议正式版的发布到现在已经快有一年了，越来越多的网站部署了 HTTP2，HTTP2 的广泛应用带来了更好的浏览体验，只要是 Modern 浏览器都支持，所以部署 HTTP2 并不会带来太多困扰。&lt;br&gt;虽然 h2 有 h2c （HTTP/2 Cleartext） 可以通过非加密通道传输，但是支持的浏览器初期还是比较少的，所以目前部署 h2 还是需要走加密的，不过由于 Let’s Encrypt 大力推行免费证书和证书的廉价化，部署 h2 的成本并不高。&lt;/p&gt;
&lt;h2 id=&quot;HTTP-2-0介绍&quot;&gt;&lt;a href=&quot;#HTTP-2-0介绍&quot; class=&quot;headerlink&quot; title=&quot;HTTP 2.0介绍&quot;&gt;&lt;/a&gt;HTTP 2.0介绍&lt;/h2&gt;&lt;p&gt;HTTP 2.0即超文本传输协议 2.0，是下一代HTTP协议。是由互联网工程任务组（IETF）的Hypertext Transfer Protocol Bis (httpbis)工作小组进行开发。是自1999年http1.1发布后的首个更新。&lt;br&gt;HTTP/2 协议是从 SPDY 演变而来，SPDY 已经完成了使命并很快就会退出历史舞台（例如 Chrome 将在「2016 年初结束对 SPDY 的支持」；Nginx、Apache 也已经全面支持 HTTP/2 ，并也不再支持 SPDY）。&lt;br&gt;一般的大家把 HTTP2 简称为 h2，尽管有些朋友可能不怎么愿意，但是这个简称已经默认化了，特别是体现在浏览器对 HTTP2 都是这个简写的。&lt;br&gt;普通的 HTTPS 网站浏览会比 HTTP 网站稍微慢一些，因为需要处理加密任务，而配置了 h2 的 HTTPS，在低延时的情况下速度会比 HTTP 更快更稳定。&lt;br&gt;现在电信劫持事件频发，网站部署了 HTTPS 加密后可以杜绝大部分劫持，但不是完全。像电子商务行业对 HTTPS 加密可是标配啊，因此部署 h2 更是势在必行。&lt;/p&gt;
&lt;h2 id=&quot;环境信息&quot;&gt;&lt;a href=&quot;#环境信息&quot; class=&quot;headerlink&quot; title=&quot;环境信息&quot;&gt;&lt;/a&gt;环境信息&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;操作系统版本&lt;/th&gt;
&lt;th&gt;IP地址&lt;/th&gt;
&lt;th&gt;作用&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;res&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.7.180&lt;/td&gt;
&lt;td&gt;Res静态资源服务器&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;我这里使用的Nginx是openresty-1.11.2.5。&lt;/p&gt;
&lt;h2 id=&quot;安装Nginx&quot;&gt;&lt;a href=&quot;#安装Nginx&quot; class=&quot;headerlink&quot; title=&quot;安装Nginx&quot;&gt;&lt;/a&gt;安装Nginx&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;1.安装依赖包&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# yum install readline-devel pcre-devel openssl-devel gcc -y
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;首先确认当前openssl版本，最低要求1.0.2，如果不满足，还得手动下载openssl，然后在编译时使用–with-openssl指定openssl目录。我这里是满足条件的：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# openssl version
OpenSSL 1.0.2k-fips  26 Jan 2017
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;2.安装Openresty&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@res local]# cd /usr/local/
[root@res local]# wget https://openresty.org/download/openresty-1.11.2.5.tar.gz
[root@res local]# tar zxf openresty-1.11.2.5.tar.gz 
[root@res local]# cd openresty-1.11.2.5/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;一些较低的版本里是不支持HTTP 2.0的，我这里使用的Openresty-1.11.2.5是支持的。另外，对于官方的Nginx，默认编译的 Nginx 并不包含 h2 模块，我们需要加入参数来编译，截止发文，Nginx 1.9 开发版及以上版本源码需要自己加入编译参数，从软件源仓库下载的则默认编译。 Tengine 可以同时部署 h2 和 SPDY 保证兼容性，Nginx 则是一刀切不再支持 SPDY。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#查看可供编译的参数是否有--with-http_v2_module
[root@res openresty-1.11.2.5]# ./configure --help
[root@res openresty-1.11.2.5]#./configure --with-http_v2_module
[root@res openresty-1.11.2.5]#  gmake &amp;amp;&amp;amp; gmake install
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;配置Nginx&quot;&gt;&lt;a href=&quot;#配置Nginx&quot; class=&quot;headerlink&quot; title=&quot;配置Nginx&quot;&gt;&lt;/a&gt;配置Nginx&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;[root@res conf]# pwd
/usr/local/nginx/nginx/conf
[root@res conf]# cp nginx.conf nginx.conf.bak
[root@res conf]# vim nginx.conf
server {
    listen       80;
    server_name  res.wisedu.com;

    rewrite ^(.*)$  https://$host$1 permanent; #http强制转https

}

server {
    listen       443 ssl http2;
    server_name  res.wisedu.com;

    ssl on;
    #证书和私钥
    ssl_certificate /opt/ssl/214199023800937.pem;
    ssl_certificate_key /opt/ssl/214199023800937.key;
    ...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;关于ssl使用的证书，由于是公司对外的服务环境，证书是购买的。当然做实验时也可以建立私有CA，模拟颁发证书。&lt;/p&gt;
&lt;p&gt;启动Nginx：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl start nginx.service
&lt;/code&gt;&lt;/pre&gt;</content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;最近将公司前端组件的Nginx配置了http 2.0，特此在此记录一下。突然发现博客里竟然没有什么关于Nginx的文章，毕竟是一直在使用的，后面会写几篇关于自己这几年使用Nginx的一些总结。&lt;br&gt;
    
    </summary>
    
      <category term="Nginx" scheme="http://yoursite.com/categories/Nginx/"/>
    
    
      <category term="Nginx" scheme="http://yoursite.com/tags/Nginx/"/>
    
  </entry>
  
  <entry>
    <title>Redis安装部署</title>
    <link href="http://yoursite.com/2018/01/10/Redis%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
    <id>http://yoursite.com/2018/01/10/Redis安装部署/</id>
    <published>2018-01-10T01:12:43.000Z</published>
    <updated>2018-01-10T03:45:22.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;安装Redis&quot;&gt;&lt;a href=&quot;#安装Redis&quot; class=&quot;headerlink&quot; title=&quot;安装Redis&quot;&gt;&lt;/a&gt;安装Redis&lt;/h2&gt;&lt;p&gt;我这里安装的版本是 3.2.2&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@log2 ~]# yum install readline-devel pcre-devel openssl-devel -y
[root@log2 local]# tar zxf redis-3.2.2.tar.gz 
[root@log2 local]# cd redis-3.2.2/
[root@log2 redis-3.2.2]# make
[root@log2 redis-3.2.2]# make install
&lt;/code&gt;&lt;/pre&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;配置Redis&quot;&gt;&lt;a href=&quot;#配置Redis&quot; class=&quot;headerlink&quot; title=&quot;配置Redis&quot;&gt;&lt;/a&gt;配置Redis&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;[root@log2 redis-3.2.2]# cp redis.conf /etc/
[root@log2 redis-3.2.2]# vim /etc/redis.conf
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;1.配置redis后台启动&lt;/strong&gt;&lt;br&gt;打开/etc/redis.conf，将daemonize处修改为yes。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;################################# GENERAL #####################################

# By default Redis does not run as a daemon. Use &amp;apos;yes&amp;apos; if you need it.
# Note that Redis will write a pid file in /var/run/redis.pid when daemonized.
daemonize yes
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;2.配置Redis持久化策略&lt;/strong&gt;&lt;br&gt;使用RDB和AOF双持久化策略：其中默认开启了RDB持久化，我们只需要开启AOF持久化。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# AOF and RDB persistence can be enabled at the same time without problems.
# If the AOF is enabled on startup Redis will load the AOF, that is the file
# with the better durability guarantees.
#
# Please check http://redis.io/topics/persistence for more information.

appendonly yes
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;3.配置redis日志文件&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Specify the log file name. Also the empty string can be used to force
# Redis to log on the standard output. Note that if you use standard
# output for logging but daemonize, logs will be sent to /dev/null
logfile &amp;quot;/var/log/redis.log&amp;quot;   
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;4.配置数据文件存放路径，在/目录下面创建/RedisData目录&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@log2 redis-3.2.2]# mkdir /RedisData

 # The working directory.
#
# The DB will be written inside this directory, with the filename specified
# above using the &amp;apos;dbfilename&amp;apos; configuration directive.
#
# The Append Only File will also be created inside this directory.
#
# Note that you must specify a directory here, not a file name.
dir /RedisData
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;5.修改Redis监听地址&lt;/strong&gt;&lt;br&gt;注释掉 #bind 127.0.0.1&lt;br&gt;否则redis只会监听在127.0.0.1的某个端口上。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;6.修改Redis监听端口&lt;/strong&gt;&lt;br&gt;为了安全，强烈建议修改redis的监听端口。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Accept connections on the specified port, default is 6379 (IANA #815344).
# If port 0 is specified Redis will not listen on a TCP socket.
port 6400
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;7.在redis3.2之后，redis增加了protected-mode，在这个模式下，即使注释掉了bind 127.0.0.1，再访问redis的时候还是报错，所以要如下设置：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# By default protected mode is enabled. You should disable it only if
# you are sure you want clients from other hosts to connect to Redis
# even if no authentication is configured, nor a specific set of interfaces
# are explicitly listed using the &amp;quot;bind&amp;quot; directive.
protected-mode no
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;启动-停止&quot;&gt;&lt;a href=&quot;#启动-停止&quot; class=&quot;headerlink&quot; title=&quot;启动/停止&quot;&gt;&lt;/a&gt;启动/停止&lt;/h2&gt;&lt;p&gt;1.启动&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@log2 ~]# /usr/local/bin/redis-server /etc/redis.conf
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.停止&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@log2 ~]# /usr/local/redis-3.2.2/src/redis-cli -p 6400 shutdown
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;Redis简单使用&quot;&gt;&lt;a href=&quot;#Redis简单使用&quot; class=&quot;headerlink&quot; title=&quot;Redis简单使用&quot;&gt;&lt;/a&gt;Redis简单使用&lt;/h2&gt;&lt;h3 id=&quot;使用客户端连接redis-server&quot;&gt;&lt;a href=&quot;#使用客户端连接redis-server&quot; class=&quot;headerlink&quot; title=&quot;使用客户端连接redis-server&quot;&gt;&lt;/a&gt;使用客户端连接redis-server&lt;/h3&gt;&lt;p&gt;在Redis的安装目录中有redis客户端，即redis-cli（Redis Command Line Interface），它是Redis自带的基于命令行的Redis的客户端。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cd /usr/local/bin/
# ./redis-cli -h 172.16.206.30 -p 6400
172.16.206.30:6400&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;向Redis服务器发送命令&quot;&gt;&lt;a href=&quot;#向Redis服务器发送命令&quot; class=&quot;headerlink&quot; title=&quot;向Redis服务器发送命令&quot;&gt;&lt;/a&gt;向Redis服务器发送命令&lt;/h3&gt;&lt;p&gt;redis-cli连上redis服务后，可以在命令行发送命令。&lt;br&gt;&lt;strong&gt;1.ping，测试客户端与redis的连接是否正常，如果正常会收到回复PONG&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# ./redis-cli -h 172.16.206.30 -p 6400
172.16.206.30:6400&amp;gt; ping
PONG
172.16.206.30:6400&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;2.set/get，使用set和get可以向redis设置数据、获取数据&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;172.16.206.30:6400&amp;gt; set name wisedu
OK
172.16.206.30:6400&amp;gt; get name
&amp;quot;wisedu&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;3.del，删除指定key的内容&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;172.16.206.30:6400&amp;gt; del name
(integer) 1
172.16.206.30:6400&amp;gt; get name
(nil)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;4.keys *，查看当前库中所有的key&lt;/strong&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;安装Redis&quot;&gt;&lt;a href=&quot;#安装Redis&quot; class=&quot;headerlink&quot; title=&quot;安装Redis&quot;&gt;&lt;/a&gt;安装Redis&lt;/h2&gt;&lt;p&gt;我这里安装的版本是 3.2.2&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@log2 ~]# yum install readline-devel pcre-devel openssl-devel -y
[root@log2 local]# tar zxf redis-3.2.2.tar.gz 
[root@log2 local]# cd redis-3.2.2/
[root@log2 redis-3.2.2]# make
[root@log2 redis-3.2.2]# make install
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="NoSQL" scheme="http://yoursite.com/categories/NoSQL/"/>
    
    
      <category term="Redis" scheme="http://yoursite.com/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Python操作InfluxDB</title>
    <link href="http://yoursite.com/2017/12/20/Python%E6%93%8D%E4%BD%9CInfluxDB/"/>
    <id>http://yoursite.com/2017/12/20/Python操作InfluxDB/</id>
    <published>2017-12-20T11:50:06.000Z</published>
    <updated>2017-12-22T13:13:55.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;InfluxDB-API-Client-Libraries&quot;&gt;&lt;a href=&quot;#InfluxDB-API-Client-Libraries&quot; class=&quot;headerlink&quot; title=&quot;InfluxDB API Client Libraries&quot;&gt;&lt;/a&gt;InfluxDB API Client Libraries&lt;/h2&gt;&lt;p&gt;上一篇文章介绍了安装部署InfluxDB和它的一些基本概念，接着就得来处理Nginx access.log，并将处理结果存储在InfluxDB中。&lt;br&gt;InfluxDB支持多种语言使用其客户端库来进行交互，具体参见官方文档：&lt;br&gt;&lt;a href=&quot;https://docs.influxdata.com/influxdb/v1.4/tools/api_client_libraries/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://docs.influxdata.com/influxdb/v1.4/tools/api_client_libraries/&lt;/a&gt;&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;br&gt;我这里使用Python语言来处理Nginx access.log并将结果存储于InfluxDB中。&lt;/p&gt;
&lt;h2 id=&quot;下载安装模块influxdb&quot;&gt;&lt;a href=&quot;#下载安装模块influxdb&quot; class=&quot;headerlink&quot; title=&quot;下载安装模块influxdb&quot;&gt;&lt;/a&gt;下载安装模块influxdb&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;# pip install influxdb
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;编写代码&quot;&gt;&lt;a href=&quot;#编写代码&quot; class=&quot;headerlink&quot; title=&quot;编写代码&quot;&gt;&lt;/a&gt;编写代码&lt;/h2&gt;&lt;p&gt;Nginx access.log的一行日志如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;line =&amp;apos;res.wisedu.com 172.16.6.4 [20/Dec/2017:09:20:17 +0800] &amp;quot;GET /statistics/res?/bh_apis/1.0/module-bhMenu.html&amp;amp;callback=__jp0 HTTP/1.1&amp;quot; 200 0 &amp;quot;-&amp;quot; &amp;quot;Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)&amp;quot; 0.000 -&amp;apos;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;代码如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# -*- coding: utf-8 -*-

import re
from influxdb import InfluxDBClient

def read_log(path): # 生成器generator
    &amp;apos;&amp;apos;&amp;apos;一行一行读取日志并返回&amp;apos;&amp;apos;&amp;apos;
    with open(path) as f:
        yield from f

def write_influxDB(lst):
    &amp;apos;&amp;apos;&amp;apos;写入InfluxDB数据库&amp;apos;&amp;apos;&amp;apos;
    client.write_points(lst)

def regular_line(line):
    &amp;apos;&amp;apos;&amp;apos;利用正则分析一行日志，存于字典中&amp;apos;&amp;apos;&amp;apos;
    o = re.compile(pattern)
    m = o.search(line)
    field_dict = m.groupdict()

    return field_dict

def main():
    &amp;apos;&amp;apos;&amp;apos;主函数&amp;apos;&amp;apos;&amp;apos;
    for line in read_log(path):
        field_dict = regular_line(line)
        lst = []
        point_dict = {}
        point_dict[&amp;apos;measurement&amp;apos;] = &amp;apos;res_access_log&amp;apos;
        point_dict[&amp;apos;fields&amp;apos;] = field_dict
        lst.append(point_dict)

        write_influxDB(lst)

if __name__ == &amp;apos;__main__&amp;apos;:
    pattern = &amp;apos;(?P&amp;lt;host&amp;gt;[\w+\.]+\w+) (?P&amp;lt;ip&amp;gt;\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}) \[(?P&amp;lt;time_local&amp;gt;.*)\]&amp;apos;
    pattern += &amp;apos; &amp;quot;(?P&amp;lt;method&amp;gt;\w+) (?P&amp;lt;url&amp;gt;[^\s]*) (?P&amp;lt;version&amp;gt;[\w\/\d\.]*)&amp;quot; (?P&amp;lt;status&amp;gt;\d+) (?P&amp;lt;length&amp;gt;\d+)&amp;apos;
    pattern += &amp;apos; &amp;quot;(?P&amp;lt;http_referer&amp;gt;[^\s]*)&amp;quot; &amp;quot;(?P&amp;lt;ua&amp;gt;.*)&amp;quot; (?P&amp;lt;request_time&amp;gt;[\d\.]*) (?P&amp;lt;upstream_response_time&amp;gt;[\d\.]*)&amp;apos;

    path = &amp;quot;logs/res.statistics.log&amp;quot;
    client = InfluxDBClient(host=&amp;apos;172.16.7.151&amp;apos;, port=8086, username=&amp;apos;root&amp;apos;, password=&amp;apos;wisedu123&amp;apos;, database=&amp;apos;mydb&amp;apos;)

    main()
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;Chronograf查看表数据&quot;&gt;&lt;a href=&quot;#Chronograf查看表数据&quot; class=&quot;headerlink&quot; title=&quot;Chronograf查看表数据&quot;&gt;&lt;/a&gt;Chronograf查看表数据&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/TimeSeriesDatabases/7.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;InfluxDB-API-Client-Libraries&quot;&gt;&lt;a href=&quot;#InfluxDB-API-Client-Libraries&quot; class=&quot;headerlink&quot; title=&quot;InfluxDB API Client Libraries&quot;&gt;&lt;/a&gt;InfluxDB API Client Libraries&lt;/h2&gt;&lt;p&gt;上一篇文章介绍了安装部署InfluxDB和它的一些基本概念，接着就得来处理Nginx access.log，并将处理结果存储在InfluxDB中。&lt;br&gt;InfluxDB支持多种语言使用其客户端库来进行交互，具体参见官方文档：&lt;br&gt;&lt;a href=&quot;https://docs.influxdata.com/influxdb/v1.4/tools/api_client_libraries/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://docs.influxdata.com/influxdb/v1.4/tools/api_client_libraries/&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="时序数据库" scheme="http://yoursite.com/categories/%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="InfluxDB" scheme="http://yoursite.com/tags/InfluxDB/"/>
    
  </entry>
  
  <entry>
    <title>时序数据库InfluxDB</title>
    <link href="http://yoursite.com/2017/12/15/%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93InfluxDB/"/>
    <id>http://yoursite.com/2017/12/15/时序数据库InfluxDB/</id>
    <published>2017-12-15T14:00:36.000Z</published>
    <updated>2018-01-10T07:55:24.000Z</updated>
    
    <content type="html">&lt;blockquote&gt;
&lt;p&gt;最近帮助公司前端小伙伴处理他们的nginx访问日志，log的数据是半结构化的数据，同时也是典型的时序数据，每一条数据都带有时间戳。于是考虑使用时间序列数据库存储，而不会去使用mysql或是mongodb(zabbix用的是mysql，它在IO上面遇到了瓶颈)。现在时间序列的数据库是有很多的，比如graphite、opentsdb以及新生的influxdb。这次我使用了InfluxDB，在此记录下学习过程，同时也希望能够帮助到其他学习的同学。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;InfluxDB介绍&quot;&gt;&lt;a href=&quot;#InfluxDB介绍&quot; class=&quot;headerlink&quot; title=&quot;InfluxDB介绍&quot;&gt;&lt;/a&gt;InfluxDB介绍&lt;/h2&gt;&lt;p&gt;什么是时间序列数据？最简单的定义就是数据格式里包含timestamp字段的数据。比如股票市场的价格，环境中的温度，主机的CPU使用率等。但是又有什么数据是不包含timestamp的呢？几乎所有的数据都可以打上一个timestamp字段。时间序列数据更重要的一个属性是如何去查询它。在查询的时候，对于时间序列我们总是会带上一个时间范围去过滤数据。同时查询的结果里也总是会包含timestamp字段。&lt;br&gt;InfluxDB 是一个开源分布式时序、事件和指标数据库。使用 Go 语言编写，无需外部依赖。其设计目标是实现分布式和水平伸缩扩展。&lt;br&gt;它有三大特性：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Time Series （时间序列）：你可以使用与时间有关的相关函数（如最大，最小，求和等）&lt;/li&gt;
&lt;li&gt;Metrics（度量）：你可以实时对大量数据进行计算&lt;/li&gt;
&lt;li&gt;Eevents（事件）：它支持任意的事件数据&lt;br&gt;特点：&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;schemaless(无结构)，可以是任意数量的列&lt;/li&gt;
&lt;li&gt;min, max, sum, count, mean, median 一系列函数，方便统计&lt;/li&gt;
&lt;li&gt;Native HTTP API, 内置http支持，使用http读写&lt;/li&gt;
&lt;li&gt;Powerful Query Language 类似sql&lt;/li&gt;
&lt;li&gt;Built-in Explorer 自带web管理界面。（从1.4版本开始去除了自带的web管理界面）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;安装部署InfluxDB&quot;&gt;&lt;a href=&quot;#安装部署InfluxDB&quot; class=&quot;headerlink&quot; title=&quot;安装部署InfluxDB&quot;&gt;&lt;/a&gt;安装部署InfluxDB&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;influxdb下载地址：&lt;/strong&gt;&lt;br&gt;&lt;a href=&quot;https://portal.influxdata.com/downloads&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://portal.influxdata.com/downloads&lt;/a&gt;&lt;br&gt;&lt;strong&gt;influxdb文档：&lt;/strong&gt;&lt;br&gt;&lt;a href=&quot;http://docs.influxdata.com/influxdb/v1.4/introduction/getting_started/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://docs.influxdata.com/influxdb/v1.4/introduction/getting_started/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.下载安装&lt;/strong&gt;&lt;br&gt;对于在不同操作系统上安装，官网都有说明，我这里使用的是CentOS 7。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/TimeSeriesDatabases/1.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/TimeSeriesDatabases/2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@docdesginer local]# wget https://dl.influxdata.com/influxdb/releases/influxdb-1.4.2.x86_64.rpm
[root@docdesginer local]# yum localinstall influxdb-1.4.2.x86_64.rpm
[root@docdesginer local]# rpm -ql influxdb
/etc/influxdb/influxdb.conf
/etc/logrotate.d/influxdb
/usr/bin/influx
/usr/bin/influx_inspect
/usr/bin/influx_stress
/usr/bin/influx_tsm
/usr/bin/influxd
/usr/lib/influxdb/scripts/influxdb.service
/usr/lib/influxdb/scripts/init.sh
/usr/share/man/man1/influx.1.gz
/usr/share/man/man1/influx_inspect.1.gz
/usr/share/man/man1/influx_stress.1.gz
/usr/share/man/man1/influx_tsm.1.gz
/usr/share/man/man1/influxd-backup.1.gz
/usr/share/man/man1/influxd-config.1.gz
/usr/share/man/man1/influxd-restore.1.gz
/usr/share/man/man1/influxd-run.1.gz
/usr/share/man/man1/influxd-version.1.gz
/usr/share/man/man1/influxd.1.gz
/var/lib/influxdb
/var/log/influxdb
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;2.启动&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@docdesginer local]# systemctl start influxdb 
[root@docdesginer local]# systemctl status influxdb
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;3.登录&lt;/strong&gt;&lt;br&gt;客户端工具：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@docdesginer local]# /usr/bin/influx
Connected to http://localhost:8086 version 1.4.2
InfluxDB shell version: 1.4.2
&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查看数据库：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; SHOW DATABASES
name: databases
name
----
_internal
&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;HTTP API访问：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@docdesginer ~]# curl -G http://localhost:8086/query --data-urlencode &amp;quot;q=SHOW DATABASES&amp;quot;
{&amp;quot;results&amp;quot;:[{&amp;quot;statement_id&amp;quot;:0,&amp;quot;series&amp;quot;:[{&amp;quot;name&amp;quot;:&amp;quot;databases&amp;quot;,&amp;quot;columns&amp;quot;:[&amp;quot;name&amp;quot;],&amp;quot;values&amp;quot;:[[&amp;quot;_internal&amp;quot;]]}]}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;此时数据库中还有没有用户，还没开启认证。&lt;/p&gt;
&lt;h2 id=&quot;开启认证&quot;&gt;&lt;a href=&quot;#开启认证&quot; class=&quot;headerlink&quot; title=&quot;开启认证&quot;&gt;&lt;/a&gt;开启认证&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;1.创建管理员&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@docdesginer local]# /usr/bin/influx
Connected to http://localhost:8086 version 1.4.2
InfluxDB shell version: 1.4.2
&amp;gt; CREATE USER root WITH PASSWORD &amp;apos;wisedu123&amp;apos; WITH ALL PRIVILEGES
&amp;gt; quit
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;2.修改配置文件，开启认证&lt;/strong&gt;&lt;br&gt;By default, authentication is disabled in the configuration file. Enable authentication by setting the auth-enabled option to true in the [http] section of the configuration file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[http]  
  enabled = true  
  bind-address = &amp;quot;:8086&amp;quot;  
  auth-enabled = true 
  log-enabled = true  
  write-tracing = false  
  pprof-enabled = false  
  https-enabled = false  
  https-certificate = &amp;quot;/etc/ssl/influxdb.pem&amp;quot; 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;重启InfluxDB：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@docdesginer ~]# systemctl restart influxdb 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;3.再次登录&lt;/strong&gt;&lt;br&gt;客户端工具连接数据库：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@docdesginer ~]# influx
Connected to http://localhost:8086 version 1.4.2
InfluxDB shell version: 1.4.2
&amp;gt; show databases;
ERR: unable to parse authentication credentials
Warning: It is possible this error is due to not setting a database.
Please set a database with the command &amp;quot;use &amp;lt;database&amp;gt;&amp;quot;.
&amp;gt;

[root@docdesginer ~]# /usr/bin/influx
Connected to http://localhost:8086 version 1.4.2
InfluxDB shell version: 1.4.2
&amp;gt; auth
username: root
password: 
&amp;gt; show databases;
name: databases
name
----
_internal
&amp;gt; 
或者
[root@docdesginer ~]# /usr/bin/influx -username root -password wisedu123 -precision rfc3339 
Connected to http://localhost:8086 version 1.4.2
InfluxDB shell version: 1.4.2
&amp;gt; show databases;
name: databases
name
----
_internal
&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;HTTP API：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@docdesginer ~]# curl -G http://localhost:8086/query --data-urlencode &amp;quot;q=SHOW DATABASES&amp;quot;                  
{&amp;quot;error&amp;quot;:&amp;quot;unable to parse authentication credentials&amp;quot;}
[root@docdesginer ~]# curl -G http://localhost:8086/query -u root:wisedu123 --data-urlencode &amp;quot;q=SHOW DATABASES&amp;quot;
{&amp;quot;results&amp;quot;:[{&amp;quot;statement_id&amp;quot;:0,&amp;quot;series&amp;quot;:[{&amp;quot;name&amp;quot;:&amp;quot;databases&amp;quot;,&amp;quot;columns&amp;quot;:[&amp;quot;name&amp;quot;],&amp;quot;values&amp;quot;:[[&amp;quot;_internal&amp;quot;]]}]}
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;InfluxDB概念&quot;&gt;&lt;a href=&quot;#InfluxDB概念&quot; class=&quot;headerlink&quot; title=&quot;InfluxDB概念&quot;&gt;&lt;/a&gt;InfluxDB概念&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;1.influxdb相关名词（可类比关系型数据库）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;database：数据库。&lt;/li&gt;
&lt;li&gt;measurement：数据库中的表。它就是tag，field，time的容器；对于influxDB的measurement来说，field是必须的，并且不能根据field来排序；Tag是可选的，tag可以用来做索引，tag是以字符串的形式存放的。&lt;/li&gt;
&lt;li&gt;points：表里面的一行数据。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;2.influxDB中独有的概念&lt;/strong&gt;&lt;br&gt;（1）Point由时间戳（time）、数据（field）和标签（tags）组成。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;time：每条数据记录的时间，也是数据库自动生成的主索引；&lt;/li&gt;
&lt;li&gt;fields：各种记录的值；&lt;/li&gt;
&lt;li&gt;tags：各种有索引的属性。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;InfluxDb不需要做schema定义，这意味着你可以随意的添加measurements, tags, and fields at any time。&lt;/p&gt;
&lt;p&gt;（2）series&lt;br&gt;a series is the collection of data that share a retention policy, measurement, and tag set&lt;br&gt;所有在数据库中的数据，都需要通过图表来展示，而这个series表示这个表里面的数据，可以在图表上画成几条线：通过tags排列组合算出来。&lt;br&gt;其实一个series就是一个测点，或者说一条曲线，那么retention policy, measurement, tagset就共同组成了一个定位测点序列的唯一标识。&lt;br&gt;point，就是某个series的同一个时刻的多个field的value，就组成了一个point；其实就是一条曲线上的一个点。&lt;/p&gt;
&lt;p&gt;（3）retention policy&lt;br&gt;保留策略，用于决定要保留多久的数据，保存几个备份，以及集群的策略等。&lt;/p&gt;
&lt;h2 id=&quot;InfluxDB基本操作&quot;&gt;&lt;a href=&quot;#InfluxDB基本操作&quot; class=&quot;headerlink&quot; title=&quot;InfluxDB基本操作&quot;&gt;&lt;/a&gt;InfluxDB基本操作&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;连接数据库：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@docdesginer ~]# /usr/bin/influx -username root -password wisedu123 -precision rfc3339   
Connected to http://localhost:8086 version 1.4.2
InfluxDB shell version: 1.4.2
&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这里的-precision参数指定了时间戳的格式为rfc3339，也可以不使用该参数。&lt;br&gt;&lt;strong&gt;查看数据库：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; SHOW DATABASES
name: databases
name
----
_internal
&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;创建数据库：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; CREATE DATABASE mydb
&amp;gt; SHOW DATABASES
name: databases
name
----
_internal
mydb
&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;进入数据库：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; USE mydb
Using database mydb
&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;插入数据：&lt;/strong&gt;&lt;br&gt;influxDB存储数据采用的是Line Protocol格式。&lt;br&gt;Line Protocol格式：写入数据库的Point的固定格式。格式如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;measurement&amp;gt;[,&amp;lt;tag-key&amp;gt;=&amp;lt;tag-value&amp;gt;...] &amp;lt;field-key&amp;gt;=&amp;lt;field-value&amp;gt;[,&amp;lt;field2-key&amp;gt;=&amp;lt;field2-value&amp;gt;...] [unix-nano-timestamp]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;比如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;weather,location=us-midwest temperature=82 1465839830100400200
  |    -------------------- --------------  |
  |             |             |             |
  |             |             |             |
+-----------+--------+-+---------+-+---------+
|measurement|,tag_set| |field_set| |timestamp|
+-----------+--------+-+---------+-+---------+
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【注意】：最后的timestamp是 unix时间戳*1000000000 的值，或者使用 %Y-%m-%dT%H:%M:%SZ 这种格式。使用其他格式在插入时会报错。&lt;/p&gt;
&lt;p&gt;示例：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; INSERT cpu,host=serverA,region=us_west value=0.64
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cpu是表名&lt;/li&gt;
&lt;li&gt;host=serverA,region=us_west 是tag&lt;/li&gt;
&lt;li&gt;value=0.64是field&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;想对此格式有详细的了解参见&lt;a href=&quot;https://docs.influxdata.com/influxdb/v1.4/write_protocols/line_protocol_tutorial/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;官方文档&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;查询数据：&lt;/strong&gt;&lt;br&gt;influxDB是支持类sql语句的，具体的查询语法都差不多。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; select * from cpu
name: cpu
time                           host    region  value
----                           ----    ------  -----
2017-12-15T13:17:09.660446488Z serverA us_west 0.64
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【注意】：InfluxDB集群功能已经不再开源。要想使用集群服务需要购买企业版。开源版和企业版的主要区别就是企业版的InfluxDB支持集群，而开源版不支持，此外企业版提供了先进的备份/恢复功能，而开源版本没有。但InfluxDB单机版性能也足够支撑中小公司的业务了。&lt;/p&gt;
&lt;h2 id=&quot;InfluxDB数据保存策略（Retention-Policies）&quot;&gt;&lt;a href=&quot;#InfluxDB数据保存策略（Retention-Policies）&quot; class=&quot;headerlink&quot; title=&quot;InfluxDB数据保存策略（Retention Policies）&quot;&gt;&lt;/a&gt;InfluxDB数据保存策略（Retention Policies）&lt;/h2&gt;&lt;p&gt;InfluxDB每秒可以处理成千上万条数据，要将这些数据全部保存下来会占用大量的存储空间，有时我们可能并不需要将所有历史数据进行存储，因此，InfluxDB推出了数据保留策略（Retention Policies），用来让我们自定义数据的保留时间。&lt;br&gt;InfluxDB的数据保留策略（RP） 用来定义数据在InfluxDB中存放的时间，或者定义保存某个期间的数据。&lt;br&gt;一个数据库可以有多个保留策略，但每个策略必须是独一无二的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.查询策略&lt;/strong&gt;&lt;br&gt;可以通过如下语句查看数据库的现有策略：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; SHOW RETENTION POLICIES ON mydb
name    duration shardGroupDuration replicaN default
----    -------- ------------------ -------- -------
autogen 0s       168h0m0s           1        true
&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：数据库mydb只有一个策略，各字段的含义如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;name：名称，此示例名称为 autogen。当你创建一个数据库的时候，InfluxDB会自动为数据库创建一个名叫 autogen 的策略，这个策略会永久保存数据。你可以重命名这个策略，并且在InfluxDB的配置文件中禁止掉自动创建策略。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;duration：数据保存时间，0代表无限制&lt;/li&gt;
&lt;li&gt;shardGroupDuration：shardGroup的存储时间，shardGroup是InfluxDB的一个基本储存结构。&lt;/li&gt;
&lt;li&gt;replicaN：全称是REPLICATION，副本个数&lt;/li&gt;
&lt;li&gt;default：是否是默认策略。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这里有两个概念：&lt;br&gt;&lt;strong&gt;shard：&lt;/strong&gt;&lt;br&gt;    shard 在 InfluxDB 中是一个比较重要的概念，它和 retention policy 相关联。每一个存储策略下会存在许多 shard，每一个 shard 存储一个指定时间段内的数据，并且不重复，例如 7点-8点 的数据落入 shard0 中，8点-9点的数据则落入 shard1 中。每一个 shard 都对应一个底层的 tsm 存储引擎，有独立的 cache、wal、tsm file。&lt;br&gt;创建数据库时会自动创建一个默认存储策略，永久保存数据，对应的在此存储策略下的 shard 所保存的数据的时间段为 7 天，也就是上面查询时看到的168h。计算的函数如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;func shardGroupDuration(d time.Duration) time.Duration {
    if d &amp;gt;= 180*24*time.Hour || d == 0 { // 6 months or 0
        return 7 * 24 * time.Hour
    } else if d &amp;gt;= 2*24*time.Hour { // 2 days
        return 1 * 24 * time.Hour
    }
    return 1 * time.Hour
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果创建一个新的 retention policy 设置数据的保留时间为 1 天，则单个 shard 所存储数据的时间间隔为 1 小时，超过1个小时的数据会被存放到下一个 shard 中。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;shard group：&lt;/strong&gt;&lt;br&gt;    shard group是shards的逻辑容器。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.创建策略&lt;/strong&gt;&lt;br&gt;语法：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CREATE RETENTION POLICY &amp;lt;retention_policy_name&amp;gt; ON &amp;lt;database_name&amp;gt; DURATION &amp;lt;duration&amp;gt; REPLICATION &amp;lt;n&amp;gt; [SHARD DURATION &amp;lt;duration&amp;gt;] [DEFAULT]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其中：SHARD DURATION子句决定了每个shard group存储的时间间隔，在永久存储的策略里这个子句是无效的。这个子句是可选的。shard group duration默认由策略的 duration 决定。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Retention Policy’s DURATION&lt;/th&gt;
&lt;th&gt;Shard Group Duration&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&amp;lt; 2 days&lt;/td&gt;
&lt;td&gt;1 hour&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;gt;= 2 days and &amp;lt;= 6 months&lt;/td&gt;
&lt;td&gt;1 day&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;gt; 6 months&lt;/td&gt;
&lt;td&gt;7 days&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;示例1：为数据库mydb创建一个策略&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CREATE RETENTION POLICY &amp;quot;one_day_only&amp;quot; ON &amp;quot;mydb&amp;quot; DURATION 1d REPLICATION 1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;示例2：为数据库mydb创建一个默认策略。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CREATE RETENTION POLICY &amp;quot;one_day_only&amp;quot; ON &amp;quot;mydb&amp;quot; DURATION 23h60m REPLICATION 1 DEFAULT
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;3.修改策略&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ALTER RETENTION POLICY &amp;lt;retention_policy_name&amp;gt; ON &amp;lt;database_name&amp;gt; DURATION &amp;lt;duration&amp;gt; REPLICATION &amp;lt;n&amp;gt; SHARD DURATION &amp;lt;duration&amp;gt; DEFAULT
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;4.删除策略&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;DROP RETENTION POLICY &amp;lt;retention_policy_name&amp;gt; ON &amp;lt;database_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【注意】：策略这个关键词“POLICY”在使用是应该大写，小写应该会出错。&lt;br&gt;当一个表使用的策略不是默认策略时，在进行操作时一定要显式的指定策略名称，否则会出现错误。&lt;/p&gt;
&lt;h2 id=&quot;Chronograf介绍&quot;&gt;&lt;a href=&quot;#Chronograf介绍&quot; class=&quot;headerlink&quot; title=&quot;Chronograf介绍&quot;&gt;&lt;/a&gt;Chronograf介绍&lt;/h2&gt;&lt;p&gt;Influxdb在1.3以后版本已经关闭了内置的8086的web管理功能，需要单独的工具来管理。而这个工具就是Chronograf。&lt;br&gt;其实Chronograf是TICK技术栈的一个组成部分。TICK是InfluxdDB公司推出的监控套件，承包指标采集、分析、画图等时序数据库上下游的工作，有点模仿日志分析系统ELK套件的意思。TICK包含：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;T = Telegraf is a plugin-driven server agent for collecting and reporting metrics.&lt;/li&gt;
&lt;li&gt;I = InfluxDB is a time series database built from the ground up to handle high write and query loads.&lt;/li&gt;
&lt;li&gt;C = Chronograf is a graphing and visualization application for performing ad hoc exploration of data.&lt;/li&gt;
&lt;li&gt;K = Kapacitor is a data processing framework proving alerting, anomaly detection and action frameworks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;也就是说：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Telegraf：数据采集&lt;/li&gt;
&lt;li&gt;InfluxDB：数据接收和存储&lt;/li&gt;
&lt;li&gt;Chronograf：数据汇总展示，报警等。&lt;/li&gt;
&lt;li&gt;Kapacitor：数据处理，比如监控策略等&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;安装Chronograf&quot;&gt;&lt;a href=&quot;#安装Chronograf&quot; class=&quot;headerlink&quot; title=&quot;安装Chronograf&quot;&gt;&lt;/a&gt;安装Chronograf&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;1.安装部署&lt;/strong&gt;&lt;br&gt;下载地址：&lt;a href=&quot;https://portal.influxdata.com/downloads&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://portal.influxdata.com/downloads&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/TimeSeriesDatabases/3.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@docdesginer local]# wget https://dl.influxdata.com/chronograf/releases/chronograf-1.3.10.0.x86_64.rpm
[root@docdesginer local]# yum localinstall chronograf-1.3.10.0.x86_64.rpm
[root@docdesginer local]# rpm -ql chronograf
/etc/logrotate.d/chronograf
/usr/bin/chronograf
/usr/lib/chronograf/scripts/chronograf.service
/usr/lib/chronograf/scripts/init.sh
/usr/share/chronograf/canned/apache.json
/usr/share/chronograf/canned/consul.json
/usr/share/chronograf/canned/consul_agent.json
...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;2.启动&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@docdesginer local]# systemctl start chronograf
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;3.访问Chronograf&lt;/strong&gt;&lt;br&gt;浏览器输入&lt;a href=&quot;http://IP:8888&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://IP:8888&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/TimeSeriesDatabases/4.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;查询：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/TimeSeriesDatabases/5.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/TimeSeriesDatabases/6.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;在查询时，最好数据库名和表名都加上引号。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;最近帮助公司前端小伙伴处理他们的nginx访问日志，log的数据是半结构化的数据，同时也是典型的时序数据，每一条数据都带有时间戳。于是考虑使用时间序列数据库存储，而不会去使用mysql或是mongodb(zabbix用的是mysql，它在IO上面遇到了瓶颈)。现在时间序列的数据库是有很多的，比如graphite、opentsdb以及新生的influxdb。这次我使用了InfluxDB，在此记录下学习过程，同时也希望能够帮助到其他学习的同学。&lt;br&gt;
    
    </summary>
    
      <category term="时序数据库" scheme="http://yoursite.com/categories/%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="InfluxDB" scheme="http://yoursite.com/tags/InfluxDB/"/>
    
  </entry>
  
  <entry>
    <title>nexus搭建maven私服</title>
    <link href="http://yoursite.com/2017/11/19/nexus%E6%90%AD%E5%BB%BAmaven%E7%A7%81%E6%9C%8D/"/>
    <id>http://yoursite.com/2017/11/19/nexus搭建maven私服/</id>
    <published>2017-11-19T06:49:41.000Z</published>
    <updated>2017-11-19T08:06:13.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;为什么要搭建私服&quot;&gt;&lt;a href=&quot;#为什么要搭建私服&quot; class=&quot;headerlink&quot; title=&quot;为什么要搭建私服&quot;&gt;&lt;/a&gt;为什么要搭建私服&lt;/h2&gt;&lt;p&gt;私服不是Maven的核心概念，它仅仅是一种衍生出来的特殊的Maven仓库。通过建立自己的私服，就可以降低中央仓库负荷、节省外网带宽、加速Maven构建、自己部署构建等，从而高效地使用Maven。Nexus也是当前最流行的Maven仓库管理软件。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;下载Nexus&quot;&gt;&lt;a href=&quot;#下载Nexus&quot; class=&quot;headerlink&quot; title=&quot;下载Nexus&quot;&gt;&lt;/a&gt;下载Nexus&lt;/h2&gt;&lt;p&gt;最新nexus下载地址：&lt;a href=&quot;http://www.sonatype.org/nexus/go&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://www.sonatype.org/nexus/go&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Maven/1.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;Nexus是典型的Java Web应用，它有两种安装包，一种是包含Jetty容器的Bundle包，另一种是不包含Web容器的war包。&lt;/p&gt;
&lt;h2 id=&quot;安装Nexus&quot;&gt;&lt;a href=&quot;#安装Nexus&quot; class=&quot;headerlink&quot; title=&quot;安装Nexus&quot;&gt;&lt;/a&gt;安装Nexus&lt;/h2&gt;&lt;h3 id=&quot;安装JDK1-8&quot;&gt;&lt;a href=&quot;#安装JDK1-8&quot; class=&quot;headerlink&quot; title=&quot;安装JDK1.8&quot;&gt;&lt;/a&gt;安装JDK1.8&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;# mkdir /usr/java
# tar zxf /usr/local/jdk-8u73-linux-x64.gz -C /usr/java/
# vim /etc/profile
export JAVA_HOME=/usr/java/jdk1.8.0_73
export PATH=$JAVA_HOME/bin:$PATH
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
# source /etc/profile
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;安装Nexus-1&quot;&gt;&lt;a href=&quot;#安装Nexus-1&quot; class=&quot;headerlink&quot; title=&quot;安装Nexus&quot;&gt;&lt;/a&gt;安装Nexus&lt;/h3&gt;&lt;p&gt;上传Nexus安装包到/opt目录下。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@hadoop33 opt]# mkdir nexus
[root@hadoop33 opt]# tar zxf nexus-2.14.0-01-bundle.tar.gz -C /opt/nexus
[root@hadoop33 opt]# cd nexus/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Maven/2.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;nexus-2.6.2-01: 该目录包含了Nexus运行所需要的文件，如启动脚本、依赖jar包等。&lt;br&gt;sonatype-work：该目录包含Nenus生成的配置文件、日志文件、仓库文件等。&lt;br&gt;其中第一个目录是运行Nexus必须的，而第二个不是必须的，Nexus会在运行的时候动态创建该目录。&lt;/p&gt;
&lt;h3 id=&quot;启动Nexus&quot;&gt;&lt;a href=&quot;#启动Nexus&quot; class=&quot;headerlink&quot; title=&quot;启动Nexus&quot;&gt;&lt;/a&gt;启动Nexus&lt;/h3&gt;&lt;p&gt;默认端口为8081，如需修改请查看配置文件 conf/nexus.properties&lt;br&gt;它本身不建议在root用户下使用，如果我们需要在root用户下启动服务，要先配置 bin/nexus 文件中的 RUN_AS_USER=root&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Maven/3.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@hadoop33 nexus]# cd nexus-2.14.0-01/bin/
[root@hadoop33 bin]# ./nexus start
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Maven/4.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;停止-Nexus&quot;&gt;&lt;a href=&quot;#停止-Nexus&quot; class=&quot;headerlink&quot; title=&quot;停止 Nexus&quot;&gt;&lt;/a&gt;停止 Nexus&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@hadoop33 bin]# ./nexus stop
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;访问&quot;&gt;&lt;a href=&quot;#访问&quot; class=&quot;headerlink&quot; title=&quot;访问&quot;&gt;&lt;/a&gt;访问&lt;/h2&gt;&lt;p&gt;启动后访问首页： &lt;a href=&quot;http://172.16.206.33:8081/nexus/index.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.206.33:8081/nexus/index.html&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Maven/5.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;点击右上角的Login in&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Maven/6.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;登录默认账号/密码 admin/admin123。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如何修改 nexus 账号密码：&lt;/strong&gt;先停止 nexus，打开 %NEXUS_HOME%/sonatype-work/nexus/conf/security.xml，修改即可。&lt;br&gt;nexus 的密码采用 SHA1 加密算法 ( 在线加密工具 )，将加密后的 SHA1 串（小写）拷贝覆盖原来的。重启 nexus：./nexus restart。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Maven/7.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;配置Nexus&quot;&gt;&lt;a href=&quot;#配置Nexus&quot; class=&quot;headerlink&quot; title=&quot;配置Nexus&quot;&gt;&lt;/a&gt;配置Nexus&lt;/h2&gt;&lt;p&gt;Nexus常用功能就是：指定私服的中央地址、将自己的Maven项目指定到私服地址、从私服下载中央库的项目索引、从私服仓库下载依赖组件、将第三方项目jar上传到私服供其他项目组使用。&lt;br&gt;登录&lt;a href=&quot;http://172.16.206.33:8081/nexus/index.html，点击左侧菜单栏的“Repositories”。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.206.33:8081/nexus/index.html，点击左侧菜单栏的“Repositories”。&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Maven/8.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;一般用到的仓库种类是hosted、proxy。Hosted代表宿主仓库，用来发布一些第三方不允许的组件，比如oracle驱动、比如商业软件jar包。Proxy代表代理远程的仓库，最典型的就是Maven官方中央仓库、JBoss仓库等等。如果构建的Maven项目本地仓库没有依赖包，那么就会去这个代理站点去下载，那么如果代理站点也没有此依赖包，就回去远程中央仓库下载依赖，这些中央仓库就是proxy。代理站点下载成功后再下载至本机。一般情况下Maven这个自带的默认仓库一般情况下已经够大多数项目使用了。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;hosted   类型的仓库，内部项目的发布仓库&lt;/li&gt;
&lt;li&gt;releases 内部的模块中release模块的发布仓库&lt;/li&gt;
&lt;li&gt;snapshots 发布内部的SNAPSHOT模块的仓库&lt;/li&gt;
&lt;li&gt;3rd party 第三方依赖的仓库，这个数据通常是由内部人员自行下载之后发布上去&lt;/li&gt;
&lt;li&gt;proxy   类型的仓库，从远程中央仓库中寻找数据的仓库&lt;/li&gt;
&lt;li&gt;group   类型的仓库，组仓库用来方便我们开发人员进行设置的仓库&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;maven项目索引&quot;&gt;&lt;a href=&quot;#maven项目索引&quot; class=&quot;headerlink&quot; title=&quot;maven项目索引&quot;&gt;&lt;/a&gt;maven项目索引&lt;/h3&gt;&lt;p&gt;下载Maven项目索引，项目索引是为了使用者能够在私服站点查找依赖使用的功能。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Maven/9.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;保存后后台会运行一个任务，点击菜单栏的Scheduled Tasks选项即可看到有个任务在RUNNING。 下载完成后，Maven索引就可以使用了，在搜索栏输入要搜索的项，就可以查到相关的信息。例如spring-core&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Maven/10.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;就可以检索出它的相关信息，包括怎么配置依赖信息。我们要想使用这个私服仓库，先在项目pom中配置相关私服信息。&lt;/p&gt;
&lt;h2 id=&quot;Maven项目配置&quot;&gt;&lt;a href=&quot;#Maven项目配置&quot; class=&quot;headerlink&quot; title=&quot;Maven项目配置&quot;&gt;&lt;/a&gt;Maven项目配置&lt;/h2&gt;&lt;p&gt;在pom.xml文件中指定仓库：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;repositories&amp;gt;
    &amp;lt;repository&amp;gt;
        &amp;lt;id&amp;gt;nexus&amp;lt;/id&amp;gt;
        &amp;lt;url&amp;gt;http://172.16.206.33:8081/nexus/content/groups/public/&amp;lt;/url&amp;gt;
        &amp;lt;snapshots&amp;gt;&amp;lt;enabled&amp;gt;true&amp;lt;/enabled&amp;gt;&amp;lt;/snapshots&amp;gt;
        &amp;lt;releases&amp;gt;&amp;lt;enabled&amp;gt;true&amp;lt;/enabled&amp;gt;&amp;lt;/releases&amp;gt;
    &amp;lt;/repository&amp;gt;
&amp;lt;/repositories&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在pom.xml文件中指定插件仓库：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;pluginRepositories&amp;gt;
    &amp;lt;pluginRepository&amp;gt;
        &amp;lt;id&amp;gt;nexus&amp;lt;/id&amp;gt;
        &amp;lt;url&amp;gt;http://172.16.206.33:8081/nexus/content/groups/public/&amp;lt;/url&amp;gt;
        &amp;lt;snapshots&amp;gt;&amp;lt;enabled&amp;gt;true&amp;lt;/enabled&amp;gt;&amp;lt;/snapshots&amp;gt;
        &amp;lt;releases&amp;gt;&amp;lt;enabled&amp;gt;true&amp;lt;/enabled&amp;gt;&amp;lt;/releases&amp;gt;
    &amp;lt;/pluginRepository&amp;gt;
&amp;lt;/pluginRepositories&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;以上的配置使得只有本项目才在私服下载组件。这个Maven项目构建的时候会从私服下载相关依赖。&lt;br&gt;上面的配置仅仅是在此项目中生效，对于其他项目还是不起作用。如果相对Maven的其他项目也生效的话。需要修改全局的settings.xml文件。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Maven/11.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;追加激活profile：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;activeProfiles&amp;gt;  
     &amp;lt;activeProfile&amp;gt;central&amp;lt;/activeProfile&amp;gt;  
&amp;lt;/activeProfiles
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;之后所有本机的Maven项目就在私服下载组件。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;为什么要搭建私服&quot;&gt;&lt;a href=&quot;#为什么要搭建私服&quot; class=&quot;headerlink&quot; title=&quot;为什么要搭建私服&quot;&gt;&lt;/a&gt;为什么要搭建私服&lt;/h2&gt;&lt;p&gt;私服不是Maven的核心概念，它仅仅是一种衍生出来的特殊的Maven仓库。通过建立自己的私服，就可以降低中央仓库负荷、节省外网带宽、加速Maven构建、自己部署构建等，从而高效地使用Maven。Nexus也是当前最流行的Maven仓库管理软件。&lt;br&gt;
    
    </summary>
    
      <category term="maven" scheme="http://yoursite.com/categories/maven/"/>
    
    
      <category term="nexus" scheme="http://yoursite.com/tags/nexus/"/>
    
  </entry>
  
  <entry>
    <title>Filebeat日志收集器</title>
    <link href="http://yoursite.com/2017/10/24/Filebeat%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E5%99%A8/"/>
    <id>http://yoursite.com/2017/10/24/Filebeat日志收集器/</id>
    <published>2017-10-24T02:26:16.000Z</published>
    <updated>2017-11-13T07:38:39.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;Beats工具&quot;&gt;&lt;a href=&quot;#Beats工具&quot; class=&quot;headerlink&quot; title=&quot;Beats工具&quot;&gt;&lt;/a&gt;Beats工具&lt;/h2&gt;&lt;p&gt;Beats是elastic公司的一款轻量级数据采集产品，它是从packetbeat发展出来的数据收集器系统，它包含了几个子产品：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Packetbeat(用于监控网络流量)&lt;/li&gt;
&lt;li&gt;Filebeat(用于监听日志数据)&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Metricbeat(用于搜集CPU、内存、磁盘的信息以及Nginx、Redis等服务的数据)&lt;/li&gt;
&lt;li&gt;Winlogbeat(用于搜集windows事件日志)&lt;/li&gt;
&lt;li&gt;Heartbeat(用于监控服务的可用性)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Beats可以直接把数据发送给Elasticsearch或者发送给Logstash，然后Logstash发送给Elasticsearch，然后进行后续的数据分析活动。&lt;br&gt;由于他们都是基于libbeat写出来的，提供了统一的数据发送方法，输入配置解析，日志记录框架等功能。所有的beat工具，在配置上基本相同，只是input输入的地方各有差异。&lt;br&gt;如果有其他特殊需求，可以使用Go语言借助libbeat库方便地开发自己的Beat工具，社区中有很多工具可以参考。&lt;/p&gt;
&lt;h2 id=&quot;Logstash和Filebeat&quot;&gt;&lt;a href=&quot;#Logstash和Filebeat&quot; class=&quot;headerlink&quot; title=&quot;Logstash和Filebeat&quot;&gt;&lt;/a&gt;Logstash和Filebeat&lt;/h2&gt;&lt;p&gt;上一篇文章中介绍过，Logstash是跑在JVM上的，需要消耗较多的系统资源，而Filebeat则是一个轻量级的日志采集工具，占用资源更少。我们完全可以在每台机器上安装启动个Filebeat，由Filebeat来采集日志，将数据传输给Redis或者Kafka，然后logstash去获取，利用filter功能过滤分析，然后存储到elasticsearch中。架构图如下：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/8.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;环境信息&quot;&gt;&lt;a href=&quot;#环境信息&quot; class=&quot;headerlink&quot; title=&quot;环境信息&quot;&gt;&lt;/a&gt;环境信息&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;操作系统版本&lt;/th&gt;
&lt;th&gt;IP地址&lt;/th&gt;
&lt;th&gt;安装软件&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;hadoop16&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.206.16&lt;/td&gt;
&lt;td&gt;elasticsearch-5.6.3.zip、kibana-5.6.3-linux-x86_64.tar.gz&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;spark32&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.206.32&lt;/td&gt;
&lt;td&gt;logstash-5.6.3.tar.gz、filebeat-5.6.3-linux-x86_64.tar.gz、Nginx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;osb30&lt;/td&gt;
&lt;td&gt;Redhat 6.5&lt;/td&gt;
&lt;td&gt;172.16.206.30&lt;/td&gt;
&lt;td&gt;redis-3.2.2.tar.gz&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&quot;安装配置Redis&quot;&gt;&lt;a href=&quot;#安装配置Redis&quot; class=&quot;headerlink&quot; title=&quot;安装配置Redis&quot;&gt;&lt;/a&gt;安装配置Redis&lt;/h2&gt;&lt;h3 id=&quot;安装Redis&quot;&gt;&lt;a href=&quot;#安装Redis&quot; class=&quot;headerlink&quot; title=&quot;安装Redis&quot;&gt;&lt;/a&gt;安装Redis&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;# yum install readline-devel pcre-devel openssl-devel -y
# cd /usr/local/
# tar zxf redis-3.2.2.tar.gz 
# cd redis-3.2.2/
# make
# make install
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;配置Redis&quot;&gt;&lt;a href=&quot;#配置Redis&quot; class=&quot;headerlink&quot; title=&quot;配置Redis&quot;&gt;&lt;/a&gt;配置Redis&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;# cd /usr/local/redis-3.2.2/
# cp redis.conf /etc/
# vim /etc/redis.conf
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;1.打开/etc/redis.conf，将daemonize处修改为yes。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/9.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;2.配置Redis持久化策略&lt;br&gt;使用RDB和AOF双持久化策略：其中默认开启了RDB持久化，我们只需要开启AOF持久化。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/10.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;3.配置redis日志文件&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/11.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;4.配置数据文件存放路径，在/目录下面创建/RedisData目录&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# mkdir /RedisData
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/12.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;5.修改Redis监听地址&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/13.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;6.修改Redis监听端口&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/14.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;7.在redis3.2之后，redis增加了protected-mode，在这个模式下，即使注释掉了bind 127.0.0.1，再访问redis的时候还是报错，所以要如下设置&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/15.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;启动-停止Redis&quot;&gt;&lt;a href=&quot;#启动-停止Redis&quot; class=&quot;headerlink&quot; title=&quot;启动/停止Redis&quot;&gt;&lt;/a&gt;启动/停止Redis&lt;/h3&gt;&lt;p&gt;启动：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# /usr/local/bin/redis-server /etc/redis.conf
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;停止：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# /usr/local/redis-3.2.2/src/redis-cli -p 6400 shutdown
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;安装配置Filebeat&quot;&gt;&lt;a href=&quot;#安装配置Filebeat&quot; class=&quot;headerlink&quot; title=&quot;安装配置Filebeat&quot;&gt;&lt;/a&gt;安装配置Filebeat&lt;/h2&gt;&lt;p&gt;filebeat的工作流程：当开启filebeat程序的时候，它会启动一个或多个探测器（prospectors）去检测指定的日志目录或文件，对于探测器找出的每一个日志文件，filebeat启动收割进程（harvester），每一个收割进程读取一个日志文件的新内容，并发送这些新的日志数据到处理程序（spooler），处理程序会集合这些事件，最后filebeat会发送集合的数据到你指定的地点（Elasticsearch、Logstash、Kafka或者Redis）。&lt;/p&gt;
&lt;h3 id=&quot;安装Filebeat&quot;&gt;&lt;a href=&quot;#安装Filebeat&quot; class=&quot;headerlink&quot; title=&quot;安装Filebeat&quot;&gt;&lt;/a&gt;安装Filebeat&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@spark32 opt]# wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-5.6.3-linux-x86_64.tar.gz
[root@spark32 opt]# ln -sv filebeat-5.6.3-linux-x86_64 filebeat
‘filebeat’ -&amp;gt; ‘filebeat-5.6.3-linux-x86_64’
[root@spark32 opt]# cd filebeat
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;配置Filebeat&quot;&gt;&lt;a href=&quot;#配置Filebeat&quot; class=&quot;headerlink&quot; title=&quot;配置Filebeat&quot;&gt;&lt;/a&gt;配置Filebeat&lt;/h3&gt;&lt;p&gt;1.定义你的日志文件的路径（一个或多个）&lt;br&gt;对于大多数的基本filebeat配置，可以定义一个单一探测器针对一个单一的路径，例如：&lt;br&gt;filebeat.prospectors:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- input_type: log
  paths:
    - /usr/local/openresty/nginx/logs/host.access.log
  #json.keys_under_root: true 若收取日志格式为json的log，请开启此配置
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.定义输出日志，我这里输出到redis中&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;output.redis:
  hosts: [&amp;quot;172.16.206.30:6400&amp;quot;]
  #password: &amp;quot;my_password&amp;quot;
  key: &amp;quot;filebeat&amp;quot;
  db: 0
  timeout: 5
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;启动Filebeat&quot;&gt;&lt;a href=&quot;#启动Filebeat&quot; class=&quot;headerlink&quot; title=&quot;启动Filebeat&quot;&gt;&lt;/a&gt;启动Filebeat&lt;/h3&gt;&lt;p&gt;测试配置文件语法：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cd /opt/filebeat/
# ./filebeat -configtest -e
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;启动：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# ./filebeat &amp;amp;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查看数据是否进入redis：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@osb30 src]# ./redis-cli -p 6400 
127.0.0.1:6400&amp;gt; help @LIST
127.0.0.1:6400&amp;gt; LLEN filebeat
(integer) 15
127.0.0.1:6400&amp;gt; LINDEX filebeat 1
&amp;quot;{\&amp;quot;@timestamp\&amp;quot;:\&amp;quot;2017-10-23T07:13:37.551Z\&amp;quot;,\&amp;quot;beat\&amp;quot;:{\&amp;quot;hostname\&amp;quot;:\&amp;quot;spark32\&amp;quot;,\&amp;quot;name\&amp;quot;:\&amp;quot;spark32\&amp;quot;,\&amp;quot;version\&amp;quot;:\&amp;quot;5.6.3\&amp;quot;},\&amp;quot;input_type\&amp;quot;:\&amp;quot;log\&amp;quot;,\&amp;quot;message\&amp;quot;:\&amp;quot;172.16.4.81 - - [19/Oct/2017:09:47:17 +0800] \\\&amp;quot;GET /favicon.ico HTTP/1.1\\\&amp;quot; 404 576 \\\&amp;quot;http://172.16.206.32:808/\\\&amp;quot; \\\&amp;quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\\\&amp;quot; \\\&amp;quot;-\\\&amp;quot;\&amp;quot;,\&amp;quot;offset\&amp;quot;:439,\&amp;quot;source\&amp;quot;:\&amp;quot;/usr/local/openresty/nginx/logs/host.access.log\&amp;quot;,\&amp;quot;type\&amp;quot;:\&amp;quot;log\&amp;quot;}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;浏览器访问下nginx，再次查看redis数据：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;127.0.0.1:6400&amp;gt; LLEN filebeat
(integer) 16
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;安装配置Elasticsearch&quot;&gt;&lt;a href=&quot;#安装配置Elasticsearch&quot; class=&quot;headerlink&quot; title=&quot;安装配置Elasticsearch&quot;&gt;&lt;/a&gt;安装配置Elasticsearch&lt;/h2&gt;&lt;p&gt;关于Elasticsearch的安装部署详见上一篇博客&lt;a href=&quot;http://jkzhao.github.io/2017/10/20/ELK%E5%AE%9E%E6%88%98/#more&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;《ELK实战》&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&quot;安装配置Logstash&quot;&gt;&lt;a href=&quot;#安装配置Logstash&quot; class=&quot;headerlink&quot; title=&quot;安装配置Logstash&quot;&gt;&lt;/a&gt;安装配置Logstash&lt;/h2&gt;&lt;p&gt;Logstash从redis中取数据，输出结果到Elasticsearch中。关于Logstash的安装部署详见上一篇博客&lt;a href=&quot;http://jkzhao.github.io/2017/10/20/ELK%E5%AE%9E%E6%88%98/#more&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;《ELK实战》&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id=&quot;配置Logstash&quot;&gt;&lt;a href=&quot;#配置Logstash&quot; class=&quot;headerlink&quot; title=&quot;配置Logstash&quot;&gt;&lt;/a&gt;配置Logstash&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;# cd /opt/logstash-5.6.3/conf
# vim logstash_redis.conf
input {
  redis {
    host  =&amp;gt;  &amp;quot;172.16.206.30&amp;quot;
    port  =&amp;gt;  &amp;quot;6400&amp;quot;
    data_type  =&amp;gt;  &amp;quot;list&amp;quot;
    key  =&amp;gt;  &amp;quot;filebeat&amp;quot;
  }
}

filter {
  grok {
    match =&amp;gt; { &amp;quot;message&amp;quot; =&amp;gt; &amp;quot;%{NGINXACCESS}&amp;quot; }
  }
}

output {
  elasticsearch {
    hosts    =&amp;gt;  [&amp;quot;172.16.206.16:9200&amp;quot;]
    action   =&amp;gt;  &amp;quot;index&amp;quot;
    index    =&amp;gt;  &amp;quot;filebeat-%{+YYYY.MM.dd}&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;启动Logstash&quot;&gt;&lt;a href=&quot;#启动Logstash&quot; class=&quot;headerlink&quot; title=&quot;启动Logstash&quot;&gt;&lt;/a&gt;启动Logstash&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@spark32 logstash-5.6.3]# bin/logstash -f /opt/logstash-5.6.3/conf/logstash_redis.conf &amp;amp;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查看redis中的数据，发现已经全部被消费了：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;127.0.0.1:6400&amp;gt; LLEN filebeat
(integer) 0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查看Elasticsearch中的索引：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# curl -XGET &amp;apos;172.16.206.16:9200/_cat/indices&amp;apos;
yellow open filebeat-2017.10.23 PQe6qyVfSSG7GHQJUOPWtA 5 1 18 0 92.8kb 92.8kb
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;安装配置Kibana&quot;&gt;&lt;a href=&quot;#安装配置Kibana&quot; class=&quot;headerlink&quot; title=&quot;安装配置Kibana&quot;&gt;&lt;/a&gt;安装配置Kibana&lt;/h2&gt;&lt;p&gt;关于Kibana的安装部署详见上一篇博客&lt;a href=&quot;http://jkzhao.github.io/2017/10/20/ELK%E5%AE%9E%E6%88%98/#more&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;《ELK实战》&lt;/a&gt;。&lt;br&gt;浏览器访问：&lt;a href=&quot;http://172.16.206.16:5601/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.206.16:5601/&lt;/a&gt;&lt;br&gt;1.左侧菜单点击“Management”，点击“Index Patterns”。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/16.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;2.点击“Create Index Pattern”。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/17.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;3.输入索引名字，点击“Create”&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/18.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;4.点击Discover，选择上一步创建的索引。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/19.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Beats工具&quot;&gt;&lt;a href=&quot;#Beats工具&quot; class=&quot;headerlink&quot; title=&quot;Beats工具&quot;&gt;&lt;/a&gt;Beats工具&lt;/h2&gt;&lt;p&gt;Beats是elastic公司的一款轻量级数据采集产品，它是从packetbeat发展出来的数据收集器系统，它包含了几个子产品：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Packetbeat(用于监控网络流量)&lt;/li&gt;
&lt;li&gt;Filebeat(用于监听日志数据)
    
    </summary>
    
      <category term="日志" scheme="http://yoursite.com/categories/%E6%97%A5%E5%BF%97/"/>
    
    
      <category term="ELK" scheme="http://yoursite.com/tags/ELK/"/>
    
  </entry>
  
  <entry>
    <title>ELK实战</title>
    <link href="http://yoursite.com/2017/10/20/ELK%E5%AE%9E%E6%88%98/"/>
    <id>http://yoursite.com/2017/10/20/ELK实战/</id>
    <published>2017-10-20T01:33:57.000Z</published>
    <updated>2017-11-13T07:38:32.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;ELK介绍&quot;&gt;&lt;a href=&quot;#ELK介绍&quot; class=&quot;headerlink&quot; title=&quot;ELK介绍&quot;&gt;&lt;/a&gt;ELK介绍&lt;/h2&gt;&lt;p&gt;ELK由Elasticsearch、Logstash和Kibana三部分组件组成。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Elasticsearch是个开源分布式搜索引擎，它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制，restful风格接口，多数据源，自动搜索负载等。&lt;/li&gt;
&lt;li&gt;Logstash是一个完全开源的工具，它可以对你的日志进行收集、分析，并将其存储供以后使用。&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;kibana 是一个开源和免费的工具，它可以为 Logstash 和 ElasticSearch 提供的日志分析友好的 Web 界面，可以帮助您汇总、分析和搜索重要数据日志。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;Elasticsearch&quot;&gt;&lt;a href=&quot;#Elasticsearch&quot; class=&quot;headerlink&quot; title=&quot;Elasticsearch&quot;&gt;&lt;/a&gt;Elasticsearch&lt;/h3&gt;&lt;p&gt;Elasticsearch是一个基于Lucene实现的开源、分布式、Restful的全文本搜索引擎。（仅支持文本搜索）此外，它还是一个分布式实时文档存储，其中每个文档的每个field均是被索引的数据，且可被搜索；也是一个带实时分析功能的分布式搜索引擎，能够扩展至数以百计的节点实时处理PB级的数据。&lt;br&gt;Elasticsearch借助于Lucene的API，在Lucene之外又重新封装了一层实现构建搜索引擎中的搜索组件。除此之外，Elasticsearch还新增了更强大的功能。比如把自己构建为分布式，分布式地将Lucene所提供的索引组建成shard形式，分布于多个节点之上，从而构建成分布式实时查询的组件。&lt;/p&gt;
&lt;h3 id=&quot;Logstash&quot;&gt;&lt;a href=&quot;#Logstash&quot; class=&quot;headerlink&quot; title=&quot;Logstash&quot;&gt;&lt;/a&gt;Logstash&lt;/h3&gt;&lt;p&gt;支持多数据获取机制，通过TCP/UDP协议、文件、syslog、windows EventLogs及STDIN等。获取到数据后，它支持对数据执行过滤、修改等操作。&lt;/p&gt;
&lt;p&gt;logstash配置框架：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;input {
  ...
}

filter {
  ...
}

output {
  ...
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Logstash是高度插件化的，支持4种类型的插件(每一类都有数10种具体的实现)：&lt;br&gt;input、filter、codec、output  &lt;/p&gt;
&lt;h4 id=&quot;input插件&quot;&gt;&lt;a href=&quot;#input插件&quot; class=&quot;headerlink&quot; title=&quot;input插件&quot;&gt;&lt;/a&gt;input插件&lt;/h4&gt;&lt;p&gt;下面介绍input插件几种常见的实现。&lt;br&gt;&lt;strong&gt;1.File：&lt;/strong&gt;&lt;br&gt;从指定的文件中读取事件流；其工作特性类似于tail -1，不断将文件的最后一行读出来；不过第一次读取文件时是从第1行开始的；文件中的每一行都被识别为一个事件。对于Logstash而言，每一个独立的信息就是一个事件，而对于文本文件来讲，每一个事件是用一行来表示的，如果期望将多行识别为一个事件的话，就需要codec插件。logstash使用FileWatch(Ruby Gem库)机制来监听文件的变化，FileWatch是Linux内核中提供的一个功能。可以一下子监听多个文件。文件状态记录在.sincedb数据库中。另外，FIle插件还能够自动识别你的日志滚动操作，日志一般达到某个体积、或者满足多少天后会滚动，File也能识别。它会按照上一次那个日志文件所在的位置读取，自动进行滚动，读到最新的文件。&lt;br&gt;&lt;strong&gt;2.udp插件:&lt;/strong&gt;&lt;br&gt;如果我们安装的某个程序，它能够通过udp的某个端口输出自己相关日志信息或者事件。Logstash通过udp协议从网络连接来读取Message。其必备参数为port，用于指明自己监听的端口，别的主机向这个端口发事件，host则用来指明自己监听的地址。&lt;br&gt;&lt;strong&gt;3.redis插件：&lt;/strong&gt;&lt;br&gt;允许Logstash从redis读数据。支持redis channel和lists两种方式来获取数据。&lt;/p&gt;
&lt;h4 id=&quot;filter插件&quot;&gt;&lt;a href=&quot;#filter插件&quot; class=&quot;headerlink&quot; title=&quot;filter插件&quot;&gt;&lt;/a&gt;filter插件&lt;/h4&gt;&lt;p&gt;filter插件主要用于将event通过output发出之前，对其实现某些处理功能。&lt;br&gt;&lt;strong&gt;1.grok：&lt;/strong&gt;用于分析并结构化文本数据。把每一个事件字段切好，做成结构化的形式。使得我们后续可以分析。目前能处理syslog、apache、nginx的日志等。目前Logstash提供120种grok模式，因为我们要分析文本，必须提供模式。&lt;br&gt;简单举个例子说明下什么是模式：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;55.3.244.1 GET /index.html 15824 0.043
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;比如对于上面一条日志，我们定义如下的模式：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;那么经过grok过滤后，这个事件后将会加有分析后的字段：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;client =&amp;gt; 55.3.244.1
method =&amp;gt; GET
request =&amp;gt; /index.html
bytes =&amp;gt; 15824
duration =&amp;gt; 0.043
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Logstash中默认自带了很多pattern，但是如果没有你需要的，就需要自己在文件中定义你需要的模式。&lt;br&gt;&lt;strong&gt;grok语法格式：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;%{SYNTAX:SEMANTIC}
    SYNTAX表示预定义模式名称。就是Logstash解压后pattern相关文件里已经定义好的模式，如果没有的，可以自己写在上面那个文件里，名字得全大写。具体的文件在下面的实战中会讲到。
    SEMANTIC表示匹配到的文本的自定义的标识符。比如识别处理的ip可能是clientip也可能是server端的ip，我们可以自定义名字。
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;output插件&quot;&gt;&lt;a href=&quot;#output插件&quot; class=&quot;headerlink&quot; title=&quot;output插件&quot;&gt;&lt;/a&gt;output插件&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;1.stdout：&lt;/strong&gt;标准输出插件。&lt;br&gt;&lt;strong&gt;2.elasticsearch：&lt;/strong&gt;将结果输出到Elasticsearch中。&lt;br&gt;elasticsearch插件常见配置参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;index：数据存储在ES中的哪个索引中。默认“logstash-%{+YYYY.MM.dd}”，每天使用一个单独的索引。&lt;/li&gt;
&lt;li&gt;workers：执行output的线程数。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;3.redis插件：&lt;/strong&gt;将结果输出到redis中。&lt;br&gt;Logstash使用redis作为输入或输出插件时，尤其是输出插件时，它有两种数据类型可以用来保存Logstash输出的数据。一种是list，一种是channel。一般使用list，list比较简单。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;【注意】：Logstash版本不同，每种插件支持的参数也不同，具体看&lt;a href=&quot;https://www.elastic.co/guide/en/logstash/current/index.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;官方文档&lt;/a&gt;。&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&quot;环境信息&quot;&gt;&lt;a href=&quot;#环境信息&quot; class=&quot;headerlink&quot; title=&quot;环境信息&quot;&gt;&lt;/a&gt;环境信息&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;操作系统版本&lt;/th&gt;
&lt;th&gt;IP地址&lt;/th&gt;
&lt;th&gt;安装软件&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;hadoop16&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.206.16&lt;/td&gt;
&lt;td&gt;elasticsearch-5.6.3.zip、kibana-5.6.3-linux-x86_64.tar.gz&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;spark32&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.206.32&lt;/td&gt;
&lt;td&gt;logstash-5.6.3.tar.gz&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&quot;Elasticsearch-1&quot;&gt;&lt;a href=&quot;#Elasticsearch-1&quot; class=&quot;headerlink&quot; title=&quot;Elasticsearch&quot;&gt;&lt;/a&gt;Elasticsearch&lt;/h2&gt;&lt;h3 id=&quot;安装JDK8&quot;&gt;&lt;a href=&quot;#安装JDK8&quot; class=&quot;headerlink&quot; title=&quot;安装JDK8&quot;&gt;&lt;/a&gt;安装JDK8&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;# tar zxf jdk-8u73-linux-x64.gz -C /usr/java/
# vim /etc/profile
export JAVA_HOME=/usr/java/jdk1.8.0_73
export PATH=$JAVA_HOME/bin:$PATH
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
# source /etc/profile
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;新建用户&quot;&gt;&lt;a href=&quot;#新建用户&quot; class=&quot;headerlink&quot; title=&quot;新建用户&quot;&gt;&lt;/a&gt;新建用户&lt;/h3&gt;&lt;p&gt;【注意】:elasticsearch不能使用root用户去启动。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# groupadd -g 510 es
# useradd -g 510 -u 510 es
# echo &amp;quot;wisedu123&amp;quot; | passwd --stdin es &amp;amp;&amp;gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;安装配置Elasticsearch&quot;&gt;&lt;a href=&quot;#安装配置Elasticsearch&quot; class=&quot;headerlink&quot; title=&quot;安装配置Elasticsearch&quot;&gt;&lt;/a&gt;安装配置Elasticsearch&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@hadoop16 opt]# unzip -oq elasticsearch-5.6.3.zip
[root@hadoop16 opt]# vim elasticsearch-5.6.3/config/elasticsearch.yml 
cluster.name: loges
node.name: hadoop16
network.host: 172.16.206.16
bootstrap.memory_lock: true
[root@hadoop16 opt]# vim /etc/security/limits.conf
es - memlock -1
[root@hadoop16 opt]# chown -R es.es elasticsearch-5.6.3
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;配置说明：&lt;br&gt;bootstrap.memory_lock: true&lt;br&gt;这个配置的作用是保护Elasticsearch使用的内存防止其被swapped。&lt;/p&gt;
&lt;h3 id=&quot;优化Elasticsearch&quot;&gt;&lt;a href=&quot;#优化Elasticsearch&quot; class=&quot;headerlink&quot; title=&quot;优化Elasticsearch&quot;&gt;&lt;/a&gt;优化Elasticsearch&lt;/h3&gt;&lt;p&gt;1.配置操作系统文件描述符数&lt;br&gt;输入下面的命令进行查看：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ ulimit -a
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;找到open files那行：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;open files                      (-n) 1024
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;设置需要修改：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/security/limits.conf
es               -       nofile          65536
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.增大虚拟内存mmap count配置&lt;br&gt;备注：如果你以.deb或.rpm包安装，则默认不需要设置此项，因为已经被自动设置，查看方式为：&lt;br&gt;sysctl vm.max_map_count&lt;br&gt;如果是手动安装，以root身份执行如下命令：&lt;br&gt;sysctl vm.max_map_count=262144&lt;br&gt;并修改文件使设置永久生效：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/sysctl.conf    
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;加一行：vm.max_map_count = 262144&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# sysctl -p
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;改完后，重启elasticsearch。&lt;br&gt;如果需要需改JVM大小，请修改 jvm.options 配置文件。&lt;/p&gt;
&lt;h3 id=&quot;启动Elasticsearch&quot;&gt;&lt;a href=&quot;#启动Elasticsearch&quot; class=&quot;headerlink&quot; title=&quot;启动Elasticsearch&quot;&gt;&lt;/a&gt;启动Elasticsearch&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@hadoop16 opt]# su - es
[es@hadoop16 ~]$ /opt/elasticsearch-5.6.3/bin/elasticsearch -d
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;检验bootstrap.mlockall: true是否生效：&lt;br&gt;curl &lt;a href=&quot;http://172.16.206.16:9200/_nodes/process?pretty&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.206.16:9200/_nodes/process?pretty&lt;/a&gt;&lt;br&gt;关注这个这个请求返回数据中的mlockall的值，如果为false，则说明锁定内存失败，这可能由于运行elasticsearch的用户不具备这样的权限。解决该问题的方法是：&lt;br&gt;在运行elasticsearch之前，以root身份执行&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ulimit -l unlimited
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后再次重启elasticsearch。并查看上面的请求中的mlockall的值是否为true。&lt;br&gt;【注意】:这时候需要在root执行ulimit -l unlimited的shell终端上su - es，然后重启elasticsearch。因为这是命令行设置的ulimit -l unlimited，只对当前会话生效。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# ulimit -l unlimited
# su - es
$ ps -ef|grep elasticsearch
$ kill -9 27189
$ /usr/local/elasticsearch/bin/elasticsearch -d
$ curl http://172.16.206.16:9200/_nodes/process?pretty
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;要想永久修改锁定内存大小无限制，需修改/etc/security/limits.conf，添加下面的内容，改完不需要重启系统，但是需要重新打开一个shell建立会话。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;es - memlock -1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其中，es代表运行elasticsearch的用户，-表示同时设置了soft和hard，memlock代表设置的是”锁定内存”这个类型，-1(unlimited或者infinity)代表没限制。&lt;br&gt;【补充】: 要使 /etc/security/limits.conf 文件配置生效，必须要确保 pam_limits.so 文件被加入到相关的启动文件中，启动文件位于/etc/pam.d路径下，如该路径下sshd、login、system-auth等，一般是system-auth文件负责加载该so文件。只要加载了pam_limits.so，则配置就会生效，无需重启系统。&lt;/p&gt;
&lt;h3 id=&quot;安装配置head插件&quot;&gt;&lt;a href=&quot;#安装配置head插件&quot; class=&quot;headerlink&quot; title=&quot;安装配置head插件&quot;&gt;&lt;/a&gt;安装配置head插件&lt;/h3&gt;&lt;p&gt;项目地址：&lt;a href=&quot;https://github.com/mobz/elasticsearch-head&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/mobz/elasticsearch-head&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;安装head插件&quot;&gt;&lt;a href=&quot;#安装head插件&quot; class=&quot;headerlink&quot; title=&quot;安装head插件&quot;&gt;&lt;/a&gt;安装head插件&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@hadoop16 opt]# git clone git://github.com/mobz/elasticsearch-head.git
[root@hadoop16 opt]# yum -y install epel-release
[root@hadoop16 opt]# yum install nodejs -y
[root@hadoop16 opt]# cd elasticsearch-head/
[root@hadoop16 elasticsearch-head]# npm install
npm: relocation error: npm: symbol SSL_set_cert_cb, version libssl.so.10 not defined in file libssl.so.10 with link time reference
[root@hadoop16 elasticsearch-head]# yum update openssl -y
[root@hadoop16 elasticsearch-head]# npm install 
# 如果下载速度太慢，可以如下方式安装
[root@hadoop16 elasticsearch-head]# npm install -gd express --registry=http://registry.npm.taobao.org
# 为了避免每次安装都需要--registry参数，可以使用如下命令进行永久设置：
[root@hadoop16 elasticsearch-head]# npm config set registry http://registry.npm.taobao.org 
[root@hadoop16 elasticsearch-head]# npm install grunt --save-dev
[root@hadoop16 elasticsearch-head]# npm install grunt-contrib-clean 
[root@hadoop16 elasticsearch-head]# npm install grunt-contrib-concat 
[root@hadoop16 elasticsearch-head]# npm install grunt-contrib-watch 
[root@hadoop16 elasticsearch-head]# npm install grunt-contrib-connect 
[root@hadoop16 elasticsearch-head]# npm install grunt-contrib-copy 
[root@hadoop16 elasticsearch-head]# npm install grunt-contrib-jasmine 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;奇怪的是最后一个没有安装成功，是因为该模块依赖了phantomjs。但是配置之后，依然无法安装。直接启动就可以了：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@hadoop16 elasticsearch-head]# grunt server 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;以上安装head插件的步骤太过复杂，我们可以将下载下来的elasticsearch-head包放入tomcat中，直接启动tomcat就可以访问head插件了。本实验环境下是采用将head插件放入tomcat中运行。&lt;/p&gt;
&lt;h4 id=&quot;配置&quot;&gt;&lt;a href=&quot;#配置&quot; class=&quot;headerlink&quot; title=&quot;配置&quot;&gt;&lt;/a&gt;配置&lt;/h4&gt;&lt;p&gt;1.修改Elasticsearch的配置文件elasticsearch.yml，增加跨域的配置(需要重启es才能生效)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;http.cors.enabled: true
http.cors.allow-origin: &amp;quot;*&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.编辑head/Gruntfile.js，修改服务器监听地址，增加hostname属性，将其值设置为*。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@hadoop16 elasticsearch-head]# pwd
/opt/apache-tomcat-8.5.23/webapps/elasticsearch-head
[root@hadoop16 elasticsearch-head]# vim Gruntfile.js
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;以下两种配置都是可以的：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Type1
connect: {
        hostname: &amp;apos;*&amp;apos;,
        server: {
                options: {
                        port: 9100,
                        base: &amp;apos;.&amp;apos;,
                        keepalive: true
                }
        }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;　　&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Type 2
connect: {
        server: {
                options: {
                        hostname: &amp;apos;*&amp;apos;,
                        port: 9100,
                        base: &amp;apos;.&amp;apos;,
                        keepalive: true
                }
        }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3.编辑head/_site/app.js，修改head连接es的地址，将localhost修改为es的IP地址&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@hadoop16 elasticsearch-head]# pwd
/opt/apache-tomcat-8.5.23/webapps/elasticsearch-head
[root@hadoop16 elasticsearch-head]# vim _site/app.js
# 原配置
this.base_uri = this.config.base_uri || this.prefs.get(&amp;quot;app-base_uri&amp;quot;) || &amp;quot;http://localhost:9200&amp;quot;;
# 将localhost修改为ES的IP地址
this.base_uri = this.config.base_uri || this.prefs.get(&amp;quot;app-base_uri&amp;quot;) || &amp;quot;http://YOUR-ES-IP:9200&amp;quot;;
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;启动&quot;&gt;&lt;a href=&quot;#启动&quot; class=&quot;headerlink&quot; title=&quot;启动&quot;&gt;&lt;/a&gt;启动&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@hadoop16 elasticsearch-head]# cd /opt/apache-tomcat-8.5.23/
[root@hadoop16 apache-tomcat-8.5.23]# bin/startup.sh 
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;Logstash-1&quot;&gt;&lt;a href=&quot;#Logstash-1&quot; class=&quot;headerlink&quot; title=&quot;Logstash&quot;&gt;&lt;/a&gt;Logstash&lt;/h2&gt;&lt;h3 id=&quot;安装JDK8-1&quot;&gt;&lt;a href=&quot;#安装JDK8-1&quot; class=&quot;headerlink&quot; title=&quot;安装JDK8&quot;&gt;&lt;/a&gt;安装JDK8&lt;/h3&gt;&lt;p&gt;Logstash是jruby研发的，需要跑在JVM上，所以需要安装JDK。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# tar zxf jdk-8u73-linux-x64.gz -C /usr/java/
# vim /etc/profile
export JAVA_HOME=/usr/java/jdk1.8.0_73
export PATH=$JAVA_HOME/bin:$PATH
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
# source /etc/profile
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;安装配置Logstash&quot;&gt;&lt;a href=&quot;#安装配置Logstash&quot; class=&quot;headerlink&quot; title=&quot;安装配置Logstash&quot;&gt;&lt;/a&gt;安装配置Logstash&lt;/h3&gt;&lt;p&gt;我这里选择二进制包安装。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 opt]# tar zxf logstash-5.6.3.tar.gz 
[root@spark32 opt]# cd logstash-5.6.3/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;由于我这里选择是二进制安装，pattern文件位置在：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 logstash-5.6.3]# ls /opt/logstash-5.6.3/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/grok-patterns
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;收集Nginx-access日志&quot;&gt;&lt;a href=&quot;#收集Nginx-access日志&quot; class=&quot;headerlink&quot; title=&quot;收集Nginx access日志&quot;&gt;&lt;/a&gt;收集Nginx access日志&lt;/h3&gt;&lt;p&gt;Logstash收集Nginx日志，输出到Elasticsearch中。&lt;/p&gt;
&lt;h4 id=&quot;创建Nginx-pattern&quot;&gt;&lt;a href=&quot;#创建Nginx-pattern&quot; class=&quot;headerlink&quot; title=&quot;创建Nginx pattern&quot;&gt;&lt;/a&gt;创建Nginx pattern&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# vim /opt/logstash-5.6.3/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns/grok-patterns
# Nginx log
NGUSERNAME [a-zA-Z\.\@\-\+_%]+
NGUSER %{NGUSERNAME}
NGINXACCESS %{IPORHOST:clientip} - %{NOTSPACE:remote_user} \[%{HTTPDATE:timestamp}\] \&amp;quot;(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\&amp;quot; %{NUMBER:response} (?:%{NUMBER:bytes}|-) %{QS:referrer} %{QS:agent} %{NOTSPACE:http_x_forwarded_for}
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;创建Logstash配置文件&quot;&gt;&lt;a href=&quot;#创建Logstash配置文件&quot; class=&quot;headerlink&quot; title=&quot;创建Logstash配置文件&quot;&gt;&lt;/a&gt;创建Logstash配置文件&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@spark32 logstash-5.6.3]# cd /opt/logstash-5.6.3
[root@spark32 logstash-5.6.3]# mkdir conf
[root@spark32 logstash-5.6.3]# cd conf
[root@spark32 conf]# vim logstash_nginx.conf
input {
  file {
    path  =&amp;gt;  [&amp;quot;/usr/local/openresty/nginx/logs/host.access.log&amp;quot;]
    type  =&amp;gt;  &amp;quot;nginxlog&amp;quot;
    start_position  =&amp;gt;  &amp;quot;beginning&amp;quot;
  }
}

filter {
  grok {
    match =&amp;gt; { &amp;quot;message&amp;quot; =&amp;gt; &amp;quot;%{NGINXACCESS}&amp;quot; }
  }
}

output {
  elasticsearch {
    hosts    =&amp;gt;  [&amp;quot;172.16.206.16:9200&amp;quot;]
    action   =&amp;gt;  &amp;quot;index&amp;quot;
    index    =&amp;gt;  &amp;quot;logstash-%{+YYYY.MM.dd}&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;启动Logstash&quot;&gt;&lt;a href=&quot;#启动Logstash&quot; class=&quot;headerlink&quot; title=&quot;启动Logstash&quot;&gt;&lt;/a&gt;启动Logstash&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@spark32 logstash-5.6.3]# bin/logstash -f /opt/logstash-5.6.3/conf/logstash_nginx.conf -t
Sending Logstash&amp;apos;s logs to /opt/logstash-5.6.3/logs which is now configured via log4j2.properties
[2017-10-20T17:07:14,793][INFO ][logstash.modules.scaffold] Initializing module {:module_name=&amp;gt;&amp;quot;fb_apache&amp;quot;, :directory=&amp;gt;&amp;quot;/opt/logstash-5.6.3/modules/fb_apache/configuration&amp;quot;}
[2017-10-20T17:07:14,797][INFO ][logstash.modules.scaffold] Initializing module {:module_name=&amp;gt;&amp;quot;netflow&amp;quot;, :directory=&amp;gt;&amp;quot;/opt/logstash-5.6.3/modules/netflow/configuration&amp;quot;}
[2017-10-20T17:07:14,803][INFO ][logstash.setting.writabledirectory] Creating directory {:setting=&amp;gt;&amp;quot;path.queue&amp;quot;, :path=&amp;gt;&amp;quot;/opt/logstash-5.6.3/data/queue&amp;quot;}
[2017-10-20T17:07:14,804][INFO ][logstash.setting.writabledirectory] Creating directory {:setting=&amp;gt;&amp;quot;path.dead_letter_queue&amp;quot;, :path=&amp;gt;&amp;quot;/opt/logstash-5.6.3/data/dead_letter_queue&amp;quot;}
Configuration OK
[2017-10-20T17:07:14,999][INFO ][logstash.runner          ] Using config.test_and_exit mode. Config Validation Result: OK. Exiting Logstash
[root@spark32 logstash-5.6.3]# bin/logstash -f /opt/logstash-5.6.3/conf/logstash_nginx.conf &amp;amp; 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;记录收集到日志的文件位置在:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 file]# ls -a
.  ..  .sincedb_650663ba19529187a32a8b9dc99049f8
[root@spark32 file]# pwd
/opt/logstash-5.6.3/data/plugins/inputs/file
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查看Elasticsearch索引：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[es@hadoop16 elasticsearch-5.6.3]$ curl -XGET &amp;apos;172.16.206.16:9200/_cat/indices&amp;apos;
yellow open logstash-2017.10.20 DVARGYZ2R9CfT-xyLrhyAQ 5 1 7 0 49.9kb 49.9kb
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;Kibana&quot;&gt;&lt;a href=&quot;#Kibana&quot; class=&quot;headerlink&quot; title=&quot;Kibana&quot;&gt;&lt;/a&gt;Kibana&lt;/h2&gt;&lt;h3 id=&quot;安装配置kibana&quot;&gt;&lt;a href=&quot;#安装配置kibana&quot; class=&quot;headerlink&quot; title=&quot;安装配置kibana&quot;&gt;&lt;/a&gt;安装配置kibana&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@hadoop16 opt]# tar zxf kibana-5.6.3-linux-x86_64.tar.gz 
[root@hadoop16 opt]# ln -sv kibana-5.6.3-linux-x86_64 kibana
‘kibana’ -&amp;gt; ‘kibana-5.6.3-linux-x86_64’
[root@hadoop16 opt]# cd kibana
[root@hadoop16 kibana]# cd config/
[root@hadoop16 config]# vim kibana.yml 
server.host: &amp;quot;172.16.206.16&amp;quot;
elasticsearch.url: &amp;quot;http://172.16.206.16:9200&amp;quot;
[root@hadoop16 config]# cd ..
[root@hadoop16 kibana]# bin/kibana &amp;amp;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;访问&quot;&gt;&lt;a href=&quot;#访问&quot; class=&quot;headerlink&quot; title=&quot;访问&quot;&gt;&lt;/a&gt;访问&lt;/h3&gt;&lt;p&gt;浏览器输入：&lt;a href=&quot;http://172.16.206.16:5601/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.206.16:5601/&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/1.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/2.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;点击左侧菜单的Discover：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/3.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;比如我用curl访问下nginx，然后去kibana中搜索。要稍微等一下，等日志进到Elasticsearch中。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@hadoop16 kibana]# curl http://172.16.206.32:808
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/4.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/5.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;总结：&lt;/strong&gt;&lt;br&gt;1.Logstash 主要的特点就是它的灵活性，因为它有很多插件。然后它清楚的文档已经直白的配置格式让它可以再多种场景下应用。这样的良性循环让我们可以在网上找到很多资源，几乎可以处理任何问题。&lt;br&gt;2.Logstash不支持缓存，当然我们可以使用redis或者kafka作为中心缓冲池，架构如下：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/6.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;3.Logstash是在jvm跑的，资源消耗比较大，太重量级了。后来作者又用golang写了一个功能较少但是资源消耗也小的轻量级的logstash-forwarder。后来这个人加入了elastic公司。因为elastic公司本身还收购了另一个开源项目packetbeat，而这个项目专门就是用golang写的，有整个团队，所以elastic公司干脆把logstash-forwarder的开发工作也合并到同一个golang团队来搞，于是新的项目就叫filebeat了。当然也可以自己写agent，用go、python都可以写。这样我们就可以使用轻量的日志传输工具，将数据从服务器端经由一个或多个 Logstash 中心服务器传输到 Elasticsearch。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/ELK/7.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;ELK介绍&quot;&gt;&lt;a href=&quot;#ELK介绍&quot; class=&quot;headerlink&quot; title=&quot;ELK介绍&quot;&gt;&lt;/a&gt;ELK介绍&lt;/h2&gt;&lt;p&gt;ELK由Elasticsearch、Logstash和Kibana三部分组件组成。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Elasticsearch是个开源分布式搜索引擎，它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制，restful风格接口，多数据源，自动搜索负载等。&lt;/li&gt;
&lt;li&gt;Logstash是一个完全开源的工具，它可以对你的日志进行收集、分析，并将其存储供以后使用。
    
    </summary>
    
      <category term="日志" scheme="http://yoursite.com/categories/%E6%97%A5%E5%BF%97/"/>
    
    
      <category term="ELK" scheme="http://yoursite.com/tags/ELK/"/>
    
  </entry>
  
  <entry>
    <title>EFK收集Kubernetes应用日志</title>
    <link href="http://yoursite.com/2017/10/12/EFK%E6%94%B6%E9%9B%86Kubernetes%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97/"/>
    <id>http://yoursite.com/2017/10/12/EFK收集Kubernetes应用日志/</id>
    <published>2017-10-12T06:53:08.000Z</published>
    <updated>2017-12-04T02:36:46.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;EFK介绍&quot;&gt;&lt;a href=&quot;#EFK介绍&quot; class=&quot;headerlink&quot; title=&quot;EFK介绍&quot;&gt;&lt;/a&gt;EFK介绍&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Logstash(或者Fluentd)负责收集日志 &lt;/li&gt;
&lt;li&gt;Elasticsearch存储日志并提供搜索 &lt;/li&gt;
&lt;li&gt;Kibana负责日志查询和展示&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;官方地址：&lt;a href=&quot;https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch&lt;/a&gt;&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;通过在每台node上部署一个以DaemonSet方式运行的fluentd来收集每台node上的日志。Fluentd将docker日志目录/var/lib/docker/containers和/var/log目录挂载到Pod中，然后Pod会在node节点的/var/log/pods目录中创建新的目录，可以区别不同的容器日志输出，该目录下有一个日志文件链接到/var/lib/docker/contianers目录下的容器日志输出。&lt;/p&gt;
&lt;h2 id=&quot;配置efk-rbac-yaml文件&quot;&gt;&lt;a href=&quot;#配置efk-rbac-yaml文件&quot; class=&quot;headerlink&quot; title=&quot;配置efk-rbac.yaml文件&quot;&gt;&lt;/a&gt;配置efk-rbac.yaml文件&lt;/h2&gt;&lt;p&gt;EFK服务也需要一个efk-rbac.yaml文件，配置serviceaccount为efk。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 opt]# mkdir efk
[root@node1 opt]# cd efk
[root@node1 efk]# vim efk-rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: efk
  namespace: kube-system

---

kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: efk
subjects:
  - kind: ServiceAccount
    name: efk
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;配置-es-controller-yaml&quot;&gt;&lt;a href=&quot;#配置-es-controller-yaml&quot; class=&quot;headerlink&quot; title=&quot;配置 es-controller.yaml&quot;&gt;&lt;/a&gt;配置 es-controller.yaml&lt;/h2&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;19&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;20&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;21&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;22&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;23&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;24&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;25&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;26&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;27&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;28&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;29&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;30&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;31&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;32&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;33&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;34&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;35&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;36&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;37&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;38&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;39&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;40&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;41&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;42&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;43&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;44&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;45&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;46&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;47&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;48&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;49&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;50&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;51&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;[root@node1 efk]# vim es-controller.yaml&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;apiVersion: v1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;kind: ReplicationController&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  name: elasticsearch-logging-v1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  namespace: kube-system&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  labels:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    k8s-app: elasticsearch-logging&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    version: v1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    kubernetes.io/cluster-service: &amp;quot;true&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    addonmanager.kubernetes.io/mode: Reconcile&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  replicas: 2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  selector:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    k8s-app: elasticsearch-logging&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    version: v1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  template:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      labels:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        k8s-app: elasticsearch-logging&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        version: v1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        kubernetes.io/cluster-service: &amp;quot;true&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      serviceAccountName: efk&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      containers:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      - image: index.tenxcloud.com/jimmy/elasticsearch:v2.4.1-2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        name: elasticsearch-logging&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        resources:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          # need more cpu upon initialization, therefore burstable class&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          limits:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            cpu: 1000m&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          requests:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            cpu: 100m&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        ports:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        - containerPort: 9200&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          name: db&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          protocol: TCP&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        - containerPort: 9300&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          name: transport&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          protocol: TCP&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        volumeMounts:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        - name: es-persistent-storage&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          mountPath: /data&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        env:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        - name: &amp;quot;NAMESPACE&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          valueFrom:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            fieldRef:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;              fieldPath: metadata.namespace&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      volumes:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      - name: es-persistent-storage&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        emptyDir: &amp;#123;&amp;#125;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;配置-es-service-yaml&quot;&gt;&lt;a href=&quot;#配置-es-service-yaml&quot; class=&quot;headerlink&quot; title=&quot;配置 es-service.yaml&quot;&gt;&lt;/a&gt;配置 es-service.yaml&lt;/h2&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;[root@node1 efk]# vim es-service.yaml&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;apiVersion: v1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;kind: Service&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  name: elasticsearch-logging&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  namespace: kube-system&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  labels:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    k8s-app: elasticsearch-logging&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    kubernetes.io/cluster-service: &amp;quot;true&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    addonmanager.kubernetes.io/mode: Reconcile&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    kubernetes.io/name: &amp;quot;Elasticsearch&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  ports:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  - port: 9200&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    protocol: TCP&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    targetPort: db&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  selector:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    k8s-app: elasticsearch-logging&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;配置-fluentd-es-ds-yaml&quot;&gt;&lt;a href=&quot;#配置-fluentd-es-ds-yaml&quot; class=&quot;headerlink&quot; title=&quot;配置 fluentd-es-ds.yaml&quot;&gt;&lt;/a&gt;配置 fluentd-es-ds.yaml&lt;/h2&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;19&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;20&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;21&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;22&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;23&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;24&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;25&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;26&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;27&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;28&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;29&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;30&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;31&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;32&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;33&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;34&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;35&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;36&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;37&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;38&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;39&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;40&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;41&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;42&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;43&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;44&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;45&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;46&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;47&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;48&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;49&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;50&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;51&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;52&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;53&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;54&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;55&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;56&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;apiVersion: extensions/v1beta1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;kind: DaemonSet&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  name: fluentd-es-v1.22&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  namespace: kube-system&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  labels:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    k8s-app: fluentd-es&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    kubernetes.io/cluster-service: &amp;quot;true&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    addonmanager.kubernetes.io/mode: Reconcile&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    version: v1.22&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  template:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      labels:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        k8s-app: fluentd-es&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        kubernetes.io/cluster-service: &amp;quot;true&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        version: v1.22&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      # This annotation ensures that fluentd does not get evicted if the node&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      # supports critical pod annotation based priority scheme.&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      # Note that this does not guarantee admission on the nodes (#40573).&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      annotations:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        scheduler.alpha.kubernetes.io/critical-pod: &amp;apos;&amp;apos;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    spec:  &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      serviceAccountName: efk&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      containers:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      - name: fluentd-es&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        image: index.tenxcloud.com/jimmy/fluentd-elasticsearch:1.22&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        command:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          - &amp;apos;/bin/sh&amp;apos;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          - &amp;apos;-c&amp;apos;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          - &amp;apos;/usr/sbin/td-agent 2&amp;gt;&amp;amp;1 &amp;gt;&amp;gt; /var/log/fluentd.log&amp;apos;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        resources:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          limits:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            memory: 200Mi&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          requests:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            cpu: 100m&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            memory: 200Mi&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        volumeMounts:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        - name: varlog&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          mountPath: /var/log&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        - name: varlibdockercontainers&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          mountPath: /var/lib/docker/containers&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          readOnly: true&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      nodeSelector:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        beta.kubernetes.io/fluentd-ds-ready: &amp;quot;true&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      tolerations:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      - key : &amp;quot;node.alpha.kubernetes.io/ismaster&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        effect: &amp;quot;NoSchedule&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      terminationGracePeriodSeconds: 30&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      volumes:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      - name: varlog&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        hostPath:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          path: /var/log&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      - name: varlibdockercontainers&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        hostPath:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          path: /var/lib/docker/containers&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;配置-kibana-controller-yaml&quot;&gt;&lt;a href=&quot;#配置-kibana-controller-yaml&quot; class=&quot;headerlink&quot; title=&quot;配置 kibana-controller.yaml&quot;&gt;&lt;/a&gt;配置 kibana-controller.yaml&lt;/h2&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;19&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;20&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;21&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;22&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;23&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;24&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;25&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;26&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;27&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;28&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;29&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;30&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;31&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;32&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;33&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;34&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;35&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;36&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;37&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;38&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;apiVersion: extensions/v1beta1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;kind: Deployment&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  name: kibana-logging&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  namespace: kube-system&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  labels:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    k8s-app: kibana-logging&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    kubernetes.io/cluster-service: &amp;quot;true&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    addonmanager.kubernetes.io/mode: Reconcile&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  replicas: 1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  selector:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    matchLabels:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      k8s-app: kibana-logging&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  template:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      labels:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        k8s-app: kibana-logging&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      serviceAccountName: efk&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      containers:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      - name: kibana-logging&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        image: index.tenxcloud.com/jimmy/kibana:v4.6.1-1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        resources:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          # keep request = limit to keep this container in guaranteed class&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          limits:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            cpu: 100m&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          requests:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            cpu: 100m&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        env:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          - name: &amp;quot;ELASTICSEARCH_URL&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            value: &amp;quot;http://elasticsearch-logging:9200&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          - name: &amp;quot;KIBANA_BASE_URL&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            value: &amp;quot;/api/v1/proxy/namespaces/kube-system/services/kibana-logging&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        ports:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        - containerPort: 5601&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          name: ui&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          protocol: TCP&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;配置-kibana-service-yaml&quot;&gt;&lt;a href=&quot;#配置-kibana-service-yaml&quot; class=&quot;headerlink&quot; title=&quot;配置 kibana-service.yaml&quot;&gt;&lt;/a&gt;配置 kibana-service.yaml&lt;/h2&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;apiVersion: v1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;kind: Service&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  name: kibana-logging&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  namespace: kube-system&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  labels:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    k8s-app: kibana-logging&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    kubernetes.io/cluster-service: &amp;quot;true&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    addonmanager.kubernetes.io/mode: Reconcile&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    kubernetes.io/name: &amp;quot;Kibana&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  ports:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  - port: 5601&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    protocol: TCP&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    targetPort: ui&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  selector:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    k8s-app: kibana-logging&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 efk]# ls
efk-rbac.yaml  es-controller.yaml  es-service.yaml  fluentd-es-ds.yaml  kibana-controller.yaml  kibana-service.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;给-Node-设置标签&quot;&gt;&lt;a href=&quot;#给-Node-设置标签&quot; class=&quot;headerlink&quot; title=&quot;给 Node 设置标签&quot;&gt;&lt;/a&gt;给 Node 设置标签&lt;/h2&gt;&lt;p&gt;定义 DaemonSet fluentd-es-v1.22 时设置了 nodeSelector beta.kubernetes.io/fluentd-ds-ready=true ，所以需要在期望运行 fluentd 的 Node 上设置该标签；&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 efk]# kubectl label nodes 172.16.7.151 beta.kubernetes.io/fluentd-ds-ready=true
node &amp;quot;172.16.7.151&amp;quot; labeled
[root@node1 efk]# kubectl label nodes 172.16.7.152 beta.kubernetes.io/fluentd-ds-ready=true
node &amp;quot;172.16.7.152&amp;quot; labeled
[root@node1 efk]# kubectl label nodes 172.16.7.153 beta.kubernetes.io/fluentd-ds-ready=true
node &amp;quot;172.16.7.153&amp;quot; labeled
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;执行定义文件&quot;&gt;&lt;a href=&quot;#执行定义文件&quot; class=&quot;headerlink&quot; title=&quot;执行定义文件&quot;&gt;&lt;/a&gt;执行定义文件&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;[root@node1 efk]# kubectl create -f .
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;检查执行结果&quot;&gt;&lt;a href=&quot;#检查执行结果&quot; class=&quot;headerlink&quot; title=&quot;检查执行结果&quot;&gt;&lt;/a&gt;检查执行结果&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;[root@node1 efk]# kubectl get deployment -n kube-system|grep kibana
kibana-logging         1         1         1            1           1h

[root@node1 efk]# kubectl get pods -n kube-system|grep -E &amp;apos;elasticsearch|fluentd|kibana&amp;apos;
elasticsearch-logging-v1-nw3p3          1/1       Running   0          43m
elasticsearch-logging-v1-pp89h          1/1       Running   0          43m
fluentd-es-v1.22-cqd1s                  1/1       Running   0          15m
fluentd-es-v1.22-f5ljr                  0/1       Error     6          15m
fluentd-es-v1.22-x24jx                  1/1       Running   0          15m
kibana-logging-4293390753-kg8kx         1/1       Running   0          1h

[root@node1 efk]# kubectl get service  -n kube-system|grep -E &amp;apos;elasticsearch|kibana&amp;apos;
elasticsearch-logging   10.254.50.63     &amp;lt;none&amp;gt;        9200/TCP                        1h
kibana-logging          10.254.169.159   &amp;lt;none&amp;gt;        5601/TCP                        1h
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;kibana Pod 第一次启动时会用&lt;strong&gt;较长时间(10-20分钟)&lt;/strong&gt;来优化和 Cache 状态页面，可以 tailf 该 Pod 的日志观察进度。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 efk]# kubectl logs kibana-logging-4293390753-86h5d -n kube-system -f
ELASTICSEARCH_URL=http://elasticsearch-logging:9200
server.basePath: /api/v1/proxy/namespaces/kube-system/services/kibana-logging
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-10-13T00:51:31Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;info&amp;quot;,&amp;quot;optimize&amp;quot;],&amp;quot;pid&amp;quot;:5,&amp;quot;message&amp;quot;:&amp;quot;Optimizing and caching bundles for kibana and statusPage. This may take a few minutes&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-10-13T01:13:36Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;info&amp;quot;,&amp;quot;optimize&amp;quot;],&amp;quot;pid&amp;quot;:5,&amp;quot;message&amp;quot;:&amp;quot;Optimization of bundles for kibana and statusPage complete in 1324.64 seconds&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-10-13T01:13:37Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;status&amp;quot;,&amp;quot;plugin:kibana@1.0.0&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:5,&amp;quot;state&amp;quot;:&amp;quot;green&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Status changed from uninitialized to green - Ready&amp;quot;,&amp;quot;prevState&amp;quot;:&amp;quot;uninitialized&amp;quot;,&amp;quot;prevMsg&amp;quot;:&amp;quot;uninitialized&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-10-13T01:13:38Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;status&amp;quot;,&amp;quot;plugin:elasticsearch@1.0.0&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:5,&amp;quot;state&amp;quot;:&amp;quot;yellow&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Status changed from uninitialized to yellow - Waiting for Elasticsearch&amp;quot;,&amp;quot;prevState&amp;quot;:&amp;quot;uninitialized&amp;quot;,&amp;quot;prevMsg&amp;quot;:&amp;quot;uninitialized&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-10-13T01:13:39Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;status&amp;quot;,&amp;quot;plugin:kbn_vislib_vis_types@1.0.0&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:5,&amp;quot;state&amp;quot;:&amp;quot;green&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Status changed from uninitialized to green - Ready&amp;quot;,&amp;quot;prevState&amp;quot;:&amp;quot;uninitialized&amp;quot;,&amp;quot;prevMsg&amp;quot;:&amp;quot;uninitialized&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-10-13T01:13:39Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;status&amp;quot;,&amp;quot;plugin:markdown_vis@1.0.0&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:5,&amp;quot;state&amp;quot;:&amp;quot;green&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Status changed from uninitialized to green - Ready&amp;quot;,&amp;quot;prevState&amp;quot;:&amp;quot;uninitialized&amp;quot;,&amp;quot;prevMsg&amp;quot;:&amp;quot;uninitialized&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-10-13T01:13:39Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;status&amp;quot;,&amp;quot;plugin:metric_vis@1.0.0&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:5,&amp;quot;state&amp;quot;:&amp;quot;green&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Status changed from uninitialized to green - Ready&amp;quot;,&amp;quot;prevState&amp;quot;:&amp;quot;uninitialized&amp;quot;,&amp;quot;prevMsg&amp;quot;:&amp;quot;uninitialized&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-10-13T01:13:39Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;status&amp;quot;,&amp;quot;plugin:spyModes@1.0.0&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:5,&amp;quot;state&amp;quot;:&amp;quot;green&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Status changed from uninitialized to green - Ready&amp;quot;,&amp;quot;prevState&amp;quot;:&amp;quot;uninitialized&amp;quot;,&amp;quot;prevMsg&amp;quot;:&amp;quot;uninitialized&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-10-13T01:13:40Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;status&amp;quot;,&amp;quot;plugin:statusPage@1.0.0&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:5,&amp;quot;state&amp;quot;:&amp;quot;green&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Status changed from uninitialized to green - Ready&amp;quot;,&amp;quot;prevState&amp;quot;:&amp;quot;uninitialized&amp;quot;,&amp;quot;prevMsg&amp;quot;:&amp;quot;uninitialized&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-10-13T01:13:40Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;status&amp;quot;,&amp;quot;plugin:table_vis@1.0.0&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:5,&amp;quot;state&amp;quot;:&amp;quot;green&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Status changed from uninitialized to green - Ready&amp;quot;,&amp;quot;prevState&amp;quot;:&amp;quot;uninitialized&amp;quot;,&amp;quot;prevMsg&amp;quot;:&amp;quot;uninitialized&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-10-13T01:13:40Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;listening&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:5,&amp;quot;message&amp;quot;:&amp;quot;Server running at http://0.0.0.0:5601&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-10-13T01:13:45Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;status&amp;quot;,&amp;quot;plugin:elasticsearch@1.0.0&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:5,&amp;quot;state&amp;quot;:&amp;quot;yellow&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Status changed from yellow to yellow - No existing Kibana index found&amp;quot;,&amp;quot;prevState&amp;quot;:&amp;quot;yellow&amp;quot;,&amp;quot;prevMsg&amp;quot;:&amp;quot;Waiting for Elasticsearch&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-10-13T01:13:49Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;status&amp;quot;,&amp;quot;plugin:elasticsearch@1.0.0&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:5,&amp;quot;state&amp;quot;:&amp;quot;green&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Status changed from yellow to green - Kibana index ready&amp;quot;,&amp;quot;prevState&amp;quot;:&amp;quot;yellow&amp;quot;,&amp;quot;prevMsg&amp;quot;:&amp;quot;No existing Kibana index found&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;访问-kibana&quot;&gt;&lt;a href=&quot;#访问-kibana&quot; class=&quot;headerlink&quot; title=&quot;访问 kibana&quot;&gt;&lt;/a&gt;访问 kibana&lt;/h2&gt;&lt;h3 id=&quot;通过-kube-apiserver-访问：获取-kibana-服务-URL&quot;&gt;&lt;a href=&quot;#通过-kube-apiserver-访问：获取-kibana-服务-URL&quot; class=&quot;headerlink&quot; title=&quot;通过 kube-apiserver 访问：获取 kibana 服务 URL&quot;&gt;&lt;/a&gt;通过 kube-apiserver 访问：获取 kibana 服务 URL&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@node1 efk]# kubectl cluster-info
Kubernetes master is running at https://172.16.7.151:6443
Elasticsearch is running at https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/elasticsearch-logging
Heapster is running at https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/heapster
Kibana is running at https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/kibana-logging
KubeDNS is running at https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/kube-dns
kubernetes-dashboard is running at https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard
monitoring-grafana is running at https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana
monitoring-influxdb is running at https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb

To further debug and diagnose cluster problems, use &amp;apos;kubectl cluster-info dump&amp;apos;.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;浏览器访问 URL： &lt;a href=&quot;https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/kibana-logging/app/kibana&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/kibana-logging/app/kibana&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;通过-kubectl-proxy-访问：创建代理&quot;&gt;&lt;a href=&quot;#通过-kubectl-proxy-访问：创建代理&quot; class=&quot;headerlink&quot; title=&quot;通过 kubectl proxy 访问：创建代理&quot;&gt;&lt;/a&gt;通过 kubectl proxy 访问：创建代理&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@node1 efk]# kubectl proxy --address=&amp;apos;172.16.7.151&amp;apos; --port=8086 --accept-hosts=&amp;apos;^*$&amp;apos; &amp;amp;        
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;浏览器访问 URL：&lt;a href=&quot;http://172.16.7.151:8086/api/v1/proxy/namespaces/kube-system/services/kibana-logging&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.151:8086/api/v1/proxy/namespaces/kube-system/services/kibana-logging&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/16.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;如果你在这里发现Create按钮是灰色的无法点击，且Time-filed name中没有选项，fluentd要读取/var/log/containers/目录下的log日志，这些日志是从/var/lib/docker/containers/${CONTAINER_ID}/${CONTAINER_ID}-json.log链接过来的，查看你的docker配置，—-log-driver需要设置为json-file格式，默认的可能是journald。&lt;/p&gt;
&lt;p&gt;查看当前的–log-driver:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker version
Client:
 Version:         1.12.6
 API version:     1.24
 Package version: docker-1.12.6-32.git88a4867.el7.centos.x86_64
 Go version:      go1.7.4
 Git commit:      88a4867/1.12.6
 Built:           Mon Jul  3 16:02:02 2017
 OS/Arch:         linux/amd64

Server:
 Version:         1.12.6
 API version:     1.24
 Package version: docker-1.12.6-32.git88a4867.el7.centos.x86_64
 Go version:      go1.7.4
 Git commit:      88a4867/1.12.6
 Built:           Mon Jul  3 16:02:02 2017
 OS/Arch:         linux/amd64   
[root@node1 efk]# docker info |grep &amp;apos;Logging Driver&amp;apos;
 WARNING: Usage of loopback devices is strongly discouraged for production use. Use `--storage-opt dm.thinpooldev` to specify a custom block storage device.
WARNING: bridge-nf-call-ip6tables is disabled
Logging Driver: journald
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;修改当前版本docker的–log-driver:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# vim /etc/sysconfig/docker
OPTIONS=&amp;apos;--selinux-enabled --log-driver=json-file --signature-verification=false&amp;apos;
[root@node1 efk]# systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【注意】：本来修改这个参数应该在在/etc/docker/daemon.json文件中添加&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
  &amp;quot;log-driver&amp;quot;: &amp;quot;json-file&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;但是在该版本中，–log-driver是在文件/etc/sysconfig/docker中定义的。&lt;br&gt;在docker-ce版本中，默认的–log-driver是json-file。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;遇到的问题：&lt;/strong&gt;&lt;br&gt;由于之前在/etc/docker/daemon.json中配置–log-driver，重启导致docker程序启动失败，等到后来在/etc/sysconfig/docker配置文件中配置好后，启动docker却发现当前node变成NotReady状态，所有的Pod也变为Unknown状态。查看kubelet状态，发现kubelet程序已经挂掉了。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# kubectl get nodes
NAME           STATUS     AGE       VERSION
172.16.7.151   NotReady   28d       v1.6.0
172.16.7.152   Ready      28d       v1.6.0
172.16.7.153   Ready      28d       v1.6.0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;启动kubelet：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# systemctl start kubelet
[root@node1 ~]# kubectl get nodes
NAME           STATUS    AGE       VERSION
172.16.7.151   Ready     28d       v1.6.0
172.16.7.152   Ready     28d       v1.6.0
172.16.7.153   Ready     28d       v1.6.0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;浏览器再次访问 kibana URL：&lt;a href=&quot;http://172.16.7.151:8086/api/v1/proxy/namespaces/kube-system/services/kibana-logging&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.151:8086/api/v1/proxy/namespaces/kube-system/services/kibana-logging&lt;/a&gt;&lt;br&gt;此时就会发现有Create按钮了。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/17.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;在 Settings -&amp;gt; Indices 页面创建一个 index（相当于 mysql 中的一个 database），去掉已经勾选的 &lt;strong&gt;Index contains time-based events&lt;/strong&gt;，使用默认的 &lt;strong&gt;logstash-*&lt;/strong&gt; pattern，点击 Create ;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/18.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;创建Index后，可以在 Discover 下看到 ElasticSearch logging 中汇聚的日志。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/19.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;EFK介绍&quot;&gt;&lt;a href=&quot;#EFK介绍&quot; class=&quot;headerlink&quot; title=&quot;EFK介绍&quot;&gt;&lt;/a&gt;EFK介绍&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Logstash(或者Fluentd)负责收集日志 &lt;/li&gt;
&lt;li&gt;Elasticsearch存储日志并提供搜索 &lt;/li&gt;
&lt;li&gt;Kibana负责日志查询和展示&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;官方地址：&lt;a href=&quot;https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="容器编排" scheme="http://yoursite.com/categories/%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92/"/>
    
    
      <category term="Kubernetes" scheme="http://yoursite.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes架构</title>
    <link href="http://yoursite.com/2017/10/09/Kubernetes%E6%9E%B6%E6%9E%84/"/>
    <id>http://yoursite.com/2017/10/09/Kubernetes架构/</id>
    <published>2017-10-09T06:49:31.000Z</published>
    <updated>2017-11-13T07:39:34.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;Kubernetes整体架构&quot;&gt;&lt;a href=&quot;#Kubernetes整体架构&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes整体架构&quot;&gt;&lt;/a&gt;Kubernetes整体架构&lt;/h2&gt;&lt;p&gt;Kubernetes的整体架构如下图：&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/14.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;Kubernetes主要由以下几个核心组件组成:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;etcd保存了整个集群的状态; &lt;/li&gt;
&lt;li&gt;apiserver提供了资源操作的唯一入口,并提供认证、授权、访问控制、API注册和 发现等机制;&lt;/li&gt;
&lt;li&gt;controller manager负责维护集群的状态,比如故障检测、自动扩展、滚动更新等; - scheduler负责资源的调度,按照预定的调度策略将Pod调度到相应的机器上;&lt;/li&gt;
&lt;li&gt;kubelet负责维护容器的生命周期,同时也负责Volume(CVI)和网络(CNI)的管 理;&lt;/li&gt;
&lt;li&gt;Container runtime负责镜像管理以及Pod和容器的真正运行(CRI); &lt;/li&gt;
&lt;li&gt;kube-proxy负责为Service提供cluster内部的服务发现和负载均衡;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;除了核心组件,还有一些推荐的Add-ons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kube-dns负责为整个集群提供DNS服务 &lt;/li&gt;
&lt;li&gt;Ingress Controller为服务提供外网入口 &lt;/li&gt;
&lt;li&gt;Heapster提供资源监控 &lt;/li&gt;
&lt;li&gt;Dashboard提供GUI&lt;/li&gt;
&lt;li&gt;Federation提供跨可用区的集群 &lt;/li&gt;
&lt;li&gt;Fluentd-elasticsearch提供集群日志采集、存储与查询&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Kubernetes总体包含两种角色，一个是Master节点，负责集群调度、对外接口、访问控制、对象的生命周期维护等工作；另一个是Node节点，负责维护容器的生命周期，例如创建、删除、停止Docker容器，负责容器的服务抽象和负载均衡等工作。其中Master节点上，运行着三个核心组件：API Server, Scheduler, Controller Mananger。Node节点上运行两个核心组件：Kubelet， Kube-Proxy。API Server提供Kubernetes集群访问的统一接口，Scheduler, Controller Manager, Kubelet, Kube-Proxy等组件都通过API Server进行通信，API Server将Pod, Service, Replication Controller, Daemonset等对象存储在ETCD集群中。ETCD是CoreOS开发的高效、稳定的强一致性Key-Value数据库，ETCD本身可以搭建成集群对外服务，它负责存储Kubernetes所有对象的生命周期，是Kubernetes的最核心的组件。&lt;/p&gt;
&lt;h2 id=&quot;核心组件介绍&quot;&gt;&lt;a href=&quot;#核心组件介绍&quot; class=&quot;headerlink&quot; title=&quot;核心组件介绍&quot;&gt;&lt;/a&gt;核心组件介绍&lt;/h2&gt;&lt;p&gt;下面先大概介绍一下Kubernetes的核心组件的功能：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;API Server: 提供了资源对象的唯一操作入口，其他所有的组件都必须通过它提供的API来操作资源对象。它以RESTful风格的API对外提供接口。所有Kubernetes资源对象的生命周期维护都是通过调用API Server的接口来完成，例如，用户通过kubectl创建一个Pod，即是通过调用API Server的接口创建一个Pod对象，并储存在ETCD集群中。&lt;/li&gt;
&lt;li&gt;Controller Manager: 集群内部的管理控制中心，主要目的是实现Kubernetes集群的故障检测和自动恢复等工作。它包含两个核心组件：Node Controller和Replication Controller。其中Node Controller负责计算节点的加入和退出，可以通过Node Controller实现计算节点的扩容和缩容。Replication Controller用于Kubernetes资源对象RC的管理，应用的扩容、缩容以及滚动升级都是有Replication Controller来实现。&lt;/li&gt;
&lt;li&gt;Scheduler: 集群中的调度器，负责Pod在集群的中的调度和分配。&lt;/li&gt;
&lt;li&gt;Kubelet: 负责本Node节点上的Pod的创建、修改、监控、删除等Pod的全生命周期管理，Kubelet实时向API Server发送所在计算节点（Node）的信息。&lt;/li&gt;
&lt;li&gt;Kube-Proxy: 实现Service的抽象，为一组Pod抽象的服务（Service）提供统一接口并提供负载均衡功能。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;核心原理介绍&quot;&gt;&lt;a href=&quot;#核心原理介绍&quot; class=&quot;headerlink&quot; title=&quot;核心原理介绍&quot;&gt;&lt;/a&gt;核心原理介绍&lt;/h2&gt;&lt;h3 id=&quot;API-Server&quot;&gt;&lt;a href=&quot;#API-Server&quot; class=&quot;headerlink&quot; title=&quot;API Server&quot;&gt;&lt;/a&gt;API Server&lt;/h3&gt;&lt;p&gt;1.如何访问Kubernetes API，Kubernetes API通过一个kube-apiserver的进程提供服务，该进程运行在Kubernetes Master节点上，默认的情况下，监听两个端口：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本地端口: 默认值为8080，用于接收HTTP请求，非认证授权的HTTP请求通过该端口访问API Server。&lt;/li&gt;
&lt;li&gt;安全端口：默认值为6443，用于接收HTTPS请求，用于基于Token文件或者客户端证书及HTTP Base的认证，用于基于策略的授权，Kubernetes默认情况下不启动HTTPS安全访问机制。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;用户可以通过编程方式访问API接口，也可以通过curl命令来直接访问它，例如，我们在Master节点上访问API Server：&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;19&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;20&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;21&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;22&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;23&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;24&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;25&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;26&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;27&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;28&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;29&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;30&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;31&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;32&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;33&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;34&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;35&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;36&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;37&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;38&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;39&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;40&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;41&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;42&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;43&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;44&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;45&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;46&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;47&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;48&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;[root@node1 ~]# curl http://172.16.7.151:8080/ap/&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  &amp;quot;paths&amp;quot;: [&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/api&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/api/v1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/apps&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/apps/v1beta1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/authentication.k8s.io&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/authentication.k8s.io/v1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/authentication.k8s.io/v1beta1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/authorization.k8s.io&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/authorization.k8s.io/v1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/authorization.k8s.io/v1beta1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/autoscaling&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/autoscaling/v1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/autoscaling/v2alpha1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/batch&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/batch/v1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/batch/v2alpha1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/certificates.k8s.io&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/certificates.k8s.io/v1beta1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/extensions&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/extensions/v1beta1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/policy&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/policy/v1beta1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/rbac.authorization.k8s.io&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/rbac.authorization.k8s.io/v1alpha1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/rbac.authorization.k8s.io/v1beta1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/settings.k8s.io&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/settings.k8s.io/v1alpha1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/storage.k8s.io&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/storage.k8s.io/v1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/apis/storage.k8s.io/v1beta1&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/healthz&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/healthz/ping&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/healthz/poststarthook/bootstrap-controller&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/healthz/poststarthook/ca-registration&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/healthz/poststarthook/extensions/third-party-resources&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/healthz/poststarthook/rbac/bootstrap-roles&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/logs&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/metrics&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/swagger-ui/&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/swaggerapi/&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/ui/&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;/version&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  ]&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;Kubernetes还提供了一个代理程序——Kubectl Proxy，它既能作为API Server的反向代理，也能作为普通客户端访问API Server，使用方法如下：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;# kubectl proxy --port=9090 &amp;amp;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;# curl http://172.16.7.151:9090/api&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  &amp;quot;kind&amp;quot;: &amp;quot;APIVersions&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  &amp;quot;versions&amp;quot;: [&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;quot;v1&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  ],&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  &amp;quot;serverAddressByClientCIDRs&amp;quot;: [&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      &amp;quot;clientCIDR&amp;quot;: &amp;quot;0.0.0.0/0&amp;quot;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      &amp;quot;serverAddress&amp;quot;: &amp;quot;172.16.7.151:6443&amp;quot;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  ]&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;2.集群功能模块之间的通信&lt;br&gt;API Server是整个集群的核心，负责集群各个模块之间的通信。集群内部的功能模块通过API Server将信息存入ETCD，其他模块通过API Server读取这些信息，从而实现各模块之间的信息交互。比如，Node节点上的Kubelet每个一个时间周期，通过API Server报告自身状态，API Server接收这些信息后，将节点状态信息保存到ETCd中。Controller Manager中的Node Controller通过API Server定期读取这些节点状态信息，并做相应处理。Scheduler监听到某个Pod创建的信息后，检索所有符合该Pod要求的节点列表，并将Pod绑定到节点李彪中最符合要求的节点上：如果Scheduler监听到某个Pod被删除，则删除本节点上的相应Pod实例。&lt;br&gt;从上面的通信过程可以看出，API Server的访问压力很大，这也是限制（制约）Kubernetes集群规模的关键，缓解API Server的压力可以通过缓存来实现，通过watch/list操作，将资源对象的信息缓存到本地，这种方法在一定程度上缓解了API Server的压力，但是不是最好的解决办法。&lt;/p&gt;
&lt;h3 id=&quot;Controller-Manager&quot;&gt;&lt;a href=&quot;#Controller-Manager&quot; class=&quot;headerlink&quot; title=&quot;Controller Manager&quot;&gt;&lt;/a&gt;Controller Manager&lt;/h3&gt;&lt;p&gt;Controller Manager作为集群的内部管理控制中心，负责集群内的Node，Pod，RC，服务端点（Endpoint），命名空间（Namespace），服务账号（ServiceAccount）、资源配额（ResourceQuota）等的管理并执行自动化修复流程，确保集群出处于预期的工作状态，比如，RC实现自动控制Pod活跃副本数，如果Pod出错退出，RC自动创建一个新的Pod，来保持活跃的Pod的个数。&lt;br&gt;Controller Manager包含Replication Controller、Node Controller、ResourceQuota Controller、Namespace Controller、ServiceAccount Controller、Token Controller、Server Controller以及Endpoint Controller等多个控制器，Controller Manager是这些Controller的管理者。&lt;/p&gt;
&lt;h3 id=&quot;Scheduler&quot;&gt;&lt;a href=&quot;#Scheduler&quot; class=&quot;headerlink&quot; title=&quot;Scheduler&quot;&gt;&lt;/a&gt;Scheduler&lt;/h3&gt;&lt;p&gt;Kubernetes Scheduler负责Pod的调度管理，它负责将要创建的Pod按照一定的规则分配在某个适合的Node上。&lt;br&gt;Scheduler的默认调度流程分为以下两步：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;预选调度过程，即遍历所有目标Node，筛选出符合要求的候选节点。为此，Kubernetes内置了多种预选策略供用户选择。&lt;/li&gt;
&lt;li&gt;确定最优节点，在第一步的基础上，采用优选策略为每个候选节点打分，分值最高的胜出。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Scheduler的调度流程是通过插件方式加载“调度算法提供者”具体实现的，一个调度算法提供者其实就是包括了一组预选策略与一组有限选择策略的结构体，注册算法插件的函数如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;func RegisterAlgorithmProvider(name string, predicateKeys, priorityKeys util.StringSet)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;它包含3个参数：“name string”参数为算法名，“predicateKeys”为为算法用到的预选策略集合，”priorityKeys”为算法用到的优选策略集合。&lt;br&gt;Scheduler中可用的预算策略包含：NoDiskConflict,  PodFitResources, PodSelectorMatches,  PodFitHost,  CheckNodeLabelPresence,  CheckServiceAffinity和PodFitsPorts策略等。其默认的AlgorithmProvider加载的预选策略Predicates包括：PodFitsPorts,  PodFitsResources,  NoDiskConflict,  MatchNodeSelector和HostName，即每个节点只有通过前面的五个默认预选策略后，才能初步被选中，进入下一个流程。&lt;/p&gt;
&lt;h3 id=&quot;kubelet&quot;&gt;&lt;a href=&quot;#kubelet&quot; class=&quot;headerlink&quot; title=&quot;kubelet&quot;&gt;&lt;/a&gt;kubelet&lt;/h3&gt;&lt;p&gt;在Kubernetes集群中，每个计算节点（Node）上会运行一个守护进程：Kubelet。它用于处理Master节点下发到本节点的任务，管理Pod以及Pod中的容器。每个Kubelet进程会在API Server上注册自身节点的信息，定期向API Server汇报节点资源的使用情况，并通过cAdvise监控容器和节点资源。&lt;br&gt;Kubelet主要功能：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;节点管理：kubelet可以自动向API Server注册自己，它可以采集所在计算节点的资源信息和使用情况并提交给API Server，通过启动/停止kubelet进程来实现计算节点的扩容、缩容。&lt;/li&gt;
&lt;li&gt;Pod管理：kubelet通过API Server监听ETCD目录，同步Pod清单，当发现有新的Pod绑定到所在的节点，则按照Pod清单的要求创建改清单。如果发现本地的Pod被删除，则kubelet通过docker client删除该容器。&lt;/li&gt;
&lt;li&gt;健康检查：Pod通过两类探针来检查容器的健康状态。一个是LivenessProbe探针，用于判断容器是否健康，如果LivenessProbe探针探测到容器不健康，则kubelet将删除该容器，并根据容器的重启策略做相应的处理。另一类是ReadnessProbe探针，用于判断容器是否启动完成，且准备接受请求，如果ReadnessProbe探针检测到失败，则Pod的状态被修改。Enpoint Controller将从Service的Endpoint中删除包含该容器的IP地址的Endpoint条目。kubelet定期调用LivenessProbe探针来诊断容器的健康状况，它目前支持三种探测：HTTP的方式发送GET请求; TCP方式执行Connect目的端口; Exec的方式，执行一个脚本。&lt;/li&gt;
&lt;li&gt;cAdvisor资源监控: 在Kubernetes集群中，应用程序的执行情况可以在不同的级别上检测到，这些级别包含Container，Pod，Service和整个集群。作为Kubernetes集群的一部分，Kubernetes希望提供给用户各个级别的资源使用信息，这将使用户能够更加深入地了解应用的执行情况，并找到可能的瓶颈。Heapster项目为Kubernetes提供了一个基本的监控平台，他是集群级别的监控和事件数据集成器。Heapster通过收集所有节点的资源使用情况，将监控信息实时推送至一个可配置的后端，用于存储和可视化展示。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;kube-proxy&quot;&gt;&lt;a href=&quot;#kube-proxy&quot; class=&quot;headerlink&quot; title=&quot;kube-proxy&quot;&gt;&lt;/a&gt;kube-proxy&lt;/h3&gt;&lt;p&gt;每台机器上都运行一个kube-proxy服务,它监听API server中service和endpoint的变化 情况,并通过iptables等来为服务配置负载均衡(仅支持TCP和UDP)。&lt;br&gt;kube-proxy可以直接运行在物理机上,也可以以static pod或者daemonset的方式运行。 kube-proxy当前支持以下几种实现：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;userspace:最早的负载均衡方案,它在用户空间监听一个端口,所有服务通过 iptables转发到这个端口,然后在其内部负载均衡到实际的Pod。该方式最主要的问 题是效率低,有明显的性能瓶颈。 &lt;/li&gt;
&lt;li&gt;iptables:目前推荐的方案,完全以iptables规则的方式来实现service负载均衡。该 方式最主要的问题是在服务多的时候产生太多的iptables规则(社区有人提到过几万 条),大规模下也有性能问题 &lt;/li&gt;
&lt;li&gt;winuserspace:同userspace,但仅工作在windows上&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;另外,基于ipvs的方案正在讨论中,大规模情况下可以大幅提升性 能,比如slide里面提供的示例将服务延迟从小时缩短到毫秒级。&lt;br&gt;kube-proxy目前仅支持TCP和UDP,不支持HTTP路由,并且也没有健康检查机制。这 些可以通过自定义Ingress Controller的方法来解决。&lt;/p&gt;
&lt;h3 id=&quot;kube-dns&quot;&gt;&lt;a href=&quot;#kube-dns&quot; class=&quot;headerlink&quot; title=&quot;kube-dns&quot;&gt;&lt;/a&gt;kube-dns&lt;/h3&gt;&lt;p&gt;kube-dns为Kubernetes集群提供命名服务,一般通过addon的方式部署,从v1.3版本开 始,成为了一个内建的自启动服务。&lt;/p&gt;
&lt;h2 id=&quot;Kubernetes应用部署模型&quot;&gt;&lt;a href=&quot;#Kubernetes应用部署模型&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes应用部署模型&quot;&gt;&lt;/a&gt;Kubernetes应用部署模型&lt;/h2&gt;&lt;p&gt;主要包括Pod、Replication controller、Label和Service。&lt;/p&gt;
&lt;h3 id=&quot;Pod&quot;&gt;&lt;a href=&quot;#Pod&quot; class=&quot;headerlink&quot; title=&quot;Pod&quot;&gt;&lt;/a&gt;Pod&lt;/h3&gt;&lt;p&gt;Kubernetes的最小部署单元是Pod而不是容器。作为First class API公民，Pods能被创建，调度和管理。简单地来说，像一个豌豆荚中的豌豆一样，一个Pod中的应用容器同享同一个上下文：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;PID 名字空间。但是在docker中不支持&lt;/li&gt;
&lt;li&gt;网络名字空间，在同一Pod中的多个容器访问同一个IP和端口空间。&lt;/li&gt;
&lt;li&gt;IPC名字空间，同一个Pod中的应用能够使用SystemV IPC和POSIX消息队列进行通信。&lt;/li&gt;
&lt;li&gt;UTS名字空间，同一个Pod中的应用共享一个主机名。&lt;/li&gt;
&lt;li&gt;Pod中的各个容器应用还可以访问Pod级别定义的共享卷。&lt;br&gt;从生命周期来说，Pod应该是短暂的而不是长久的应用。 Pods被调度到节点，保持在这个节点上直到被销毁。当节点死亡时，分配到这个节点的Pods将会被删掉。将来可能会实现Pod的迁移特性。在实际使用时，我们一般不直接创建Pods, 我们通过replication controller来负责Pods的创建，复制，监控和销毁。一个Pod可以包括多个容器，他们直接往往相互协作完成一个应用功能。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;Replication-controller和ReplicaSet&quot;&gt;&lt;a href=&quot;#Replication-controller和ReplicaSet&quot; class=&quot;headerlink&quot; title=&quot;Replication controller和ReplicaSet&quot;&gt;&lt;/a&gt;Replication controller和ReplicaSet&lt;/h3&gt;&lt;p&gt;复制控制器确保Pod的一定数量的份数(replica)在运行。如果超过这个数量，控制器会杀死一些，如果少了，控制器会启动一些。控制器也会在节点失效、维护的时候来保证这个数量。所以强烈建议即使我们的份数是1，也要使用复制控制器，而不是直接创建Pod。&lt;br&gt;在生命周期上讲，复制控制器自己不会终止，但是跨度不会比Service强。Service能够横跨多个复制控制器管理的Pods。而且在一个Service的生命周期内，复制控制器能被删除和创建。Service和客户端程序是不知道复制控制器的存在的。&lt;br&gt;复制控制器创建的Pods应该是可以互相替换的和语义上相同的，这个对无状态服务特别合适。&lt;br&gt;在新版本的Kubernetes中建议使用ReplicaSet(也简称为rs)来取代 ReplicationController。ReplicaSet跟ReplicationController没有本质的不同,只是名字不 一样,并且ReplicaSet支持集合式的selector(ReplicationController仅支持等式)。&lt;br&gt;虽然也ReplicaSet可以独立使用,但建议使用 Deployment 来自动管理ReplicaSet,这 样就无需担心跟其他机制的不兼容问题(比如ReplicaSet不支持rolling-update但 Deployment支持),并且还支持版本记录、回滚、暂停升级等高级特性。&lt;br&gt;Pod是临时性的对象，被创建和销毁，而且不会恢复。复制器动态地创建和销毁Pod。虽然Pod会分配到IP地址，但是这个IP地址都不是持久的。这样就产生了一个疑问：外部如何消费Pod提供的服务呢？&lt;/p&gt;
&lt;h3 id=&quot;Service&quot;&gt;&lt;a href=&quot;#Service&quot; class=&quot;headerlink&quot; title=&quot;Service&quot;&gt;&lt;/a&gt;Service&lt;/h3&gt;&lt;p&gt;Service定义了一个Pod的逻辑集合和访问这个集合的策略。集合是通过定义Service时提供的Label选择器完成的。举个例子，我们假定有3个Pod的备份来完成一个图像处理的后端。这些后端备份逻辑上是相同的，前端不关心哪个后端在给它提供服务。虽然组成这个后端的实际Pod可能变化，前端客户端不会意识到这个变化，也不会跟踪后端。Service就是用来实现这种分离的抽象。&lt;br&gt;对于Service，我们还可以定义Endpoint，Endpoint把Service和Pod动态地连接起来。&lt;/p&gt;
&lt;h4 id=&quot;Service-Cluster-IP和-kuber-proxy&quot;&gt;&lt;a href=&quot;#Service-Cluster-IP和-kuber-proxy&quot; class=&quot;headerlink&quot; title=&quot;Service Cluster IP和 kuber proxy&quot;&gt;&lt;/a&gt;Service Cluster IP和 kuber proxy&lt;/h4&gt;&lt;p&gt;每个代理节点都运行了一个kube-proxy进程。这个进程从服务进程那边拿到Service和Endpoint对象的变化。 对每一个Service, 它在本地打开一个端口。 到这个端口的任意连接都会代理到后端Pod集合中的一个Pod IP和端口。在创建了服务后，服务Endpoint模型会体现后端Pod的 IP和端口列表，kube-proxy就是从这个endpoint维护的列表中选择服务后端的。另外Service对象的sessionAffinity属性也会帮助kube-proxy来选择哪个具体的后端。缺省情况下，后端Pod的选择是随机的。可以设置service.spec.sessionAffinity 成”ClientIP”来指定同一个ClientIP的流量代理到同一个后端。在实现上，kube-proxy会用IPtables规则把访问Service的Cluster IP和端口的流量重定向到这个本地端口。下面的部分会讲什么是service的Cluster IP。&lt;br&gt;注意：在0.18以前的版本中Cluster IP叫PortalNet IP。&lt;/p&gt;
&lt;h4 id=&quot;Pod-IP-and-Service-Cluster-IP&quot;&gt;&lt;a href=&quot;#Pod-IP-and-Service-Cluster-IP&quot; class=&quot;headerlink&quot; title=&quot;Pod IP and Service Cluster IP&quot;&gt;&lt;/a&gt;Pod IP and Service Cluster IP&lt;/h4&gt;&lt;p&gt;Pod IP 地址是实际存在于某个网卡(可以是虚拟设备)上的，但Service Cluster IP就不一样了，没有网络设备为这个地址负责。它是由kube-proxy使用Iptables规则重新定向到其本地端口，再均衡到后端Pod的。我们前面说的Service环境变量和DNS都使用Service的Cluster IP和端口。&lt;br&gt;就拿上面我们提到的图像处理程序为例。当我们的Service被创建时，Kubernetes给它分配一个地址10.0.0.1。这个地址从我们启动API的service-cluster-ip-range参数(旧版本为portal_net参数)指定的地址池中分配，比如–service-cluster-ip-range=10.0.0.0/16。假设这个Service的端口是1234。集群内的所有kube-proxy都会注意到这个Service。当proxy发现一个新的service后，它会在本地节点打开一个任意端口，建相应的iptables规则，重定向服务的IP和port到这个新建的端口，开始接受到达这个服务的连接。&lt;br&gt;当一个客户端访问这个service时，这些iptable规则就开始起作用，客户端的流量被重定向到kube-proxy为这个service打开的端口上，kube-proxy随机选择一个后端pod来服务客户。这个流程如下图所示：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/15.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;根据Kubernetes的网络模型，使用Service Cluster IP和Port访问Service的客户端可以坐落在任意代理节点上。外部要访问Service，我们就需要给Service外部访问IP。&lt;/p&gt;
&lt;h4 id=&quot;外部访问Service&quot;&gt;&lt;a href=&quot;#外部访问Service&quot; class=&quot;headerlink&quot; title=&quot;外部访问Service&quot;&gt;&lt;/a&gt;外部访问Service&lt;/h4&gt;&lt;p&gt;Service对象在Cluster IP range池中分配到的IP只能在内部访问，如果服务作为一个应用程序内部的层次，还是很合适的。如果这个Service作为前端服务，准备为集群外的客户提供业务，我们就需要给这个服务提供公共IP了。&lt;br&gt;外部访问者是访问集群代理节点的访问者。为这些访问者提供服务，我们可以在定义Service时指定其spec.publicIPs，一般情况下publicIP 是代理节点的物理IP地址。和先前的Cluster IP range上分配到的虚拟的IP一样，kube-proxy同样会为这些publicIP提供Iptables 重定向规则，把流量转发到后端的Pod上。有了publicIP，我们就可以使用load balancer等常用的互联网技术来组织外部对服务的访问了。&lt;br&gt;spec.publicIPs在新的版本中标记为过时了，代替它的是spec.type=NodePort，这个类型的service，系统会给它在集群的各个代理节点上分配一个节点级别的端口，能访问到代理节点的客户端都能访问这个端口，从而访问到服务。&lt;/p&gt;
&lt;h3 id=&quot;Label和Label-selector&quot;&gt;&lt;a href=&quot;#Label和Label-selector&quot; class=&quot;headerlink&quot; title=&quot;Label和Label selector&quot;&gt;&lt;/a&gt;Label和Label selector&lt;/h3&gt;&lt;p&gt;Label标签在Kubernetes模型中占着非常重要的作用。Label表现为key/value对，附加到Kubernetes管理的对象上，典型的就是Pods。它们定义了这些对象的识别属性，用来组织和选择这些对象。Label可以在对象创建时附加在对象上，也可以对象存在时通过API管理对象的Label。&lt;br&gt;在定义了对象的Label后，其它模型可以用Label 选择器（selector)来定义其作用的对象。&lt;br&gt;Label选择器有两种，分别是Equality-based和Set-based。&lt;br&gt;比如如下Equality-based选择器样例：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;environment = production
tier != frontend
environment = production，tier != frontend
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;对于上面的选择器，第一条匹配Label具有environment key且等于production的对象，第二条匹配具有tier key，但是值不等于frontend的对象。由于kubernetes使用AND逻辑，第三条匹配production但不是frontend的对象。&lt;br&gt;Set-based选择器样例：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;environment in (production, qa)
tier notin (frontend, backend)
partition
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;第一条选择具有environment key，而且值是production或者qa的label附加的对象。第二条选择具有tier key，但是其值不是frontend和backend。第三条选则具有partition key的对象，不对value进行校验。&lt;br&gt;replication controller复制控制器和Service都用label和label selctor来动态地配备作用对象。复制控制器在定义的时候就指定了其要创建Pod的Label和自己要匹配这个Pod的selector， API服务器应该校验这个定义。我们可以动态地修改replication controller创建的Pod的Label用于调式，数据恢复等。一旦某个Pod由于Label改变从replication controller移出来后，replication controller会马上启动一个新的Pod来确保复制池子中的份数。对于Service，Label selector可以用来选择一个Service的后端Pods。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Kubernetes整体架构&quot;&gt;&lt;a href=&quot;#Kubernetes整体架构&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes整体架构&quot;&gt;&lt;/a&gt;Kubernetes整体架构&lt;/h2&gt;&lt;p&gt;Kubernetes的整体架构如下图：&lt;br&gt;
    
    </summary>
    
      <category term="容器编排" scheme="http://yoursite.com/categories/%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92/"/>
    
    
      <category term="Kubernetes" scheme="http://yoursite.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Jenkins + Docker 持续集成</title>
    <link href="http://yoursite.com/2017/09/30/Jenkins-Docker-%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/"/>
    <id>http://yoursite.com/2017/09/30/Jenkins-Docker-持续集成/</id>
    <published>2017-09-30T08:56:51.000Z</published>
    <updated>2017-11-14T02:17:53.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;Jenkins介绍&quot;&gt;&lt;a href=&quot;#Jenkins介绍&quot; class=&quot;headerlink&quot; title=&quot;Jenkins介绍&quot;&gt;&lt;/a&gt;Jenkins介绍&lt;/h2&gt;&lt;p&gt;Jenkins是一个开源软件项目，是基于Java开发的一种持续集成工具，用于监控持续重复的工作，旨在提供一个开放易用的软件平台，使软件的持续集成变成可能。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;安装部署Jenkins&quot;&gt;&lt;a href=&quot;#安装部署Jenkins&quot; class=&quot;headerlink&quot; title=&quot;安装部署Jenkins&quot;&gt;&lt;/a&gt;安装部署Jenkins&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://jenkins.io/download/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://jenkins.io/download/&lt;/a&gt;&lt;br&gt;我这里下载war包安装，版本：1.642.3 LTS .war&lt;/p&gt;
&lt;h3 id=&quot;环境信息&quot;&gt;&lt;a href=&quot;#环境信息&quot; class=&quot;headerlink&quot; title=&quot;环境信息&quot;&gt;&lt;/a&gt;环境信息&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;操作系统版本&lt;/th&gt;
&lt;th&gt;IP地址&lt;/th&gt;
&lt;th&gt;安装软件&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;osb30&lt;/td&gt;
&lt;td&gt;Redhat 6.5&lt;/td&gt;
&lt;td&gt;172.16.206.30&lt;/td&gt;
&lt;td&gt;jenkins&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&quot;新建Jenkins用户&quot;&gt;&lt;a href=&quot;#新建Jenkins用户&quot; class=&quot;headerlink&quot; title=&quot;新建Jenkins用户&quot;&gt;&lt;/a&gt;新建Jenkins用户&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@osb30 ~]# groupadd jenkins
[root@osb30 ~]# useradd -g jenkins jenkins
[root@osb30 ~]# id jenkins
uid=501(jenkins) gid=501(jenkins) groups=501(jenkins)
[root@osb30 ~]# echo &amp;quot;wisedu&amp;quot; | passwd --stdin jenkins &amp;amp;&amp;gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;Jenkins安装方式&quot;&gt;&lt;a href=&quot;#Jenkins安装方式&quot; class=&quot;headerlink&quot; title=&quot;Jenkins安装方式&quot;&gt;&lt;/a&gt;Jenkins安装方式&lt;/h3&gt;&lt;p&gt;安装jenkins有两种方式，tomcat方式部署和java部署启动。本次实验我以tomcat下部署启动为例。&lt;/p&gt;
&lt;h4 id=&quot;tomcat方式部署&quot;&gt;&lt;a href=&quot;#tomcat方式部署&quot; class=&quot;headerlink&quot; title=&quot;tomcat方式部署&quot;&gt;&lt;/a&gt;tomcat方式部署&lt;/h4&gt;&lt;p&gt;1.首先安装tomcat和JAVA，配置环境变量（此步骤不再讲述，java配置不可缺少）&lt;br&gt;我这里安装的是jdk 1.8.0_65。&lt;/p&gt;
&lt;p&gt;2.将从官网下载下来的jenkins.war文件放入tomcat下的webapps目录下，进入tomcat的/bin目录下，启动tomcat即启动jenkins。&lt;br&gt;我这里用的是tomcat8。&lt;/p&gt;
&lt;p&gt;3.启动jenkins时，会自动在webapps目录下建立jenkins目录，访问地址为：&lt;a href=&quot;http://localhost:8080/jenkins&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://localhost:8080/jenkins&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[jenkins@osb30 ~]$ tar zxf apache-tomcat-8.0.30.tar.gz
[jenkins@osb30 ~]$ mv jenkins.war apache-tomcat-8.0.30/webapps/
[jenkins@osb30 ~]$ cd apache-tomcat-8.0.30
[jenkins@osb30 apache-tomcat-8.0.30]$ bin/startup.sh
Jenkins home directory: /home/jenkins/.jenkins found at: $user.home/.jenkins
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果启动时报错：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Caused by:java.awt.AWTError: Can&amp;apos;t connect to X11 window server using &amp;apos;:0&amp;apos; as the value of the DISPLAY varible...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;解决：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[jenkins@osb30 ~]$ cd apache-tomcat-8.0.30/bin/
[jenkins@osb30 bin]$ vim catalina.sh 
JAVA_OPTS=&amp;quot;-Xms1024m -Xmx1024m -Djava.awt.headless=true&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;4.访问jenkins&lt;br&gt;&lt;a href=&quot;http://172.16.206.30:8080/jenkins&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.206.30:8080/jenkins&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;java部署启动jenkins&quot;&gt;&lt;a href=&quot;#java部署启动jenkins&quot; class=&quot;headerlink&quot; title=&quot;java部署启动jenkins&quot;&gt;&lt;/a&gt;java部署启动jenkins&lt;/h4&gt;&lt;p&gt;切换到jenkins.war存放的目录，输入如下命令：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ java -jar jenkins.war   
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可以修改启动端口&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ java -jar jenkins.war --httpPort=8000
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后在浏览器中（推荐用火狐、chrome）输入&lt;a href=&quot;http://localhost:8080，localhost可以是本机的ip，也可以是计算机名。就可以打开jenkins；修改端口后，访问地址的端口需同步变更。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://localhost:8080，localhost可以是本机的ip，也可以是计算机名。就可以打开jenkins；修改端口后，访问地址的端口需同步变更。&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;Jenkins授权和访问控制&quot;&gt;&lt;a href=&quot;#Jenkins授权和访问控制&quot; class=&quot;headerlink&quot; title=&quot;Jenkins授权和访问控制&quot;&gt;&lt;/a&gt;Jenkins授权和访问控制&lt;/h3&gt;&lt;p&gt;默认地Jenkins不包含任何的安全检查，任何人可以修改Jenkins设置，job和启动build等。显然地在大规模的公司需要多个部门一起协调工作的时候，没有任何安全检查会带来很多的问题。 我们可以通过下面的方式来增强Jenkins的安全：&lt;br&gt;访问jenkins：&lt;a href=&quot;http://172.16.206.30:8080/jenkins，&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.206.30:8080/jenkins，&lt;/a&gt;&lt;br&gt;点击系统管理—&amp;gt; Configure Global Security，点击”启用安全”，可以看到可以使用多种方式来增强Jenkins的授权和访问控制：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/44.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/45.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;如上图所示，默认是”任何用户可以做任何事情(没有任何限制)”。&lt;br&gt;我们在”安全域”选择”Jenkins专有用户数据库”，”允许用户注册”；并先在“授权策略”点击“任何用户可以做任何事情(没有任何限制)”， 防止注册之后无法再管理jenkins。此时就可以刷新一下jenkins的页面看到右上角有登录、注册的按钮。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/46.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h4 id=&quot;注册管理员账号&quot;&gt;&lt;a href=&quot;#注册管理员账号&quot; class=&quot;headerlink&quot; title=&quot;注册管理员账号&quot;&gt;&lt;/a&gt;注册管理员账号&lt;/h4&gt;&lt;p&gt;1.点击注册，首先注册一个管理员账号。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/47.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;2.点击系统管理—&amp;gt; Configure Global Security，在“授权策略”选择”安全矩阵”，添加用户/组——添加admin账户——为admin账户添加所有权限，为匿名用户勾选你希望对方了解的功能。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/48.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;【注意】：匿名用户一定要开启此处的可读权限，若不开启，后面github或者bitbucket的webhook自动构建会没有权限。&lt;br&gt;并且勾选上该项，点击保存。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/49.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;做完此部操作之后，即可用admin帐号登录，取消登录用户可以做任何事的权限。&lt;br&gt;以上操作，即可完成jenkins的授权和访问控制。&lt;/p&gt;
&lt;h3 id=&quot;Jenkins系统配置&quot;&gt;&lt;a href=&quot;#Jenkins系统配置&quot; class=&quot;headerlink&quot; title=&quot;Jenkins系统配置&quot;&gt;&lt;/a&gt;Jenkins系统配置&lt;/h3&gt;&lt;p&gt;登录jenkins——系统管理——系统设置，为jenkins添加上需要的功能配置，有如下几个方面：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/50.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h4 id=&quot;jdk版本&quot;&gt;&lt;a href=&quot;#jdk版本&quot; class=&quot;headerlink&quot; title=&quot;jdk版本&quot;&gt;&lt;/a&gt;jdk版本&lt;/h4&gt;&lt;p&gt;在jdk的选项，点击”新增JDK”，取消自动安装，输入jdk别名（名称随意），JAVA_HOME大家应该都很了解，在此处填写jenkins所在服务器安装的java程序的HOME位置即可，根据不同操作系统填写不同路径，如win7 D:\Java\jdk1.8   linux /usr/lib/jvm/jdk1.7.0_51。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/51.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;设置完了请记得保存。&lt;/p&gt;
&lt;h4 id=&quot;git-svn版本控制添加&quot;&gt;&lt;a href=&quot;#git-svn版本控制添加&quot; class=&quot;headerlink&quot; title=&quot;git/svn版本控制添加&quot;&gt;&lt;/a&gt;git/svn版本控制添加&lt;/h4&gt;&lt;p&gt;根据使用的版本选择控制版本的应用程序的路径，如jdk配置即可。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/52.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;【注意】:如果使用Git作为版本控制库，Jenkins默认情况下是没有安装Git的。我们需要到插件管理界面中选中Git，然后点击直接安装。&lt;br&gt;点击系统管理—&amp;gt;管理插件—&amp;gt;可选插件，在右上角”过滤”处输入git进行搜索：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/53.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;找到Git client plugin和Git plugin，在前面打上√，点击直接安装。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/54.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/55.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;安装成功后，重启jenkins。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[jenkins@osb30 ~]$ cd apache-tomcat-8.0.30
[jenkins@osb30 apache-tomcat-8.0.30]$ bin/shutdown.sh
[jenkins@osb30 apache-tomcat-8.0.30]$ bin/startup.sh ;tail -f logs/catalina.out
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;Jenkins添加maven配置&quot;&gt;&lt;a href=&quot;#Jenkins添加maven配置&quot; class=&quot;headerlink&quot; title=&quot;Jenkins添加maven配置&quot;&gt;&lt;/a&gt;Jenkins添加maven配置&lt;/h4&gt;&lt;p&gt;先判断jenkins所在主机是否安装了maven：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# mvn –version
-bash: mvn: command not found
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果没有安装，请先安装maven。&lt;/p&gt;
&lt;h5 id=&quot;CentOS-安装maven&quot;&gt;&lt;a href=&quot;#CentOS-安装maven&quot; class=&quot;headerlink&quot; title=&quot;CentOS 安装maven&quot;&gt;&lt;/a&gt;CentOS 安装maven&lt;/h5&gt;&lt;pre&gt;&lt;code&gt;[root@osb30 ~]# cd /usr/local/
[root@osb30 local]# wget http://apache.opencas.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz

[root@osb30 local]# tar zxf apache-maven-3.3.9-bin.tar.gz
[root@osb30 local]# ln -s apache-maven-3.3.9 maven
[root@osb30 local]# vim /etc/profile
# 添加如下配置：
# Maven configuration.
MAVEN_HOME=/usr/local/maven
export PATH=$MAVEN_HOME/bin:$PATH
[root@osb30 local]# source /etc/profile

[root@osb30 local]# mvn -version
Apache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-11T00:41:47+08:00)
Maven home: /usr/local/maven
Java version: 1.8.0_65, vendor: Oracle Corporation
Java home: /usr/java/jdk1.8.0_65/jre
Default locale: en_US, platform encoding: UTF-8
OS name: &amp;quot;linux&amp;quot;, version: &amp;quot;2.6.32-431.el6.x86_64&amp;quot;, arch: &amp;quot;amd64&amp;quot;, family: &amp;quot;unix&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;h5 id=&quot;Jenkins配置maven&quot;&gt;&lt;a href=&quot;#Jenkins配置maven&quot; class=&quot;headerlink&quot; title=&quot;Jenkins配置maven&quot;&gt;&lt;/a&gt;Jenkins配置maven&lt;/h5&gt;&lt;p&gt;安装完成后，登录jenkins。点击系统管理—&amp;gt;系统设置。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/56.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;Jenkins构建maven风格的job&quot;&gt;&lt;a href=&quot;#Jenkins构建maven风格的job&quot; class=&quot;headerlink&quot; title=&quot;Jenkins构建maven风格的job&quot;&gt;&lt;/a&gt;Jenkins构建maven风格的job&lt;/h2&gt;&lt;h3 id=&quot;新建maven任务&quot;&gt;&lt;a href=&quot;#新建maven任务&quot; class=&quot;headerlink&quot; title=&quot;新建maven任务&quot;&gt;&lt;/a&gt;新建maven任务&lt;/h3&gt;&lt;p&gt;登录jenkins，点击新建。输入Item名称，选择“构建一个maven项目”，点击OK。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/57.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;构建任务配置&quot;&gt;&lt;a href=&quot;#构建任务配置&quot; class=&quot;headerlink&quot; title=&quot;构建任务配置&quot;&gt;&lt;/a&gt;构建任务配置&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/58.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;源码管理配置&quot;&gt;&lt;a href=&quot;#源码管理配置&quot; class=&quot;headerlink&quot; title=&quot;源码管理配置&quot;&gt;&lt;/a&gt;源码管理配置&lt;/h3&gt;&lt;p&gt;进入配置页面，找到”源码管理”。我这里是svn，输入项目所在版本库的地址。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/59.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;构建触发器配置&quot;&gt;&lt;a href=&quot;#构建触发器配置&quot; class=&quot;headerlink&quot; title=&quot;构建触发器配置&quot;&gt;&lt;/a&gt;构建触发器配置&lt;/h3&gt;&lt;p&gt;在”源码管理”下面是”构建触发器”。&lt;br&gt;”构建触发器”是一个持续集成的触发器插件，可以根据已经完成构建的结果，触发新Job或者传递参数。默认的选项是Build whenever a SNAPSHOT dependency is built，意思是依赖于快照的构建，意思是依赖于快照的构建，当代码有更新时就构建项目。&lt;br&gt;Build periodically和Poll SCM可以设置定时自动构建。两者区别如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Poll SCM：定时检查源码变更（根据SCM软件的版本号），如果有更新就checkout最新code下来，然后执行构建动作。&lt;/li&gt;
&lt;li&gt;Build periodically：定时进行项目构建（它不care源码是否发生变化）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我这里设置为每12小时构建一次。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/60.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;Maven构建设置&quot;&gt;&lt;a href=&quot;#Maven构建设置&quot; class=&quot;headerlink&quot; title=&quot;Maven构建设置&quot;&gt;&lt;/a&gt;Maven构建设置&lt;/h3&gt;&lt;h4 id=&quot;Pre-Step&quot;&gt;&lt;a href=&quot;#Pre-Step&quot; class=&quot;headerlink&quot; title=&quot;Pre Step&quot;&gt;&lt;/a&gt;Pre Step&lt;/h4&gt;&lt;p&gt;Pre Steps选项用来配置构建前的工作，这里不作更改。&lt;/p&gt;
&lt;h4 id=&quot;配置Root-POM和Goals-and-options&quot;&gt;&lt;a href=&quot;#配置Root-POM和Goals-and-options&quot; class=&quot;headerlink&quot; title=&quot;配置Root POM和Goals and options&quot;&gt;&lt;/a&gt;配置Root POM和Goals and options&lt;/h4&gt;&lt;p&gt;因为是Maven项目，所以Build选项有Root POM和Goals and options的设置。Root POM:填写你项目的pom.xml文件的位置，注意：是相对位置，如果该文件不存在，会有红色字提示。&lt;br&gt;比如我这里是：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/61.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h4 id=&quot;Post-Steps&quot;&gt;&lt;a href=&quot;#Post-Steps&quot; class=&quot;headerlink&quot; title=&quot;Post Steps&quot;&gt;&lt;/a&gt;Post Steps&lt;/h4&gt;&lt;p&gt;在maven项目创建完成后，我们还需要实现每次构建完成，将war发布到阿里云主机上，以实现自动发布。我们通过添加shell实现自动发布。&lt;/p&gt;
&lt;p&gt;找到Post steps下有个Execute shell：&lt;br&gt;【注意】:Jenkins在执行该shell脚本的时候是以jenkins这个用户身份去执行。某些场景下请注意环境变量PATH。&lt;br&gt;将构建完成后，所要采取的动作，shell脚本脚本内容如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#!/bin/bash

# Stop tomcat.
ssh root@114.55.29.246 &amp;apos;/usr/local/apache-tomcat-7.0.65/bin/shutdown.sh&amp;apos; &amp;amp;&amp;gt;/dev/null
sleep 10

# Check the stop is successful or not.
if ssh root@114.55.29.246 &amp;apos;ps -ef|grep tomcat |grep -v &amp;quot;grep&amp;quot;&amp;apos; &amp;amp;&amp;gt;/dev/null; then
  echo &amp;quot;Tomcat stop failed.Please check the problem.&amp;quot;
  exit 5
fi

# Backup previous version and delete the war in the path /usr/local/apache-tomcat-7.0.65/webapps/.
ssh root@114.55.29.246 &amp;apos;/usr/bin/cp -f /usr/local/apache-tomcat-7.0.65/webapps/*.war /backups/*war&amp;apos;
ssh root@114.55.29.246 &amp;apos;rm -rf /usr/local/apache-tomcat-7.0.65/webapps/*&amp;apos;

# Copy the newest war to aliyun ECS.
scp /home/jenkins/.jenkins/workspace/godseye/godseye-parent/godseye-container/target/godseye-container-aliyun.war root@114.55.29.246:/usr/local/apache-tomcat-7.0.65/webapps/godseye.war &amp;amp;&amp;gt;/dev/null

# Start the tomcat.
ssh root@114.55.29.246 &amp;apos;/usr/local/apache-tomcat-7.0.65/bin/startup.sh&amp;apos; &amp;amp;&amp;gt;/dev/null
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;配置阿里云主机信任内网的这台jenkins主机：&lt;br&gt;由于是war包在内网服务器上，发布的环境是在阿里云主机上，所以要配置主机互信，防止scp war包时还需要输入密码。我这里内网服务器ip是172.16.206.30，外网是114.55.29.246。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[jenkins@osb30 ~]$ ssh-keygen -t rsa -f .ssh/id_rsa
[jenkins@osb30 ~]$ ssh-copy-id -i .ssh/id_rsa.pub root@114.55.29.246
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;Jenkins邮件通知设置&quot;&gt;&lt;a href=&quot;#Jenkins邮件通知设置&quot; class=&quot;headerlink&quot; title=&quot;Jenkins邮件通知设置&quot;&gt;&lt;/a&gt;Jenkins邮件通知设置&lt;/h2&gt;&lt;h3 id=&quot;配置jenkins自带的邮件功能&quot;&gt;&lt;a href=&quot;#配置jenkins自带的邮件功能&quot; class=&quot;headerlink&quot; title=&quot;配置jenkins自带的邮件功能&quot;&gt;&lt;/a&gt;配置jenkins自带的邮件功能&lt;/h3&gt;&lt;p&gt;1.找到系统设置&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/62.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;2.填写系统管理员邮箱&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/63.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;3.找到邮件通知，输入SMTP服务器地址，点击高级，输入发件人帐号和密码&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/64.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;4.勾选上”通过发送测试邮件测试配置”，然后输入收件人帐号&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/65.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;此时我们已经可以发送邮件了。在具体job配置处，找到”构建设置”，输入收件人信箱，但是你会发现只能在构建失败时发邮件。可以安装插件Email Extension Plugin来自定义。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/66.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;安装使用插件Email-Extension-Plugin&quot;&gt;&lt;a href=&quot;#安装使用插件Email-Extension-Plugin&quot; class=&quot;headerlink&quot; title=&quot;安装使用插件Email Extension Plugin&quot;&gt;&lt;/a&gt;安装使用插件Email Extension Plugin&lt;/h3&gt;&lt;p&gt;1.安装插件Email Extension Plugin&lt;br&gt;该插件支持jenkins 1.5以上的版本。&lt;br&gt;在系统管理-插件管理-安装Email Extension Plugin。它可根据构建的结果，发送构建报告。该插件支持jenkins 1.5以上的版本。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/67.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;【注意】:安装完如果使用Email Extension Plugin，就可以弃用自带的那个邮件功能了。&lt;/p&gt;
&lt;p&gt;2.配置使用插件Email Extension Plugin&lt;br&gt;点击”系统配置”—&amp;gt;”系统设置”。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/68.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;找到Extended E-mail Notification处，输入如下的配置：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/69.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/70.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;构建通知：$PROJECT_NAME - Build # $BUILD_NUMBER - $BUILD_STATUS!

&amp;lt;hr/&amp;gt;
(本邮件是程序自动下发的，请勿回复！)&amp;lt;br/&amp;gt;&amp;lt;hr/&amp;gt;
项目名称：$PROJECT_NAME&amp;lt;br/&amp;gt;&amp;lt;hr/&amp;gt;
构建编号：$BUILD_NUMBER&amp;lt;br/&amp;gt;&amp;lt;hr/&amp;gt;
svn版本号：${SVN_REVISION}&amp;lt;br/&amp;gt;&amp;lt;hr/&amp;gt;
构建状态：$BUILD_STATUS&amp;lt;br/&amp;gt;&amp;lt;hr/&amp;gt;
触发原因：${CAUSE}&amp;lt;br/&amp;gt;&amp;lt;hr/&amp;gt;
构建日志地址：&amp;lt;a href=&amp;quot;${BUILD_URL}console&amp;quot;&amp;gt;${BUILD_URL}console&amp;lt;/a&amp;gt;&amp;lt;br/&amp;gt;&amp;lt;hr/&amp;gt;
构建地址：&amp;lt;a href=&amp;quot;$BUILD_URL&amp;quot;&amp;gt;$BUILD_URL&amp;lt;/a&amp;gt;&amp;lt;br/&amp;gt;&amp;lt;hr/&amp;gt;
变更集:${JELLY_SCRIPT,template=&amp;quot;html&amp;quot;}&amp;lt;br/&amp;gt;&amp;lt;hr/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;点击下面的保存。&lt;br&gt;然后去job配置页面激活这个插件。找到需要发邮件的项目，点击进去。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/71.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;点击配置，点击”增加构建后操作步骤”，选择Editable Email Notification。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/72.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;附上构建日志，点击高级设置。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/73.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;配置Triggers：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/74.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;更详细的介绍：&lt;a href=&quot;http://www.cnblogs.com/zz0412/p/jenkins_jj_01.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://www.cnblogs.com/zz0412/p/jenkins_jj_01.html&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;sonar&quot;&gt;&lt;a href=&quot;#sonar&quot; class=&quot;headerlink&quot; title=&quot;sonar&quot;&gt;&lt;/a&gt;sonar&lt;/h2&gt;&lt;p&gt;官方文档：&lt;a href=&quot;http://docs.sonarqube.org/display/SONARQUBE45/Documentation&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://docs.sonarqube.org/display/SONARQUBE45/Documentation&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;sonar简介&quot;&gt;&lt;a href=&quot;#sonar简介&quot; class=&quot;headerlink&quot; title=&quot;sonar简介&quot;&gt;&lt;/a&gt;sonar简介&lt;/h3&gt;&lt;p&gt;Sonar是一个用于代码质量管理的开源平台，用于管理Java源代码的质量。通过插件机制，Sonar 可以集成不同的测试工具，代码分析工具，以及持续集成工具，比如pmd-cpd、checkstyle、findbugs、Jenkins。通过不同的插件对这些结果进行再加工处理，通过量化的方式度量代码质量的变化，从而可以方便地对不同规模和种类的工程进行代码质量管理。&lt;br&gt;与持续集成工具（例如 Hudson/Jenkins 等）不同，Sonar 并不是简单地把不同的代码检查工具结果（例如 FindBugs，PMD 等）直接显示在 Web 页面上，而是通过不同的插件对这些结果进行再加工处理，通过量化的方式度量代码质量的变化，从而可以方便地对不同规模和种类的工程进行代码质量管理。&lt;br&gt;在对其他工具的支持方面，Sonar 不仅提供了对 IDE 的支持，可以在 Eclipse 和 IntelliJ IDEA 这些工具里联机查看结果；同时 Sonar 还对大量的持续集成工具提供了接口支持，可以很方便地在持续集成中使用 Sonar。&lt;br&gt;此外，Sonar 的插件还可以对 Java 以外的其他编程语言提供支持，对国际化以及报告文档化也有良好的支持。&lt;/p&gt;
&lt;h3 id=&quot;环境要求&quot;&gt;&lt;a href=&quot;#环境要求&quot; class=&quot;headerlink&quot; title=&quot;环境要求&quot;&gt;&lt;/a&gt;环境要求&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;http://docs.sonarqube.org/display/SONAR/Requirements&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://docs.sonarqube.org/display/SONAR/Requirements&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;新建用户&quot;&gt;&lt;a href=&quot;#新建用户&quot; class=&quot;headerlink&quot; title=&quot;新建用户&quot;&gt;&lt;/a&gt;新建用户&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@osb30 ~]# groupadd sonar
[root@osb30 ~]# useradd -g sonar sonar
[root@osb30 ~]# id sonar
uid=502(sonar) gid=502(sonar) groups=502(sonar)
[root@osb30 ~]# echo &amp;quot;wisedu&amp;quot; | passwd --stdin sonar &amp;amp;&amp;gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;安装jdk&quot;&gt;&lt;a href=&quot;#安装jdk&quot; class=&quot;headerlink&quot; title=&quot;安装jdk&quot;&gt;&lt;/a&gt;安装jdk&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[sonar@osb30 ~]$ java -version
java version &amp;quot;1.8.0_65&amp;quot;
Java(TM) SE Runtime Environment (build 1.8.0_65-b17)
Java HotSpot(TM) 64-Bit Server VM (build 25.65-b01, mixed mode)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;安装配置数据库&quot;&gt;&lt;a href=&quot;#安装配置数据库&quot; class=&quot;headerlink&quot; title=&quot;安装配置数据库&quot;&gt;&lt;/a&gt;安装配置数据库&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@osb30 ~]# mysql -uroot –p
mysql&amp;gt; CREATE DATABASE sonar CHARACTER SET utf8 COLLATE utf8_general_ci;
mysql&amp;gt; CREATE USER &amp;apos;sonar&amp;apos; IDENTIFIED BY &amp;apos;sonar&amp;apos;;
mysql&amp;gt; GRANT ALL ON sonar.* TO &amp;apos;sonar&amp;apos;@&amp;apos;%&amp;apos; IDENTIFIED BY &amp;apos;wisedu&amp;apos;;
mysql&amp;gt; GRANT ALL ON sonar.* TO &amp;apos;sonar&amp;apos;@&amp;apos;localhost&amp;apos; IDENTIFIED BY &amp;apos;wisedu&amp;apos;;
mysql&amp;gt; FLUSH PRIVILEGES;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;安装sonar&quot;&gt;&lt;a href=&quot;#安装sonar&quot; class=&quot;headerlink&quot; title=&quot;安装sonar&quot;&gt;&lt;/a&gt;安装sonar&lt;/h3&gt;&lt;p&gt;我这里用的版本是SonarQube 4.5.7 (LTS *)，上传该软件到sonar用户的家目录下。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[sonar@osb30 ~]$ unzip -oq sonarqube-4.5.7.zip
[sonar@osb30 ~]$ vim sonarqube-4.5.7/conf/sonar.properties
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;修改如下字段(就是配置数据库信息，其他不用动)：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sonar.jdbc.username:                       sonar
sonar.jdbc.password:                       wisedu
sonar.jdbc.url:                            jdbc:mysql://localhost:3306/sonar?useUnicode=true&amp;amp;characterEncoding=utf8&amp;amp;rewriteBatchedStatements=true

# Optional properties
sonar.jdbc.driverClassName:                com.mysql.jdbc.Driver
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;启动sonar&quot;&gt;&lt;a href=&quot;#启动sonar&quot; class=&quot;headerlink&quot; title=&quot;启动sonar&quot;&gt;&lt;/a&gt;启动sonar&lt;/h3&gt;&lt;p&gt;Sonar默认集成了jetty容器，可以直接启动提供服务，也可以通过脚本构建为war包，部署在tomcat容器中。&lt;br&gt;Sonar默认的端口是”9000”、默认的上下文路径是”/”、默认的网络接口是”0.0.0.0”，默认的管理员帐号和密码为:admin/admin，这些参数都可以在配置文件sonar.properties中修改。我这里修改下port，因为本机的9000端口被其他程序占用了。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[sonar@osb30 ~]$ vim sonarqube-4.5.7/conf/sonar.properties
sonar.web.port=9003
[sonar@osb30 ~]$ sonarqube-4.5.7/bin/linux-x86-64/sonar.sh start
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查看日志：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[sonar@osb30 ~]$ tail -f sonarqube-4.5.7/logs/sonar.log
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可以看到第一次启动时，初始化语句：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/75.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;关闭sonar&quot;&gt;&lt;a href=&quot;#关闭sonar&quot; class=&quot;headerlink&quot; title=&quot;关闭sonar&quot;&gt;&lt;/a&gt;关闭sonar&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[sonar@osb30 ~]$ sonarqube-4.5.7/bin/linux-x86-64/sonar.sh stop
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;访问sonar&quot;&gt;&lt;a href=&quot;#访问sonar&quot; class=&quot;headerlink&quot; title=&quot;访问sonar&quot;&gt;&lt;/a&gt;访问sonar&lt;/h3&gt;&lt;p&gt;浏览器输入&lt;a href=&quot;http://172.16.206.30:9003/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.206.30:9003/&lt;/a&gt;&lt;br&gt;默认的管理员帐号和密码为:admin/admin。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/76.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/77.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/78.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;sonar插件&quot;&gt;&lt;a href=&quot;#sonar插件&quot; class=&quot;headerlink&quot; title=&quot;sonar插件&quot;&gt;&lt;/a&gt;sonar插件&lt;/h3&gt;&lt;p&gt;Sonar支持多种插件，插件的下载地址为：&lt;a href=&quot;http://docs.codehaus.org/display/SONAR/Plugin+Library&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://docs.codehaus.org/display/SONAR/Plugin+Library&lt;/a&gt;&lt;br&gt;将下载后的插件上传到${SONAR_HOME}extensions\plugins目录下，重新启动sonar。&lt;/p&gt;
&lt;p&gt;sonar默认集成了Java Ecosystem插件，该插件是一组插件的合集:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Java [sonar-java-plugin]：java源代码解析，计算指标等&lt;/li&gt;
&lt;li&gt;Squid [sonar-squid-java-plugin]：检查违反Sonar定义规则的代码&lt;/li&gt;
&lt;li&gt;Checkstyle [sonar-checkstyle-plugin]：使用CheckStyle检查违反统一代码编写风格的代码&lt;/li&gt;
&lt;li&gt;FindBugs [sonar-findbugs-plugin]：使用FindBugs检查违反规则的缺陷代码&lt;/li&gt;
&lt;li&gt;PMD [sonar-pmd-plugin]：使用pmd检查违反规则的代码&lt;/li&gt;
&lt;li&gt;Surefire [sonar-surefire-plugin]：使用Surefire执行单元测试&lt;/li&gt;
&lt;li&gt;Cobertura [sonar-cobertura-plugin]：使用Cobertura获取代码覆盖率&lt;/li&gt;
&lt;li&gt;JaCoCo [sonar-jacoco-plugin]：使用JaCOCO获取代码覆盖率&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;与jenkins集成&quot;&gt;&lt;a href=&quot;#与jenkins集成&quot; class=&quot;headerlink&quot; title=&quot;与jenkins集成&quot;&gt;&lt;/a&gt;与jenkins集成&lt;/h3&gt;&lt;p&gt;可以通过maven集成，也可以直接与jenkins集成。我这里选择直接与jenkins集成。&lt;/p&gt;
&lt;h4 id=&quot;通过maven集成&quot;&gt;&lt;a href=&quot;#通过maven集成&quot; class=&quot;headerlink&quot; title=&quot;通过maven集成&quot;&gt;&lt;/a&gt;通过maven集成&lt;/h4&gt;&lt;p&gt;修改maven的主配置文件（${MAVEN_HOME}/conf/settings.xml文件或者 ~/.m2/settings.xml文件），在其中增加访问Sonar数据库及Sonar服务地址，添加如下配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;profile&amp;gt;
&amp;lt;id&amp;gt;sonar&amp;lt;/id&amp;gt;
&amp;lt;properties&amp;gt;
    &amp;lt;sonar.jdbc.url&amp;gt;jdbc:mysql://localhost:3306/sonar&amp;lt;/sonar.jdbc.url&amp;gt;
    &amp;lt;sonar.jdbc.driver&amp;gt;com.mysql.jdbc.Driver&amp;lt;/sonar.jdbc.driver&amp;gt;
    &amp;lt;sonar.jdbc.username&amp;gt;sonar&amp;lt;/sonar.jdbc.username&amp;gt;
    &amp;lt;sonar.jdbc.password&amp;gt;sonar&amp;lt;/sonar.jdbc.password&amp;gt;
    &amp;lt;sonar.host.url&amp;gt;http://localhost:9003&amp;lt;/sonar.host.url&amp;gt; &amp;lt;!-- Sonar服务器访问地址 --&amp;gt;
&amp;lt;/properties&amp;gt;
&amp;lt;/profile&amp;gt;

&amp;lt;activeProfiles&amp;gt;
  &amp;lt;activeProfile&amp;gt;sonar&amp;lt;/activeProfile&amp;gt;
&amp;lt;/activeProfiles&amp;gt;

...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这部分内容具体可参照网上&lt;a href=&quot;http://www.cnblogs.com/gao241/p/3190701.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://www.cnblogs.com/gao241/p/3190701.html&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;直接与Jenkins集成&quot;&gt;&lt;a href=&quot;#直接与Jenkins集成&quot; class=&quot;headerlink&quot; title=&quot;直接与Jenkins集成&quot;&gt;&lt;/a&gt;直接与Jenkins集成&lt;/h4&gt;&lt;p&gt;在jenkins的插件管理中选择安装sonar jenkins plugin，该插件可以使项目每次构建都调用sonar进行代码度量。&lt;/p&gt;
&lt;p&gt;1.安装插件&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/79.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;2.系统配置添加sonar的配置&lt;br&gt;进入系统配置页面对sonar插件进行配置，如下图：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/80.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/81.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;然后点击下面的保存。&lt;/p&gt;
&lt;p&gt;3.配置构建项目，增加Post Build Action&lt;br&gt;点击要构建的项目，在点击左侧的配置。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/82.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;在页面的最下面找到”构建后操作”，选择SonarQube。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/83.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/84.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;It is no longer recommended to use SonarQube maven builder. It is preferable to set up SonarQube in the build environment and use a standard Jenkins maven target.&lt;br&gt;【解决】：&lt;br&gt;&lt;a href=&quot;http://docs.sonarqube.org/display/SCAN/Analyzing+with+SonarQube+Scanner+for+Jenkins&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://docs.sonarqube.org/display/SCAN/Analyzing+with+SonarQube+Scanner+for+Jenkins&lt;/a&gt;&lt;br&gt;修改Build处：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/85.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;最后去jenkins构建项目，构建完查看sonar控制台：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/86.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;常见问题&quot;&gt;&lt;a href=&quot;#常见问题&quot; class=&quot;headerlink&quot; title=&quot;常见问题&quot;&gt;&lt;/a&gt;常见问题&lt;/h3&gt;&lt;p&gt;Jenkins构建完成后，sonar扫描代码报错：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/87.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;解决：&lt;br&gt;卸载sonar的JavaScript插件。&lt;/p&gt;
&lt;h2 id=&quot;Jenkins与Docker结合&quot;&gt;&lt;a href=&quot;#Jenkins与Docker结合&quot; class=&quot;headerlink&quot; title=&quot;Jenkins与Docker结合&quot;&gt;&lt;/a&gt;Jenkins与Docker结合&lt;/h2&gt;&lt;p&gt;我这里没有使用Docker Pipeline，直接在构建完成后，执行shell脚本，这样更灵活。&lt;/p&gt;
&lt;h3 id=&quot;部署流程&quot;&gt;&lt;a href=&quot;#部署流程&quot; class=&quot;headerlink&quot; title=&quot;部署流程&quot;&gt;&lt;/a&gt;部署流程&lt;/h3&gt;&lt;p&gt;1.研发push到svn代码库&lt;br&gt;2.Jenkins 构建，pull svn代码 使用maven进行编译打包&lt;br&gt;3.打包生成的代码，生成一个新版本的镜像，push到本地docker仓库harbor&lt;br&gt;4.发布，测试机器 pull 新版本的镜像，并删除原来的容器，重新运行新版本镜像。&lt;/p&gt;
&lt;h3 id=&quot;环境说明&quot;&gt;&lt;a href=&quot;#环境说明&quot; class=&quot;headerlink&quot; title=&quot;环境说明&quot;&gt;&lt;/a&gt;环境说明&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;操作系统版本&lt;/th&gt;
&lt;th&gt;IP地址&lt;/th&gt;
&lt;th&gt;用途&lt;/th&gt;
&lt;th&gt;安装软件&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;osb30&lt;/td&gt;
&lt;td&gt;Redhat 6.5&lt;/td&gt;
&lt;td&gt;172.16.206.30&lt;/td&gt;
&lt;td&gt;svn代码库、Jenkins、Docker&lt;/td&gt;
&lt;td&gt;jenkins、svn、Docker 1.7.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;spark32&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.206.32&lt;/td&gt;
&lt;td&gt;本地docker仓库、业务部署测试环境&lt;/td&gt;
&lt;td&gt;harbor、Docker 17.06.1-ce&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&quot;配置&quot;&gt;&lt;a href=&quot;#配置&quot; class=&quot;headerlink&quot; title=&quot;配置&quot;&gt;&lt;/a&gt;配置&lt;/h3&gt;&lt;p&gt;由于在Jenkins机器上docker是使用root用户运行的，而Jenkins是使用普通用户jenkins运行的，所以要先配置下jenkins用户可以使用docker命令。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@osb30 ~]# visudo
jenkins ALL=(root)      NOPASSWD: /usr/bin/docker
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;另外在Jenkins机器上配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Disable &amp;quot;ssh hostname sudo &amp;lt;cmd&amp;gt;&amp;quot;, because it will show the password in clear.
#         You have to run &amp;quot;ssh -t hostname sudo &amp;lt;cmd&amp;gt;&amp;quot;.
#
#Defaults    requiretty
Defaults:jenkins !requiretty
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果不配置这个，在执行下面脚本时，会报错误：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+ cp -f /home/jenkins/.jenkins/workspace/godseyeBranchForNov/godseye-container/target/godseye-container-wisedu.war /home/jenkins/docker-file/godseye_war/godseye.war
+ sudo docker login -u jkzhao -p Wisedu123 -e 01115004@wisedu.com 172.16.206.32
sudo: sorry, you must have a tty to run sudo
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在172.16.206.32机器上配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# visudo
#
#Defaults    requiretty
Defaults:root !requiretty
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;否则在机器172.16.206.32机器上执行脚本时会报错：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[SSH] executing...
sudo: sorry, you must have a tty to run sudo
docker: invalid reference format.
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;安装插件&quot;&gt;&lt;a href=&quot;#安装插件&quot; class=&quot;headerlink&quot; title=&quot;安装插件&quot;&gt;&lt;/a&gt;安装插件&lt;/h3&gt;&lt;p&gt;登录Jenkins，点击“系统管理”，点击“管理插件”，搜索插件“SSH plugin”，进行安装。&lt;br&gt;登录Jenkins，点击“Credentials”，点击“Add domain”。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/88.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/89.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/90.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;点击“系统管理”，“系统配置”，找到“SSH remote hosts”。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/91.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;配置Post-Steps&quot;&gt;&lt;a href=&quot;#配置Post-Steps&quot; class=&quot;headerlink&quot; title=&quot;配置Post Steps&quot;&gt;&lt;/a&gt;配置Post Steps&lt;/h3&gt;&lt;p&gt;项目其他的配置不变，见上面的章节。&lt;br&gt;【注意】：脚本中用到的仓库和认证的账号需要先在harbor新建好。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/92.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Jenkins机器：编译完成后，build生成一个新版本的镜像，push到远程docker仓库

# Variables
JENKINS_WAR_HOME=&amp;apos;/home/jenkins/.jenkins/workspace/godseyeBranchForNov/godseye-container/target&amp;apos;
DOCKERFILE_HOME=&amp;apos;/home/jenkins/docker-file/godseye_war&amp;apos;
HARBOR_IP=&amp;apos;172.16.206.32&amp;apos;
REPOSITORIES=&amp;apos;godseye_war/godseye&amp;apos;
HARBOR_USER=&amp;apos;jkzhao&amp;apos;
HARBOR_USER_PASSWD=&amp;apos;Wisedu123&amp;apos;
HARBOR_USER_EMAIL=&amp;apos;01115004@wisedu.com&amp;apos;

# Copy the newest war to docker-file directory.
\cp -f ${JENKINS_WAR_HOME}/godseye-container-wisedu.war ${DOCKERFILE_HOME}/godseye.war 

# Delete image early version.
sudo docker login -u ${HARBOR_USER} -p ${HARBOR_USER_PASSWD} -e ${HARBOR_USER_EMAIL} ${HARBOR_IP}  
IMAGE_ID=`sudo docker images | grep ${REPOSITORIES} | awk &amp;apos;{print $3}&amp;apos;`
if [ -n &amp;quot;${IMAGE_ID}&amp;quot; ];then
    sudo docker rmi ${IMAGE_ID}
fi

# Build image.
cd ${DOCKERFILE_HOME}
TAG=`date +%Y%m%d-%H%M%S`
sudo docker build -t ${HARBOR_IP}/${REPOSITORIES}:${TAG} . &amp;amp;&amp;gt;/dev/null

# Push to the harbor registry.
sudo docker push ${HARBOR_IP}/${REPOSITORIES}:${TAG} &amp;amp;&amp;gt;/dev/null
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/93.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 拉取镜像，发布
HARBOR_IP=&amp;apos;172.16.206.32&amp;apos;
REPOSITORIES=&amp;apos;godseye_war/godseye&amp;apos;
HARBOR_USER=&amp;apos;jkzhao&amp;apos;
HARBOR_USER_PASSWD=&amp;apos;Wisedu123&amp;apos;

# 登录harbor
#docker login -u ${HARBOR_USER} -p ${HARBOR_USER_PASSWD} ${HARBOR_IP}

# Stop container, and delete the container.
CONTAINER_ID=`docker ps | grep &amp;quot;godseye_web&amp;quot; | awk &amp;apos;{print $1}&amp;apos;`
if [ -n &amp;quot;$CONTAINER_ID&amp;quot; ]; then
    docker stop $CONTAINER_ID
    docker rm $CONTAINER_ID
else #如果容器启动时失败了，就需要docker ps -a才能找到那个容器
    CONTAINER_ID=`docker ps -a | grep &amp;quot;godseye_web&amp;quot; | awk &amp;apos;{print $1}&amp;apos;`
    if [ -n &amp;quot;$CONTAINER_ID&amp;quot; ]; then  # 如果是第一次在这台机器上拉取运行容器，那么docker ps -a也是找不到这个容器的
        docker rm $CONTAINER_ID
    fi
fi

# Delete godseye_web image early version.
IMAGE_ID=`sudo docker images | grep ${REPOSITORIES} | awk &amp;apos;{print $3}&amp;apos;`
if [ -n &amp;quot;${IMAGE_ID}&amp;quot; ];then
    docker rmi ${IMAGE_ID}
fi

# Pull image.
TAG=`curl -s http://${HARBOR_IP}/api/repositories/${REPOSITORIES}/tags | jq &amp;apos;.[-1]&amp;apos; | sed &amp;apos;s/\&amp;quot;//g&amp;apos;` #最后的sed是为了去掉tag前后的双引号
docker pull ${HARBOR_IP}/${REPOSITORIES}:${TAG} &amp;amp;&amp;gt;/dev/null

# Run.
docker run -d --name godseye_web -p 8080:8080 ${HARBOR_IP}/${REPOSITORIES}:${TAG}
&lt;/code&gt;&lt;/pre&gt;</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Jenkins介绍&quot;&gt;&lt;a href=&quot;#Jenkins介绍&quot; class=&quot;headerlink&quot; title=&quot;Jenkins介绍&quot;&gt;&lt;/a&gt;Jenkins介绍&lt;/h2&gt;&lt;p&gt;Jenkins是一个开源软件项目，是基于Java开发的一种持续集成工具，用于监控持续重复的工作，旨在提供一个开放易用的软件平台，使软件的持续集成变成可能。&lt;br&gt;
    
    </summary>
    
      <category term="持续集成" scheme="http://yoursite.com/categories/%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/"/>
    
    
      <category term="Docker" scheme="http://yoursite.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>在Kubernetes上使用Traefik</title>
    <link href="http://yoursite.com/2017/09/25/%E5%9C%A8Kubernetes%E4%B8%8A%E4%BD%BF%E7%94%A8Traefik/"/>
    <id>http://yoursite.com/2017/09/25/在Kubernetes上使用Traefik/</id>
    <published>2017-09-25T03:30:01.000Z</published>
    <updated>2017-12-01T13:22:25.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;Traefik介绍&quot;&gt;&lt;a href=&quot;#Traefik介绍&quot; class=&quot;headerlink&quot; title=&quot;Traefik介绍&quot;&gt;&lt;/a&gt;Traefik介绍&lt;/h2&gt;&lt;p&gt;traefik 是一个前端负载均衡器，对于微服务架构尤其是 kubernetes 等编排工具具有良好的支持；同 nginx 等相比，traefik 能够自动感知后端容器变化，从而实现自动服务发现。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;br&gt;由于微服务架构以及 Docker 技术和 kubernetes 编排工具最近几年才开始逐渐流行，所以一开始的反向代理服务器比如 nginx、apache 并未提供其支持，毕竟他们也不是先知；所以才会出现 Ingress Controller 这种东西来做 kubernetes 和前端负载均衡器如 nginx 之间做衔接；即 Ingress Controller 的存在就是为了能跟 kubernetes 交互，又能写 nginx 配置，还能 reload 它，这是一种折中方案；而 traefik 天生就是提供了对 kubernetes 的支持，也就是说 traefik 本身就能跟 kubernetes API 交互，感知后端变化，因此可以得知: 在使用 traefik 时，Ingress Controller 已经没什么用了，整体架构如下：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/12.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;部署测试用的两个服务&quot;&gt;&lt;a href=&quot;#部署测试用的两个服务&quot; class=&quot;headerlink&quot; title=&quot;部署测试用的两个服务&quot;&gt;&lt;/a&gt;部署测试用的两个服务&lt;/h2&gt;&lt;p&gt;部署两个服务nginx1-7和nginx1-8，后面用Traefik去负载这两个服务：&lt;br&gt;nginx1-7.yaml：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: frontend
spec:
  ports:
    - port: 80
      targetPort: 80
  selector:
    app: nginx1-7
---
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: nginx1-7-deployment
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx1-7
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;nginx1-8.yaml：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: my-nginx
spec:
  ports:
    - port: 80
      targetPort: 80
  selector:
    app: nginx1-8
---
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: nginx1-8-deployment
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx1-8
    spec:
      containers:
      - name: nginx
        image: nginx:1.8
        ports:
        - containerPort: 80
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;运行两个服务：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 nginx_ingress]# kubectl create -f nginx1-7.yaml
service &amp;quot;frontend&amp;quot; created
deployment &amp;quot;nginx1-7-deployment&amp;quot; created
[root@node1 nginx_ingress]# kubectl create -f nginx1-8.yaml
service &amp;quot;my-nginx&amp;quot; created
deployment &amp;quot;nginx1-8-deployment&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;Role-Based-Access-Control-configuration-Kubernetes-1-6-only&quot;&gt;&lt;a href=&quot;#Role-Based-Access-Control-configuration-Kubernetes-1-6-only&quot; class=&quot;headerlink&quot; title=&quot;Role Based Access Control configuration (Kubernetes 1.6+ only)&quot;&gt;&lt;/a&gt;Role Based Access Control configuration (Kubernetes 1.6+ only)&lt;/h2&gt;&lt;p&gt;我这里部署的是1.6.0集群，开启了RBAC，授权需要使用角色和绑定角色。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 traefik]# pwd
/opt/traefik
[root@node1 traefik]# vim ingress-rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ingress
  namespace: kube-system

---

kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: ingress
subjects:
  - kind: ServiceAccount
    name: ingress
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;部署Traefik&quot;&gt;&lt;a href=&quot;#部署Traefik&quot; class=&quot;headerlink&quot; title=&quot;部署Traefik&quot;&gt;&lt;/a&gt;部署Traefik&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;[root@node1 traefik]# pwd
/opt/traefik
[root@node1 traefik]# vim traefik-deploy.yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: traefik-ingress-lb
  namespace: kube-system
  labels:
    k8s-app: traefik-ingress-lb
spec:
  template:
    metadata:
      labels:
        k8s-app: traefik-ingress-lb
        name: traefik-ingress-lb
    spec:
      terminationGracePeriodSeconds: 60
      hostNetwork: true
      restartPolicy: Always
      serviceAccountName: ingress
      containers:
      - image: traefik
        name: traefik-ingress-lb
        resources:
          limits:
            cpu: 200m
            memory: 30Mi
          requests:
            cpu: 100m
            memory: 20Mi
        ports:
        - name: http
          containerPort: 80
          hostPort: 80
        - name: admin
          containerPort: 8580
          hostPort: 8580
        args:
        - --web
        - --web.address=:8580
        - --kubernetes  
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其中 traefik 监听 node 的 80 和 8580 端口，80 提供正常服务，8580 是其自带的 UI 界面，原本默认是 8080，因为环境里端口冲突了，所以这里临时改一下。&lt;br&gt;【注意】：这里用的是Deploy类型，没有限定该pod运行在哪个主机上。&lt;/p&gt;
&lt;h2 id=&quot;部署-Ingress&quot;&gt;&lt;a href=&quot;#部署-Ingress&quot; class=&quot;headerlink&quot; title=&quot;部署 Ingress&quot;&gt;&lt;/a&gt;部署 Ingress&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;[root@node1 traefik]# cat traefik.yaml 
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: traefik-ingress
  namespace: default
spec:
  rules:
  - host: traefik.nginx.io
    http:
      paths:
      - path: /
        backend:
          serviceName: my-nginx
          servicePort: 80
  - host: traefik.frontend.io
    http:
      paths:
      - path: /
        backend:
          serviceName: frontend
          servicePort: 80
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其中的backend中要配置default namespace中启动的service名字，如果你没有配置namespace名字，默认使用default namespace，如果你在其他namespace中创建服务想要暴露到kubernetes集群外部，可以创建新的ingress.yaml文件，同时在文件中指定该namespace，其他配置与上面的文件格式相同。path就是URL地址后的路径，如traefik.frontend.io/path，service将会接受path这个路径，host最好使用service-name.filed1.filed2.domain-name这种类似主机名称的命名方式，方便区分服务。&lt;br&gt;根据实际环境中部署的service的名字和端口自行修改，有新service增加时，修改该文件后可以使用kubectl replace -f traefik.yaml来更新。&lt;/p&gt;
&lt;h2 id=&quot;部署Traefik-UI&quot;&gt;&lt;a href=&quot;#部署Traefik-UI&quot; class=&quot;headerlink&quot; title=&quot;部署Traefik UI&quot;&gt;&lt;/a&gt;部署Traefik UI&lt;/h2&gt;&lt;p&gt;traefik 本身还提供了一套 UI 供我们使用，其同样以 Ingress 方式暴露，只需要创建一下即可。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 traefik]# cat traefik-ui-service.yaml 
apiVersion: v1
kind: Service
metadata:
  name: traefik-web-ui
  namespace: kube-system
spec:
  selector:
    k8s-app: traefik-ingress-lb
  ports:
  - name: web
    port: 80
    targetPort: 8580
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: traefik-web-ui
  namespace: kube-system
spec:
  rules:
  - host: traefik-ui.local
    http:
      paths:
      - path: /
        backend:
          serviceName: traefik-web-ui
          servicePort: web
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;最后一起创建：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 traefik]# kubectl create -f .
serviceaccount &amp;quot;ingress&amp;quot; created
clusterrolebinding &amp;quot;ingress&amp;quot; created
deployment &amp;quot;traefik-ingress-lb&amp;quot; created
service &amp;quot;traefik-web-ui&amp;quot; created
ingress &amp;quot;traefik-web-ui&amp;quot; created
ingress &amp;quot;traefik-ingress&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;访问测试&quot;&gt;&lt;a href=&quot;#访问测试&quot; class=&quot;headerlink&quot; title=&quot;访问测试&quot;&gt;&lt;/a&gt;访问测试&lt;/h2&gt;&lt;p&gt;查看traefik pod被分配到了哪台主机上：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 traefik]# kubectl get pods -n kube-system -l k8s-app=traefik-ingress-lb -o wide                       
NAME                                  READY     STATUS    RESTARTS   AGE       IP             NODE
traefik-ingress-lb-4237248072-1dg9n   1/1       Running   0          2m        172.16.7.152   172.16.7.152
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;浏览器输入&lt;a href=&quot;http://172.16.7.152:8580/，将可以看到dashboard。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.152:8580/，将可以看到dashboard。&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/13.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;左侧黄色部分部分列出的是所有的rule，右侧绿色部分是所有的backend。&lt;/p&gt;
&lt;p&gt;在Kubernetes集群的任意一个节点上执行。假如现在我要访问nginx的”/“路径。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -H Host:traefik.nginx.io http://172.16.7.152/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果需要在kubernetes集群以外访问就需要设置DNS，或者修改本机的hosts文件。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;172.16.7.152 traefik.nginx.io
172.16.7.152 traefik.frontend.io
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;所有访问这些地址的流量都会发送给172.16.7.152这台主机，就是我们启动traefik的主机。&lt;br&gt;Traefik会解析http请求header里的Host参数将流量转发给Ingress配置里的相应service。&lt;br&gt;修改hosts后就就可以在kubernetes集群外访问以上两个service。&lt;/p&gt;
&lt;h2 id=&quot;健康检查&quot;&gt;&lt;a href=&quot;#健康检查&quot; class=&quot;headerlink&quot; title=&quot;健康检查&quot;&gt;&lt;/a&gt;健康检查&lt;/h2&gt;&lt;p&gt;关于健康检查，测试可以使用 kubernetes 的 Liveness Probe 实现，如果 Liveness Probe检查失败，则 traefik 会自动移除该 pod。&lt;br&gt;【示例】：我们定义一个 test-health 的 deployment，健康检查方式是 cat /tmp/health，容器启动 2 分钟后会删掉这个文件，模拟健康检查失败。&lt;br&gt;test-health的deployment：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 traefik]# vim test-health-deploy.yaml
apiVersion: v1
kind: Deployment
apiVersion: extensions/v1beta1
metadata:
  name: test
  namespace: default
  labels:
    test: alpine
spec:
  replicas: 1
  selector:
    matchLabels:
      test: alpine
  template:
    metadata:
      labels:
        test: alpine
        name: test
    spec:
      containers:
      - image: mritd/alpine:3.4
        name: alpine
        resources:
          limits:
            cpu: 200m
            memory: 30Mi
          requests:
            cpu: 100m
            memory: 20Mi
        ports:
        - name: http
          containerPort: 80
        args:
        command:
        - &amp;quot;bash&amp;quot;
        - &amp;quot;-c&amp;quot;
        - &amp;quot;echo ok &amp;gt; /tmp/health;sleep 120;rm -f /tmp/health&amp;quot;
        livenessProbe:
          exec:
            command:
            - cat
            - /tmp/health
          initialDelaySeconds: 20
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;test-health 的 service：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 traefik]# vim test-health-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: test 
  labels:
    name: test
spec:
  ports:
  - port: 8123
    targetPort: 80
  selector:
    name: test
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;test-health的 Ingress：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 traefik]# vim test-health-ingress.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test
spec:
  rules:
  - host: test.com
    http:
      paths:
      - path: /
        backend:
          serviceName: test
          servicePort: 8123
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;全部创建好以后，进入 traefik ui 界面，可以观察到每隔 2 分钟健康检查失败后，kubernetes 重建 pod，同时 traefik 会从后端列表中移除这个 pod。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Traefik介绍&quot;&gt;&lt;a href=&quot;#Traefik介绍&quot; class=&quot;headerlink&quot; title=&quot;Traefik介绍&quot;&gt;&lt;/a&gt;Traefik介绍&lt;/h2&gt;&lt;p&gt;traefik 是一个前端负载均衡器，对于微服务架构尤其是 kubernetes 等编排工具具有良好的支持；同 nginx 等相比，traefik 能够自动感知后端容器变化，从而实现自动服务发现。&lt;br&gt;
    
    </summary>
    
      <category term="容器编排" scheme="http://yoursite.com/categories/%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92/"/>
    
    
      <category term="Kubernetes" scheme="http://yoursite.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes Ingress实战</title>
    <link href="http://yoursite.com/2017/09/24/Kubernetes-Ingress%E5%AE%9E%E6%88%98/"/>
    <id>http://yoursite.com/2017/09/24/Kubernetes-Ingress实战/</id>
    <published>2017-09-24T08:57:22.000Z</published>
    <updated>2017-11-25T12:50:03.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;服务发现与负载均衡&quot;&gt;&lt;a href=&quot;#服务发现与负载均衡&quot; class=&quot;headerlink&quot; title=&quot;服务发现与负载均衡&quot;&gt;&lt;/a&gt;服务发现与负载均衡&lt;/h2&gt;&lt;p&gt;在前面的安装部署kubernetes集群中已经简单用示例来演示了Pod和Service，Kubernetes通过Service资源在Kubernetes集群内针对容器实现了服务发现和负载均衡。而Service就是kubernetes服务发现与负载均衡中的一种。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;br&gt;目前，kubernetes中的负载均衡大致可以分为以下几种机制，每种机制都有其特定的应用场景：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Service：直接用Service提供cluster内部的负载均衡，并借助cloud provider提供的LB提供外部访问&lt;/li&gt;
&lt;li&gt;Ingress Controller：还是用Service提供cluster内部的负载均衡，但是通过自定义LB提供外部访问&lt;/li&gt;
&lt;li&gt;Service Load Balancer：把load balancer直接跑在容器中，实现Bare Metal的Service Load Balancer&lt;/li&gt;
&lt;li&gt;Custom Load Balancer：自定义负载均衡，并替代kube-proxy，一般在物理部署Kubernetes时使用，方便接入公司已有的外部服务&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;Service&quot;&gt;&lt;a href=&quot;#Service&quot; class=&quot;headerlink&quot; title=&quot;Service&quot;&gt;&lt;/a&gt;Service&lt;/h3&gt;&lt;p&gt;Service是对一组提供相同功能的Pods的抽象，并为它们提供一个统一的入口。借助Service，应用可以方便的实现服务发现与负载均衡，并实现应用的零宕机升级。Service通过标签来选取服务后端，一般配合Replication Controller或者Deployment来保证后端容器的正常运行。这些匹配标签的Pod IP和端口列表组成endpoints,由kube-proxy负责将服务IP负载均衡到这些endpoints上。&lt;br&gt;Service有四种类型:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ClusterIP:默认类型,自动分配一个仅cluster内部可以访问的虚拟IP &lt;/li&gt;
&lt;li&gt;NodePort:在ClusterIP基础上为Service在每台机器上绑定一个端口,这样就可以 通过 &lt;nodeip&gt;:NodePort 来访问该服务 &lt;/nodeip&gt;&lt;/li&gt;
&lt;li&gt;LoadBalancer:在NodePort的基础上,借助cloud provider创建一个外部的负载均 衡器,并将请求转发到 &lt;nodeip&gt;:NodePort&lt;/nodeip&gt;&lt;/li&gt;
&lt;li&gt;ExternalName:将服务通过DNS CNAME记录方式转发到指定的域名(通&lt;br&gt;过 spec.externlName 设定)。需要kube-dns版本在1.7以上。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;另外，也可以将已有的服务以Service的形式加入到Kubernetes集群中来，只需要在创建 Service的时候不指定Label selector，而是在Service创建好后手动为其添加endpoint。&lt;/p&gt;
&lt;h3 id=&quot;Ingress和Ingress-Controller简介&quot;&gt;&lt;a href=&quot;#Ingress和Ingress-Controller简介&quot; class=&quot;headerlink&quot; title=&quot;Ingress和Ingress Controller简介&quot;&gt;&lt;/a&gt;Ingress和Ingress Controller简介&lt;/h3&gt;&lt;p&gt;Service虽然解决了服务发现和负载均衡的问题，但它在使用上还是有一些限制，比如&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;只支持4层负载均衡，没有7层功能 &lt;/li&gt;
&lt;li&gt;对外访问的时候，NodePort类型需要在外部搭建额外的负载均衡，而LoadBalancer要求kubernetes必须跑在支持的cloud provider上面。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;Ingress&quot;&gt;&lt;a href=&quot;#Ingress&quot; class=&quot;headerlink&quot; title=&quot;Ingress&quot;&gt;&lt;/a&gt;Ingress&lt;/h4&gt;&lt;p&gt;Ingress就是为了解决这些限制而引入的新资源，主要用来将服务暴露到cluster外面，并 且可以自定义服务的访问策略。比如想要通过负载均衡器实现不同子域名到不同服务的访问:&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;foo.bar.com --|                 |-&amp;gt; foo.bar.com s1:80&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;              | 178.91.123.132  |&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;bar.foo.com --|                 |-&amp;gt; bar.foo.com s2:80&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;可以这样来定义Ingress:&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;apiVersion: extensions/v1beta1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;kind: Ingress&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  name: test&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  rules:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  - host: foo.bar.com&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    http:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      paths:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      - backend:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          serviceName: s1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          servicePort: 80&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  - host: bar.foo.com&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    http:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      paths:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      - backend:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          serviceName: s2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          servicePort: 80&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;【注意】：Ingress本身并不会自动创建负载均衡器，cluster中需要运行一个ingress controller来根据Ingress的定义来管理负载均衡器。目前社区提供了&lt;a href=&quot;https://github.com/kubernetes/ingress/tree/master/controllers&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;nginx和gce的参考实现&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;简单的说，ingress就是从kubernetes集群外访问集群的入口，将用户的URL请求转发到 不同的service上。Ingress相当于nginx、apache等负载均衡方向代理服务器，其中还包括规则定义，即URL的路由信息，路由信息得的刷新由Ingress controller来提供。&lt;/p&gt;
&lt;h4 id=&quot;Ingress-Controller&quot;&gt;&lt;a href=&quot;#Ingress-Controller&quot; class=&quot;headerlink&quot; title=&quot;Ingress Controller&quot;&gt;&lt;/a&gt;Ingress Controller&lt;/h4&gt;&lt;p&gt;Ingress Controller 实质上可以理解为是个监视器，Ingress Controller 通过不断地跟 kubernetes API 打交道，实时的感知后端 service、pod 等变化，比如新增和减少 pod，service 增加与减少等;当得到这些变化信息后，Ingress Controller 再结合下文的 Ingress 生成配置，然后更新反向代理负载均衡器，并刷新其配置，达到服务发现的作用。&lt;/p&gt;
&lt;h2 id=&quot;Ingress实战&quot;&gt;&lt;a href=&quot;#Ingress实战&quot; class=&quot;headerlink&quot; title=&quot;Ingress实战&quot;&gt;&lt;/a&gt;Ingress实战&lt;/h2&gt;&lt;p&gt;在使用Ingress resource之前，有必要先了解下面几件事情。Ingress是beta版本的resource，在kubernetes1.1之前还没有。你需要一个Ingress Controller来实现Ingress，单纯的创建一个Ingress没有任何意义。目前社区提供了&lt;a href=&quot;https://github.com/kubernetes/ingress/tree/master/controllers&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;nginx和gce的参考实现&lt;/a&gt;。当然还有其他实现，&lt;a href=&quot;https://github.com/nginxinc/kubernetes-ingress&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;开源的 NGINX 和 NGINX Plus 开发了相应的 Ingress controller&lt;/a&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GCE/GKE会在master节点上部署一个ingress controller。你可以在一个pod中部署任意个自定义的ingress controller。你必须正确地annotate每个ingress，比如 &lt;a href=&quot;https://github.com/kubernetes/ingress/tree/master/controllers/nginx#running-multiple-ingress-controllers&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;运行多个ingress controller&lt;/a&gt; 和 &lt;a href=&quot;https://github.com/kubernetes/ingress/blob/master/controllers/gce/BETA_LIMITATIONS.md#disabling-glbc&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;关闭glbc&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;在非GCE/GKE的环境中，你需要在pod中部署一个controller。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;使用-NGINX-和-NGINX-Plus-的-Ingress-Controller-进行-Kubernetes-的负载均衡&quot;&gt;&lt;a href=&quot;#使用-NGINX-和-NGINX-Plus-的-Ingress-Controller-进行-Kubernetes-的负载均衡&quot; class=&quot;headerlink&quot; title=&quot;使用 NGINX 和 NGINX Plus 的 Ingress Controller 进行 Kubernetes 的负载均衡&quot;&gt;&lt;/a&gt;使用 NGINX 和 NGINX Plus 的 Ingress Controller 进行 Kubernetes 的负载均衡&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/nginxinc/kubernetes-ingress/tree/master/examples/complete-example&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/nginxinc/kubernetes-ingress/tree/master/examples/complete-example&lt;/a&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;19&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;20&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;21&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;22&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;[root@node1 nginx_ingress]# kubectl create -f nginx-ingress-rbac.yaml&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;[root@node1 nginx_ingress]# kubectl create -f default-server-secret.yaml &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;secret &amp;quot;default-server-secret&amp;quot; created&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;[root@node1 nginx_ingress]# kubectl create -f nginx-ingress-rc.yaml&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;replicationcontroller &amp;quot;nginx-ingress-rc&amp;quot; created&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;[root@node1 nginx_ingress]# kubectl get pods -l app=nginx-ingress -o wide &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;NAME                     READY     STATUS    RESTARTS   AGE       IP            NODE&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;nginx-ingress-rc-rs1vh   1/1       Running   0          37s       172.30.87.4   172.16.7.151&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;# 查看pod日志&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;[root@node1 nginx_ingress]# kubectl logs nginx-ingress-rc-rs1vh&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;I0924 07:27:37.663514       1 main.go:58] Starting NGINX Ingress controller Version 1.0.0&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2017/09/24 07:27:37 [notice] 20#20: signal process started&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;I0924 07:27:37.975349       1 event.go:218] Event(v1.ObjectReference&amp;#123;Kind:&amp;quot;Secret&amp;quot;, Namespace:&amp;quot;default&amp;quot;, Name:&amp;quot;default-server-secret&amp;quot;, UID:&amp;quot;4e2d9567-9f5a-11e7-9acc-005056b7609a&amp;quot;, APIVersion:&amp;quot;v1&amp;quot;, ResourceVersion:&amp;quot;1019701&amp;quot;, FieldPath:&amp;quot;&amp;quot;&amp;#125;): type: &amp;apos;Normal&amp;apos; reason: &amp;apos;Updated&amp;apos; the default server Secret default/default-server-secret was updated&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2017/09/24 07:27:37 [notice] 26#26: signal process started&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2017/09/24 07:27:38 [notice] 30#30: signal process started&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2017/09/24 07:27:38 [notice] 34#34: signal process started&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2017/09/24 07:27:38 [notice] 38#38: signal process started&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;I0924 07:27:38.073475       1 event.go:218] Event(v1.ObjectReference&amp;#123;Kind:&amp;quot;Ingress&amp;quot;, Namespace:&amp;quot;kube-system&amp;quot;, Name:&amp;quot;traefik-web-ui&amp;quot;, UID:&amp;quot;5d604da9-9f61-11e7-9acc-005056b7609a&amp;quot;, APIVersion:&amp;quot;extensions&amp;quot;, ResourceVersion:&amp;quot;1024008&amp;quot;, FieldPath:&amp;quot;&amp;quot;&amp;#125;): type: &amp;apos;Normal&amp;apos; reason: &amp;apos;AddedOrUpdated&amp;apos; Configuration for kube-system/traefik-web-ui was added or updated&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2017/09/24 07:27:38 [notice] 44#44: signal process started&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;I0924 07:27:38.100887       1 event.go:218] Event(v1.ObjectReference&amp;#123;Kind:&amp;quot;Ingress&amp;quot;, Namespace:&amp;quot;default&amp;quot;, Name:&amp;quot;traefik-ingress&amp;quot;, UID:&amp;quot;5d693739-9f61-11e7-9acc-005056b7609a&amp;quot;, APIVersion:&amp;quot;extensions&amp;quot;, ResourceVersion:&amp;quot;1024009&amp;quot;, FieldPath:&amp;quot;&amp;quot;&amp;#125;): type: &amp;apos;Normal&amp;apos; reason: &amp;apos;AddedOrUpdated&amp;apos; Configuration for default/traefik-ingress was added or updated&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;配置需要测试的service&quot;&gt;&lt;a href=&quot;#配置需要测试的service&quot; class=&quot;headerlink&quot; title=&quot;配置需要测试的service&quot;&gt;&lt;/a&gt;配置需要测试的service&lt;/h3&gt;&lt;p&gt;部署两个服务nginx 1.7和nginx 1.8：&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;19&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;20&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;21&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;22&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;23&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;24&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;25&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;26&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;27&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;28&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;nginx1-7.yaml&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;apiVersion: v1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;kind: Service&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  name: frontend&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  ports:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    - port: 80&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      targetPort: 80&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  selector:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    app: nginx1-7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;---&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;apiVersion: apps/v1beta1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;kind: Deployment&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  name: nginx1-7-deployment&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  replicas: 2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  template:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      labels:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        app: nginx1-7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      containers:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      - name: nginx&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        image: nginx:1.7.9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        ports:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        - containerPort: 80&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;19&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;20&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;21&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;22&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;23&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;24&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;25&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;26&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;27&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;28&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;nginx1-8.yaml&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;apiVersion: v1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;kind: Service&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  name: my-nginx&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  ports:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    - port: 80&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      targetPort: 80&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  selector:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    app: nginx1-8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;---&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;apiVersion: apps/v1beta1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;kind: Deployment&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  name: nginx1-8-deployment&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  replicas: 2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  template:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      labels:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        app: nginx1-8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      containers:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      - name: nginx&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        image: nginx:1.8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        ports:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        - containerPort: 80&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;[root@node1 nginx_ingress]# kubectl create -f nginx1-7.yaml&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;service &amp;quot;frontend&amp;quot; created&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;deployment &amp;quot;nginx1-7-deployment&amp;quot; created&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;[root@node1 nginx_ingress]# kubectl create -f nginx1-8.yaml&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;service &amp;quot;my-nginx&amp;quot; created&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;deployment &amp;quot;nginx1-8-deployment&amp;quot; created&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;创建Ingress&quot;&gt;&lt;a href=&quot;#创建Ingress&quot; class=&quot;headerlink&quot; title=&quot;创建Ingress&quot;&gt;&lt;/a&gt;创建Ingress&lt;/h3&gt;&lt;p&gt;假设这两个服务要暴露到集群外部。要创建一个ingress：&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;19&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;[root@node1 nginx_ingress]# vim test-ingress.yaml&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;apiVersion: extensions/v1beta1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;kind: Ingress&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  name: test&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  rules:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  - host: n17.my.com&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    http:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      paths:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      - backend:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          serviceName: nginx1-7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          servicePort: 80&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  - host: n18.my.com&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    http:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      paths:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      - backend:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          serviceName: nginx1-8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;          servicePort: 80&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;[root@node1 nginx_ingress]# kubectl create -f test-ingress.yaml&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;ingress &amp;quot;test&amp;quot; created&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;[root@node1 nginx_ingress]# kubectl get ing&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;NAME              HOSTS                                  ADDRESS   PORTS     AGE&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;test              n17.my.com,n18.my.com                            80        52s&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;打开客户机的/etc/hosts，配置172.16.7.151和n17.my.com,n18.my.com的对应关系，然后在浏览器访问n17.my.com或n18.my.com就可以访问到对应的服务。&lt;br&gt;如果想修改访问规则，修改test-ingress.yaml，使用kubectl replace -f更新就可以了。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;服务发现与负载均衡&quot;&gt;&lt;a href=&quot;#服务发现与负载均衡&quot; class=&quot;headerlink&quot; title=&quot;服务发现与负载均衡&quot;&gt;&lt;/a&gt;服务发现与负载均衡&lt;/h2&gt;&lt;p&gt;在前面的安装部署kubernetes集群中已经简单用示例来演示了Pod和Service，Kubernetes通过Service资源在Kubernetes集群内针对容器实现了服务发现和负载均衡。而Service就是kubernetes服务发现与负载均衡中的一种。&lt;br&gt;
    
    </summary>
    
      <category term="容器编排" scheme="http://yoursite.com/categories/%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92/"/>
    
    
      <category term="Kubernetes" scheme="http://yoursite.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes监控:部署Heapster、InfluxDB和Grafana</title>
    <link href="http://yoursite.com/2017/09/21/Kubernetes%E7%9B%91%E6%8E%A7-%E9%83%A8%E7%BD%B2Heapster%E3%80%81InfluxDB%E5%92%8CGrafana/"/>
    <id>http://yoursite.com/2017/09/21/Kubernetes监控-部署Heapster、InfluxDB和Grafana/</id>
    <published>2017-09-21T05:40:01.000Z</published>
    <updated>2017-11-24T07:23:49.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;Kubernetes-监控方案&quot;&gt;&lt;a href=&quot;#Kubernetes-监控方案&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes 监控方案&quot;&gt;&lt;/a&gt;Kubernetes 监控方案&lt;/h2&gt;&lt;p&gt;可选的方案：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Heapster + InfluxDB + Grafana&lt;/li&gt;
&lt;li&gt;Prometheus + Grafana&lt;/li&gt;
&lt;li&gt;Cadvisor + InfluxDB + Grafana&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本篇文章介绍的是Heapster + InfluxDB + Grafana，kubernetes集群（1.6.0）搭建见前面的文章。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Heapster、InfluxDB和Grafana介绍&quot;&gt;&lt;a href=&quot;#Heapster、InfluxDB和Grafana介绍&quot; class=&quot;headerlink&quot; title=&quot;Heapster、InfluxDB和Grafana介绍&quot;&gt;&lt;/a&gt;Heapster、InfluxDB和Grafana介绍&lt;/h2&gt;&lt;p&gt;开源软件cAdvisor（Container cAdvisor）是用于监控容器运行状态的利器之一（cAdvisor项目的主页为&lt;a href=&quot;https://github.com/cAdvisor），它被用于多个与Docker相关的开源项目中。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/cAdvisor），它被用于多个与Docker相关的开源项目中。&lt;/a&gt;&lt;br&gt;在kubernetes系统中，cAdvisor已经被默认集成到了kubelet组件内，当kubelet服务启动时，它会自动启动cAdvisor服务，然后cAdvisor会实时采集所在节点的性能指标及节点上运行的容器的性能指标。kubelet的启动参数–cadvisor-port可自定义cAdvisor对外提供服务的端口号，默认是4194。&lt;br&gt;cAdvisor提供了web页面可供浏览器访问，例如本kubernetes集群中的一个Node的ip是172.16.7.151，那么浏览器输入&lt;a href=&quot;http://172.16.7.151:4194可以访问cAdvisor的监控页面。cAdvisor主页显示了主机的实时运行状态，包括CPU使用情况、内存使用情况、网络吞吐量及文件系统使用情况等信息。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.151:4194可以访问cAdvisor的监控页面。cAdvisor主页显示了主机的实时运行状态，包括CPU使用情况、内存使用情况、网络吞吐量及文件系统使用情况等信息。&lt;/a&gt;&lt;br&gt;但是cAdvisor只提供了单机的容器资源占用情况，而在大规模容器集群中，需要对所有的Node和全部容器进行性能监控。这就需要一套工具来实现集群性能数据的采集、存储和展示：Heapster、InfluxDB和Grafana。&lt;br&gt;Heapster提供了整个集群的资源监控，并支持持久化数据存储到InfluxDB、Google Cloud Monitoring或者其他的存储后端。Heapster从kubelet提供的API采集节点和容器的资源占用。另外，Heapster的 /metrics API提供了Prometheus格式的数据。&lt;br&gt;InfluxDB是一个开源分布式时序、事件和指标数据库；而Grafana则是InfluxDB的 dashboard，提供了强大的图表展示功能。它们常被组合使用展示图表化的监控数据。&lt;br&gt;Heapster、InfluxDB和Grafana均以Pod的形式启动和运行，其中Heapster需要与Kubernetes Master进行安全连接。&lt;/p&gt;
&lt;h2 id=&quot;安装配置Heapster、InfluxDB和Grafana&quot;&gt;&lt;a href=&quot;#安装配置Heapster、InfluxDB和Grafana&quot; class=&quot;headerlink&quot; title=&quot;安装配置Heapster、InfluxDB和Grafana&quot;&gt;&lt;/a&gt;安装配置Heapster、InfluxDB和Grafana&lt;/h2&gt;&lt;p&gt;到&lt;a href=&quot;https://github.com/kubernetes/heapster/releases&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;heapster release 页面&lt;/a&gt;下载heapster。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 opt]# wget https://github.com/kubernetes/heapster/archive/v1.3.0.zip
[root@node1 opt]# unzip v1.3.0.zip
[root@node1 opt]# cd heapster-1.3.0/deploy/kube-config/influxdb
[root@node1 influxdb]# ls *.yaml
grafana-deployment.yaml  heapster-deployment.yaml  influxdb-deployment.yaml
grafana-service.yaml     heapster-service.yaml     influxdb-service.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;创建heapster的rbac配置heapster-rbac.yaml。已经修改好的 yaml 文件见：&lt;a href=&quot;https://github.com/rootsongjc/follow-me-install-kubernetes-cluster/tree/master/manifests/heapster&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;heapster&lt;/a&gt;&lt;br&gt;下面比对的是我自己修改的和源文件。或者直接点直接去上面的地址下载这几个配置文件替换原有的配置文件。&lt;/p&gt;
&lt;h3 id=&quot;修改-grafana-deployment-yaml&quot;&gt;&lt;a href=&quot;#修改-grafana-deployment-yaml&quot; class=&quot;headerlink&quot; title=&quot;修改 grafana-deployment.yaml&quot;&gt;&lt;/a&gt;修改 grafana-deployment.yaml&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;diff grafana-deployment.yaml.orig grafana-deployment.yaml
16c16
&amp;lt;         image: gcr.io/google_containers/heapster-grafana-amd64:v4.0.2
---
&amp;gt;         image: sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-grafana-amd64:v4.0.2
40,41c40,41
&amp;lt;           # value: /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/
&amp;lt;           value: /
---
&amp;gt;           value: /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/
&amp;gt;           #value: /
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;根据上面的差异修改源文件grafana-deployment.yaml，并将image地址改为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;index.tenxcloud.com/jimmy/heapster-grafana-amd64:v4.0.2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果后续使用 kube-apiserver 或者 kubectl proxy 访问 grafana dashboard，则必须将 GF_SERVER_ROOT_URL 设置为/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/，否则后续访问grafana时访问时提示找不到&lt;a href=&quot;http://10.64.3.7:8086/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/api/dashboards/home&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://10.64.3.7:8086/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/api/dashboards/home&lt;/a&gt; 页面。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;修改-heapster-deployment-yaml&quot;&gt;&lt;a href=&quot;#修改-heapster-deployment-yaml&quot; class=&quot;headerlink&quot; title=&quot;修改 heapster-deployment.yaml&quot;&gt;&lt;/a&gt;修改 heapster-deployment.yaml&lt;/h3&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;19&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;20&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;21&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;22&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;23&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;24&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;25&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;[root@node1 influxdb]# pwd&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;/opt/heapster-1.3.0/deploy/kube-config/influxdb&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;[root@node1 influxdb]# vim heapster-deployment.yaml &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;apiVersion: extensions/v1beta1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;kind: Deployment&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  name: heapster&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  namespace: kube-system&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  replicas: 1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  template:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    metadata:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      labels:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        task: monitoring&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        k8s-app: heapster&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    spec:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      serviceAccountName: heapster&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      containers:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      - name: heapster&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        image: index.tenxcloud.com/jimmy/heapster-amd64:v1.3.0-beta.1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        imagePullPolicy: IfNotPresent&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        command:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        - /heapster&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        - --source=kubernetes:https://kubernetes.default&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        - --sink=influxdb:http://monitoring-influxdb:8086&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;将image地址改为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;index.tenxcloud.com/jimmy/heapster-amd64:v1.3.0-beta.1 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】:Heapster需要设置的启动参数如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;source：配置采集源，为Master URL地址：–source=kubernetes:&lt;a href=&quot;https://kubernetes.default&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://kubernetes.default&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;sink：配置后端存储系统，使用InfluxDB系统：–sink=influxdb:&lt;a href=&quot;http://monitoring-influxdb:8086&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://monitoring-influxdb:8086&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其他参数可以通过进入heapster容器执行 # heapster –help 命令查看和设置。&lt;/p&gt;
&lt;p&gt;【注意】：URL中的主机名地址使用的是InfluxDB的Service名字，这需要DNS服务正常工作，如果没有配置DNS服务，则也可以使用Service的ClusterIP地址。&lt;br&gt;另外，InfluxDB服务的名称没有加上命名空间，是因为Heapster服务与InfluxDB服务属于相同的命名空间kube-system。也可以使用上命名空间的全服务名，例如：&lt;a href=&quot;http://monitoring-influxdb.kube-system:8086&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://monitoring-influxdb.kube-system:8086&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;修改-influxdb-deployment-yaml&quot;&gt;&lt;a href=&quot;#修改-influxdb-deployment-yaml&quot; class=&quot;headerlink&quot; title=&quot;修改 influxdb-deployment.yaml&quot;&gt;&lt;/a&gt;修改 influxdb-deployment.yaml&lt;/h3&gt;&lt;p&gt;influxdb 官方建议使用命令行或 HTTP API 接口来查询数据库，从 v1.1.0 版本开始默认关闭 admin UI，将在后续版本中移除 admin UI 插件。&lt;br&gt;开启镜像中 admin UI的办法如下：先导出镜像中的 influxdb 配置文件，开启 admin 插件后，再将配置文件内容写入 ConfigMap，最后挂载到镜像中，达到覆盖原始配置的目的。&lt;br&gt;【注意】：manifests 目录已经提供了 &lt;a href=&quot;https://github.com/opsnull/follow-me-install-kubernetes-cluster/blob/master/manifests/heapster/influxdb-cm.yaml&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;修改后的 ConfigMap 定义文件&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 导出镜像中的 influxdb 配置文件
[root@node1 influxdb]# docker run --rm --entrypoint &amp;apos;cat&amp;apos;  -ti lvanneo/heapster-influxdb-amd64:v1.1.1 /etc/config.toml &amp;gt;config.toml.orig
[root@node1 influxdb]# cp config.toml.orig config.toml 
# 修改配置：启用 admin 接口
[root@node1 influxdb]# vim config.toml
[admin]
  enabled = true
# 将修改后的配置写入到 ConfigMap 对象中(kubectl 可以通过 --namespace 或者 -n 选项指定namespace。如果不指定, 默认为default)
[root@node1 influxdb]# kubectl create configmap influxdb-config --from-file=config.toml -n kube-system
configmap &amp;quot;influxdb-config&amp;quot; created
# 将 ConfigMap 中的配置文件挂载到 Pod 中，达到覆盖原始配置的目的
diff influxdb-deployment.yaml.orig influxdb-deployment.yaml
16c16
&amp;lt;         image: grc.io/google_containers/heapster-influxdb-amd64:v1.1.1
---
&amp;gt;         image: sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-influxdb-amd64:v1.1.1
19a20,21
&amp;gt;         - mountPath: /etc/
&amp;gt;           name: influxdb-config
22a25,27
&amp;gt;       - name: influxdb-config
&amp;gt;         configMap:
&amp;gt;           name: influxdb-config
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;根据上面的差异修改源文件influxdb-deployment.yaml，并将image地址改为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;index.tenxcloud.com/jimmy/heapster-influxdb-amd64:v1.1.1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后删除这两个文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 influxdb]# rm -f config.toml config.toml.orig 
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;修改-influxdb-service-yaml&quot;&gt;&lt;a href=&quot;#修改-influxdb-service-yaml&quot; class=&quot;headerlink&quot; title=&quot;修改 influxdb-service.yaml&quot;&gt;&lt;/a&gt;修改 influxdb-service.yaml&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;diff influxdb-service.yaml.orig influxdb-service.yaml
12a13
&amp;gt;   type: NodePort
15a17,20
&amp;gt;     name: http
&amp;gt;   - port: 8083
&amp;gt;     targetPort: 8083
&amp;gt;     name: admin
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;定义端口类型为 NodePort，将InfluxDB暴露在宿主机Node的端口上，以便后续浏览器访问 influxdb 的 admin UI 界面。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;执行所有定义文件进行安装&quot;&gt;&lt;a href=&quot;#执行所有定义文件进行安装&quot; class=&quot;headerlink&quot; title=&quot;执行所有定义文件进行安装&quot;&gt;&lt;/a&gt;执行所有定义文件进行安装&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@node1 influxdb]# pwd
/opt/heapster-1.3.0/deploy/kube-config/influxdb
[root@node1 influxdb]# ls
grafana-deployment.yaml  heapster-deployment.yaml  heapster-service.yaml  influxdb-deployment.yaml
grafana-service.yaml     heapster-rbac.yaml        influxdb-cm.yaml       influxdb-service.yaml
[root@node1 influxdb]# kubectl create -f . 
deployment &amp;quot;monitoring-grafana&amp;quot; created
service &amp;quot;monitoring-grafana&amp;quot; created
deployment &amp;quot;heapster&amp;quot; created
serviceaccount &amp;quot;heapster&amp;quot; created
clusterrolebinding &amp;quot;heapster&amp;quot; created
service &amp;quot;heapster&amp;quot; created
deployment &amp;quot;monitoring-influxdb&amp;quot; created
service &amp;quot;monitoring-influxdb&amp;quot; created
Error from server (AlreadyExists): error when creating &amp;quot;influxdb-cm.yaml&amp;quot;: configmaps &amp;quot;influxdb-config&amp;quot; already exists
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;检查执行结果&quot;&gt;&lt;a href=&quot;#检查执行结果&quot; class=&quot;headerlink&quot; title=&quot;检查执行结果&quot;&gt;&lt;/a&gt;检查执行结果&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;1.检查 Deployment&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get deployments -n kube-system | grep -E &amp;apos;heapster|monitoring&amp;apos;
heapster               1         1         1            1           12m
monitoring-grafana     1         1         1            1           12m
monitoring-influxdb    1         1         1            1           12m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;2.检查 Pods&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get pods -n kube-system | grep -E &amp;apos;heapster|monitoring&amp;apos;
heapster-2291216627-6hv9s               1/1       Running   0          10m
monitoring-grafana-2490289118-n54fk     1/1       Running   0          10m
monitoring-influxdb-1450237832-029q8    1/1       Running   0          10m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;3.检查 kubernets dashboard 界面，看是显示各 Nodes、Pods 的 CPU、内存、负载等利用率曲线图&lt;/strong&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/4.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;访问-grafana&quot;&gt;&lt;a href=&quot;#访问-grafana&quot; class=&quot;headerlink&quot; title=&quot;访问 grafana&quot;&gt;&lt;/a&gt;访问 grafana&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;1.通过 kube-apiserver 访问&lt;/strong&gt;&lt;br&gt;获取 monitoring-grafana 服务 URL：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 influxdb]# kubectl cluster-info
Kubernetes master is running at https://172.16.7.151:6443
Heapster is running at https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/heapster
KubeDNS is running at https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/kube-dns
kubernetes-dashboard is running at https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard
monitoring-grafana is running at https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana
monitoring-influxdb is running at https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb

To further debug and diagnose cluster problems, use &amp;apos;kubectl cluster-info dump&amp;apos;.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;浏览器访问 URL： &lt;a href=&quot;http://172.16.7.151:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.151:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/5.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.通过 kubectl proxy 访问&lt;/strong&gt;&lt;br&gt;创建代理:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl proxy --address=&amp;apos;172.16.7.151&amp;apos; --port=8086 --accept-hosts=&amp;apos;^*$&amp;apos;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;浏览器访问 URL：&lt;a href=&quot;http://172.16.7.151:8086/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.151:8086/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/5.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3.Grafana页面查看和操作&lt;/strong&gt;&lt;br&gt;浏览器访问 URL： &lt;a href=&quot;http://172.16.7.151:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.151:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana&lt;/a&gt;&lt;br&gt;点击“Home”下拉列表，选择cluster，如下图。图中显示了Cluster集群的整体信息，以折线图的形式展示了集群范围内各Node的CPU使用率、内存使用情况等信息。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/10.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;点击“Home”下拉列表，选择Pods，如下图。图中展示了Pod的信息，以折线图的形式展示了集群范围内各Pod的CPU使用率、内存使用情况、网络流量、文件系统使用情况等信息。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/11.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;访问-influxdb-admin-UI&quot;&gt;&lt;a href=&quot;#访问-influxdb-admin-UI&quot; class=&quot;headerlink&quot; title=&quot;访问 influxdb admin UI&quot;&gt;&lt;/a&gt;访问 influxdb admin UI&lt;/h2&gt;&lt;p&gt;获取 influxdb http 8086 映射的 NodePort：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 influxdb]# kubectl get svc -n kube-system|grep influxdb
monitoring-influxdb    10.254.66.133    &amp;lt;nodes&amp;gt;       8086:32570/TCP,8083:31601/TCP   17m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;通过 kube-apiserver 的非安全端口访问 influxdb 的 admin UI 界面：&lt;a href=&quot;http://172.16.7.151:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb:8083/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.151:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb:8083/&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/6.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;在页面的 “Connection Settings” 的 Host 中输入 node IP， Port 中输入 8086 映射的 nodePort 如上面的 32570，点击 “Save” 即可（我的集群中的地址是172.16.7.151:32570）。&lt;br&gt;通过右上角齿轮按钮可以修改连接属性。单击右上角的Database下拉列表可以选择数据库，heapster创建的数据库名为k8s。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/7.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/8.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;heapster采集的metric&quot;&gt;&lt;a href=&quot;#heapster采集的metric&quot; class=&quot;headerlink&quot; title=&quot;heapster采集的metric&quot;&gt;&lt;/a&gt;heapster采集的metric&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;metric名称&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;cpu/limit&lt;/td&gt;
&lt;td&gt;CPU hard limit，单位为毫秒&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;cpu/usage&lt;/td&gt;
&lt;td&gt;全部Core的CPU累计使用时间&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;cpu/usage_rate&lt;/td&gt;
&lt;td&gt;全部Core的CPU累计使用率，单位为毫秒&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;filesystem/limit&lt;/td&gt;
&lt;td&gt;文件系统总空间限制，单位为字节&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;filesystem/usage&lt;/td&gt;
&lt;td&gt;文件系统已用的空间，单位为字节&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory/limit&lt;/td&gt;
&lt;td&gt;Memory hard limit，单位为字节&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory/major_page_faults&lt;/td&gt;
&lt;td&gt;major page faults数量&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory/major_page_faults_rate&lt;/td&gt;
&lt;td&gt;每秒的major page faults数量&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory/node_allocatable&lt;/td&gt;
&lt;td&gt;Node可分配的内存容量&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory/node_capacity&lt;/td&gt;
&lt;td&gt;Node的内存容量&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory/node_reservation&lt;/td&gt;
&lt;td&gt;Node保留的内存share&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory/node_utilization&lt;/td&gt;
&lt;td&gt;Node的内存使用值&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory/page_faults&lt;/td&gt;
&lt;td&gt;page faults数量&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory/page_faults_rate&lt;/td&gt;
&lt;td&gt;每秒的page faults数量&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory/request&lt;/td&gt;
&lt;td&gt;Memory request，单位为字节&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory/usage&lt;/td&gt;
&lt;td&gt;总内存使用量&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory/working_set&lt;/td&gt;
&lt;td&gt;总的Working set usage，Working set是指不会被kernel移除的内存&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;network/rx&lt;/td&gt;
&lt;td&gt;累计接收的网络流量字节数&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;network/rx_errors&lt;/td&gt;
&lt;td&gt;累计接收的网络流量错误数&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;network/rx_errors_rate&lt;/td&gt;
&lt;td&gt;每秒接收的网络流量错误数&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;network/rx_rate&lt;/td&gt;
&lt;td&gt;每秒接收的网络流量字节数&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;network/tx&lt;/td&gt;
&lt;td&gt;累计发送的网络流量字节数&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;network/tx_errors&lt;/td&gt;
&lt;td&gt;累计发送的网络流量错误数&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;network/tx_errors_rate&lt;/td&gt;
&lt;td&gt;每秒发送的网络流量错误数&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;network/tx_rate&lt;/td&gt;
&lt;td&gt;每秒发送的网络流量字节数&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;uptime&lt;/td&gt;
&lt;td&gt;容器启动总时长&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;每个metric可以看作一张数据库表，表中每条记录由一组label组成，可以看成字段。如下表所示：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Label名称&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;pod_id&lt;/td&gt;
&lt;td&gt;系统生成的Pod唯一名称&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;pod_name&lt;/td&gt;
&lt;td&gt;用户指定的Pod名称&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;pod_namespace&lt;/td&gt;
&lt;td&gt;Pod所属的namespace&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;container_base_image&lt;/td&gt;
&lt;td&gt;容器的镜像名称&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;container_name&lt;/td&gt;
&lt;td&gt;用户指定的容器名称&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;host_id&lt;/td&gt;
&lt;td&gt;用户指定的Node主机名&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;hostname&lt;/td&gt;
&lt;td&gt;容器运行所在主机名&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;labels&lt;/td&gt;
&lt;td&gt;逗号分隔的Label列表&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;namespace_id&lt;/td&gt;
&lt;td&gt;Pod所属的namespace的UID&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;resource_id&lt;/td&gt;
&lt;td&gt;资源ID&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;可以使用SQL SELECT语句对每个metric进行查询，例如查询CPU的使用时间：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;select * from &amp;quot;cpu/usage&amp;quot; limit 10
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;结果如下图所示：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/9.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Kubernetes-监控方案&quot;&gt;&lt;a href=&quot;#Kubernetes-监控方案&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes 监控方案&quot;&gt;&lt;/a&gt;Kubernetes 监控方案&lt;/h2&gt;&lt;p&gt;可选的方案：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Heapster + InfluxDB + Grafana&lt;/li&gt;
&lt;li&gt;Prometheus + Grafana&lt;/li&gt;
&lt;li&gt;Cadvisor + InfluxDB + Grafana&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本篇文章介绍的是Heapster + InfluxDB + Grafana，kubernetes集群（1.6.0）搭建见前面的文章。&lt;br&gt;
    
    </summary>
    
      <category term="容器编排" scheme="http://yoursite.com/categories/%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92/"/>
    
    
      <category term="Kubernetes" scheme="http://yoursite.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes1.6集群上(开启了TLS)安装Dashboard</title>
    <link href="http://yoursite.com/2017/09/19/Kubernetes1-6%E9%9B%86%E7%BE%A4%E4%B8%8A-%E5%BC%80%E5%90%AF%E4%BA%86TLS-%E5%AE%89%E8%A3%85Dashboard/"/>
    <id>http://yoursite.com/2017/09/19/Kubernetes1-6集群上-开启了TLS-安装Dashboard/</id>
    <published>2017-09-19T01:14:24.000Z</published>
    <updated>2017-11-22T07:38:50.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;配置和安装-dashboard&quot;&gt;&lt;a href=&quot;#配置和安装-dashboard&quot; class=&quot;headerlink&quot; title=&quot;配置和安装 dashboard&quot;&gt;&lt;/a&gt;配置和安装 dashboard&lt;/h2&gt;&lt;p&gt;这是接着上一篇《二进制方式部署Kubernetes 1.6.0集群(开启TLS)》写的。&lt;br&gt;Kubernetes Dashboard is a general purpose, web-based UI for Kubernetes clusters. It allows users to manage applications running in the cluster and troubleshoot them, as well as manage the cluster itself.&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;配置dashboard&quot;&gt;&lt;a href=&quot;#配置dashboard&quot; class=&quot;headerlink&quot; title=&quot;配置dashboard&quot;&gt;&lt;/a&gt;配置dashboard&lt;/h3&gt;&lt;p&gt;官方文件目录：&lt;a href=&quot;https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dashboard&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dashboard&lt;/a&gt;&lt;br&gt;我使用的文件:&lt;br&gt;从 &lt;a href=&quot;https://github.com/rootsongjc/follow-me-install-kubernetes-cluster/tree/master/manifests/dashboard&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/rootsongjc/follow-me-install-kubernetes-cluster/tree/master/manifests/dashboard&lt;/a&gt; 下载3个文件下来，并上传到/opt/kube-dashboard/目录下。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 opt]# mkdir kube-dashboard
[root@node1 opt]# cd kube-dashboard/
[root@node1 kube-dashboard]# ls
dashboard-controller.yaml  dashboard-rbac.yaml  dashboard-service.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;修改dashboard-controller.yaml文件，将里面的image改为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;index.tenxcloud.com/jimmy/kubernetes-dashboard-amd64:v1.6.0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;由于 kube-apiserver 启用了 RBAC 授权，而官方源码目录的 dashboard-controller.yaml 没有定义授权的 ServiceAccount，所以后续访问 kube-apiserver 的 API 时会被拒绝，web中提示：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Forbidden (403)

User &amp;quot;system:serviceaccount:kube-system:default&amp;quot; cannot list jobs.batch in the namespace &amp;quot;default&amp;quot;. (get jobs.batch)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;因此，增加了一个dashboard-rbac.yaml文件，定义一个名为 dashboard 的 ServiceAccount，然后将它和 Cluster Role view 绑定。&lt;/p&gt;
&lt;h3 id=&quot;执行所有定义的文件&quot;&gt;&lt;a href=&quot;#执行所有定义的文件&quot; class=&quot;headerlink&quot; title=&quot;执行所有定义的文件&quot;&gt;&lt;/a&gt;执行所有定义的文件&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;# pwd
/opt/kube-dashboard
# ls
dashboard-controller.yaml  dashboard-rbac.yaml  dashboard-service.yaml
# kubectl create -f .
deployment &amp;quot;kubernetes-dashboard&amp;quot; created
serviceaccount &amp;quot;dashboard&amp;quot; created
clusterrolebinding &amp;quot;dashboard&amp;quot; created
service &amp;quot;kubernetes-dashboard&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;检查执行结果&quot;&gt;&lt;a href=&quot;#检查执行结果&quot; class=&quot;headerlink&quot; title=&quot;检查执行结果&quot;&gt;&lt;/a&gt;检查执行结果&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;1.查看分配的 NodePort&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get services kubernetes-dashboard -n kube-system
NAME                   CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubernetes-dashboard   10.254.207.77   &amp;lt;nodes&amp;gt;       80:32281/TCP   41s
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;NodePort 32281映射到 dashboard pod 80端口。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;2.检查 controller&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get deployment kubernetes-dashboard  -n kube-system
NAME                   DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kubernetes-dashboard   1         1         1            1           13m
# kubectl get pods  -n kube-system | grep dashboard
kubernetes-dashboard-2888692679-tv54g   1/1       Running   0          13m
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;访问dashboard&quot;&gt;&lt;a href=&quot;#访问dashboard&quot; class=&quot;headerlink&quot; title=&quot;访问dashboard&quot;&gt;&lt;/a&gt;访问dashboard&lt;/h3&gt;&lt;p&gt;有以下三种方式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kubernetes-dashboard 服务暴露了 NodePort，可以使用 &lt;a href=&quot;http://NodeIP:nodePort&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://NodeIP:nodePort&lt;/a&gt; 地址访问 dashboard；&lt;/li&gt;
&lt;li&gt;通过 kube-apiserver 访问 dashboard（https 6443端口和http 8080端口方式）；&lt;/li&gt;
&lt;li&gt;通过 kubectl proxy 访问 dashboard&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;使用-http-NodeIP-nodePort-地址访问-dashboard&quot;&gt;&lt;a href=&quot;#使用-http-NodeIP-nodePort-地址访问-dashboard&quot; class=&quot;headerlink&quot; title=&quot;使用 http://NodeIP:nodePort 地址访问 dashboard&quot;&gt;&lt;/a&gt;使用 &lt;a href=&quot;http://NodeIP:nodePort&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://NodeIP:nodePort&lt;/a&gt; 地址访问 dashboard&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# kubectl get services kubernetes-dashboard -n kube-system
NAME                   CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubernetes-dashboard   10.254.207.77   &amp;lt;nodes&amp;gt;       80:32281/TCP   41s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后检查出这个pod是运行在集群中哪个服务器上的，我这里是检查是运行在node1节点上的，所以浏览器输入&lt;a href=&quot;http://172.16.7.151:32281/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.151:32281/&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/3.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h4 id=&quot;通过-kubectl-proxy-访问-dashboard&quot;&gt;&lt;a href=&quot;#通过-kubectl-proxy-访问-dashboard&quot; class=&quot;headerlink&quot; title=&quot;通过 kubectl proxy 访问 dashboard&quot;&gt;&lt;/a&gt;通过 kubectl proxy 访问 dashboard&lt;/h4&gt;&lt;p&gt;1.启动代理&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 kube-dashboard]# kubectl proxy --address=&amp;apos;172.16.7.151&amp;apos; --port=8086 --accept-hosts=&amp;apos;^*$&amp;apos;         
Starting to serve on 172.16.7.151:8086
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;需要指定 –accept-hosts 选项，否则浏览器访问 dashboard 页面时提示 “Unauthorized”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2.访问&lt;br&gt;浏览器访问 URL：&lt;a href=&quot;http://172.16.7.151:8086/ui&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.151:8086/ui&lt;/a&gt; 自动跳转到：&lt;a href=&quot;http://172.16.7.151:8086/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard/#/workload?namespace=default&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.151:8086/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard/#/workload?namespace=default&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/3.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h4 id=&quot;通过-kube-apiserver-访问dashboard&quot;&gt;&lt;a href=&quot;#通过-kube-apiserver-访问dashboard&quot; class=&quot;headerlink&quot; title=&quot;通过 kube-apiserver 访问dashboard&quot;&gt;&lt;/a&gt;通过 kube-apiserver 访问dashboard&lt;/h4&gt;&lt;p&gt;1.获取集群服务地址列表&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# kubectl cluster-info
Kubernetes master is running at https://172.16.7.151:6443
KubeDNS is running at https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/kube-dns
kubernetes-dashboard is running at https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard

To further debug and diagnose cluster problems, use &amp;apos;kubectl cluster-info dump&amp;apos;.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.导入证书&lt;br&gt;将生成的admin.pem证书转换格式。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# cd /etc/kubernetes/ssl/
[root@node1 ~]# openssl pkcs12 -export -in admin.pem  -out admin.p12 -inkey admin-key.pem
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;将生成的admin.p12证书导入的你的电脑，导出的时候记住你设置的密码，导入的时候还要用到。&lt;br&gt;如果你不想使用https的话，可以直接访问insecure port 8080端口:&lt;a href=&quot;http://172.16.7.151:8080/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.151:8080/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/3.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;由于缺少 Heapster 插件，当前 dashboard 不能展示 Pod、Nodes 的 CPU、内存等 metric 图形。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;配置和安装-dashboard&quot;&gt;&lt;a href=&quot;#配置和安装-dashboard&quot; class=&quot;headerlink&quot; title=&quot;配置和安装 dashboard&quot;&gt;&lt;/a&gt;配置和安装 dashboard&lt;/h2&gt;&lt;p&gt;这是接着上一篇《二进制方式部署Kubernetes 1.6.0集群(开启TLS)》写的。&lt;br&gt;Kubernetes Dashboard is a general purpose, web-based UI for Kubernetes clusters. It allows users to manage applications running in the cluster and troubleshoot them, as well as manage the cluster itself.&lt;br&gt;
    
    </summary>
    
      <category term="容器编排" scheme="http://yoursite.com/categories/%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92/"/>
    
    
      <category term="Kubernetes" scheme="http://yoursite.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>二进制方式部署Kubernetes 1.6.0集群(开启TLS)</title>
    <link href="http://yoursite.com/2017/09/15/%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2Kubernetes-1-6-0%E9%9B%86%E7%BE%A4-%E5%BC%80%E5%90%AFTLS/"/>
    <id>http://yoursite.com/2017/09/15/二进制方式部署Kubernetes-1-6-0集群-开启TLS/</id>
    <published>2017-09-15T06:06:09.000Z</published>
    <updated>2017-11-19T06:01:35.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;Kubernetes简介&quot;&gt;&lt;a href=&quot;#Kubernetes简介&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes简介&quot;&gt;&lt;/a&gt;Kubernetes简介&lt;/h2&gt;&lt;p&gt;Kubernetes是谷歌开源的容器集群管理系统，是Google多年大规模容器管理技术Borg的 开源版本，主要功能包括:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于容器的应用部署、维护和滚动升级&lt;/li&gt;
&lt;li&gt;负载均衡和服务发现&lt;/li&gt;
&lt;li&gt;跨机器和跨地区的集群调度&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;自动伸缩&lt;/li&gt;
&lt;li&gt;无状态服务和有状态服务 &lt;/li&gt;
&lt;li&gt;广泛的Volume支持 &lt;/li&gt;
&lt;li&gt;插件机制保证扩展性&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;之前尝试使用kubeadm自动化部署集群，使用yum去安装kubeadm等工具，但是不翻墙的情况下，这种方式在国内几乎是不可能安装成功的。于是改为采用二进制文件部署Kubernetes集群，同时开启了集群的TLS安全认证。本篇实践是参照opsnull的文章&lt;a href=&quot;https://mp.weixin.qq.com/s/bvCZUl6LQhlqDVv_TNeDFg&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;创建 kubernetes 各组件 TLS 加密通信的证书和秘钥&lt;/a&gt;，结合公司的实际情况进行部署的。&lt;/p&gt;
&lt;h2 id=&quot;环境信息&quot;&gt;&lt;a href=&quot;#环境信息&quot; class=&quot;headerlink&quot; title=&quot;环境信息&quot;&gt;&lt;/a&gt;环境信息&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;操作系统版本&lt;/th&gt;
&lt;th&gt;IP地址&lt;/th&gt;
&lt;th&gt;角色&lt;/th&gt;
&lt;th&gt;安装软件&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;node1&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.7.151&lt;/td&gt;
&lt;td&gt;Kubernetes Master、Node&lt;/td&gt;
&lt;td&gt;etcd 3.2.7、kube-apiserver、kube-scheduler、kube-controller-manager、kubelet、kube-proxy、etcd 3.2.7、flannel 0.7.1、docker 1.12.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;node2&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.7.152&lt;/td&gt;
&lt;td&gt;Kubernetes Node&lt;/td&gt;
&lt;td&gt;kubelet、kube-proxy、flannel 0.7.1、etcd 3.2.7、docker 1.12.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;node3&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.7.153&lt;/td&gt;
&lt;td&gt;Kubernetes Node&lt;/td&gt;
&lt;td&gt;kubelet、kube-proxy、flannel 0.7.1、etcd 3.2.7、docker 1.12.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;spark32&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.206.32&lt;/td&gt;
&lt;td&gt;Harbor&lt;/td&gt;
&lt;td&gt;docker-ce 17.06.1、docker-compose 1.15.0、harbor-online-installer-v1.1.2.tar&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;spark32主机是harbor私有镜像仓库，关于harbor的安装部署见之前的博客《企业级Docker Registry——Harbor搭建和使用》。&lt;/p&gt;
&lt;h2 id=&quot;创建TLS加密通信的证书和密钥&quot;&gt;&lt;a href=&quot;#创建TLS加密通信的证书和密钥&quot; class=&quot;headerlink&quot; title=&quot;创建TLS加密通信的证书和密钥&quot;&gt;&lt;/a&gt;创建TLS加密通信的证书和密钥&lt;/h2&gt;&lt;p&gt;kubernetes各组件需要使用TLS证书对通信进行加密，这里我使用CloudFlare的PKI工具集&lt;a href=&quot;https://github.com/cloudflare/cfssl&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;cfssl&lt;/a&gt;来生成CA和其它证书。&lt;br&gt;生成的CA证书和密钥文件如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ca-key.pem&lt;/li&gt;
&lt;li&gt;ca.pem&lt;/li&gt;
&lt;li&gt;kubernetes-key.pem&lt;/li&gt;
&lt;li&gt;kubernetes.pem&lt;/li&gt;
&lt;li&gt;kube-proxy.pem&lt;/li&gt;
&lt;li&gt;kube-proxy-key.pem&lt;/li&gt;
&lt;li&gt;admin.pem&lt;/li&gt;
&lt;li&gt;admin-key.pem&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;各组件使用证书的情况如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;etcd：使用ca.pem、kubernetes-key.pem、kubernetes.pem；&lt;/li&gt;
&lt;li&gt;kube-apiserver：使用ca.pem、kubernetes-key.pem、kubernetes.pem；&lt;/li&gt;
&lt;li&gt;kubelet：使用ca.pem；&lt;/li&gt;
&lt;li&gt;kube-proxy：使用ca.pem、kube-proxy-key.pem、kube-proxy.pem；&lt;/li&gt;
&lt;li&gt;kubectl：使用ca.pem、admin-key.pem、admin.pem&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;kube-controller、kube-scheduler当前需要和kube-apiserver部署在同一台机器上且使用非安全端口通信，故不需要证书。&lt;/p&gt;
&lt;h3 id=&quot;安装CFSSL&quot;&gt;&lt;a href=&quot;#安装CFSSL&quot; class=&quot;headerlink&quot; title=&quot;安装CFSSL&quot;&gt;&lt;/a&gt;安装CFSSL&lt;/h3&gt;&lt;p&gt;有两种方式安装，一是二进制源码包安装，二是使用go命令安装。&lt;br&gt;1.方式一：二进制源码包安装&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
# chmod +x cfssl_linux-amd64
# mv cfssl_linux-amd64 /root/local/bin/cfssl

# wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
# chmod +x cfssljson_linux-amd64
# mv cfssljson_linux-amd64 /root/local/bin/cfssljson

# wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64
# chmod +x cfssl-certinfo_linux-amd64
# mv cfssl-certinfo_linux-amd64 /root/local/bin/cfssl-certinfo

# export PATH=/root/local/bin:$PATH
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.方式二：使用go命令安装&lt;br&gt;安装go(需要go 1.6+)：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 下载地址：https://golang.org/dl/
[root@node1 ~]# cd /usr/local/
[root@node1 local]# wget https://storage.googleapis.com/golang/go1.9.linux-amd64.tar.gz
[root@node1 local]# tar zxf go1.9.linux-amd64.tar.gz
[root@node1 local]# vim /etc/profile
# Go
export GO_HOME=/usr/local/go
export PATH=$GO_HOME/bin:$PATH 
[root@node1 local]# source /etc/profile
# 查看版本信息
[root@node1 local]# go version
go version go1.9 linux/amd64
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;安装cfssl:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 local]# go get -u github.com/cloudflare/cfssl/cmd/...
[root@node1 local]# ls /root/go/bin/
cfssl  cfssl-bundle  cfssl-certinfo  cfssljson  cfssl-newkey  cfssl-scan  mkbundle  multirootca
[root@node1 local]# mv /root/go/bin/* /usr/local/bin/
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;创建CA&quot;&gt;&lt;a href=&quot;#创建CA&quot; class=&quot;headerlink&quot; title=&quot;创建CA&quot;&gt;&lt;/a&gt;创建CA&lt;/h3&gt;&lt;p&gt;1.创建 CA 配置文件&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 local]# mkdir /opt/ssl
[root@node1 local]# cd /opt/ssl/
[root@node1 ssl]# cfssl print-defaults config &amp;gt; config.json
[root@node1 ssl]# cfssl print-defaults csr &amp;gt; csr.json
# 创建CA配置文件
[root@node1 ssl]# vim ca-config.json
{
  &amp;quot;signing&amp;quot;: {
    &amp;quot;default&amp;quot;: {
      &amp;quot;expiry&amp;quot;: &amp;quot;8760h&amp;quot;
    },
    &amp;quot;profiles&amp;quot;: {
      &amp;quot;kubernetes&amp;quot;: {
        &amp;quot;usages&amp;quot;: [
            &amp;quot;signing&amp;quot;,
            &amp;quot;key encipherment&amp;quot;,
            &amp;quot;server auth&amp;quot;,
            &amp;quot;client auth&amp;quot;
        ],
        &amp;quot;expiry&amp;quot;: &amp;quot;8760h&amp;quot;
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;部分字段说明：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ca-config.json：&lt;/strong&gt;可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个 profile；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;signing：&lt;/strong&gt;表示该证书可用于签名其它证书；生成的 ca.pem 证书中 CA=TRUE；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;server auth：&lt;/strong&gt;表示client可以用该 CA 对server提供的证书进行验证；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;client auth：&lt;/strong&gt;表示server可以用该 CA 对client提供的证书进行验证。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2.创建 CA 证书签名请求&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# vim ca-csr.json
{
  &amp;quot;CN&amp;quot;: &amp;quot;kubernetes&amp;quot;,
  &amp;quot;key&amp;quot;: {
    &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;,
    &amp;quot;size&amp;quot;: 2048
  },
  &amp;quot;names&amp;quot;: [
    {
      &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;,
      &amp;quot;ST&amp;quot;: &amp;quot;BeiJing&amp;quot;,
      &amp;quot;L&amp;quot;: &amp;quot;BeiJing&amp;quot;,
      &amp;quot;O&amp;quot;: &amp;quot;k8s&amp;quot;,
      &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;部分字段说明：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;“CN”：&lt;/strong&gt;Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;“O”：&lt;/strong&gt;Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;3.生成 CA 证书和私钥&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca
2017/09/10 04:22:13 [INFO] generating a new CA key and certificate from CSR
2017/09/10 04:22:13 [INFO] generate received request
2017/09/10 04:22:13 [INFO] received CSR
2017/09/10 04:22:13 [INFO] generating key: rsa-2048
2017/09/10 04:22:13 [INFO] encoded CSR
2017/09/10 04:22:13 [INFO] signed certificate with serial number 348968532213237181927470194452366329323573808966
[root@node1 ssl]# ls ca*
ca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;创建-Kubernetes-证书&quot;&gt;&lt;a href=&quot;#创建-Kubernetes-证书&quot; class=&quot;headerlink&quot; title=&quot;创建 Kubernetes 证书&quot;&gt;&lt;/a&gt;创建 Kubernetes 证书&lt;/h3&gt;&lt;p&gt;1.创建 kubernetes 证书签名请求&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# vim kubernetes-csr.json
{
    &amp;quot;CN&amp;quot;: &amp;quot;kubernetes&amp;quot;,
    &amp;quot;hosts&amp;quot;: [
      &amp;quot;127.0.0.1&amp;quot;,
      &amp;quot;172.16.7.151&amp;quot;,
      &amp;quot;172.16.7.152&amp;quot;,
      &amp;quot;172.16.7.153&amp;quot;,
      &amp;quot;172.16.206.32&amp;quot;,
      &amp;quot;10.254.0.1&amp;quot;,
      &amp;quot;kubernetes&amp;quot;,
      &amp;quot;kubernetes.default&amp;quot;,
      &amp;quot;kubernetes.default.svc&amp;quot;,
      &amp;quot;kubernetes.default.svc.cluster&amp;quot;,
      &amp;quot;kubernetes.default.svc.cluster.local&amp;quot;
    ],
    &amp;quot;key&amp;quot;: {
        &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;,
        &amp;quot;size&amp;quot;: 2048
    },
    &amp;quot;names&amp;quot;: [
        {
            &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;,
            &amp;quot;ST&amp;quot;: &amp;quot;BeiJing&amp;quot;,
            &amp;quot;L&amp;quot;: &amp;quot;BeiJing&amp;quot;,
            &amp;quot;O&amp;quot;: &amp;quot;k8s&amp;quot;,
            &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot;
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;部分字段说明：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果 hosts 字段不为空则需要指定授权使用该证书的 IP 或域名列表，由于该证书后续被 etcd 集群和 kubernetes master 集群使用，所以上面分别指定了 etcd 集群、kubernetes master 集群的主机 IP；&lt;/li&gt;
&lt;li&gt;还需要添加kube-apiserver注册的名为 kubernetes 服务的 IP（一般是 kue-apiserver 指定的 service-cluster-ip-range 网段的第一个IP，如 10.254.0.1)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2.生成 kubernetes 证书和私钥&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes
2017/09/10 07:44:27 [INFO] generate received request
2017/09/10 07:44:27 [INFO] received CSR
2017/09/10 07:44:27 [INFO] generating key: rsa-2048
2017/09/10 07:44:27 [INFO] encoded CSR
2017/09/10 07:44:27 [INFO] signed certificate with serial number 695308968867503306176219705194671734841389082714
[root@node1 ssl]# ls kubernetes*
kubernetes.csr  kubernetes-csr.json  kubernetes-key.pem  kubernetes.pem
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;或者直接在命令行上指定相关参数：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# echo &amp;apos;{&amp;quot;CN&amp;quot;:&amp;quot;kubernetes&amp;quot;,&amp;quot;hosts&amp;quot;:[&amp;quot;&amp;quot;],&amp;quot;key&amp;quot;:{&amp;quot;algo&amp;quot;:&amp;quot;rsa&amp;quot;,&amp;quot;size&amp;quot;:2048}}&amp;apos; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes -hostname=&amp;quot;127.0.0.1,172.16.7.151,172.16.7.152,172.16.7.153,172.16.206.32,10.254.0.1,kubernetes,kubernetes.default&amp;quot; - | cfssljson -bare kubernetes
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;创建-Admin-证书&quot;&gt;&lt;a href=&quot;#创建-Admin-证书&quot; class=&quot;headerlink&quot; title=&quot;创建 Admin 证书&quot;&gt;&lt;/a&gt;创建 Admin 证书&lt;/h3&gt;&lt;p&gt;1.创建 admin 证书签名请求&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# vim admin-csr.json
{
  &amp;quot;CN&amp;quot;: &amp;quot;admin&amp;quot;,
  &amp;quot;hosts&amp;quot;: [],
  &amp;quot;key&amp;quot;: {
    &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;,
    &amp;quot;size&amp;quot;: 2048
  },
  &amp;quot;names&amp;quot;: [
    {
      &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;,
      &amp;quot;ST&amp;quot;: &amp;quot;BeiJing&amp;quot;,
      &amp;quot;L&amp;quot;: &amp;quot;BeiJing&amp;quot;,
      &amp;quot;O&amp;quot;: &amp;quot;system:masters&amp;quot;,
      &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;说明：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;后续 kube-apiserver 使用 RBAC 对客户端(如 kubelet、kube-proxy、Pod)请求进行授权；&lt;/li&gt;
&lt;li&gt;kube-apiserver 预定义了一些 RBAC 使用的 RoleBindings，如 cluster-admin 将 Groupsystem:masters 与 Role cluster-admin 绑定，该 Role 授予了调用kube-apiserver 的所有 API的权限；&lt;/li&gt;
&lt;li&gt;OU 指定该证书的 Group 为 system:masters，kubelet 使用该证书访问 kube-apiserver时 ，由于证书被 CA 签名，所以认证通过，同时由于证书用户组为经过预授权的system:masters，所以被授予访问所有 API 的权限。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2.生成 admin 证书和私钥&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin
2017/09/10 20:01:05 [INFO] generate received request
2017/09/10 20:01:05 [INFO] received CSR
2017/09/10 20:01:05 [INFO] generating key: rsa-2048
2017/09/10 20:01:05 [INFO] encoded CSR
2017/09/10 20:01:05 [INFO] signed certificate with serial number 580169825175224945071583937498159721917720511011
2017/09/10 20:01:05 [WARNING] This certificate lacks a &amp;quot;hosts&amp;quot; field. This makes it unsuitable for websites. For more information see the Baseline Requirements for the Issuance and Management of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 (&amp;quot;Information Requirements&amp;quot;).
[root@node1 ssl]# ls admin*
admin.csr  admin-csr.json  admin-key.pem  admin.pem
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;创建-Kube-Proxy-证书&quot;&gt;&lt;a href=&quot;#创建-Kube-Proxy-证书&quot; class=&quot;headerlink&quot; title=&quot;创建 Kube-Proxy 证书&quot;&gt;&lt;/a&gt;创建 Kube-Proxy 证书&lt;/h3&gt;&lt;p&gt;1.创建 kube-proxy 证书签名请求&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# vim kube-proxy-csr.json
{
  &amp;quot;CN&amp;quot;: &amp;quot;system:kube-proxy&amp;quot;,
  &amp;quot;hosts&amp;quot;: [],
  &amp;quot;key&amp;quot;: {
    &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;,
    &amp;quot;size&amp;quot;: 2048
  },
  &amp;quot;names&amp;quot;: [
    {
      &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;,
      &amp;quot;ST&amp;quot;: &amp;quot;BeiJing&amp;quot;,
      &amp;quot;L&amp;quot;: &amp;quot;BeiJing&amp;quot;,
      &amp;quot;O&amp;quot;: &amp;quot;k8s&amp;quot;,
      &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;说明：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CN 指定该证书的 User 为 system:kube-proxy；&lt;/li&gt;
&lt;li&gt;kube-apiserver 预定义的 RoleBinding cluster-admin 将User system:kube-proxy 与 Rolesystem:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2.生成 kube-proxy 客户端证书和私钥&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes  kube-proxy-csr.json | cfssljson -bare kube-proxy
2017/09/10 20:07:55 [INFO] generate received request
2017/09/10 20:07:55 [INFO] received CSR
2017/09/10 20:07:55 [INFO] generating key: rsa-2048
2017/09/10 20:07:55 [INFO] encoded CSR
2017/09/10 20:07:55 [INFO] signed certificate with serial number 655306618453852718922516297333812428130766975244
2017/09/10 20:07:55 [WARNING] This certificate lacks a &amp;quot;hosts&amp;quot; field. This makes it unsuitable for websites. For more information see the Baseline Requirements for the Issuance and Management of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 (&amp;quot;Information Requirements&amp;quot;).
[root@node1 ssl]# ls kube-proxy*
kube-proxy.csr  kube-proxy-csr.json  kube-proxy-key.pem  kube-proxy.pem
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;校验证书&quot;&gt;&lt;a href=&quot;#校验证书&quot; class=&quot;headerlink&quot; title=&quot;校验证书&quot;&gt;&lt;/a&gt;校验证书&lt;/h3&gt;&lt;p&gt;以校验Kubernetes证书为例。&lt;/p&gt;
&lt;h4 id=&quot;使用openssl命令校验证书&quot;&gt;&lt;a href=&quot;#使用openssl命令校验证书&quot; class=&quot;headerlink&quot; title=&quot;使用openssl命令校验证书&quot;&gt;&lt;/a&gt;使用openssl命令校验证书&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# openssl x509 -noout -text -in kubernetes.pem 
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number:
            79:ca:bb:84:73:15:b1:db:aa:24:d7:a3:60:65:b0:55:27:a7:e8:5a
    Signature Algorithm: sha256WithRSAEncryption
        Issuer: C=CN, ST=BeiJing, L=BeiJing, O=k8s, OU=System, CN=kubernetes
        Validity
            Not Before: Sep 10 11:39:00 2017 GMT
            Not After : Sep 10 11:39:00 2018 GMT
        Subject: C=CN, ST=BeiJing, L=BeiJing, O=k8s, OU=System, CN=kubernetes
...
        X509v3 extensions:
            X509v3 Key Usage: critical
                Digital Signature, Key Encipherment
            X509v3 Extended Key Usage: 
                TLS Web Server Authentication, TLS Web Client Authentication
            X509v3 Basic Constraints: critical
                CA:FALSE
            X509v3 Subject Key Identifier: 
                79:48:C1:1B:81:DD:9C:75:04:EC:B6:35:26:5E:82:AA:2E:45:F6:C5
            X509v3 Subject Alternative Name: 
                DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster, DNS:kubernetes.default.svc.cluster.local, IP Address:127.0.0.1, IP Address:172.16.7.151, IP Address:172.16.7.152, IP Address:172.16.7.153, IP Address:172.16.206.32, IP Address:10.254.0.1
...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;确认 Issuer 字段的内容和 ca-csr.json 一致；&lt;/li&gt;
&lt;li&gt;确认 Subject 字段的内容和 kubernetes-csr.json 一致；&lt;/li&gt;
&lt;li&gt;确认 X509v3 Subject Alternative Name 字段的内容和 kubernetes-csr.json 一致；&lt;/li&gt;
&lt;li&gt;确认 X509v3 Key Usage、Extended Key Usage 字段的内容和 ca-config.json 中 kubernetesprofile 一致。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;使用-Cfssl-Certinfo-命令校验&quot;&gt;&lt;a href=&quot;#使用-Cfssl-Certinfo-命令校验&quot; class=&quot;headerlink&quot; title=&quot;使用 Cfssl-Certinfo 命令校验&quot;&gt;&lt;/a&gt;使用 Cfssl-Certinfo 命令校验&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# cfssl-certinfo -cert kubernetes.pem
{
  &amp;quot;subject&amp;quot;: {
    &amp;quot;common_name&amp;quot;: &amp;quot;kubernetes&amp;quot;,
    &amp;quot;country&amp;quot;: &amp;quot;CN&amp;quot;,
    &amp;quot;organization&amp;quot;: &amp;quot;k8s&amp;quot;,
    &amp;quot;organizational_unit&amp;quot;: &amp;quot;System&amp;quot;,
    &amp;quot;locality&amp;quot;: &amp;quot;BeiJing&amp;quot;,
    &amp;quot;province&amp;quot;: &amp;quot;BeiJing&amp;quot;,
    &amp;quot;names&amp;quot;: [
      &amp;quot;CN&amp;quot;,
      &amp;quot;BeiJing&amp;quot;,
      &amp;quot;BeiJing&amp;quot;,
      &amp;quot;k8s&amp;quot;,
      &amp;quot;System&amp;quot;,
      &amp;quot;kubernetes&amp;quot;
    ]
  },
  &amp;quot;issuer&amp;quot;: {
    &amp;quot;common_name&amp;quot;: &amp;quot;kubernetes&amp;quot;,
    &amp;quot;country&amp;quot;: &amp;quot;CN&amp;quot;,
    &amp;quot;organization&amp;quot;: &amp;quot;k8s&amp;quot;,
    &amp;quot;organizational_unit&amp;quot;: &amp;quot;System&amp;quot;,
    &amp;quot;locality&amp;quot;: &amp;quot;BeiJing&amp;quot;,
    &amp;quot;province&amp;quot;: &amp;quot;BeiJing&amp;quot;,
    &amp;quot;names&amp;quot;: [
      &amp;quot;CN&amp;quot;,
      &amp;quot;BeiJing&amp;quot;,
      &amp;quot;BeiJing&amp;quot;,
      &amp;quot;k8s&amp;quot;,
      &amp;quot;System&amp;quot;,
      &amp;quot;kubernetes&amp;quot;
    ]
  },
  &amp;quot;serial_number&amp;quot;: &amp;quot;695308968867503306176219705194671734841389082714&amp;quot;,
  &amp;quot;sans&amp;quot;: [
    &amp;quot;kubernetes&amp;quot;,
    &amp;quot;kubernetes.default&amp;quot;,
    &amp;quot;kubernetes.default.svc&amp;quot;,
    &amp;quot;kubernetes.default.svc.cluster&amp;quot;,
    &amp;quot;kubernetes.default.svc.cluster.local&amp;quot;,
    &amp;quot;127.0.0.1&amp;quot;,
    &amp;quot;172.16.7.151&amp;quot;,
    &amp;quot;172.16.7.152&amp;quot;,
    &amp;quot;172.16.7.153&amp;quot;,
    &amp;quot;172.16.206.32&amp;quot;,
    &amp;quot;10.254.0.1&amp;quot;
  ],
  &amp;quot;not_before&amp;quot;: &amp;quot;2017-09-10T11:39:00Z&amp;quot;,
  &amp;quot;not_after&amp;quot;: &amp;quot;2018-09-10T11:39:00Z&amp;quot;,
  &amp;quot;sigalg&amp;quot;: &amp;quot;SHA256WithRSA&amp;quot;,
  &amp;quot;authority_key_id&amp;quot;: &amp;quot;&amp;quot;,
  &amp;quot;subject_key_id&amp;quot;: &amp;quot;79:48:C1:1B:81:DD:9C:75:4:EC:B6:35:26:5E:82:AA:2E:45:F6:C5&amp;quot;,
...
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;分发证书&quot;&gt;&lt;a href=&quot;#分发证书&quot; class=&quot;headerlink&quot; title=&quot;分发证书&quot;&gt;&lt;/a&gt;分发证书&lt;/h3&gt;&lt;p&gt;将生成的证书和秘钥文件（后缀名为.pem）拷贝到所有机器的 /etc/kubernetes/ssl 目录下备用:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# mkdir -p /etc/kubernetes/ssl
[root@node1 ssl]# cp *.pem /etc/kubernetes/ssl
[root@node1 ssl]# scp -p *.pem root@172.16.7.152:/etc/kubernetes/ssl/
[root@node1 ssl]# scp -p *.pem root@172.16.7.153:/etc/kubernetes/ssl/
[root@node1 ssl]# scp -p *.pem root@172.16.206.32:/etc/kubernetes/ssl/
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;下载和配置-kubectl-kubecontrol-命令行工具&quot;&gt;&lt;a href=&quot;#下载和配置-kubectl-kubecontrol-命令行工具&quot; class=&quot;headerlink&quot; title=&quot;下载和配置 kubectl(kubecontrol) 命令行工具&quot;&gt;&lt;/a&gt;下载和配置 kubectl(kubecontrol) 命令行工具&lt;/h2&gt;&lt;h3 id=&quot;下载kubectl&quot;&gt;&lt;a href=&quot;#下载kubectl&quot; class=&quot;headerlink&quot; title=&quot;下载kubectl&quot;&gt;&lt;/a&gt;下载kubectl&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@node1 local]# wget https://dl.k8s.io/v1.6.0/kubernetes-client-linux-amd64.tar.gz
[root@node1 local]# tar zxf kubernetes-client-linux-amd64.tar.gz
[root@node1 local]# cp kubernetes/client/bin/kube* /usr/bin/
[root@node1 local]# chmod +x /usr/bin/kube*
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;创建-kubectl-kubeconfig-文件&quot;&gt;&lt;a href=&quot;#创建-kubectl-kubeconfig-文件&quot; class=&quot;headerlink&quot; title=&quot;创建 kubectl kubeconfig 文件&quot;&gt;&lt;/a&gt;创建 kubectl kubeconfig 文件&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@node1 local]# cd /etc/kubernetes/
[root@node1 kubernetes]# export KUBE_APISERVER=&amp;quot;https://172.16.7.151:6443&amp;quot;
# 设置集群参数
[root@node1 kubernetes]# kubectl config set-cluster kubernetes \
&amp;gt; --certificate-authority=/etc/kubernetes/ssl/ca.pem \
&amp;gt; --embed-certs=true \
&amp;gt; --server=${KUBE_APISERVER}
Cluster &amp;quot;kubernetes&amp;quot; set.
# 设置客户端认证参数
[root@node1 kubernetes]# kubectl config set-credentials admin \
&amp;gt; --client-certificate=/etc/kubernetes/ssl/admin.pem \
&amp;gt; --embed-certs=true \
&amp;gt; --client-key=/etc/kubernetes/ssl/admin-key.pem
User &amp;quot;admin&amp;quot; set.
# 设置上下文参数
[root@node1 kubernetes]# kubectl config set-context kubernetes \
&amp;gt; --cluster=kubernetes \
&amp;gt; --user=admin
Context &amp;quot;kubernetes&amp;quot; set
# 设置默认上下文
[root@node1 kubernetes]# kubectl config use-context kubernetes
Switched to context &amp;quot;kubernetes&amp;quot;.
[root@node1 kubernetes]# ls ~/.kube/config 
/root/.kube/config
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;admin.pem 证书 OU 字段值为 system:masters，kube-apiserver 预定义的 RoleBinding cluster-admin 将 Groupsystem:masters 与 Role cluster admin 绑定，该 Role 授予了调用kube-apiserver 相关 API 的权限；&lt;/li&gt;
&lt;li&gt;生成的 kubeconfig 被保存到 ~/.kube/config 文件。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;创建-kubeconfig-文件&quot;&gt;&lt;a href=&quot;#创建-kubeconfig-文件&quot; class=&quot;headerlink&quot; title=&quot;创建 kubeconfig 文件&quot;&gt;&lt;/a&gt;创建 kubeconfig 文件&lt;/h2&gt;&lt;p&gt;kubelet、kube-proxy 等 Node 机器上的进程与 Master 机器的 kube-apiserver 进程通信时需要认证和授权.&lt;br&gt;kubernetes 1.4 开始支持由 kube-apiserver 为客户端生成 TLS 证书的 TLS Bootstrapping 功能，这样就不需要为每个客户端生成证书了；该功能当前仅支持为 kubelet 生成证书。&lt;/p&gt;
&lt;h3 id=&quot;创建-TLS-Bootstrapping-Token&quot;&gt;&lt;a href=&quot;#创建-TLS-Bootstrapping-Token&quot; class=&quot;headerlink&quot; title=&quot;创建 TLS Bootstrapping Token&quot;&gt;&lt;/a&gt;创建 TLS Bootstrapping Token&lt;/h3&gt;&lt;h4 id=&quot;Token-auth-file&quot;&gt;&lt;a href=&quot;#Token-auth-file&quot; class=&quot;headerlink&quot; title=&quot;Token auth file&quot;&gt;&lt;/a&gt;Token auth file&lt;/h4&gt;&lt;p&gt;Token可以是任意的包涵128 bit的字符串，可以使用安全的随机数发生器生成。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d &amp;apos; &amp;apos;)
[root@node1 ssl]# cat &amp;gt; token.csv &amp;lt;&amp;lt;EOF
&amp;gt; ${BOOTSTRAP_TOKEN},kubelet-bootstrap,10001,&amp;quot;system:kubelet-bootstrap&amp;quot;
&amp;gt; EOF
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;将token.csv发到所有机器（Master 和 Node）的 /etc/kubernetes/ 目录。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# cp token.csv /etc/kubernetes/
[root@node1 ssl]# scp -p token.csv root@172.16.7.152:/etc/kubernetes/
[root@node1 ssl]# scp -p token.csv root@172.16.7.153:/etc/kubernetes/
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;创建-kubelet-bootstrapping-kubeconfig-文件&quot;&gt;&lt;a href=&quot;#创建-kubelet-bootstrapping-kubeconfig-文件&quot; class=&quot;headerlink&quot; title=&quot;创建 kubelet bootstrapping kubeconfig 文件&quot;&gt;&lt;/a&gt;创建 kubelet bootstrapping kubeconfig 文件&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# cd /etc/kubernetes
[root@node1 kubernetes]# export KUBE_APISERVER=&amp;quot;https://172.16.7.151:6443&amp;quot;
# 设置集群参数
[root@node1 kubernetes]# kubectl config set-cluster kubernetes \
&amp;gt; --certificate-authority=/etc/kubernetes/ssl/ca.pem \
&amp;gt; --embed-certs=true \
&amp;gt; --server=${KUBE_APISERVER} \
&amp;gt; --kubeconfig=bootstrap.kubeconfig
Cluster &amp;quot;kubernetes&amp;quot; set.
# 设置客户端认证参数
[root@node1 kubernetes]# kubectl config set-credentials kubelet-bootstrap \
&amp;gt; --token=${BOOTSTRAP_TOKEN} \
&amp;gt; --kubeconfig=bootstrap.kubeconfig
User &amp;quot;kubelet-bootstrap&amp;quot; set.
# 设置上下文参数
[root@node1 kubernetes]# kubectl config set-context default \
&amp;gt; --cluster=kubernetes \
&amp;gt; --user=kubelet-bootstrap \
&amp;gt; --kubeconfig=bootstrap.kubeconfig
Context &amp;quot;default&amp;quot; created.
# 设置默认上下文
[root@node1 kubernetes]# kubectl config use-context default --kubeconfig=bootstrap.kubeconfig
Switched to context &amp;quot;default&amp;quot;.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;–embed-certs 为 true 时表示将 certificate-authority 证书写入到生成的 bootstrap.kubeconfig 文件中；&lt;/li&gt;
&lt;li&gt;设置客户端认证参数时没有指定秘钥和证书，后续由 kube-apiserver 自动生成。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;创建-kube-proxy-kubeconfig-文件&quot;&gt;&lt;a href=&quot;#创建-kube-proxy-kubeconfig-文件&quot; class=&quot;headerlink&quot; title=&quot;创建 kube-proxy kubeconfig 文件&quot;&gt;&lt;/a&gt;创建 kube-proxy kubeconfig 文件&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@node1 kubernetes]# export KUBE_APISERVER=&amp;quot;https://172.16.7.151:6443&amp;quot;
# 设置集群参数
[root@node1 kubernetes]# kubectl config set-cluster kubernetes \
&amp;gt; --certificate-authority=/etc/kubernetes/ssl/ca.pem \
&amp;gt; --embed-certs=true \
&amp;gt; --server=${KUBE_APISERVER} \
&amp;gt; --kubeconfig=kube-proxy.kubeconfig
Cluster &amp;quot;kubernetes&amp;quot; set.
# 设置客户端认证参数
[root@node1 kubernetes]# kubectl config set-credentials kube-proxy \
&amp;gt; --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \
&amp;gt; --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \
&amp;gt; --embed-certs=true \
&amp;gt; --kubeconfig=kube-proxy.kubeconfig
User &amp;quot;kube-proxy&amp;quot; set.
# 设置上下文参数
[root@node1 kubernetes]# kubectl config set-context default \
&amp;gt; --cluster=kubernetes \
&amp;gt; --user=kube-proxy \
&amp;gt; --kubeconfig=kube-proxy.kubeconfig
Context &amp;quot;default&amp;quot; created.
# 设置默认上下文
[root@node1 kubernetes]# kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
Switched to context &amp;quot;default&amp;quot;.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;设置集群参数和客户端认证参数时 –embed-certs 都为 true，这会将 certificate-authority、client-certificate 和client-key 指向的证书文件内容写入到生成的 kube-proxy.kubeconfig 文件中；&lt;/li&gt;
&lt;li&gt;kube-proxy.pem 证书中 CN 为 system:kube-proxy，kube-apiserver 预定义的 RoleBinding cluster-admin 将Usersystem:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;分发-kubeconfig-文件&quot;&gt;&lt;a href=&quot;#分发-kubeconfig-文件&quot; class=&quot;headerlink&quot; title=&quot;分发 kubeconfig 文件&quot;&gt;&lt;/a&gt;分发 kubeconfig 文件&lt;/h3&gt;&lt;p&gt;将两个 kubeconfig 文件分发到所有 Node 机器的 /etc/kubernetes/ 目录。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 kubernetes]# scp -p bootstrap.kubeconfig root@172.16.7.152:/etc/kubernetes/
[root@node1 kubernetes]# scp -p kube-proxy.kubeconfig root@172.16.7.152:/etc/kubernetes/
[root@node1 kubernetes]# scp -p bootstrap.kubeconfig root@172.16.7.153:/etc/kubernetes/
[root@node1 kubernetes]# scp -p kube-proxy.kubeconfig root@172.16.7.153:/etc/kubernetes/
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;创建高可用-etcd-集群&quot;&gt;&lt;a href=&quot;#创建高可用-etcd-集群&quot; class=&quot;headerlink&quot; title=&quot;创建高可用 etcd 集群&quot;&gt;&lt;/a&gt;创建高可用 etcd 集群&lt;/h2&gt;&lt;p&gt;etcd 是 CoreOS 团队发起的开源项目，基于 Go 语言实现，做为一个分布式键值对存储，通过分布式锁，leader选举和写屏障(write barriers)来实现可靠的分布式协作。&lt;br&gt;kubernetes系统使用etcd存储所有数据。&lt;br&gt;CoreOS官方推荐集群规模5个为宜，我这里使用了3个节点。&lt;/p&gt;
&lt;h3 id=&quot;安装配置etcd集群&quot;&gt;&lt;a href=&quot;#安装配置etcd集群&quot; class=&quot;headerlink&quot; title=&quot;安装配置etcd集群&quot;&gt;&lt;/a&gt;安装配置etcd集群&lt;/h3&gt;&lt;p&gt;搭建etcd集群有3种方式，分别为Static, etcd Discovery, DNS Discovery。Discovery请参见&lt;a href=&quot;https://coreos.com/etcd/docs/latest/op-guide/clustering.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;官网&lt;/a&gt;。这里仅以Static方式展示一次集群搭建过程。&lt;/p&gt;
&lt;p&gt;首先请做好3个节点的时间同步。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.TLS 认证文件&lt;/strong&gt;&lt;br&gt;需要为 etcd 集群创建加密通信的 TLS 证书，这里复用以前创建的 kubernetes 证书。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# cp ca.pem kubernetes-key.pem kubernetes.pem /etc/kubernetes/ssl
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这步在之前做过，可以忽略。【注意】:kubernetes 证书的 hosts 字段列表中包含上面三台机器的 IP，否则后续证书校验会失败。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.下载二进制文件&lt;/strong&gt;&lt;br&gt;到 &lt;a href=&quot;https://github.com/coreos/etcd/releases&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/coreos/etcd/releases&lt;/a&gt; 页面下载最新版本的二进制文件，并上传到/usr/local/目录下。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 local]# tar xf etcd-v3.2.7-linux-amd64.tar
[root@node1 local]# mv etcd-v3.2.7-linux-amd64/etcd* /usr/local/bin/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;etcd集群中另外两台机器也需要如上操作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3.创建 etcd 的 systemd unit 文件&lt;/strong&gt;&lt;br&gt;配置文件模板如下，注意替换 ETCD_NAME 和 INTERNAL_IP 变量的值。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# cat etcd.service
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
EnvironmentFile=-/etc/etcd/etcd.conf
ExecStart=/usr/local/bin/etcd \
  --name ${ETCD_NAME} \
  --cert-file=/etc/kubernetes/ssl/kubernetes.pem \
  --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
  --peer-cert-file=/etc/kubernetes/ssl/kubernetes.pem \
  --peer-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
  --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \
  --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \
  --initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \
  --listen-peer-urls https://${INTERNAL_IP}:2380 \
  --listen-client-urls https://${INTERNAL_IP}:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://${INTERNAL_IP}:2379 \
  --initial-cluster-token etcd-cluster-0 \
  --initial-cluster node1=https://172.16.7.151:2380,node2=https://172.16.7.152:2380,node3=https://172.16.7.153:2380 \
  --initial-cluster-state new \
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;针对上面几个配置参数做下简单的解释：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;–name：方便理解的节点名称，默认为default，在集群中应该保持唯一，可以使用 hostname&lt;/li&gt;
&lt;li&gt;–data-dir：服务运行数据保存的路径，默认为 ${name}.etcd&lt;/li&gt;
&lt;li&gt;–snapshot-count：指定有多少事务（transaction）被提交时，触发截取快照保存到磁盘&lt;/li&gt;
&lt;li&gt;–heartbeat-interval：leader 多久发送一次心跳到 followers。默认值是 100ms&lt;/li&gt;
&lt;li&gt;–eletion-timeout：重新投票的超时时间，如果 follow 在该时间间隔没有收到心跳包，会触发重新投票，默认为 1000 ms&lt;/li&gt;
&lt;li&gt;–listen-peer-urls：和同伴通信的地址，比如 &lt;a href=&quot;http://ip:2380&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://ip:2380&lt;/a&gt;&lt;br&gt;如果有多个，使用逗号分隔。需要所有节点都能够访问，所以不要使用 localhost！&lt;/li&gt;
&lt;li&gt;–listen-client-urls：对外提供服务的地址：比如 &lt;a href=&quot;http://ip:2379,http://127.0.0.1:2379，客户端会连接到这里和&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://ip:2379,http://127.0.0.1:2379，客户端会连接到这里和&lt;/a&gt; etcd 交互&lt;/li&gt;
&lt;li&gt;–advertise-client-urls：对外公告的该节点客户端监听地址，这个值会告诉集群中其他节点&lt;/li&gt;
&lt;li&gt;–initial-advertise-peer-urls：该节点同伴监听地址，这个值会告诉集群中其他节点&lt;/li&gt;
&lt;li&gt;–initial-cluster：集群中所有节点的信息，格式为 node1=&lt;a href=&quot;http://ip1:2380,node2=http://ip2:2380,…。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://ip1:2380,node2=http://ip2:2380,…。&lt;/a&gt;&lt;br&gt;注意：这里的 node1 是节点的 –name 指定的名字；后面的 ip1:2380 是 –initial-advertise-peer-urls 指定的值&lt;/li&gt;
&lt;li&gt;–initial-cluster-state：新建集群的时候，这个值为new；假如已经存在的集群，这个值为 existing&lt;/li&gt;
&lt;li&gt;–initial-cluster-token：创建集群的token，这个值每个集群保持唯一。这样的话，如果你要重新创建集群，即使配置和之前一样，也会再次生成新的集群和节点 uuid；否则会导致多个集群之间的冲突，造成未知的错误&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所有以–init开头的配置都是在bootstrap集群的时候才会用到，后续节点的重启会被忽略。&lt;/p&gt;
&lt;p&gt;node1主机：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 local]# mkdir -p /var/lib/etcd
[root@node1 local]# cd /etc/systemd/system/
[root@node1 system]# vim etcd.service
...
ExecStart=/usr/local/bin/etcd \
  --name node1 \
...
  --initial-advertise-peer-urls https://172.16.7.151:2380 \
  --listen-peer-urls https://172.16.7.151:2380 \
  --listen-client-urls https://172.16.7.151:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://172.16.7.151:2379 \
  --initial-cluster-token etcd-cluster-0 \
...

[root@node1 system]# scp -p etcd.service root@172.16.7.152:/etc/systemd/system/
[root@node1 system]# scp -p etcd.service root@172.16.7.153:/etc/systemd/system/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;node2主机：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node2 ~]# mkdir -p /var/lib/etcd /var/lib/etcd
[root@node2 ~]# vim /etc/systemd/system/etcd.service
...
ExecStart=/usr/local/bin/etcd \
  --name node2 \
...
  --initial-advertise-peer-urls https://172.16.7.152:2380 \
  --listen-peer-urls https://172.16.7.152:2380 \
  --listen-client-urls https://172.16.7.152:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://172.16.7.152:2379 \
  --initial-cluster-token etcd-cluster-0 \
...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;node3主机：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node3 ~]# mkdir -p /var/lib/etcd /var/lib/etcd
[root@node3 ~]# vim /etc/systemd/system/etcd.service
...
ExecStart=/usr/local/bin/etcd \
  --name node2 \
...
  --initial-advertise-peer-urls https://172.16.7.153:2380 \
  --listen-peer-urls https://172.16.7.153:2380 \
  --listen-client-urls https://172.16.7.153:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://172.16.7.153:2379 \
  --initial-cluster-token etcd-cluster-0 \
...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;指定 etcd 的工作目录为 /var/lib/etcd，数据目录为 /var/lib/etcd，需在启动服务前创建这两个目录；&lt;/li&gt;
&lt;li&gt;为了保证通信安全，需要指定 etcd 的公私钥(cert-file和key-file)、Peers 通信的公私钥和 CA 证书(peer-cert-file、peer-key-file、peer-trusted-ca-file)、客户端的CA证书（trusted-ca-file）；&lt;/li&gt;
&lt;li&gt;创建 kubernetes.pem 证书时使用的 kubernetes-csr.json 文件的 hosts 字段包含所有 etcd 节点的 INTERNAL_IP，否则证书校验会出错；&lt;/li&gt;
&lt;li&gt;–initial-cluster-state 值为 new 时，–name 的参数值必须位于 –initial-cluster 列表中。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;启动-etcd-服务&quot;&gt;&lt;a href=&quot;#启动-etcd-服务&quot; class=&quot;headerlink&quot; title=&quot;启动 etcd 服务&quot;&gt;&lt;/a&gt;启动 etcd 服务&lt;/h3&gt;&lt;p&gt;集群中的节点都执行以下命令：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable etcd
# systemctl start etcd
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;验证服务&quot;&gt;&lt;a href=&quot;#验证服务&quot; class=&quot;headerlink&quot; title=&quot;验证服务&quot;&gt;&lt;/a&gt;验证服务&lt;/h3&gt;&lt;p&gt;etcdctl 是一个命令行客户端，它能提供一些简洁的命令，供用户直接跟 etcd 服务打交道，而无需基于 HTTP API 方式。这在某些情况下将很方便，例如用户对服务进行测试或者手动修改数据库内容。我们也推荐在刚接触 etcd 时通过 etcdctl 命令来熟悉相关的操作，这些操作跟 HTTP API 实际上是对应的。&lt;br&gt;在etcd集群任意一台机器上执行如下命令：&lt;/p&gt;
&lt;p&gt;1.查看集群健康状态&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 system]# etcdctl --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/kubernetes/ssl/kubernetes.pem --key-file=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints &amp;quot;https://172.16.7.151:2379&amp;quot; cluster-health            
member 31800ab6b566b2b is healthy: got healthy result from https://172.16.7.151:2379
member 9a0745d96695eec6 is healthy: got healthy result from https://172.16.7.153:2379
member e64edc68e5e81b55 is healthy: got healthy result from https://172.16.7.152:2379
cluster is healthy
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;结果最后一行为 cluster is healthy 时表示集群服务正常。&lt;/p&gt;
&lt;p&gt;2.查看集群成员，并能看出哪个是leader节点&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 system]# etcdctl --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/kubernetes/ssl/kubernetes.pem --key-file=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints &amp;quot;https://172.16.7.151:2379&amp;quot; member list
31800ab6b566b2b: name=node1 peerURLs=https://172.16.7.151:2380 clientURLs=https://172.16.7.151:2379 isLeader=false
9a0745d96695eec6: name=node3 peerURLs=https://172.16.7.153:2380 clientURLs=https://172.16.7.153:2379 isLeader=false
e64edc68e5e81b55: name=node2 peerURLs=https://172.16.7.152:2380 clientURLs=https://172.16.7.152:2379 isLeader=true
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3.删除一个节点&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 如果你想更新一个节点的IP(peerURLS)，首先你需要知道那个节点的ID
# etcdctl --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/kubernetes/ssl/kubernetes.pem --key-file=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints &amp;quot;https://172.16.7.151:2379&amp;quot; member list
31800ab6b566b2b: name=node1 peerURLs=https://172.16.7.151:2380 clientURLs=https://172.16.7.151:2379 isLeader=false
9a0745d96695eec6: name=node3 peerURLs=https://172.16.7.153:2380 clientURLs=https://172.16.7.153:2379 isLeader=false
e64edc68e5e81b55: name=node2 peerURLs=https://172.16.7.152:2380 clientURLs=https://172.16.7.152:2379 isLeader=true
# 删除一个节点
# etcdctl --endpoints &amp;quot;http://192.168.2.210:2379&amp;quot; member remove 9a0745d96695eec6
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;部署-kubernetes-master&quot;&gt;&lt;a href=&quot;#部署-kubernetes-master&quot; class=&quot;headerlink&quot; title=&quot;部署 kubernetes master&quot;&gt;&lt;/a&gt;部署 kubernetes master&lt;/h2&gt;&lt;p&gt;kubernetes master 节点包含的组件：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kube-apiserver&lt;/li&gt;
&lt;li&gt;kube-scheduler&lt;/li&gt;
&lt;li&gt;kube-controller-manager&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;目前这三个组件需要部署在同一台机器上。&lt;br&gt;kube-scheduler、kube-controller-manager 和 kube-apiserver 三者的功能紧密相关；&lt;br&gt;同时只能有一个 kube-scheduler、kube-controller-manager 进程处于工作状态，如果运行多个，则需要通过选举产生一个 leader。&lt;/p&gt;
&lt;h3 id=&quot;TLS-证书文件&quot;&gt;&lt;a href=&quot;#TLS-证书文件&quot; class=&quot;headerlink&quot; title=&quot;TLS 证书文件&quot;&gt;&lt;/a&gt;TLS 证书文件&lt;/h3&gt;&lt;p&gt;检查之前生成的证书。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 kubernetes]# ls /etc/kubernetes/ssl
admin-key.pem  admin.pem  ca-key.pem  ca.pem  kube-proxy-key.pem  kube-proxy.pem  kubernetes-key.pem  kubernetes.pem
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;下载二进制文件&quot;&gt;&lt;a href=&quot;#下载二进制文件&quot; class=&quot;headerlink&quot; title=&quot;下载二进制文件&quot;&gt;&lt;/a&gt;下载二进制文件&lt;/h3&gt;&lt;p&gt;有两种下载方式：&lt;br&gt;方式一：从 &lt;a href=&quot;https://github.com/kubernetes/kubernetes/releases&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;github release&lt;/a&gt; 页面下载发布版 tarball，解压后再执行下载脚本。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 local]# cd /opt/
[root@node1 opt]# wget https://github.com/kubernetes/kubernetes/releases/download/v1.6.0/kubernetes.tar.gz
[root@node1 opt]# tar zxf kubernetes.tar.gz 
[root@node1 opt]# cd kubernetes/
[root@node1 kubernetes]# ./cluster/get-kube-binaries.sh
Kubernetes release: v1.6.0
Server: linux/amd64  (to override, set KUBERNETES_SERVER_ARCH)
Client: linux/amd64  (autodetected)

Will download kubernetes-server-linux-amd64.tar.gz from https://storage.googleapis.com/kubernetes-release/release/v1.6.0
Will download and extract kubernetes-client-linux-amd64.tar.gz from https://storage.googleapis.com/kubernetes-release/release/v1.6.0
Is this ok? [Y]/n
y
...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;方式二：从 CHANGELOG页面 下载 client 或 server tarball 文件&lt;br&gt;server 的 tarball kubernetes-server-linux-amd64.tar.gz 已经包含了 client(kubectl) 二进制文件，所以不用单独下载kubernetes-client-linux-amd64.tar.gz文件&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;wget https://dl.k8s.io/v1.6.0/kubernetes-server-linux-amd64.tar.gz
tar -xzvf kubernetes-server-linux-amd64.tar.gz
...
cd kubernetes
tar -xzvf  kubernetes-src.tar.gz
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;将二进制文件拷贝到指定路径：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 kubernetes]# pwd
/opt/kubernetes
[root@node1 kubernetes]# cd server/
[root@node1 server]# tar zxf kubernetes-server-linux-amd64.tar.gz
[root@node1 server]# cp -r kubernetes/server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet} /usr/local/bin/
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;配置和启动-kube-apiserver&quot;&gt;&lt;a href=&quot;#配置和启动-kube-apiserver&quot; class=&quot;headerlink&quot; title=&quot;配置和启动 kube-apiserver&quot;&gt;&lt;/a&gt;配置和启动 kube-apiserver&lt;/h3&gt;&lt;h4 id=&quot;创建-kube-apiserver的service配置文件&quot;&gt;&lt;a href=&quot;#创建-kube-apiserver的service配置文件&quot; class=&quot;headerlink&quot; title=&quot;创建 kube-apiserver的service配置文件&quot;&gt;&lt;/a&gt;创建 kube-apiserver的service配置文件&lt;/h4&gt;&lt;p&gt;在/usr/lib/systemd/system/下创建kube-apiserver.service，内容如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cd /usr/lib/systemd/system/
# vim kube-apiserver.service
[Unit]
Description=Kubernetes API Service
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
After=etcd.service

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/apiserver
ExecStart=/usr/local/bin/kube-apiserver \
        $KUBE_LOGTOSTDERR \
        $KUBE_LOG_LEVEL \
        $KUBE_ETCD_SERVERS \
        $KUBE_API_ADDRESS \
        $KUBE_API_PORT \
        $KUBELET_PORT \
        $KUBE_ALLOW_PRIV \
        $KUBE_SERVICE_ADDRESSES \
        $KUBE_ADMISSION_CONTROL \
        $KUBE_API_ARGS
Restart=on-failure
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;/etc/kubernetes/config文件的内容为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/kubernetes/config
###
# kubernetes system config
#
# The following values are used to configure various aspects of all
# kubernetes services, including
#
#   kube-apiserver.service
#   kube-controller-manager.service
#   kube-scheduler.service
#   kubelet.service
#   kube-proxy.service
# logging to stderr means we get it in the systemd journal
KUBE_LOGTOSTDERR=&amp;quot;--logtostderr=true&amp;quot;

# journal message level, 0 is debug
KUBE_LOG_LEVEL=&amp;quot;--v=0&amp;quot;

# Should this cluster be allowed to run privileged docker containers
KUBE_ALLOW_PRIV=&amp;quot;--allow-privileged=true&amp;quot;

# How the controller-manager, scheduler, and proxy find the apiserver
#KUBE_MASTER=&amp;quot;--master=http://domainName:8080&amp;quot;                             
KUBE_MASTER=&amp;quot;--master=http://172.16.7.151:8080&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;该配置文件同时被kube-apiserver、kube-controller-manager、kube-scheduler、kubelet、kube-proxy使用。&lt;/p&gt;
&lt;p&gt;创建apiserver配置文件/etc/kubernetes/apiserver：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/kubernetes/apiserver
###
## kubernetes system config
##
## The following values are used to configure the kube-apiserver
##
#
## The address on the local server to listen to.
#KUBE_API_ADDRESS=&amp;quot;--insecure-bind-address=sz-pg-oam-docker-test-001.tendcloud.com&amp;quot;
KUBE_API_ADDRESS=&amp;quot;--advertise-address=172.16.7.151 --bind-address=172.16.7.151 --insecure-bind-address=172.16.7.151&amp;quot;
#
## The port on the local server to listen on.
#KUBE_API_PORT=&amp;quot;--port=8080&amp;quot;
#
## Port minions listen on
#KUBELET_PORT=&amp;quot;--kubelet-port=10250&amp;quot;
#
## Comma separated list of nodes in the etcd cluster
KUBE_ETCD_SERVERS=&amp;quot;--etcd-servers=https://172.16.7.151:2379,https://172.16.7.152:2379,https://172.16.7.153:2379&amp;quot;
#
## Address range to use for services
KUBE_SERVICE_ADDRESSES=&amp;quot;--service-cluster-ip-range=10.254.0.0/16&amp;quot;
#
## default admission control policies
KUBE_ADMISSION_CONTROL=&amp;quot;--admission-control=ServiceAccount,NamespaceLifecycle,NamespaceExists,LimitRanger,ResourceQuota&amp;quot;
#
## Add your own!
KUBE_API_ARGS=&amp;quot;--authorization-mode=RBAC --runtime-config=rbac.authorization.k8s.io/v1beta1 --kubelet-https=true --experimental-bootstrap-token-auth --token-auth-file=/etc/kubernetes/token.csv --service-node-port-range=30000-32767 --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem --client-ca-file=/etc/kubernetes/ssl/ca.pem --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem --etcd-cafile=/etc/kubernetes/ssl/ca.pem --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem --enable-swagger-ui=true --apiserver-count=3 --audit-log-maxage=30 --audit-log-maxbackup=3 --audit-log-maxsize=100 --audit-log-path=/var/lib/audit.log --event-ttl=1h&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;–authorization-mode=RBAC 指定在安全端口使用 RBAC 授权模式，拒绝未通过授权的请求；&lt;/li&gt;
&lt;li&gt;kube-scheduler、kube-controller-manager 一般和 kube-apiserver 部署在同一台机器上，它们使用非安全端口和 kube-apiserver通信;&lt;/li&gt;
&lt;li&gt;kubelet、kube-proxy、kubectl 部署在其它 Node 节点上，如果通过安全端口访问 kube-apiserver，则必须先通过 TLS 证书认证，再通过 RBAC 授权；&lt;/li&gt;
&lt;li&gt;kube-proxy、kubectl 通过在使用的证书里指定相关的 User、Group 来达到通过 RBAC 授权的目的；&lt;/li&gt;
&lt;li&gt;如果使用了 kubelet TLS Boostrap 机制，则不能再指定 –kubelet-certificate-authority、–kubelet-client-certificate 和 –kubelet-client-key 选项，否则后续 kube-apiserver 校验 kubelet 证书时出现 ”x509: certificate signed by unknown authority“ 错误；&lt;/li&gt;
&lt;li&gt;–admission-control 值必须包含 ServiceAccount；&lt;/li&gt;
&lt;li&gt;–bind-address 不能为 127.0.0.1；&lt;/li&gt;
&lt;li&gt;runtime-config配置为rbac.authorization.k8s.io/v1beta1，表示运行时的apiVersion；&lt;/li&gt;
&lt;li&gt;–service-cluster-ip-range 指定 Service Cluster IP 地址段，该地址段不能路由可达；&lt;/li&gt;
&lt;li&gt;缺省情况下 kubernetes 对象保存在 etcd /registry 路径下，可以通过 –etcd-prefix 参数进行调整。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;启动kube-apiserver&quot;&gt;&lt;a href=&quot;#启动kube-apiserver&quot; class=&quot;headerlink&quot; title=&quot;启动kube-apiserver&quot;&gt;&lt;/a&gt;启动kube-apiserver&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable kube-apiserver
# systemctl start kube-apiserver
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;启动过程中可以观察日志：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# tail -f /var/log/message
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;配置和启动-kube-controller-manager&quot;&gt;&lt;a href=&quot;#配置和启动-kube-controller-manager&quot; class=&quot;headerlink&quot; title=&quot;配置和启动 kube-controller-manager&quot;&gt;&lt;/a&gt;配置和启动 kube-controller-manager&lt;/h3&gt;&lt;h4 id=&quot;创建-kube-controller-manager-的service配置文件&quot;&gt;&lt;a href=&quot;#创建-kube-controller-manager-的service配置文件&quot; class=&quot;headerlink&quot; title=&quot;创建 kube-controller-manager 的service配置文件&quot;&gt;&lt;/a&gt;创建 kube-controller-manager 的service配置文件&lt;/h4&gt;&lt;p&gt;在/usr/lib/systemd/system/下创建kube-controller-manager.service，内容如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# pwd
/usr/lib/systemd/system
# vim kube-controller-manager.service
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/controller-manager
ExecStart=/usr/local/bin/kube-controller-manager \
            $KUBE_LOGTOSTDERR \
            $KUBE_LOG_LEVEL \
            $KUBE_MASTER \
            $KUBE_CONTROLLER_MANAGER_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;创建kube-controller-manager配置文件/etc/kubernetes/controller-manager：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/kubernetes/controller-manager
###
# The following values are used to configure the kubernetes controller-manager

# defaults from config and apiserver should be adequate

# Add your own!
KUBE_CONTROLLER_MANAGER_ARGS=&amp;quot;--address=127.0.0.1 --service-cluster-ip-range=10.254.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem  --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem --root-ca-file=/etc/kubernetes/ssl/ca.pem --leader-elect=true&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;–service-cluster-ip-range 参数指定 Cluster 中 Service 的CIDR范围，该网络在各 Node 间必须路由不可达，必须和 kube-apiserver 中的参数一致；&lt;/li&gt;
&lt;li&gt;–cluster-signing-* 指定的证书和私钥文件用来签名为 TLS BootStrap 创建的证书和私钥；&lt;/li&gt;
&lt;li&gt;–root-ca-file 用来对 kube-apiserver 证书进行校验，指定该参数后，才会在Pod 容器的 ServiceAccount 中放置该 CA 证书文件；&lt;/li&gt;
&lt;li&gt;&lt;p&gt;–address 值必须为 127.0.0.1，因为当前 kube-apiserver 期望 scheduler 和 controller-manager 在同一台机器，否则：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get componentstatuses
NAME                 STATUS      MESSAGE                                                                                        ERROR
scheduler            Unhealthy   Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: getsockopt: connection refused   
controller-manager   Healthy     ok                                                                                             
etcd-2               Unhealthy   Get http://172.20.0.113:2379/health: malformed HTTP response &amp;quot;\x15\x03\x01\x00\x02\x02&amp;quot;        
etcd-0               Healthy     {&amp;quot;health&amp;quot;: &amp;quot;true&amp;quot;}                                                                             
etcd-1               Healthy     {&amp;quot;health&amp;quot;: &amp;quot;true&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;参考：&lt;a href=&quot;https://github.com/kubernetes-incubator/bootkube/issues/64&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/kubernetes-incubator/bootkube/issues/64&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;启动-kube-controller-manager&quot;&gt;&lt;a href=&quot;#启动-kube-controller-manager&quot; class=&quot;headerlink&quot; title=&quot;启动 kube-controller-manager&quot;&gt;&lt;/a&gt;启动 kube-controller-manager&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable kube-controller-manager
# systemctl start kube-controller-manager
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;配置和启动-kube-scheduler&quot;&gt;&lt;a href=&quot;#配置和启动-kube-scheduler&quot; class=&quot;headerlink&quot; title=&quot;配置和启动 kube-scheduler&quot;&gt;&lt;/a&gt;配置和启动 kube-scheduler&lt;/h3&gt;&lt;h4 id=&quot;创建-kube-scheduler的serivce配置文件&quot;&gt;&lt;a href=&quot;#创建-kube-scheduler的serivce配置文件&quot; class=&quot;headerlink&quot; title=&quot;创建 kube-scheduler的serivce配置文件&quot;&gt;&lt;/a&gt;创建 kube-scheduler的serivce配置文件&lt;/h4&gt;&lt;p&gt;在/usr/lib/systemd/system/下创建kube-scheduler.serivce，内容如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# pwd
/usr/lib/systemd/system
# vim kube-scheduler.service
[Unit]
Description=Kubernetes Scheduler Plugin
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/scheduler
ExecStart=/usr/local/bin/kube-scheduler \
            $KUBE_LOGTOSTDERR \
            $KUBE_LOG_LEVEL \
            $KUBE_MASTER \
            $KUBE_SCHEDULER_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;创建kube-scheduler配置文件/etc/kubernetes/scheduler：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/kubernetes/scheduler
###
# kubernetes scheduler config

# default config should be adequate

# Add your own!
KUBE_SCHEDULER_ARGS=&amp;quot;--leader-elect=true --address=127.0.0.1&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;–address 值必须为 127.0.0.1，因为当前 kube-apiserver 期望 scheduler 和 controller-manager 在同一台机器。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;启动-kube-scheduler&quot;&gt;&lt;a href=&quot;#启动-kube-scheduler&quot; class=&quot;headerlink&quot; title=&quot;启动 kube-scheduler&quot;&gt;&lt;/a&gt;启动 kube-scheduler&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable kube-scheduler
# systemctl start kube-scheduler
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;验证-master-节点功能&quot;&gt;&lt;a href=&quot;#验证-master-节点功能&quot; class=&quot;headerlink&quot; title=&quot;验证 master 节点功能&quot;&gt;&lt;/a&gt;验证 master 节点功能&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;# kubectl get componentstatuses
NAME                 STATUS    MESSAGE              ERROR
scheduler            Healthy   ok                   
controller-manager   Healthy   ok                   
etcd-0               Healthy   {&amp;quot;health&amp;quot;: &amp;quot;true&amp;quot;}   
etcd-1               Healthy   {&amp;quot;health&amp;quot;: &amp;quot;true&amp;quot;}   
etcd-2               Healthy   {&amp;quot;health&amp;quot;: &amp;quot;true&amp;quot;}              
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;部署kubernetes-node节点&quot;&gt;&lt;a href=&quot;#部署kubernetes-node节点&quot; class=&quot;headerlink&quot; title=&quot;部署kubernetes node节点&quot;&gt;&lt;/a&gt;部署kubernetes node节点&lt;/h2&gt;&lt;p&gt;kubernetes node 节点包含如下组件：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Docker 1.12.6&lt;/li&gt;
&lt;li&gt;Flanneld&lt;/li&gt;
&lt;li&gt;kubelet&lt;/li&gt;
&lt;li&gt;kube-proxy&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;安装Docker&quot;&gt;&lt;a href=&quot;#安装Docker&quot; class=&quot;headerlink&quot; title=&quot;安装Docker&quot;&gt;&lt;/a&gt;安装Docker&lt;/h3&gt;&lt;p&gt;参见之前的文章《Docker镜像和容器》。&lt;/p&gt;
&lt;h3 id=&quot;安装配置Flanneld&quot;&gt;&lt;a href=&quot;#安装配置Flanneld&quot; class=&quot;headerlink&quot; title=&quot;安装配置Flanneld&quot;&gt;&lt;/a&gt;安装配置Flanneld&lt;/h3&gt;&lt;h4 id=&quot;Flannel介绍&quot;&gt;&lt;a href=&quot;#Flannel介绍&quot; class=&quot;headerlink&quot; title=&quot;Flannel介绍&quot;&gt;&lt;/a&gt;Flannel介绍&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Flannel是CoreOS团队针对Kubernetes设计的一个网络规划服务，简单来说，它的功能是让集群中的不同节点主机创建的Docker容器都具有全集群唯一的虚拟IP地址。&lt;/li&gt;
&lt;li&gt;在默认的Docker配置中，每个节点上的Docker服务会分别负责所在节点容器的IP分配。这样导致的一个问题是，不同节点上容器可能获得相同的内外IP地址。&lt;/li&gt;
&lt;li&gt;Flannel的设计目的就是为集群中的所有节点重新规划IP地址的使用规则，从而使得不同节点上的容器能够获得“同属一个内网”且”不重复的”IP地址，并让属于不同节点上的容器能够直接通过内网IP通信。&lt;/li&gt;
&lt;li&gt;Flannel实质上是一种“覆盖网络(overlay network)”，也就是将TCP数据包装在另一种网络包里面进行路由转发和通信，目前已经支持udp、vxlan、host-gw、aws-vpc、gce和alloc路由等数据转发方式，默认的节点间数据通信方式是UDP转发。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在Flannel的GitHub页面有如下的一张原理图：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数据从源容器中发出后，经由所在主机的docker0虚拟网卡转发到flannel0虚拟网卡，这是个P2P的虚拟网卡，flanneld服务监听在网卡的另外一端。（Flannel通过ETCD服务维护了一张节点间的路由表）；&lt;/li&gt;
&lt;li&gt;源主机的flanneld服务将原本的数据内容UDP封装后根据自己的路由表投递给目的节点的flanneld服务，数据到达后被解包，然后直接进入目的节点的flannel0虚拟网卡，然后被转发到目的主机的docker0虚拟网卡；&lt;/li&gt;
&lt;li&gt;最后就像本机容器通信一样由docker0路由到目标容器，这样整个数据包的传递就完成了。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;安装配置flannel&quot;&gt;&lt;a href=&quot;#安装配置flannel&quot; class=&quot;headerlink&quot; title=&quot;安装配置flannel&quot;&gt;&lt;/a&gt;安装配置flannel&lt;/h4&gt;&lt;p&gt;我这里使用yum安装，安装的版本是0.7.1。集群中的3台node都需要安装配置flannel。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# yum install -y flannel
# rpm -ql flannel
/etc/sysconfig/flanneld
/run/flannel
/usr/bin/flanneld
/usr/bin/flanneld-start
/usr/lib/systemd/system/docker.service.d/flannel.conf
/usr/lib/systemd/system/flanneld.service
/usr/lib/tmpfiles.d/flannel.conf
/usr/libexec/flannel
/usr/libexec/flannel/mk-docker-opts.sh
...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;修改flannel配置文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/sysconfig/flanneld 
# Flanneld configuration options  

# etcd url location.  Point this to the server where etcd runs
FLANNEL_ETCD_ENDPOINTS=&amp;quot;https://172.16.7.151:2379,https://172.16.7.152:2379,https://172.16.7.153:2379&amp;quot;

# etcd config key.  This is the configuration key that flannel queries
# For address range assignment
FLANNEL_ETCD_PREFIX=&amp;quot;/kube-centos/network&amp;quot;

# Any additional options that you want to pass
#FLANNEL_OPTIONS=&amp;quot;&amp;quot;
FLANNEL_OPTIONS=&amp;quot;-etcd-cafile=/etc/kubernetes/ssl/ca.pem -etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem -etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：        &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;etcd的地址FLANNEL_ETCD_ENDPOINT&lt;/li&gt;
&lt;li&gt;etcd查询的目录，包含docker的IP地址段配置。FLANNEL_ETCD_PREFIX        &lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;在etcd中初始化flannel网络数据&quot;&gt;&lt;a href=&quot;#在etcd中初始化flannel网络数据&quot; class=&quot;headerlink&quot; title=&quot;在etcd中初始化flannel网络数据&quot;&gt;&lt;/a&gt;在etcd中初始化flannel网络数据&lt;/h4&gt;&lt;p&gt;多个node上的Flanneld依赖一个etcd cluster来做集中配置服务，etcd保证了所有node上flanned所看到的配置是一致的。同时每个node上的flanned监听etcd上的数据变化，实时感知集群中node的变化。&lt;/p&gt;
&lt;p&gt;执行下面的命令为docker分配IP地址段：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# etcdctl --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/kubernetes/ssl/kubernetes.pem --key-file=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints &amp;quot;https://172.16.7.151:2379&amp;quot; mkdir /kube-centos/network
# etcdctl --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/kubernetes/ssl/kubernetes.pem --key-file=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints &amp;quot;https://172.16.7.151:2379&amp;quot; mk /kube-centos/network/config &amp;apos;{&amp;quot;Network&amp;quot;: &amp;quot;172.30.0.0/16&amp;quot;, &amp;quot;SubnetLen&amp;quot;: 24, &amp;quot;Backend&amp;quot;: { &amp;quot;Type&amp;quot;: &amp;quot;vxlan&amp;quot; }}&amp;apos;
{&amp;quot;Network&amp;quot;: &amp;quot;172.30.0.0/16&amp;quot;, &amp;quot;SubnetLen&amp;quot;: 24, &amp;quot;Backend&amp;quot;: { &amp;quot;Type&amp;quot;: &amp;quot;vxlan&amp;quot; }}
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;启动flannel&quot;&gt;&lt;a href=&quot;#启动flannel&quot; class=&quot;headerlink&quot; title=&quot;启动flannel&quot;&gt;&lt;/a&gt;启动flannel&lt;/h4&gt;&lt;p&gt;集群中的3台node都启动flannel：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl start flanneld
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;启动完成后，会在/run/flannel/目录下生成两个文件，以node1为例：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# ls /run/flannel/         
docker  subnet.env
# cd /run/flannel/
[root@node1 flannel]# cat docker 
DOCKER_OPT_BIP=&amp;quot;--bip=172.30.51.1/24&amp;quot;
DOCKER_OPT_IPMASQ=&amp;quot;--ip-masq=true&amp;quot;
DOCKER_OPT_MTU=&amp;quot;--mtu=1450&amp;quot;
DOCKER_NETWORK_OPTIONS=&amp;quot; --bip=172.30.51.1/24 --ip-masq=true --mtu=1450&amp;quot;
# cat subnet.env 
FLANNEL_NETWORK=172.30.0.0/16
FLANNEL_SUBNET=172.30.51.1/24
FLANNEL_MTU=1450
FLANNEL_IPMASQ=false
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;现在查询etcd中的内容可以看到：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# etcdctl --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/kubernetes/ssl/kubernetes.pem --key-file=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints &amp;quot;https://172.16.7.151:2379&amp;quot; ls /kube-centos/network/subnets
/kube-centos/network/subnets/172.30.51.0-24
/kube-centos/network/subnets/172.30.29.0-24
/kube-centos/network/subnets/172.30.19.0-24
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;设置docker0网桥的IP地址(集群中node节点都需要设置)：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# source /run/flannel/subnet.env
# ifconfig docker0 $FLANNEL_SUBNET
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这样docker0和flannel网桥会在同一个子网中，查看node1主机网卡：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker0: flags=4099&amp;lt;UP,BROADCAST,MULTICAST&amp;gt;  mtu 1500
        inet 172.30.51.1  netmask 255.255.255.0  broadcast 172.30.51.255
flannel.1: flags=4163&amp;lt;UP,BROADCAST,RUNNING,MULTICAST&amp;gt;  mtu 1450
        inet 172.30.51.0  netmask 255.255.255.255  broadcast 0.0.0.0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;重启docker：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;【注意】:经过测试，docker 17.06.1-ce版本重启后，docker0网桥又会被重置为172.17.0.1，docker 1.12.6版本测试是不会有问题的。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果想重新设置flannel，先停止flanneld，清理etcd里的数据，然后 ifconfig flannel.1 down，然后启动flanneld，会重新生成子网，并up flannel.1网桥设备。&lt;/p&gt;
&lt;h4 id=&quot;测试跨主机容器通信&quot;&gt;&lt;a href=&quot;#测试跨主机容器通信&quot; class=&quot;headerlink&quot; title=&quot;测试跨主机容器通信&quot;&gt;&lt;/a&gt;测试跨主机容器通信&lt;/h4&gt;&lt;p&gt;分别在node1和node2上启动一个容器，然后ping对方容器的地址：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 flannel]# docker run -i -t centos /bin/bash
[root@38be151deb71 /]# yum install net-tools -y
[root@38be151deb71 /]# ifconfig
eth0: flags=4163&amp;lt;UP,BROADCAST,RUNNING,MULTICAST&amp;gt;  mtu 1450
        inet 172.30.51.2  netmask 255.255.255.0  broadcast 0.0.0.0

[root@node2 flannel]# docker run -i -t centos /bin/bash
[root@90e85c215fda /]# yum install net-tools -y
[root@90e85c215fda /]# ifconfig
eth0: flags=4163&amp;lt;UP,BROADCAST,RUNNING,MULTICAST&amp;gt;  mtu 1450
        inet 172.30.29.2  netmask 255.255.255.0  broadcast 0.0.0.0
[root@90e85c215fda /]# ping 172.16.51.2  
PING 172.16.51.2 (172.16.51.2) 56(84) bytes of data.
64 bytes from 172.16.51.2: icmp_seq=1 ttl=254 time=1.00 ms
64 bytes from 172.16.51.2: icmp_seq=2 ttl=254 time=1.29 ms
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;补充：下载二进制包安装flannel&quot;&gt;&lt;a href=&quot;#补充：下载二进制包安装flannel&quot; class=&quot;headerlink&quot; title=&quot;补充：下载二进制包安装flannel&quot;&gt;&lt;/a&gt;补充：下载二进制包安装flannel&lt;/h4&gt;&lt;p&gt;从官网 &lt;a href=&quot;https://github.com/coreos/flannel/releases&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/coreos/flannel/releases&lt;/a&gt; 下载的flannel release 0.7.1，并将下载的文件上传到服务器的/opt/flannel/目录下。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# mkdir flannel
# cd flannel/
# tar xf flannel-v0.7.1-linux-amd64.tar  
# ls
flanneld  flannel-v0.7.1-linux-amd64.tar  mk-docker-opts.sh  README.md
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;mk-docker-opts.sh是用来Generate Docker daemon options based on flannel env file。&lt;br&gt;执行 ./mk-docker-opts.sh -i 将会生成如下两个文件环境变量文件。&lt;/p&gt;
&lt;p&gt;Flannel的&lt;a href=&quot;https://github.com/coreos/flannel/blob/master/Documentation/running.md&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;文档&lt;/a&gt;中有写Docker Integration：&lt;br&gt;Docker daemon accepts –bip argument to configure the subnet of the docker0 bridge. It also accepts –mtu to set the MTU for docker0 and veth devices that it will be creating.&lt;br&gt;Because flannel writes out the acquired subnet and MTU values into a file, the script starting Docker can source in the values and pass them to Docker daemon:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;source /run/flannel/subnet.env
docker daemon --bip=${FLANNEL_SUBNET} --mtu=${FLANNEL_MTU} &amp;amp;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Systemd users can use EnvironmentFile directive in the .service file to pull in /run/flannel/subnet.env&lt;/p&gt;
&lt;h3 id=&quot;安装和配置-kubelet&quot;&gt;&lt;a href=&quot;#安装和配置-kubelet&quot; class=&quot;headerlink&quot; title=&quot;安装和配置 kubelet&quot;&gt;&lt;/a&gt;安装和配置 kubelet&lt;/h3&gt;&lt;p&gt;kubelet 启动时向 kube-apiserver 发送 TLS bootstrapping 请求，需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper cluster 角色(role)， 然后 kubelet 才能有权限创建认证请求(certificate signing requests)：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cd /etc/kubernetes
[root@node1 kubernetes]# kubectl create clusterrolebinding kubelet-bootstrap \
&amp;gt; --clusterrole=system:node-bootstrapper \
&amp;gt; --user=kubelet-bootstrap
clusterrolebinding &amp;quot;kubelet-bootstrap&amp;quot; created  
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;【注意】：以上这步只需要在kubernetes node集群中的一台执行一次就可以了。&lt;/strong&gt;&lt;br&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;–user=kubelet-bootstrap 是在 /etc/kubernetes/token.csv 文件中指定的用户名，同时也写入了/etc/kubernetes/bootstrap.kubeconfig 文件。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;下载最新的-kubelet-和-kube-proxy-二进制文件&quot;&gt;&lt;a href=&quot;#下载最新的-kubelet-和-kube-proxy-二进制文件&quot; class=&quot;headerlink&quot; title=&quot;下载最新的 kubelet 和 kube-proxy 二进制文件&quot;&gt;&lt;/a&gt;下载最新的 kubelet 和 kube-proxy 二进制文件&lt;/h4&gt;&lt;p&gt;这个在之前安装kubernetes master时已经下载好了二进制文件，只需要复制到相应目录即可。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 kubernetes]# cd /opt/kubernetes/server/kubernetes/server/bin/
[root@node1 bin]# scp -p kubelet root@172.16.7.152:/usr/local/bin/
[root@node1 bin]# scp -p kube-proxy root@172.16.7.152:/usr/local/bin/
[root@node1 bin]# scp -p kubelet root@172.16.7.153:/usr/local/bin/
[root@node1 bin]# scp -p kube-proxy root@172.16.7.153:/usr/local/bin/
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;配置kubelet&quot;&gt;&lt;a href=&quot;#配置kubelet&quot; class=&quot;headerlink&quot; title=&quot;配置kubelet&quot;&gt;&lt;/a&gt;配置kubelet&lt;/h4&gt;&lt;p&gt;以下操作需要在集群的kubernetes node节点上都要运行，下面以node1服务器为例：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.创建 kubelet 的service配置文件：&lt;/strong&gt;&lt;br&gt;在/usr/lib/systemd/system/下创建文件kubelet.serivce：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /usr/lib/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/kubelet
ExecStart=/usr/local/bin/kubelet \
            $KUBE_LOGTOSTDERR \
            $KUBE_LOG_LEVEL \
            $KUBELET_API_SERVER \
            $KUBELET_ADDRESS \
            $KUBELET_PORT \
            $KUBELET_HOSTNAME \
            $KUBE_ALLOW_PRIV \
            $KUBELET_POD_INFRA_CONTAINER \
            $KUBELET_ARGS
Restart=on-failure

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;2.创建kubelet配置文件&lt;/strong&gt;&lt;br&gt;创建kubelet工作目录（必须创建，否则kubelet启动不了）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# mkdir /var/lib/kubelet
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;创建kubelet配置文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/kubernetes/kubelet
###
## kubernetes kubelet (minion) config
#
## The address for the info server to serve on (set to 0.0.0.0 or &amp;quot;&amp;quot; for all interfaces)
KUBELET_ADDRESS=&amp;quot;--address=172.16.7.151&amp;quot;
#
## The port for the info server to serve on
#KUBELET_PORT=&amp;quot;--port=10250&amp;quot;
#
## You may leave this blank to use the actual hostname
KUBELET_HOSTNAME=&amp;quot;--hostname-override=172.16.7.151&amp;quot;
#
## location of the api-server
KUBELET_API_SERVER=&amp;quot;--api-servers=http://172.16.7.151:8080&amp;quot;
#
## pod infrastructure container
#KUBELET_POD_INFRA_CONTAINER=&amp;quot;--pod-infra-container-image=sz-pg-oam-docker-hub-001.tendcloud.com/library/pod-infrastructure:rhel7&amp;quot;
KUBELET_POD_INFRA_CONTAINER=&amp;quot;--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure&amp;quot;
#
## Add your own!
KUBELET_ARGS=&amp;quot;--cgroup-driver=systemd --cluster-dns=10.254.0.2 --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig --require-kubeconfig --cert-dir=/etc/kubernetes/ssl --cluster-domain=cluster.local. --hairpin-mode promiscuous-bridge --serialize-image-pulls=false&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【注意】：将配置文件中的IP地址更改为你的每台node节点的IP地址（除了–api-servers=&lt;a href=&quot;http://172.16.7.151:8080这个ip地址是不用改的）。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.151:8080这个ip地址是不用改的）。&lt;/a&gt;&lt;br&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;–address 不能设置为 127.0.0.1，否则后续 Pods 访问 kubelet 的 API 接口时会失败，因为 Pods 访问的 127.0.0.1 指向自己而不是 kubelet；&lt;br&gt;如果设置了 –hostname-override 选项，则 kube-proxy 也需要设置该选项，否则会出现找不到 Node 的情况；&lt;/li&gt;
&lt;li&gt;KUBELET_POD_INFRA_CONTAINER=”–pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure”，这个是一个基础容器，每一个Pod启动的时候都会启动一个这样的容器。如果你的本地没有这个镜像，kubelet会连接外网把这个镜像下载下来。最开始的时候是在Google的registry上，因此国内因为GFW都下载不了导致Pod运行不起来。现在每个版本的Kubernetes都把这个镜像打包，你可以提前传到自己的registry上，然后再用这个参数指定。&lt;/li&gt;
&lt;li&gt;–experimental-bootstrap-kubeconfig 指向 bootstrap kubeconfig 文件，kubelet 使用该文件中的用户名和 token 向 kube-apiserver 发送 TLS Bootstrapping 请求；&lt;/li&gt;
&lt;li&gt;管理员通过了 CSR 请求后，kubelet 自动在 –cert-dir 目录创建证书和私钥文件(kubelet-client.crt 和 kubelet-client.key)，然后写入 –kubeconfig 文件；&lt;/li&gt;
&lt;li&gt;建议在 –kubeconfig 配置文件中指定 kube-apiserver 地址，如果未指定 –api-servers 选项，则必须指定 –require-kubeconfig 选项后才从配置文件中读取 kube-apiserver 的地址，否则 kubelet 启动后将找不到 kube-apiserver (日志中提示未找到 API Server），kubectl get nodes 不会返回对应的 Node 信息;&lt;/li&gt;
&lt;li&gt;–cluster-dns 指定 kubedns 的 Service IP(可以先分配，后续创建 kubedns 服务时指定该 IP)，–cluster-domain 指定域名后缀，这两个参数同时指定后才会生效。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;启动kubelet&quot;&gt;&lt;a href=&quot;#启动kubelet&quot; class=&quot;headerlink&quot; title=&quot;启动kubelet&quot;&gt;&lt;/a&gt;启动kubelet&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable kubelet
# systemctl start kubelet
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;通过-kubelet-的-TLS-证书请求&quot;&gt;&lt;a href=&quot;#通过-kubelet-的-TLS-证书请求&quot; class=&quot;headerlink&quot; title=&quot;通过 kubelet 的 TLS 证书请求&quot;&gt;&lt;/a&gt;通过 kubelet 的 TLS 证书请求&lt;/h4&gt;&lt;p&gt;kubelet 首次启动时向 kube-apiserver 发送证书签名请求，必须通过后 kubernetes 系统才会将该 Node 加入到集群。&lt;/p&gt;
&lt;p&gt;1.查看未授权的 CSR 请求&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get csr 
NAME        AGE       REQUESTOR           CONDITION
csr-fv3bj   49s       kubelet-bootstrap   Pending
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.通过 CSR 请求&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl certificate approve csr-fv3bj
certificatesigningrequest &amp;quot;csr-fv3bj&amp;quot; approved
[root@node1 kubernetes]# kubectl get csr
NAME        AGE       REQUESTOR           CONDITION
csr-fv3bj   42m       kubelet-bootstrap   Approved,Issued
# kubectl get nodes
NAME           STATUS    AGE       VERSION
172.16.7.151   Ready     18s       v1.6.0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3.查看自动生成的 kubelet kubeconfig 文件和公私钥&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 kubernetes]# ls -l /etc/kubernetes/kubelet.kubeconfig
-rw-------. 1 root root 2215 Sep 13 09:04 /etc/kubernetes/kubelet.kubeconfig
[root@node1 kubernetes]# ls -l /etc/kubernetes/ssl/kubelet*
-rw-r--r--. 1 root root 1046 Sep 13 09:04 /etc/kubernetes/ssl/kubelet-client.crt
-rw-------. 1 root root  227 Sep 13 09:02 /etc/kubernetes/ssl/kubelet-client.key
-rw-r--r--. 1 root root 1111 Sep 13 09:04 /etc/kubernetes/ssl/kubelet.crt
-rw-------. 1 root root 1675 Sep 13 09:04 /etc/kubernetes/ssl/kubelet.key
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在集群中其它的kubernetes node节点上操作完成后，查看集群kubernetes node情况如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get csr
NAME        AGE       REQUESTOR           CONDITION
csr-5n72m   3m        kubelet-bootstrap   Approved,Issued
csr-clwzj   16m       kubelet-bootstrap   Approved,Issued
csr-fv3bj   4h        kubelet-bootstrap   Approved,Issued
# kubectl get nodes
NAME           STATUS    AGE       VERSION
172.16.7.151   Ready     4h        v1.6.0
172.16.7.152   Ready     6m        v1.6.0
172.16.7.153   Ready     12s       v1.6.0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【问题】：切记每台node节点上的kubelet配置文件/etc/kubernetes/kubelet中的ip地址要改正确，否则会出现加入不了的情况。我在将node1节点的/etc/kubernetes/kubelet远程复制到node2节点上，没有修改ip，直接启动了，配置文件中写的ip地址是node1的ip地址，这就造成了node2节点并没有加入进来。采取的恢复操作是：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node2 ~]# systemctl stop kubelet
[root@node2 ~]# cd /etc/kubernetes
[root@node2 kubernetes]# rm -f kubelet.kubeconfig
[root@node2 kubernetes]# rm -rf ~/.kube/cache
# 修改/etc/kubernetes/kubelet中的ip地址
[root@node2 kubernetes]# vim /etc/kubernetes/kubelet
[root@node2 ~]# systemctl start kubelet
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这样，再次启动kubelet时，kube-apiserver才收到证书签名请求。&lt;/p&gt;
&lt;h3 id=&quot;配置-kube-proxy&quot;&gt;&lt;a href=&quot;#配置-kube-proxy&quot; class=&quot;headerlink&quot; title=&quot;配置 kube-proxy&quot;&gt;&lt;/a&gt;配置 kube-proxy&lt;/h3&gt;&lt;p&gt;上面已经把kube-proxy复制到了kubernetes node节点的/usr/local/bin/目录下了，下面开始做配置。每台kubernetes node节点都需要做如下的操作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.创建 kube-proxy 的service配置文件&lt;/strong&gt;&lt;br&gt;在/usr/lib/systemd/system/目录下创建kube-proxy.service：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /usr/lib/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/proxy
ExecStart=/usr/local/bin/kube-proxy \
            $KUBE_LOGTOSTDERR \
            $KUBE_LOG_LEVEL \
            $KUBE_MASTER \
            $KUBE_PROXY_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;2.创建kube-proxy配置文件/etc/kubernetes/proxy&lt;/strong&gt;&lt;br&gt;【注意】：需要修改每台kubernetes node的ip地址。以下以node1主机为例：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/kubernetes/proxy
###
# kubernetes proxy config

# default config should be adequate

# Add your own!
KUBE_PROXY_ARGS=&amp;quot;--bind-address=172.16.7.151 --hostname-override=172.16.7.151 --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig --cluster-cidr=10.254.0.0/16&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;–hostname-override 参数值必须与 kubelet 的值一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 iptables 规则；&lt;/li&gt;
&lt;li&gt;kube-proxy 根据 –cluster-cidr 判断集群内部和外部流量，指定 –cluster-cidr 或 –masquerade-all 选项后 kube-proxy 才会对访问 Service IP 的请求做 SNAT；&lt;/li&gt;
&lt;li&gt;–kubeconfig 指定的配置文件嵌入了 kube-apiserver 的地址、用户名、证书、秘钥等请求和认证信息；&lt;/li&gt;
&lt;li&gt;预定义的 RoleBinding cluster-admin 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;3.启动 kube-proxy&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable kube-proxy
# systemctl start kube-proxy
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;验证测试&quot;&gt;&lt;a href=&quot;#验证测试&quot; class=&quot;headerlink&quot; title=&quot;验证测试&quot;&gt;&lt;/a&gt;验证测试&lt;/h3&gt;&lt;p&gt;创建一个niginx的service试一下集群是否可用。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl run nginx --replicas=2 --labels=&amp;quot;run=load-balancer-example&amp;quot; --image=docker.io/nginx:latest --port=80
deployment &amp;quot;nginx&amp;quot; created                   
# kubectl expose deployment nginx --type=NodePort --name=example-service     
service &amp;quot;example-service&amp;quot; exposed
# kubectl describe svc example-service
Name:                   example-service
Namespace:              default
Labels:                 run=load-balancer-example
Annotations:            &amp;lt;none&amp;gt;
Selector:               run=load-balancer-example
Type:                   NodePort
IP:                     10.254.67.61
Port:                   &amp;lt;unset&amp;gt; 80/TCP
NodePort:               &amp;lt;unset&amp;gt; 32201/TCP
Endpoints:              172.30.32.2:80,172.30.87.2:80
Session Affinity:       None
Events:                 &amp;lt;none&amp;gt;

# kubectl get all
NAME                        READY     STATUS    RESTARTS   AGE
po/nginx-1931613429-nlsj1   1/1       Running   0          5m
po/nginx-1931613429-xr7zk   1/1       Running   0          5m

NAME                  CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
svc/example-service   10.254.67.61   &amp;lt;nodes&amp;gt;       80:32201/TCP   1m
svc/kubernetes        10.254.0.1     &amp;lt;none&amp;gt;        443/TCP        5h

NAME           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deploy/nginx   2         2         2            2           5m

NAME                  DESIRED   CURRENT   READY     AGE
rs/nginx-1931613429   2         2         2         5m

# curl &amp;quot;10.254.67.61:80&amp;quot; 
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&amp;lt;style&amp;gt;
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
&amp;lt;/style&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;p&amp;gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;For online documentation and support please refer to
&amp;lt;a href=&amp;quot;http://nginx.org/&amp;quot;&amp;gt;nginx.org&amp;lt;/a&amp;gt;.&amp;lt;br/&amp;gt;
Commercial support is available at
&amp;lt;a href=&amp;quot;http://nginx.com/&amp;quot;&amp;gt;nginx.com&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;&amp;lt;em&amp;gt;Thank you for using nginx.&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;浏览器输入172.16.7.151:32201或172.16.7.152:32201或者172.16.7.153:32201都可以得到nginx的页面。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;查看运行的容器（在node1和node2上分别运行了一个pod）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# docker ps
CONTAINER ID        IMAGE                                                                                     COMMAND                  CREATED             STATUS              PORTS               NAMES
7d2ef8e34e43        docker.io/nginx@sha256:fc6d2ef47e674a9ffb718b7ac361ec4e421e3a0ef2c93df79abbe4e9ffb5fa08   &amp;quot;nginx -g &amp;apos;daemon off&amp;quot;   40 minutes ago      Up 40 minutes                           k8s_nginx_nginx-1931613429-xr7zk_default_c628f12f-9912-11e7-9acc-005056b7609a_0
5bbb98fba623        registry.access.redhat.com/rhel7/pod-infrastructure                                       &amp;quot;/usr/bin/pod&amp;quot;           42 minutes ago      Up 42 minutes                           k8s_POD_nginx-1931613429-xr7zk_default_c628f12f-9912-11e7-9acc-005056b7609a_0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果想删除刚才创建的deployment：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get deployments
NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx     2         2         2            0           2m
# kubectl delete deployment nginx
deployment &amp;quot;nginx&amp;quot; deleted
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;安装和配置-kube-dns-插件&quot;&gt;&lt;a href=&quot;#安装和配置-kube-dns-插件&quot; class=&quot;headerlink&quot; title=&quot;安装和配置 kube-dns 插件&quot;&gt;&lt;/a&gt;安装和配置 kube-dns 插件&lt;/h2&gt;&lt;h3 id=&quot;kube-dns是什么&quot;&gt;&lt;a href=&quot;#kube-dns是什么&quot; class=&quot;headerlink&quot; title=&quot;kube-dns是什么&quot;&gt;&lt;/a&gt;kube-dns是什么&lt;/h3&gt;&lt;p&gt;刚才在上一步中创建了个Nginx deployment，得到了两个运行nginx服务的Pod。待Pod运行之后查看一下它们的IP，并在k8s集群内通过podIP和containerPort来访问Nginx服务。&lt;br&gt;获取Pod IP：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get pod -o yaml -l run=load-balancer-example|grep podIP 
    podIP: 172.30.32.2
    podIP: 172.30.87.2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后在Kubernetes集群的任一节点上就可以通过podIP在k8s集群内访问Nginx服务了。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# curl &amp;quot;172.30.32.2:80&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;但是这样存在几个问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每次收到获取podIP太扯了，总不能每次都要手动改程序或者配置才能访问服务吧，要怎么提前知道podIP呢？&lt;/li&gt;
&lt;li&gt;Pod在运行中可能会重建，Pod的IP地址会随着Pod的重启而变化,并 不建议直接拿Pod的IP来交互&lt;/li&gt;
&lt;li&gt;如何在多个Pod中实现负载均衡嘞？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;使用k8s Service就可以解决。Service为一组Pod(通过labels来选择)提供一个统一的入口，并为它们提供负载均衡和自动服务发现。&lt;br&gt;所以紧接着就创建了个service：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl expose deployment nginx --type=NodePort --name=example-service
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;创建之后，仍需要获取Service的Cluster-IP，再结合Port访问Nginx服务。&lt;br&gt;获取IP：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get service example-service
NAME              CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
example-service   10.254.67.61   &amp;lt;nodes&amp;gt;       80:32201/TCP   1h
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在集群内访问Service：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# curl &amp;quot;10.254.67.61:80&amp;quot; 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;而在Kubernetes cluster外面，则只能通过&lt;a href=&quot;http://node-ip:32201来访问。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://node-ip:32201来访问。&lt;/a&gt;&lt;br&gt;虽然Service解决了Pod的服务发现和负载均衡问题，但存在着类似的问题：不提前知道Service的IP，还是需要改程序或配置啊。kube-dns就是用来解决上面这个问题的。&lt;br&gt;kube-dns可以解决Service的发现问题，k8s将Service的名称当做域名注册到kube-dns中，通过Service的名称就可以访问其提供的服务。也就是说其他应用能够直接使用服务的名字，不需要关心它实际的 ip 地址，中间的转换能够自动完成。名字和 ip 之间的转换就是 DNS 系统的功能。&lt;br&gt;kubu-dns 服务不是独立的系统服务，而是一种 addon ，作为插件来安装的，不是 kubernetes 集群必须的（但是非常推荐安装）。可以把它看做运行在集群上的应用，只不过这个应用比较特殊而已。&lt;/p&gt;
&lt;h3 id=&quot;安装配置kube-dns&quot;&gt;&lt;a href=&quot;#安装配置kube-dns&quot; class=&quot;headerlink&quot; title=&quot;安装配置kube-dns&quot;&gt;&lt;/a&gt;安装配置kube-dns&lt;/h3&gt;&lt;p&gt;官方的yaml文件目录：&lt;a href=&quot;https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns。&lt;/a&gt;&lt;br&gt;kube-dns 有两种配置方式，在 1.3 之前使用 etcd + kube2sky + skydns 的方式，在 1.3 之后可以使用 kubedns + dnsmasq 的方式。&lt;br&gt;该插件直接使用kubernetes部署，实际上kube-dns插件只是运行在kube-system命名空间下的Pod，完全可以手动创建它。官方的配置文件中包含以下镜像：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.1
gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.1
gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.1
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;下载yaml文件&quot;&gt;&lt;a href=&quot;#下载yaml文件&quot; class=&quot;headerlink&quot; title=&quot;下载yaml文件&quot;&gt;&lt;/a&gt;下载yaml文件&lt;/h4&gt;&lt;p&gt;从 &lt;a href=&quot;https://github.com/rootsongjc/follow-me-install-kubernetes-cluster/tree/master/manifests/kubedns&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/rootsongjc/follow-me-install-kubernetes-cluster/tree/master/manifests/kubedns&lt;/a&gt; 下载 kubedns-cm.yaml、kubedns-sa.yaml、kubedns-controller.yaml和kubedns-svc.yaml这4个文件下来，并上传到/opt/kube-dns/目录下。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# mkdir /opt/kube-dns
# cd /opt/kube-dns/
# ls kubedns-*
kubedns-cm.yaml  kubedns-controller.yaml  kubedns-sa.yaml  kubedns-svc.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;修改kubedns-controller.yaml文件，将其中的镜像地址改为时速云的地址：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;index.tenxcloud.com/jimmy/k8s-dns-dnsmasq-nanny-amd64:1.14.1
index.tenxcloud.com/jimmy/k8s-dns-kube-dns-amd64:1.14.1
index.tenxcloud.com/jimmy/k8s-dns-sidecar-amd64:1.14.1
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;kubeDNS：提供了原来 kube2sky + etcd + skyDNS 的功能，可以单独对外提供 DNS 查询服务&lt;/li&gt;
&lt;li&gt;dnsmasq： 一个轻量级的 DNS 服务软件，可以提供 DNS 缓存功能。kubeDNS 模式下，dnsmasq 在内存中预留一块大小（默认是 1G）的地方，保存当前最常用的 DNS 查询记录，如果缓存中没有要查找的记录，它会到 kubeDNS 中查询，并把结果缓存起来。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;系统预定义的-RoleBinding&quot;&gt;&lt;a href=&quot;#系统预定义的-RoleBinding&quot; class=&quot;headerlink&quot; title=&quot;系统预定义的 RoleBinding&quot;&gt;&lt;/a&gt;系统预定义的 RoleBinding&lt;/h4&gt;&lt;p&gt;预定义的 RoleBinding system:kube-dns 将 kube-system 命名空间的 kube-dns ServiceAccount 与 system:kube-dns Role 绑定， 该 Role 具有访问 kube-apiserver DNS 相关 API 的权限。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get clusterrolebindings system:kube-dns -o yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: &amp;quot;true&amp;quot;
  creationTimestamp: 2017-09-14T00:46:08Z
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kube-dns
  resourceVersion: &amp;quot;56&amp;quot;
  selfLink: /apis/rbac.authorization.k8s.io/v1beta1/clusterrolebindingssystem%3Akube-dns
  uid: 18fa2aff-98e6-11e7-a153-005056b7609a
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kube-dns
subjects:
- kind: ServiceAccount
  name: kube-dns
  namespace: kube-system
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;kubedns-controller.yaml 中定义的 Pods 时使用了 kubedns-sa.yaml 文件定义的 kube-dns ServiceAccount，所以具有访问 kube-apiserver DNS 相关 API 的权限。&lt;/p&gt;
&lt;h4 id=&quot;配置-kube-dns-ServiceAccount&quot;&gt;&lt;a href=&quot;#配置-kube-dns-ServiceAccount&quot; class=&quot;headerlink&quot; title=&quot;配置 kube-dns ServiceAccount&quot;&gt;&lt;/a&gt;配置 kube-dns ServiceAccount&lt;/h4&gt;&lt;p&gt;无需修改。&lt;/p&gt;
&lt;h4 id=&quot;配置-kube-dns-服务&quot;&gt;&lt;a href=&quot;#配置-kube-dns-服务&quot; class=&quot;headerlink&quot; title=&quot;配置 kube-dns 服务&quot;&gt;&lt;/a&gt;配置 kube-dns 服务&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# diff kubedns-svc.yaml.base kubedns-svc.yaml
30c30
&amp;lt;   clusterIP: __PILLAR__DNS__SERVER__
---
&amp;gt;   clusterIP: 10.254.0.2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;spec.clusterIP = 10.254.0.2，即明确指定了 kube-dns Service IP，这个 IP 需要和 kubelet 的 –cluster-dns 参数值一致。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;配置-kube-dns-Deployment&quot;&gt;&lt;a href=&quot;#配置-kube-dns-Deployment&quot; class=&quot;headerlink&quot; title=&quot;配置 kube-dns Deployment&quot;&gt;&lt;/a&gt;配置 kube-dns Deployment&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# diff kubedns-controller.yaml.base kubedns-controller.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用系统已经做了 RoleBinding 的 kube-dns ServiceAccount，该账户具有访问 kube-apiserver DNS 相关 API 的权限。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;执行所有定义文件&quot;&gt;&lt;a href=&quot;#执行所有定义文件&quot; class=&quot;headerlink&quot; title=&quot;执行所有定义文件&quot;&gt;&lt;/a&gt;执行所有定义文件&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# pwd
/opt/kube-dns
# ls
kubedns-cm.yaml  kubedns-controller.yaml  kubedns-sa.yaml  kubedns-svc.yaml
# kubectl create -f .
configmap &amp;quot;kube-dns&amp;quot; created
deployment &amp;quot;kube-dns&amp;quot; created
serviceaccount &amp;quot;kube-dns&amp;quot; created
service &amp;quot;kube-dns&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在3台node节点上查看生成的kube-dns相关pod和container：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node2 ~]# docker ps
CONTAINER ID        IMAGE                                                                                                                           COMMAND                  CREATED             STATUS              PORTS               NAMES
9b1dbfde7eac        index.tenxcloud.com/jimmy/k8s-dns-sidecar-amd64@sha256:947271f3e08b1fd61c4b26478f08d3a8f10bbca90d4dec067e3b33be08066970         &amp;quot;/sidecar --v=2 --log&amp;quot;   4 hours ago         Up 4 hours                              k8s_sidecar_kube-dns-351402727-6vnsj_kube-system_efb96c05-9928-11e7-9acc-005056b7609a_0
a455dc0a9b55        index.tenxcloud.com/jimmy/k8s-dns-dnsmasq-nanny-amd64@sha256:b253876345427dbd626b145897be51d87bfd535e2cd5d7d166deb97ea37701f8   &amp;quot;/dnsmasq-nanny -v=2 &amp;quot;   4 hours ago         Up 4 hours                              k8s_dnsmasq_kube-dns-351402727-6vnsj_kube-system_efb96c05-9928-11e7-9acc-005056b7609a_0
7f18c10c8d60        index.tenxcloud.com/jimmy/k8s-dns-kube-dns-amd64@sha256:94426e872d1a4a0cf88e6c5cd928a1acbe1687871ae5fe91ed751593aa6052d3        &amp;quot;/kube-dns --domain=c&amp;quot;   4 hours ago         Up 4 hours                              k8s_kubedns_kube-dns-351402727-6vnsj_kube-system_efb96c05-9928-11e7-9acc-005056b7609a_0
a6feb213296b        registry.access.redhat.com/rhel7/pod-infrastructure                                                                             &amp;quot;/usr/bin/pod&amp;quot;           4 hours ago         Up 4 hours                              k8s_POD_kube-dns-351402727-6vnsj_kube-system_efb96c05-9928-11e7-9acc-005056b7609a_0
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;检查-kube-dns-功能&quot;&gt;&lt;a href=&quot;#检查-kube-dns-功能&quot; class=&quot;headerlink&quot; title=&quot;检查 kube-dns 功能&quot;&gt;&lt;/a&gt;检查 kube-dns 功能&lt;/h3&gt;&lt;p&gt;上面是通过 kubectl run 来启动了第一个Pod，但是并不支持所有的功能。使用kubectl run在设定很复杂的时候需要非常长的一条语句，敲半天也很容易出错，也没法保存，在碰到转义字符的时候也经常会很抓狂，所以更多场景下会使用yaml或者json文件，而使用kubectl create或者delete就可以利用这些yaml文件。通过 kubectl create -f file.yaml 来创建资源。kubectl run 并不是直接创建一个Pod，而是先创建一个Deployment资源 (replicas=1)，再由Deployment来自动创建Pod。&lt;/p&gt;
&lt;p&gt;新建一个 Deployment：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 kube-dns]# vim my-nginx.yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: my-nginx
spec:
  replicas: 2
  template:
    metadata:
      labels:
        run: my-nginx
    spec:
      containers:
      - name: my-nginx
        image: docker.io/nginx:latest                                  
        ports:
        - containerPort: 80
# kubectl create -f my-nginx.yaml
deployment &amp;quot;my-nginx&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Export 该 Deployment，生成 my-nginx 服务:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl expose deploy my-nginx 
service &amp;quot;my-nginx&amp;quot; exposed
# kubectl get services --all-namespaces |grep my-nginx
default       my-nginx          10.254.34.181   &amp;lt;none&amp;gt;        80/TCP          26s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;创建另一个 Pod，查看 /etc/resolv.conf 是否包含 kubelet 配置的 –cluster-dns 和 –cluster-domain，是否能够将服务my-nginx 解析到 Cluster IP 10.254.34.181。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 kube-dns]# vim dns-test-busybox.yaml
apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: default
spec:
  containers:
  - image: busybox
    command:
      - sleep
      - &amp;quot;3600&amp;quot;
    imagePullPolicy: IfNotPresent
    name: busybox
  restartPolicy: Always
[root@node1 kube-dns]# kubectl create -f dns-test-busybox.yaml
pod &amp;quot;busybox&amp;quot; created
[root@node1 kube-dns]# kubectl exec -ti busybox -- nslookup kubernetes.default
Server:    10.254.0.2
Address 1: 10.254.0.2 kube-dns.kube-system.svc.cluster.local

Name:      kubernetes.default
Address 1: 10.254.0.1 kubernetes.default.svc.cluster.local

kubectl exec -ti busybox -- ping my-nginx
PING my-nginx (10.254.34.181): 56 data bytes

kubectl exec -ti busybox -- ping kubernetes
PING kubernetes (10.254.0.1): 56 data bytes

kubectl exec -ti busybox -- ping kube-dns.kube-system.svc.cluster.local
PING kube-dns.kube-system.svc.cluster.local (10.254.0.2): 56 data bytes
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;从结果来看，service名称可以正常解析。&lt;br&gt;另外，使用kubernetes的时候建议不要再用docker命令操作。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Kubernetes简介&quot;&gt;&lt;a href=&quot;#Kubernetes简介&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes简介&quot;&gt;&lt;/a&gt;Kubernetes简介&lt;/h2&gt;&lt;p&gt;Kubernetes是谷歌开源的容器集群管理系统，是Google多年大规模容器管理技术Borg的 开源版本，主要功能包括:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于容器的应用部署、维护和滚动升级&lt;/li&gt;
&lt;li&gt;负载均衡和服务发现&lt;/li&gt;
&lt;li&gt;跨机器和跨地区的集群调度
    
    </summary>
    
      <category term="容器编排" scheme="http://yoursite.com/categories/%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92/"/>
    
    
      <category term="Kubernetes" scheme="http://yoursite.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>企业级Docker Registry —— Harbor搭建和使用</title>
    <link href="http://yoursite.com/2017/09/08/%E4%BC%81%E4%B8%9A%E7%BA%A7Docker-Registry-%E2%80%94%E2%80%94-Harbor%E6%90%AD%E5%BB%BA%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    <id>http://yoursite.com/2017/09/08/企业级Docker-Registry-——-Harbor搭建和使用/</id>
    <published>2017-09-08T08:38:52.000Z</published>
    <updated>2017-11-12T09:12:33.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;Harbor介绍&quot;&gt;&lt;a href=&quot;#Harbor介绍&quot; class=&quot;headerlink&quot; title=&quot;Harbor介绍&quot;&gt;&lt;/a&gt;Harbor介绍&lt;/h2&gt;&lt;p&gt;Harbor是由VMWare公司开源的容器镜像仓库。事实上，Habor是在Docker Registry上进行了相应的企业级扩展，从而获得了更加广泛的应用，这些新的企业级特性包括：管理用户界面，基于角色的访问控制 ，AD/LDAP集成以及审计日志等。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;安装部署Harbor&quot;&gt;&lt;a href=&quot;#安装部署Harbor&quot; class=&quot;headerlink&quot; title=&quot;安装部署Harbor&quot;&gt;&lt;/a&gt;安装部署Harbor&lt;/h2&gt;&lt;p&gt;官方安装文档：&lt;br&gt;&lt;a href=&quot;https://github.com/vmware/harbor/blob/master/docs/installation_guide.md&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/vmware/harbor/blob/master/docs/installation_guide.md&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;环境要求&quot;&gt;&lt;a href=&quot;#环境要求&quot; class=&quot;headerlink&quot; title=&quot;环境要求&quot;&gt;&lt;/a&gt;环境要求&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;python 2.7+&lt;/li&gt;
&lt;li&gt;docker 1.10+&lt;/li&gt;
&lt;li&gt;docker-compose 1.6.0+&lt;br&gt;Docker的安装见文章《Docker镜像和容器》和docker-compose的安装见文章《Registry私有仓库搭建及认证》。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;环境信息&quot;&gt;&lt;a href=&quot;#环境信息&quot; class=&quot;headerlink&quot; title=&quot;环境信息&quot;&gt;&lt;/a&gt;环境信息&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;操作系统版本&lt;/th&gt;
&lt;th&gt;IP地址&lt;/th&gt;
&lt;th&gt;安装软件&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;spark32&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.206.32&lt;/td&gt;
&lt;td&gt;docker-ce 17.06.1、docker-compose 1.15.0、harbor-online-installer-v1.1.2.tar&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&quot;安装部署harbor&quot;&gt;&lt;a href=&quot;#安装部署harbor&quot; class=&quot;headerlink&quot; title=&quot;安装部署harbor&quot;&gt;&lt;/a&gt;安装部署harbor&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# cd /opt/
[root@spark32 opt]# mkdir harbor
[root@spark32 opt]# cd harbor/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;下载地址：&lt;a href=&quot;https://github.com/vmware/harbor/releases&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/vmware/harbor/releases&lt;/a&gt;&lt;br&gt;由于我这里服务器可以联网，离线版本又很大，所以下载的在线安装版本的1.1.2版本，将软件上传到/opt/harbor/目录下。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 harbor]# tar xvf harbor-online-installer-v1.1.2.tar 
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;配置harbor&quot;&gt;&lt;a href=&quot;#配置harbor&quot; class=&quot;headerlink&quot; title=&quot;配置harbor&quot;&gt;&lt;/a&gt;配置harbor&lt;/h3&gt;&lt;p&gt;可配置的参数在文件harbor.cfg中。在该文件中，有两种参数，必须配置的和可选的。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;必须的：这些参数会在用户更新配置文件和执行install.sh脚本重新安装harbor起作用。你至少得设置 hostname 参数。&lt;/li&gt;
&lt;li&gt;可选的：用户可以就使用默认值，或者在启动后通过web ui进行修改。如果这些可选参数被设置在harbor.cfg文件中，它们只会在第一次启动时起作用，以后在harbor.cfg文件中修改会被忽略。也就是说在 Harbor 初次启动时，Admin Server 从 harbor.cfg 文件读取配置并记录下来。之后重新启动Harbor的过程中，只有必需的配置会从 harbor.cfg 文件读取；其他可选的配置将不再生效，需要通过 Admin Server 的管理界面来修改。&lt;br&gt;&lt;strong&gt;【注意】:如果你通过web ui设置这些参数，请在harbor启动后立即设置。尤其是，必须先设置 auth_mode 在你注册或创建任何用户之前。&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些具体参数的名字和意义详见&lt;a href=&quot;https://github.com/vmware/harbor/blob/master/docs/installation_guide.md&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/vmware/harbor/blob/master/docs/installation_guide.md&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 harbor]# cd harbor/
[root@spark32 harbor]# vim harbor.cfg
# 指定 hostname，为IP或者域名，用于登录 Web UI 界面
hostname = 172.16.206.32
# mysql 数据库 root 账户密码
db_password = wisedu123
# 邮件相关信息配置，如忘记密码发送邮件
email_server = smtp.exmail.qq.com
email_server_port = 465
email_username = 01115004@wisedu.com
email_password = zjk230640
email_from = admin &amp;lt;01115004@wisedu.com&amp;gt;
email_ssl = on
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;配置存储&quot;&gt;&lt;a href=&quot;#配置存储&quot; class=&quot;headerlink&quot; title=&quot;配置存储&quot;&gt;&lt;/a&gt;配置存储&lt;/h3&gt;&lt;p&gt;默认情况下，harbor把镜像存储在本地文件系统，在生产环境中，你可能考虑用其他的存储替代本地存储，比如S3, Openstack Swift, Ceph等等。你需要修改 common/templates/registry/config.yml 中的 storage 段。比如使用Openstack Swift，storage段的配置类似如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;storage:
  swift:
    username: admin
    password: ADMIN_PASS
    authurl: http://keystone_addr:35357/v3/auth
    tenant: admin
    domain: default
    region: regionOne
    container: docker_images
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;更详细的存储配置，见 &lt;a href=&quot;https://docs.docker.com/registry/configuration/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Registry Configuration Reference&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;完成安装和启动harbor&quot;&gt;&lt;a href=&quot;#完成安装和启动harbor&quot; class=&quot;headerlink&quot; title=&quot;完成安装和启动harbor&quot;&gt;&lt;/a&gt;完成安装和启动harbor&lt;/h3&gt;&lt;p&gt;一旦harbor.cfg和后端存储(可选的)配置完成，就可以使用install.sh脚本安装和启动harbor了。注意在线安装可能需要等待一些时间从dockerhubs下载harbor镜像。&lt;br&gt;&lt;strong&gt;【注意】：请确保主机上80和443端口没被占用。如果想修改端口，需要去修改docker-compose.yml文件&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 harbor]# pwd
/opt/harbor/harbor
[root@spark32 harbor]# ./install.sh 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我这里配置了阿里云容器加速，所以在线版安装没有问题，如果有网络问题可以选择离线包安装。&lt;/p&gt;
&lt;h2 id=&quot;访问Harbor&quot;&gt;&lt;a href=&quot;#访问Harbor&quot; class=&quot;headerlink&quot; title=&quot;访问Harbor&quot;&gt;&lt;/a&gt;访问Harbor&lt;/h2&gt;&lt;p&gt;浏览器输入&lt;a href=&quot;http://172.16.206.32&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.206.32&lt;/a&gt;&lt;br&gt;默认账号密码是admin/Harbor12345。默认是80端口，如果端口占用，我们可以去修改docker-compose.yml文件中，对应服务的端口映射。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/36.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/37.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;我们可以看到系统各个模块如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;项目：新增/删除项目，查看镜像仓库，给项目添加成员、查看操作日志、复制项目等&lt;/li&gt;
&lt;li&gt;日志：仓库各个镜像create、push、pull等操作日志&lt;/li&gt;
&lt;li&gt;系统管理 &lt;ul&gt;
&lt;li&gt;用户管理：新增/删除用户、设置管理员等&lt;/li&gt;
&lt;li&gt;复制管理：新增/删除从库目标、新建/删除/启停复制规则等&lt;/li&gt;
&lt;li&gt;配置管理：认证模式、复制、邮箱设置、系统设置等&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;其他设置 &lt;ul&gt;
&lt;li&gt;用户设置：修改用户名、邮箱、名称信息&lt;/li&gt;
&lt;li&gt;修改密码：修改用户密码&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;注意：非系统管理员用户登录，只能看到有权限的项目和日志，其他模块不可见。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;修改管理员密码&quot;&gt;&lt;a href=&quot;#修改管理员密码&quot; class=&quot;headerlink&quot; title=&quot;修改管理员密码&quot;&gt;&lt;/a&gt;修改管理员密码&lt;/h3&gt;&lt;p&gt;点击右上角的admin，点击修改密码。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/38.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;启动后相关容器&quot;&gt;&lt;a href=&quot;#启动后相关容器&quot; class=&quot;headerlink&quot; title=&quot;启动后相关容器&quot;&gt;&lt;/a&gt;启动后相关容器&lt;/h3&gt;&lt;p&gt;Harbor的所有服务组件都是在Docker中部署的。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 harbor]# ls
common  docker-compose.notary.yml  docker-compose.yml  harbor_1_1_0_template  harbor.cfg  install.sh  LICENSE  NOTICE  prepare  upgrade
[root@spark32 harbor]# docker-compose ps
       Name                     Command               State                                Ports                               
------------------------------------------------------------------------------------------------------------------------------
harbor-adminserver   /harbor/harbor_adminserver       Up                                                                       
harbor-db            docker-entrypoint.sh mysqld      Up      3306/tcp                                                         
harbor-jobservice    /harbor/harbor_jobservice        Up                                                                       
harbor-log           /bin/sh -c crond &amp;amp;&amp;amp; rm -f  ...   Up      127.0.0.1:1514-&amp;gt;514/tcp                                          
harbor-ui            /harbor/harbor_ui                Up                                                                       
nginx                nginx -g daemon off;             Up      0.0.0.0:443-&amp;gt;443/tcp, 0.0.0.0:4443-&amp;gt;4443/tcp, 0.0.0.0:80-&amp;gt;80/tcp 
registry             /entrypoint.sh serve /etc/ ...   Up      5000/tcp 
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;harbor-adminserver：用来管理系统配置，并提供了相应的 WEB 页面和 API 来供用户操作，改进了之前需用户手动修改配置文件并手动重启系统的用户体验。&lt;/li&gt;
&lt;li&gt;harbor-db : 由官方MySql镜像构成的数据库容器&lt;/li&gt;
&lt;li&gt;harbor-jobservice：是harbor的job管理模块，job在harbor里面主要是为了镜像仓库同步使用的。&lt;/li&gt;
&lt;li&gt;harbor-log : 运行着rsyslogd的容器，通过log-driver的形式收集其他容器的日志&lt;/li&gt;
&lt;li&gt;harbor-ui : 即架构中的core services, 构成此容器的代码是Harbor项目的主体&lt;/li&gt;
&lt;li&gt;nginx : 由 nginx 服务器构成的反向代理&lt;/li&gt;
&lt;li&gt;registry : 由Docker官方的开源 registry 镜像构成的容器实例&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这几个 Contianer 通过 Docker link 的形式连接在一起，在容器之间通过容器名字互相访问。对终端用户而言，只需要暴露 proxy（即Nginx）的服务端口。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;【注意】:之前的版本更新配置，需要修改harbor.cfg，然后停止并删除现有Harbor实例，再重新运行Harbor，比较繁琐。新版本的adminconsole可以使用户很方便地通过WEB界面配置认证、同步、邮件和系统等信息，修改立即生效，无需重启整个系统。&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&quot;Harbor持久化数据和日志&quot;&gt;&lt;a href=&quot;#Harbor持久化数据和日志&quot; class=&quot;headerlink&quot; title=&quot;Harbor持久化数据和日志&quot;&gt;&lt;/a&gt;Harbor持久化数据和日志&lt;/h2&gt;&lt;p&gt;默认情况下，registrys数据被持久化在宿主机的/data/目录下，甚至你删除harbor容器或者重新被创建，这部分数据也不会改变。&lt;br&gt;另外，harbor使用rsyslog来收集每一个容器日志，默认情况下，这些日志存放在宿主机的/var/log/harbor/目录下。&lt;/p&gt;
&lt;h2 id=&quot;管理Harbor的生命&quot;&gt;&lt;a href=&quot;#管理Harbor的生命&quot; class=&quot;headerlink&quot; title=&quot;管理Harbor的生命&quot;&gt;&lt;/a&gt;管理Harbor的生命&lt;/h2&gt;&lt;p&gt;可以使用docker-compose来管理harbor的启动、停止和销毁。但是注意必须切换到docker-compose.yml同级目录运行以下的命令。&lt;/p&gt;
&lt;h3 id=&quot;停止harbor&quot;&gt;&lt;a href=&quot;#停止harbor&quot; class=&quot;headerlink&quot; title=&quot;停止harbor&quot;&gt;&lt;/a&gt;停止harbor&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;# docker-compose stop
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;启动harbor&quot;&gt;&lt;a href=&quot;#启动harbor&quot; class=&quot;headerlink&quot; title=&quot;启动harbor&quot;&gt;&lt;/a&gt;启动harbor&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;# docker-compose start
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;修改配置后启动&quot;&gt;&lt;a href=&quot;#修改配置后启动&quot; class=&quot;headerlink&quot; title=&quot;修改配置后启动&quot;&gt;&lt;/a&gt;修改配置后启动&lt;/h3&gt;&lt;p&gt;先停止harbor，在修改配置文件harbor.cfg，然后运行prepare脚本应用配置，最后重新创建harbor并运行它。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# docker-compose down -v
# vim harbor.cfg
# prepare
# docker-compose up -d
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;清除harbor容器，保留镜像和数据&quot;&gt;&lt;a href=&quot;#清除harbor容器，保留镜像和数据&quot; class=&quot;headerlink&quot; title=&quot;清除harbor容器，保留镜像和数据&quot;&gt;&lt;/a&gt;清除harbor容器，保留镜像和数据&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;# docker-compose down -v
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;删除harbors数据库和镜像-用于干净的重新安装&quot;&gt;&lt;a href=&quot;#删除harbors数据库和镜像-用于干净的重新安装&quot; class=&quot;headerlink&quot; title=&quot;删除harbors数据库和镜像(用于干净的重新安装)&quot;&gt;&lt;/a&gt;删除harbors数据库和镜像(用于干净的重新安装)&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;# rm -r /data/database
# rm -r /data/registry
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;Harbor的安全机制&quot;&gt;&lt;a href=&quot;#Harbor的安全机制&quot; class=&quot;headerlink&quot; title=&quot;Harbor的安全机制&quot;&gt;&lt;/a&gt;Harbor的安全机制&lt;/h2&gt;&lt;p&gt;企业中的软件研发团队往往划分为诸多角色，如项目经理、产品经理、测试、运维等。在实际的软件开发和运维过程中，这些角色对于镜像的使用需求是不一样的。比如：开发人员需要拥有对镜像的读写(PULL/PUSH)权限以更新和改正代码；测试人员中需要读取(PULL)权限；而项目经理需要对上述的角色进行管理。&lt;br&gt;Harbor为这种需求提供了用户和成员两种管理概念。&lt;/p&gt;
&lt;h3 id=&quot;用户&quot;&gt;&lt;a href=&quot;#用户&quot; class=&quot;headerlink&quot; title=&quot;用户&quot;&gt;&lt;/a&gt;用户&lt;/h3&gt;&lt;p&gt;用户主要分两类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;管理员&lt;/li&gt;
&lt;li&gt;普通用户&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;两类用户都可以成为项目的成员。而管理员可以对用户进行管理。&lt;/p&gt;
&lt;h3 id=&quot;成员&quot;&gt;&lt;a href=&quot;#成员&quot; class=&quot;headerlink&quot; title=&quot;成员&quot;&gt;&lt;/a&gt;成员&lt;/h3&gt;&lt;p&gt;成员是对应于项目的概念，分为三类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;管理员&lt;/li&gt;
&lt;li&gt;开发者&lt;/li&gt;
&lt;li&gt;访客&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;管理员可以对开发者和访客作权限的配置和管理。测试和运维人员可以访客身份读取项目镜像，或者公共镜像库中的文件。&lt;br&gt;从项目的角度出发，项目管理员拥有最大的项目权限，如果要对用户进行禁用或限权等，可以通过修改用户在项目中的成员角色来实现，甚至将用户移除出这个项目。&lt;br&gt;下面以实际操作来演示。&lt;/p&gt;
&lt;h2 id=&quot;Harbor使用&quot;&gt;&lt;a href=&quot;#Harbor使用&quot; class=&quot;headerlink&quot; title=&quot;Harbor使用&quot;&gt;&lt;/a&gt;Harbor使用&lt;/h2&gt;&lt;p&gt;官方使用文档：&lt;a href=&quot;https://github.com/vmware/harbor/blob/master/docs/user_guide.md&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/vmware/harbor/blob/master/docs/user_guide.md&lt;/a&gt;&lt;br&gt;注意：当项目设为公开后，任何人都有此项目下镜像的读权限。命令行用户不需要“docker login”就可以拉取此项目下的镜像。所以一般需要建立私有项目。&lt;br&gt;1.登录harbor，点击“+项目”&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/39.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;2.点击左侧菜单“用户管理”，点击“+用户”&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/40.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;3.点击左侧菜单项目，选择刚才创建的项目“godseye”，在点击右侧正文中的选项卡“成员”，点击“+成员”，输入刚才创建的用户，并设置其为管理员。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/41.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;对于权限(角色)，项目管理员和开发人员可以有 push 的权限，而访客只能查看和 pull&lt;/p&gt;
&lt;p&gt;4.测试&lt;br&gt;我这里找了另外一台机器，安装了docker 1.12.6。由于这里harbor采用了默认的 http 方式连接，而 Docker 认为这是不安全的，所以在 push 之前需要调整一下 docker 配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node3 ~]# vim /etc/docker/daemon.json 
{
   &amp;quot;insecure-registries&amp;quot;: [&amp;quot;172.16.206.32&amp;quot;]
}
[root@node3 ~]# systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;登录harbor：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node3 ~]# docker login 172.16.206.32
Username: jkzhao
Password: 
Login Succeeded
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后 tag 一个 image，名称一定要标准( registryAddress[:端口]/项目/imageName[:tag] )，最后将其 push 即可&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node3 ~]# docker tag centos:centos7 172.16.206.32/godseye/centos:latest
[root@node3 ~]# docker push 172.16.206.32/godseye/centos:latest
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后到web ui上查看刚才push的镜像是否成功了：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/42.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;【补充】：如果使用的docker客户端版本比较低，比如在centos6上安装了docker 1.7.1，那么同样需要先调整docker的配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@osb30 ~]# vim /etc/sysconfig/docker
other_args=&amp;quot;--insecure-registry=172.16.206.32&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;镜像删除和空间回收&quot;&gt;&lt;a href=&quot;#镜像删除和空间回收&quot; class=&quot;headerlink&quot; title=&quot;镜像删除和空间回收&quot;&gt;&lt;/a&gt;镜像删除和空间回收&lt;/h2&gt;&lt;p&gt;Docker命令没有提供Registry镜像删除功能，日积月累，将会产生许多无用的镜像，占用大量存储空间。若要删除镜像并回收空间，需要调用docker registry API来完成，比较麻烦。Harbor提供了可视化的镜像删除界面，可以逻辑删除镜像。在维护状态下可以回收垃圾镜像的空间。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node3 ~]# docker images
REPOSITORY                           TAG                    IMAGE ID            CREATED             SIZE
java                                 openjdk-8-jre-alpine   d61ff40a5bf6        16 months ago       108.3 MB
[root@node3 ~]# docker login 172.16.206.32
Username: jkzhao
Password: 
Login Succeeded
[root@node3 ~]# docker tag java:openjdk-8-jre-alpine 172.16.206.32/godseye/jdk:openjdk-8-jre-alpine
[root@node3 ~]# docker push 172.16.206.32/godseye/jdk:openjdk-8-jre-alpine
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们先查看下宿主机上存放镜像的目录大小：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 2017-09-08]# du -sh /data/registry/docker/registry/v2/
110M    /data/registry/docker/registry/v2/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;登录harbor界面，点击godseye项目，删除刚才上传的镜像：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/43.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;但是实际上这只是逻辑删除，我们可以查看此时宿主机上存放镜像的目录大小，仍然是110M：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 2017-09-08]# du -sh /data/registry/docker/registry/v2/
110M    /data/registry/docker/registry/v2/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;此时你完全可以再次上传这个镜像，会显示这些镜像层已经存在了：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node3 ~]# docker push 172.16.206.32/godseye/jdk:openjdk-8-jre-alpine
The push refers to a repository [172.16.206.32/godseye/jdk]
2b4866cc0048: Layer already exists 
5f70bf18a086: Layer already exists 
82a47053c51a: Layer already exists 
8f01a53880b9: Layer already exists 
openjdk-8-jre-alpine: digest:sha256:56b1ffe13af2ee1c5e2c9a3d3cd8c377b5f1bc6130a87648d48ba3fffab0d5eb size: 1977
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;那么如何彻底删除这个镜像呢？&lt;/strong&gt;&lt;br&gt;1.首先去界面删除这个镜像&lt;br&gt;2.在harbor宿主机上执行如下的命令：&lt;br&gt;先找到当前的registry版本：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 harbor]# docker images vmware/registry
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
vmware/registry     2.6.1-photon        0f6c96580032        3 months ago        150MB
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;列出要删除的镜像：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 harbor]# docker run -it --name gc --rm --volumes-from registry vmware/registry:2.6.1-photon garbage-collect --dry-run /etc/registry/config.yml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;选项 –dry-run 只是在最后打印出界面删除了的但是实际上并未删除的镜像层，但是这条命令不会删除这些镜像层。&lt;br&gt;运行下面的命令删除镜像：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 harbor]# docker run -it --name gc --rm --volumes-from registry vmware/registry:2.6.1-photon garbage-collect  /etc/registry/config.yml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;再次查看存放镜像的目录大小：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 2017-09-08]# du -sh /data/registry/docker/registry/v2/                                              
70M     /data/registry/docker/registry/v2/
&lt;/code&gt;&lt;/pre&gt;</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Harbor介绍&quot;&gt;&lt;a href=&quot;#Harbor介绍&quot; class=&quot;headerlink&quot; title=&quot;Harbor介绍&quot;&gt;&lt;/a&gt;Harbor介绍&lt;/h2&gt;&lt;p&gt;Harbor是由VMWare公司开源的容器镜像仓库。事实上，Habor是在Docker Registry上进行了相应的企业级扩展，从而获得了更加广泛的应用，这些新的企业级特性包括：管理用户界面，基于角色的访问控制 ，AD/LDAP集成以及审计日志等。&lt;br&gt;
    
    </summary>
    
      <category term="容器" scheme="http://yoursite.com/categories/%E5%AE%B9%E5%99%A8/"/>
    
    
      <category term="Docker Registry" scheme="http://yoursite.com/tags/Docker-Registry/"/>
    
  </entry>
  
  <entry>
    <title>overlay实现容器跨主机通信</title>
    <link href="http://yoursite.com/2017/09/05/overlay%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B7%A8%E4%B8%BB%E6%9C%BA%E9%80%9A%E4%BF%A1/"/>
    <id>http://yoursite.com/2017/09/05/overlay实现容器跨主机通信/</id>
    <published>2017-09-05T05:42:06.000Z</published>
    <updated>2017-11-07T13:06:15.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;Docker容器跨主机通信方案&quot;&gt;&lt;a href=&quot;#Docker容器跨主机通信方案&quot; class=&quot;headerlink&quot; title=&quot;Docker容器跨主机通信方案&quot;&gt;&lt;/a&gt;Docker容器跨主机通信方案&lt;/h2&gt;&lt;p&gt;实现跨主机的容器通信有很多种方案，需要看实际的网络状况，是云上环境，私有云环境，还是混合云环境；是否有SDN对网络做特殊控制等等。网络状况不一样，适用的方案也会不一样。比如有的环境可以使用路由的方案，有的却不能使用。不考虑网络模型的话，基本是两个派别：overlay和路由方案。&lt;br&gt;Docker 1.12中把swarmkit集成到了docker中，本篇博客使用的版本是docker 1.11版本，这是我以前做的一个方案，现整理出来。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;环境信息&quot;&gt;&lt;a href=&quot;#环境信息&quot; class=&quot;headerlink&quot; title=&quot;环境信息&quot;&gt;&lt;/a&gt;环境信息&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;操作系统版本&lt;/th&gt;
&lt;th&gt;IP地址&lt;/th&gt;
&lt;th&gt;Docker版本&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;node1&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.7.151&lt;/td&gt;
&lt;td&gt;1.11.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;node2&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.7.152&lt;/td&gt;
&lt;td&gt;1.11.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&quot;升级内核&quot;&gt;&lt;a href=&quot;#升级内核&quot; class=&quot;headerlink&quot; title=&quot;升级内核&quot;&gt;&lt;/a&gt;升级内核&lt;/h2&gt;&lt;p&gt;默认内核：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# uname -r
3.10.0-229.el7.x86_64
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;1.升级内核需要使用 elrepo 的yum 源&lt;br&gt;首先我们导入 elrepo 的key&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.安装 elrepo 源&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3.在yum的ELRepo源中，mainline 为最新版本的内核&lt;br&gt;安装 ml 的内核&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# yum --enablerepo=elrepo-kernel install  kernel-ml-devel kernel-ml -y
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;4.修改内核启动顺序，默认启动的顺序应该为1，升级以后内核是往前面插入，为0&lt;br&gt;由于CentOS 7使用grub2作为引导程序 ，所以和CentOS 6有所不同，并不是修改/etc/grub.conf来修改启动项，需要如下操作：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# cat /boot/grub2/grub.cfg |grep menuentry   #查看有哪些内核选项
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/21.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# grub2-editenv list
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/22.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# grub2-set-default 0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;5.重启系统&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# shutdown -r now
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;6.查看内核版本&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# uname -r
4.5.2-1.el7.elrepo.x86_64
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;安装docker&quot;&gt;&lt;a href=&quot;#安装docker&quot; class=&quot;headerlink&quot; title=&quot;安装docker&quot;&gt;&lt;/a&gt;安装docker&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;[root@node1 ~]# vim /etc/yum.repos.d/docker.repo
[dockerrepo]
name=Docker Repository
baseurl=https://yum.dockerproject.org/repo/main/centos/$releasever/
enabled=1
gpgcheck=1
gpgkey=https://yum.dockerproject.org/gpg

[root@node1 ~]# yum install -y docker-engine
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果后面升级：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# yum update docker-engine
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;卸载：yum remove  &lt;/p&gt;
&lt;h2 id=&quot;防火墙设置和开启内核转发&quot;&gt;&lt;a href=&quot;#防火墙设置和开启内核转发&quot; class=&quot;headerlink&quot; title=&quot;防火墙设置和开启内核转发&quot;&gt;&lt;/a&gt;防火墙设置和开启内核转发&lt;/h2&gt;&lt;p&gt;停止firewalld，安装iptables-services&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# systemctl stop firewalld.service
[root@node1 ~]# systemctl disable firewalld.service
[root@node1 ~]# yum install -y iptables-services
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;修改防火墙策略：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# vim /etc/sysconfig/iptables
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/23.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;        [root@node1 ~]# systemctl start iptables.service&lt;br&gt;        [root@node1 ~]# systemctl enable iptables.service&lt;/p&gt;
&lt;p&gt;开启内核转发，在/etc/sysctl.conf中添加一行配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# vim /etc/sysctl.conf 
net.ipv4.ip_forward=1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;运行下面的命令使内核修改生效：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# sysctl -p
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;安装启动consul&quot;&gt;&lt;a href=&quot;#安装启动consul&quot; class=&quot;headerlink&quot; title=&quot;安装启动consul&quot;&gt;&lt;/a&gt;安装启动consul&lt;/h2&gt;&lt;p&gt;overlay一般需要一个全局的KV存储（sdn controller、etcd、consul）来存储各个主机节点在overlay网络中的配置信息。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# wget https://releases.hashicorp.com/consul/0.6.4/consul_0.6.4_linux_amd64.zip
# unzip -oq consul_0.6.4_linux_amd64.zip
# mv consul /usr/local/bin/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;启动consul：&lt;br&gt;host-1 Start Consul as a server in bootstrap mode:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# nohup consul agent -server -bootstrap -data-dir /tmp/consul -bind=172.16.7.151 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;host-2 Start the Consul agent:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node2 ~]# nohup consul agent -data-dir /tmp/consul -bind=172.16.7.152 &amp;amp;
[root@node2 ~]# consul join 172.16.7.151
Successfully joined cluster by contacting 1 nodes.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/24.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;启动Docker&quot;&gt;&lt;a href=&quot;#启动Docker&quot; class=&quot;headerlink&quot; title=&quot;启动Docker&quot;&gt;&lt;/a&gt;启动Docker&lt;/h2&gt;&lt;h3 id=&quot;修改docker-daemon配置&quot;&gt;&lt;a href=&quot;#修改docker-daemon配置&quot; class=&quot;headerlink&quot; title=&quot;修改docker daemon配置&quot;&gt;&lt;/a&gt;修改docker daemon配置&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;# cp /usr/lib/systemd/system/docker.service /etc/systemd/system/
# vim /etc/systemd/system/docker.service
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在ExecStart那行加上如下的选项，其中ens32是网卡名字：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--cluster-store=consul://localhost:8500 --cluster-advertise=ens32:2376
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/25.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;其中–cluster-store是指向key-value存储的地址，我这里就是consul的地址，consul里保存着整个overlay网络配置和节点信息。–cluster-advertise中是Host1和Host2互通的端口。&lt;/p&gt;
&lt;h3 id=&quot;启动Docker-1&quot;&gt;&lt;a href=&quot;#启动Docker-1&quot; class=&quot;headerlink&quot; title=&quot;启动Docker&quot;&gt;&lt;/a&gt;启动Docker&lt;/h3&gt;&lt;p&gt;执行systemctl daemon-reload使配置生效，然后执行systemctl start docker.service启动docker服务。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl start docker.service
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;加入开机自启动：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl enable docker.service
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;创建overlay-network&quot;&gt;&lt;a href=&quot;#创建overlay-network&quot; class=&quot;headerlink&quot; title=&quot;创建overlay network&quot;&gt;&lt;/a&gt;创建overlay network&lt;/h2&gt;&lt;h3 id=&quot;vxlan简介&quot;&gt;&lt;a href=&quot;#vxlan简介&quot; class=&quot;headerlink&quot; title=&quot;vxlan简介&quot;&gt;&lt;/a&gt;vxlan简介&lt;/h3&gt;&lt;p&gt;overlay network这种方式一般也是只需要三层可达，容器就能互通。overlay模式容器有独立IP，不同overlay方案之间的性能差别也是很大的。我这里采用的的vxlan技术。&lt;br&gt;vxlan(virtual Extensible LAN)虚拟可扩展局域网，是一种overlay的网络技术，使用MAC in UDP的方法进 行封装，共50字节的封装报文头。&lt;br&gt;用于对VXLAN报文进行封装/解封装，包括ARP请求报文和正常的VXLAN数据报文，在一段封装报文 后通过隧道向另一端VTEP发送封装报文，另一端VTEP接收到封装的报文解封装后根据封装的MAC地址进行转发。VTEP可由支持VXLAN的硬件设备或软件来实现。&lt;br&gt;从封装的结构上来看，VXLAN提供了将二层网络overlay在三层网络上的能力。&lt;/p&gt;
&lt;h3 id=&quot;创建overlay-network-1&quot;&gt;&lt;a href=&quot;#创建overlay-network-1&quot; class=&quot;headerlink&quot; title=&quot;创建overlay network&quot;&gt;&lt;/a&gt;创建overlay network&lt;/h3&gt;&lt;p&gt;默认情况下，docker启动后初始化3种网络，这3种都是不能删除的。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# docker network ls
NETWORK ID          NAME                DRIVER
5944745e7d6d             bridge                   bridge              
ce5d1ba0be32             host                       host                
244bb9a34016             none                      null  
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在node1主机上创建overlay network:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker network create -d overlay --subnet=10.10.10.0/24 net1
ca0c50dd3a49e028c3323024b9d6e8f837f4b76889b8d5848046ec0a5948ee2d
[root@node1 ~]# docker network ls
NETWORK ID          NAME                DRIVER
5944745e7d6d             bridge                  bridge              
ce5d1ba0be32             host                     host                
ca0c50dd3a49             net1                    overlay             
244bb9a34016             none                    null
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在其他主机上执行docker network ls，也会看到新建的这个名字叫net1的overlay网络。&lt;/p&gt;
&lt;h2 id=&quot;创建容器&quot;&gt;&lt;a href=&quot;#创建容器&quot; class=&quot;headerlink&quot; title=&quot;创建容器&quot;&gt;&lt;/a&gt;创建容器&lt;/h2&gt;&lt;p&gt;node1主机创建容器：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker run -it --net=net1 --name=contain1 --hostname=test1 --ip=10.10.10.3 --add-host test2:10.10.10.4 centos:centos7
[root@test1 /]# yum install -y iproute net-tools
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/26.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/27.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;node2上创建容器：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node2 ~]# docker run -it --net=net1 --name=contain2 --hostname=test2 --ip=10.10.10.4 --add-host test1:10.10.10.3 centos:centos7
[root@test2 /]# yum install -y iproute net-tools
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/28.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/29.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;测试容器跨主机通信&quot;&gt;&lt;a href=&quot;#测试容器跨主机通信&quot; class=&quot;headerlink&quot; title=&quot;测试容器跨主机通信&quot;&gt;&lt;/a&gt;测试容器跨主机通信&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/30.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/31.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;网络拓扑&quot;&gt;&lt;a href=&quot;#网络拓扑&quot; class=&quot;headerlink&quot; title=&quot;网络拓扑&quot;&gt;&lt;/a&gt;网络拓扑&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/32.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;容器内部有两个网络接口eth0、eth1。实际上，eth1连接到docker_gwbridge，这可以从ip就能看出。eth0即为overlay network的接口。&lt;/p&gt;
&lt;h2 id=&quot;抓包分析&quot;&gt;&lt;a href=&quot;#抓包分析&quot; class=&quot;headerlink&quot; title=&quot;抓包分析&quot;&gt;&lt;/a&gt;抓包分析&lt;/h2&gt;&lt;p&gt;在node2主机上使用tcpdump抓包，然后在windows上用wireshark分析。&lt;br&gt;1.container1容器里ping container2的ip地址：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/33.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;2.node2主机上抓包&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node2 ~]# tcpdump -i ens32 -s 0 -X -nnn -vvv -w /tmp/package.pcap
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3.把package.pcap传下来放到wireshark上分析&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/34.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/35.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;strong&gt;以在container1中ping container2，分析数据包流向：&lt;/strong&gt;&lt;br&gt;①container1(10.10.10.3)中ping container2(10.10.10.4)，根据container1的路由表，数据包可通过直连网络到达container2。于是arp请求获取container2的MAC地址(在xvlan上的arp这里不详述)，得到mac地址后，封包，从eth0发出；&lt;br&gt;②eth0桥接在net ns 1-ca0c50dd3a中的br0上，这个br0是个网桥(交换机)虚拟设备，需要将来自eth0的包转发出去，于是包转给了vxlan设备；这个可以通过arp -a看到一些端倪：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ip netns exec 1-ca0c50dd3a arp -a
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;③vxlan是个特殊设备，收到包后，由vxlan设备创建时注册的设备处理程序对包进行处理，即进行VXLAN封包（这期间会查询consul中存储的net1信息），将ICMP包整体作为UDP包的payload封装起来，并将UDP包通过宿主机的eth0发送出去。&lt;br&gt;④152宿主机收到UDP包后，发现是VXLAN包，根据VXLAN包中的相关信息（比如Vxlan Network Identifier，VNI=256)找到vxlan设备，并转给该vxlan设备处理。vxlan设备的处理程序进行解包，并将UDP中的payload取出，整体通过br0转给veth口，net1c2从eth0收到ICMP数据包，回复icmp reply。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;从这个通信过程中来看，跨主机通信过程中的步骤如下：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;容器的网络命名空间与overlay网络的网络命名空间通过一对veth pair连接起来，当容器对外通信时，veth pair起到网线的作用，将流量发送到overlay网络的网络命名空间中。 &lt;/li&gt;
&lt;li&gt;容器的veth pair对端eth2与vxlan设备通过br0这个Linux bridge桥接在一起，br0在同一宿主机上起到虚拟机交换机的作用，如果目标地址在同一宿主机上，则直接通信，如果不再则通过设置在vxlan1这个vxlan设备进行跨主机通信。 &lt;/li&gt;
&lt;li&gt;vxlan1设备上会在创建时，由docker daemon为其分配vxlan隧道ID，起到网络隔离的作用。 &lt;/li&gt;
&lt;li&gt;docker主机集群通过key/value存储共享数据，在7946端口上，相互之间通过gossip协议学习各个宿主机上运行了哪些容器。守护进程根据这些数据来在vxlan1设备上生成静态MAC转发表。 &lt;/li&gt;
&lt;li&gt;根据静态MAC转发表的设置，通过UDP端口4789，将流量转发到对端宿主机的网卡上。&lt;br&gt;根据流量包中的vxlan隧道ID，将流量转发到对端宿主机的overlay网络的网络命名空间中。 &lt;/li&gt;
&lt;li&gt;对端宿主机的overlay网络的网络命名空间中br0网桥，起到虚拟交换机的作用，将流量根据MAC地址转发到对应容器内部。&lt;/li&gt;
&lt;/ul&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Docker容器跨主机通信方案&quot;&gt;&lt;a href=&quot;#Docker容器跨主机通信方案&quot; class=&quot;headerlink&quot; title=&quot;Docker容器跨主机通信方案&quot;&gt;&lt;/a&gt;Docker容器跨主机通信方案&lt;/h2&gt;&lt;p&gt;实现跨主机的容器通信有很多种方案，需要看实际的网络状况，是云上环境，私有云环境，还是混合云环境；是否有SDN对网络做特殊控制等等。网络状况不一样，适用的方案也会不一样。比如有的环境可以使用路由的方案，有的却不能使用。不考虑网络模型的话，基本是两个派别：overlay和路由方案。&lt;br&gt;Docker 1.12中把swarmkit集成到了docker中，本篇博客使用的版本是docker 1.11版本，这是我以前做的一个方案，现整理出来。&lt;br&gt;
    
    </summary>
    
      <category term="容器" scheme="http://yoursite.com/categories/%E5%AE%B9%E5%99%A8/"/>
    
    
      <category term="Docker" scheme="http://yoursite.com/tags/Docker/"/>
    
  </entry>
  
</feed>
