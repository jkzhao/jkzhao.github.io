<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>jkzhao&#39;s blog</title>
  <subtitle>学习 总结 思考</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-09-19T01:17:15.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Zhao Jiankai</name>
    <email>jk.zhaocoder@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Kubernetes1.6集群上(开启了TLS)安装Dashboard</title>
    <link href="http://yoursite.com/2017/09/19/Kubernetes1-6%E9%9B%86%E7%BE%A4%E4%B8%8A-%E5%BC%80%E5%90%AF%E4%BA%86TLS-%E5%AE%89%E8%A3%85Dashboard/"/>
    <id>http://yoursite.com/2017/09/19/Kubernetes1-6集群上-开启了TLS-安装Dashboard/</id>
    <published>2017-09-19T01:14:24.000Z</published>
    <updated>2017-09-19T01:17:15.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;配置和安装-dashboard&quot;&gt;&lt;a href=&quot;#配置和安装-dashboard&quot; class=&quot;headerlink&quot; title=&quot;配置和安装 dashboard&quot;&gt;&lt;/a&gt;配置和安装 dashboard&lt;/h2&gt;&lt;p&gt;这是接着上一篇《二进制方式部署Kubernetes 1.6.0集群(开启TLS)》写的。&lt;br&gt;Kubernetes Dashboard is a general purpose, web-based UI for Kubernetes clusters. It allows users to manage applications running in the cluster and troubleshoot them, as well as manage the cluster itself.&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;配置dashboard&quot;&gt;&lt;a href=&quot;#配置dashboard&quot; class=&quot;headerlink&quot; title=&quot;配置dashboard&quot;&gt;&lt;/a&gt;配置dashboard&lt;/h3&gt;&lt;p&gt;官方文件目录：&lt;a href=&quot;https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dashboard&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dashboard&lt;/a&gt;&lt;br&gt;我们使用的文件:&lt;br&gt;从 &lt;a href=&quot;https://github.com/rootsongjc/follow-me-install-kubernetes-cluster/tree/master/manifests/dashboard&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/rootsongjc/follow-me-install-kubernetes-cluster/tree/master/manifests/dashboard&lt;/a&gt; 下载3个文件下来，并上传到/opt/kube-dashboard/目录下。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 opt]# mkdir kube-dashboard
[root@node1 opt]# cd kube-dashboard/
[root@node1 kube-dashboard]# ls
dashboard-controller.yaml  dashboard-rbac.yaml  dashboard-service.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;修改dashboard-controller.yaml文件，将里面的image改为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;index.tenxcloud.com/jimmy/kubernetes-dashboard-amd64:v1.6.0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;由于 kube-apiserver 启用了 RBAC 授权，而官方源码目录的 dashboard-controller.yaml 没有定义授权的 ServiceAccount，所以后续访问 kube-apiserver 的 API 时会被拒绝，web中提示：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Forbidden (403)

User &amp;quot;system:serviceaccount:kube-system:default&amp;quot; cannot list jobs.batch in the namespace &amp;quot;default&amp;quot;. (get jobs.batch)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;因此，增加了一个dashboard-rbac.yaml文件，定义一个名为 dashboard 的 ServiceAccount，然后将它和 Cluster Role view 绑定。&lt;/p&gt;
&lt;h3 id=&quot;执行所有定义的文件&quot;&gt;&lt;a href=&quot;#执行所有定义的文件&quot; class=&quot;headerlink&quot; title=&quot;执行所有定义的文件&quot;&gt;&lt;/a&gt;执行所有定义的文件&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;# pwd
/opt/kube-dashboard
# ls
dashboard-controller.yaml  dashboard-rbac.yaml  dashboard-service.yaml
# kubectl create -f .
deployment &amp;quot;kubernetes-dashboard&amp;quot; created
serviceaccount &amp;quot;dashboard&amp;quot; created
clusterrolebinding &amp;quot;dashboard&amp;quot; created
service &amp;quot;kubernetes-dashboard&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;检查执行结果&quot;&gt;&lt;a href=&quot;#检查执行结果&quot; class=&quot;headerlink&quot; title=&quot;检查执行结果&quot;&gt;&lt;/a&gt;检查执行结果&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;1.查看分配的 NodePort&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get services kubernetes-dashboard -n kube-system
NAME                   CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubernetes-dashboard   10.254.207.77   &amp;lt;nodes&amp;gt;       80:32281/TCP   41s
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;NodePort 32281映射到 dashboard pod 80端口；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;2.检查 controller&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get deployment kubernetes-dashboard  -n kube-system
NAME                   DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kubernetes-dashboard   1         1         1            1           13m
# kubectl get pods  -n kube-system | grep dashboard
kubernetes-dashboard-2888692679-tv54g   1/1       Running   0          13m
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;访问dashboard&quot;&gt;&lt;a href=&quot;#访问dashboard&quot; class=&quot;headerlink&quot; title=&quot;访问dashboard&quot;&gt;&lt;/a&gt;访问dashboard&lt;/h3&gt;&lt;p&gt;有以下三种方式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kubernetes-dashboard 服务暴露了 NodePort，可以使用 &lt;a href=&quot;http://NodeIP:nodePort&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://NodeIP:nodePort&lt;/a&gt; 地址访问 dashboard；&lt;/li&gt;
&lt;li&gt;通过 kube-apiserver 访问 dashboard（https 6443端口和http 8080端口方式）；&lt;/li&gt;
&lt;li&gt;通过 kubectl proxy 访问 dashboard&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;使用-http-NodeIP-nodePort-地址访问-dashboard&quot;&gt;&lt;a href=&quot;#使用-http-NodeIP-nodePort-地址访问-dashboard&quot; class=&quot;headerlink&quot; title=&quot;使用 http://NodeIP:nodePort 地址访问 dashboard&quot;&gt;&lt;/a&gt;使用 &lt;a href=&quot;http://NodeIP:nodePort&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://NodeIP:nodePort&lt;/a&gt; 地址访问 dashboard&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# kubectl get services kubernetes-dashboard -n kube-system
NAME                   CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubernetes-dashboard   10.254.207.77   &amp;lt;nodes&amp;gt;       80:32281/TCP   41s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后检查出这个pod是运行在集群中哪个服务器上的，我这里是检查是运行在node1节点上的，所以浏览器输入&lt;a href=&quot;http://172.16.7.151:32281/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.151:32281/&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/3.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h4 id=&quot;通过-kubectl-proxy-访问-dashboard&quot;&gt;&lt;a href=&quot;#通过-kubectl-proxy-访问-dashboard&quot; class=&quot;headerlink&quot; title=&quot;通过 kubectl proxy 访问 dashboard&quot;&gt;&lt;/a&gt;通过 kubectl proxy 访问 dashboard&lt;/h4&gt;&lt;p&gt;1.启动代理&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 kube-dashboard]# kubectl proxy --address=&amp;apos;172.16.7.151&amp;apos; --port=8086 --accept-hosts=&amp;apos;^*$&amp;apos;         
Starting to serve on 172.16.7.151:8086
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;需要指定 –accept-hosts 选项，否则浏览器访问 dashboard 页面时提示 “Unauthorized”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2.访问&lt;br&gt;浏览器访问 URL：&lt;a href=&quot;http://172.16.7.151:8086/ui&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.151:8086/ui&lt;/a&gt; 自动跳转到：&lt;a href=&quot;http://172.16.7.151:8086/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard/#/workload?namespace=default&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.151:8086/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard/#/workload?namespace=default&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/3.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h4 id=&quot;通过-kube-apiserver-访问dashboard&quot;&gt;&lt;a href=&quot;#通过-kube-apiserver-访问dashboard&quot; class=&quot;headerlink&quot; title=&quot;通过 kube-apiserver 访问dashboard&quot;&gt;&lt;/a&gt;通过 kube-apiserver 访问dashboard&lt;/h4&gt;&lt;p&gt;1.获取集群服务地址列表&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# kubectl cluster-info
Kubernetes master is running at https://172.16.7.151:6443
KubeDNS is running at https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/kube-dns
kubernetes-dashboard is running at https://172.16.7.151:6443/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard

To further debug and diagnose cluster problems, use &amp;apos;kubectl cluster-info dump&amp;apos;.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.导入证书&lt;br&gt;将生成的admin.pem证书转换格式。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# cd /etc/kubernetes/ssl/
[root@node1 ~]# openssl pkcs12 -export -in admin.pem  -out admin.p12 -inkey admin-key.pem
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;将生成的admin.p12证书导入的你的电脑，导出的时候记住你设置的密码，导入的时候还要用到。&lt;br&gt;如果你不想使用https的话，可以直接访问insecure port 8080端口:&lt;a href=&quot;http://172.16.7.151:8080/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.151:8080/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/3.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;由于缺少 Heapster 插件，当前 dashboard 不能展示 Pod、Nodes 的 CPU、内存等 metric 图形。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;配置和安装-dashboard&quot;&gt;&lt;a href=&quot;#配置和安装-dashboard&quot; class=&quot;headerlink&quot; title=&quot;配置和安装 dashboard&quot;&gt;&lt;/a&gt;配置和安装 dashboard&lt;/h2&gt;&lt;p&gt;这是接着上一篇《二进制方式部署Kubernetes 1.6.0集群(开启TLS)》写的。&lt;br&gt;Kubernetes Dashboard is a general purpose, web-based UI for Kubernetes clusters. It allows users to manage applications running in the cluster and troubleshoot them, as well as manage the cluster itself.&lt;br&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="http://yoursite.com/categories/Kubernetes/"/>
    
    
      <category term="容器编排 Docker Kubernetes" scheme="http://yoursite.com/tags/%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92-Docker-Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>二进制方式部署Kubernetes 1.6.0集群(开启TLS)</title>
    <link href="http://yoursite.com/2017/09/15/%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2Kubernetes-1-6-0%E9%9B%86%E7%BE%A4-%E5%BC%80%E5%90%AFTLS/"/>
    <id>http://yoursite.com/2017/09/15/二进制方式部署Kubernetes-1-6-0集群-开启TLS/</id>
    <published>2017-09-15T06:06:09.000Z</published>
    <updated>2017-09-21T01:48:27.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;Kubernetes简介&quot;&gt;&lt;a href=&quot;#Kubernetes简介&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes简介&quot;&gt;&lt;/a&gt;Kubernetes简介&lt;/h2&gt;&lt;p&gt;Kubernetes是谷歌开源的容器集群管理系统，是Google多年大规模容器管理技术Borg的 开源版本,主要功能包括:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于容器的应用部署、维护和滚动升级&lt;/li&gt;
&lt;li&gt;负载均衡和服务发现&lt;/li&gt;
&lt;li&gt;跨机器和跨地区的集群调度&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;自动伸缩&lt;/li&gt;
&lt;li&gt;无状态服务和有状态服务 &lt;/li&gt;
&lt;li&gt;广泛的Volume支持 &lt;/li&gt;
&lt;li&gt;插件机制保证扩展性&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;之前尝试使用kubeadm自动化部署集群，使用yum去安装kubeadm等工具，但是不翻墙的情况下，这种方式在国内几乎是不可能安装成功的。于是改为采用二进制文件部署Kubernetes集群，同时开启了集群的TLS安全认证。本篇实践是参照opsnull的文章&lt;a href=&quot;https://mp.weixin.qq.com/s/bvCZUl6LQhlqDVv_TNeDFg&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;创建 kubernetes 各组件 TLS 加密通信的证书和秘钥&lt;/a&gt;，结合公司的实际情况进行的。&lt;/p&gt;
&lt;h2 id=&quot;环境信息&quot;&gt;&lt;a href=&quot;#环境信息&quot; class=&quot;headerlink&quot; title=&quot;环境信息&quot;&gt;&lt;/a&gt;环境信息&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;操作系统版本&lt;/th&gt;
&lt;th&gt;IP地址&lt;/th&gt;
&lt;th&gt;角色&lt;/th&gt;
&lt;th&gt;安装软件&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;node1&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.7.151&lt;/td&gt;
&lt;td&gt;Kubernetes Master、Node&lt;/td&gt;
&lt;td&gt;etcd、kube-apiserver、kube-scheduler、kube-controller-manager、kubelet、kube-proxy、etcd 3.2.7、flannel 0.7.1、docker 1.12.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;node2&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.7.152&lt;/td&gt;
&lt;td&gt;Kubernetes Node&lt;/td&gt;
&lt;td&gt;kubelet、kube-proxy、flannel 0.7.1、etcd 3.2.7、docker 1.12.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;node3&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.7.153&lt;/td&gt;
&lt;td&gt;Kubernetes Node&lt;/td&gt;
&lt;td&gt;kubelet、kube-proxy、flannel0.7.1、etcd 3.2.7、docker 1.12.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;spark32&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.206.32&lt;/td&gt;
&lt;td&gt;Harbor&lt;/td&gt;
&lt;td&gt;docker-ce 17.06.1、docker-compose 1.15.0、harbor-online-installer-v1.1.2.tar&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;spark32主机是harbor私有镜像仓库，关于harbor的安装部署见之前的博客《企业级Docker Registry——Harbor搭建和使用》。&lt;/p&gt;
&lt;h2 id=&quot;创建TLS加密通信的证书和密钥&quot;&gt;&lt;a href=&quot;#创建TLS加密通信的证书和密钥&quot; class=&quot;headerlink&quot; title=&quot;创建TLS加密通信的证书和密钥&quot;&gt;&lt;/a&gt;创建TLS加密通信的证书和密钥&lt;/h2&gt;&lt;p&gt;kubernetes各组件需要使用TLS证书对通信进行加密，这里我使用CloudFlare的PKI工具集&lt;a href=&quot;https://github.com/cloudflare/cfssl&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;cfssl&lt;/a&gt;来生成CA和其它证书。&lt;br&gt;生成的CA证书和密钥文件如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ca-key.pem&lt;/li&gt;
&lt;li&gt;ca.pem&lt;/li&gt;
&lt;li&gt;kubernetes-key.pem&lt;/li&gt;
&lt;li&gt;kubernetes.pem&lt;/li&gt;
&lt;li&gt;kube-proxy.pem&lt;/li&gt;
&lt;li&gt;kube-proxy-key.pem&lt;/li&gt;
&lt;li&gt;admin.pem&lt;/li&gt;
&lt;li&gt;admin-key.pem&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;各组件使用证书的情况如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;etcd：使用ca.pem、kubernetes-key.pem、kubernetes.pem；&lt;/li&gt;
&lt;li&gt;kube-apiserver：使用ca.pem、kubernetes-key.pem、kubernetes.pem；&lt;/li&gt;
&lt;li&gt;kubelet：使用ca.pem；&lt;/li&gt;
&lt;li&gt;kube-proxy：使用ca.pem、kube-proxy-key.pem、kube-proxy.pem；&lt;/li&gt;
&lt;li&gt;kubectl：使用ca.pem、admin-key.pem、admin.pem&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;kube-controller、kube-scheduler当前需要和kube-apiserver部署在同一台机器上且使用非安全端口通信，故不需要证书。&lt;/p&gt;
&lt;h3 id=&quot;安装CFSSL&quot;&gt;&lt;a href=&quot;#安装CFSSL&quot; class=&quot;headerlink&quot; title=&quot;安装CFSSL&quot;&gt;&lt;/a&gt;安装CFSSL&lt;/h3&gt;&lt;p&gt;有两种方式安装，一是二进制源码包安装，二是使用go命令安装。&lt;br&gt;1.方式一：二进制源码包安装&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
# chmod +x cfssl_linux-amd64
# mv cfssl_linux-amd64 /root/local/bin/cfssl

# wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
# chmod +x cfssljson_linux-amd64
# mv cfssljson_linux-amd64 /root/local/bin/cfssljson

# wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64
# chmod +x cfssl-certinfo_linux-amd64
# mv cfssl-certinfo_linux-amd64 /root/local/bin/cfssl-certinfo

# export PATH=/root/local/bin:$PATH
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.方式二：使用go命令安装&lt;br&gt;安装go(需要go 1.6+)：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 下载地址：https://golang.org/dl/
[root@node1 ~]# cd /usr/local/
[root@node1 local]# wget https://storage.googleapis.com/golang/go1.9.linux-amd64.tar.gz
[root@node1 local]# tar zxf go1.9.linux-amd64.tar.gz
[root@node1 local]# vim /etc/profile
# Go
export GO_HOME=/usr/local/go
export PATH=$GO_HOME/bin:$PATH 
[root@node1 local]# source /etc/profile
# 查看版本信息
[root@node1 local]# go version
go version go1.9 linux/amd64
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;安装cfssl:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 local]# go get -u github.com/cloudflare/cfssl/cmd/...
[root@node1 local]# ls /root/go/bin/
cfssl  cfssl-bundle  cfssl-certinfo  cfssljson  cfssl-newkey  cfssl-scan  mkbundle  multirootca
[root@node1 local]# mv /root/go/bin/* /usr/local/bin/
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;创建CA&quot;&gt;&lt;a href=&quot;#创建CA&quot; class=&quot;headerlink&quot; title=&quot;创建CA&quot;&gt;&lt;/a&gt;创建CA&lt;/h3&gt;&lt;p&gt;1.创建 CA 配置文件&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 local]# mkdir /opt/ssl
[root@node1 local]# cd /opt/ssl/
[root@node1 ssl]# cfssl print-defaults config &amp;gt; config.json
[root@node1 ssl]# cfssl print-defaults csr &amp;gt; csr.json
# 创建CA配置文件
[root@node1 ssl]# vim ca-config.json
{
  &amp;quot;signing&amp;quot;: {
    &amp;quot;default&amp;quot;: {
      &amp;quot;expiry&amp;quot;: &amp;quot;8760h&amp;quot;
    },
    &amp;quot;profiles&amp;quot;: {
      &amp;quot;kubernetes&amp;quot;: {
        &amp;quot;usages&amp;quot;: [
            &amp;quot;signing&amp;quot;,
            &amp;quot;key encipherment&amp;quot;,
            &amp;quot;server auth&amp;quot;,
            &amp;quot;client auth&amp;quot;
        ],
        &amp;quot;expiry&amp;quot;: &amp;quot;8760h&amp;quot;
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;部分字段说明：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ca-config.json：&lt;/strong&gt;可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个 profile；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;signing：&lt;/strong&gt;表示该证书可用于签名其它证书；生成的 ca.pem 证书中 CA=TRUE；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;server auth：&lt;/strong&gt;表示client可以用该 CA 对server提供的证书进行验证；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;client auth：&lt;/strong&gt;表示server可以用该 CA 对client提供的证书进行验证；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2.创建 CA 证书签名请求&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# vim ca-csr.json
{
  &amp;quot;CN&amp;quot;: &amp;quot;kubernetes&amp;quot;,
  &amp;quot;key&amp;quot;: {
    &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;,
    &amp;quot;size&amp;quot;: 2048
  },
  &amp;quot;names&amp;quot;: [
    {
      &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;,
      &amp;quot;ST&amp;quot;: &amp;quot;BeiJing&amp;quot;,
      &amp;quot;L&amp;quot;: &amp;quot;BeiJing&amp;quot;,
      &amp;quot;O&amp;quot;: &amp;quot;k8s&amp;quot;,
      &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;部分字段说明：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;“CN”：&lt;/strong&gt;Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;“O”：&lt;/strong&gt;Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;3.生成 CA 证书和私钥&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca
2017/09/10 04:22:13 [INFO] generating a new CA key and certificate from CSR
2017/09/10 04:22:13 [INFO] generate received request
2017/09/10 04:22:13 [INFO] received CSR
2017/09/10 04:22:13 [INFO] generating key: rsa-2048
2017/09/10 04:22:13 [INFO] encoded CSR
2017/09/10 04:22:13 [INFO] signed certificate with serial number 348968532213237181927470194452366329323573808966
[root@node1 ssl]# ls ca*
ca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;创建-Kubernetes-证书&quot;&gt;&lt;a href=&quot;#创建-Kubernetes-证书&quot; class=&quot;headerlink&quot; title=&quot;创建 Kubernetes 证书&quot;&gt;&lt;/a&gt;创建 Kubernetes 证书&lt;/h3&gt;&lt;p&gt;1.创建 kubernetes 证书签名请求&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# vim kubernetes-csr.json
{
    &amp;quot;CN&amp;quot;: &amp;quot;kubernetes&amp;quot;,
    &amp;quot;hosts&amp;quot;: [
      &amp;quot;127.0.0.1&amp;quot;,
      &amp;quot;172.16.7.151&amp;quot;,
      &amp;quot;172.16.7.152&amp;quot;,
      &amp;quot;172.16.7.153&amp;quot;,
      &amp;quot;172.16.206.32&amp;quot;,
      &amp;quot;10.254.0.1&amp;quot;,
      &amp;quot;kubernetes&amp;quot;,
      &amp;quot;kubernetes.default&amp;quot;,
      &amp;quot;kubernetes.default.svc&amp;quot;,
      &amp;quot;kubernetes.default.svc.cluster&amp;quot;,
      &amp;quot;kubernetes.default.svc.cluster.local&amp;quot;
    ],
    &amp;quot;key&amp;quot;: {
        &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;,
        &amp;quot;size&amp;quot;: 2048
    },
    &amp;quot;names&amp;quot;: [
        {
            &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;,
            &amp;quot;ST&amp;quot;: &amp;quot;BeiJing&amp;quot;,
            &amp;quot;L&amp;quot;: &amp;quot;BeiJing&amp;quot;,
            &amp;quot;O&amp;quot;: &amp;quot;k8s&amp;quot;,
            &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot;
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;部分字段说明：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果 hosts 字段不为空则需要指定授权使用该证书的 IP 或域名列表，由于该证书后续被 etcd 集群和 kubernetes master 集群使用，所以上面分别指定了 etcd 集群、kubernetes master 集群的主机 IP；&lt;/li&gt;
&lt;li&gt;还需要添加kube-apiserver注册的名为 kubernetes 服务的 IP（一般是 kue-apiserver 指定的 service-cluster-ip-range 网段的第一个IP，如 10.254.0.1。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2.生成 kubernetes 证书和私钥&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes
2017/09/10 07:44:27 [INFO] generate received request
2017/09/10 07:44:27 [INFO] received CSR
2017/09/10 07:44:27 [INFO] generating key: rsa-2048
2017/09/10 07:44:27 [INFO] encoded CSR
2017/09/10 07:44:27 [INFO] signed certificate with serial number 695308968867503306176219705194671734841389082714
[root@node1 ssl]# ls kubernetes*
kubernetes.csr  kubernetes-csr.json  kubernetes-key.pem  kubernetes.pem
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;或者直接在命令行上指定相关参数：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# echo &amp;apos;{&amp;quot;CN&amp;quot;:&amp;quot;kubernetes&amp;quot;,&amp;quot;hosts&amp;quot;:[&amp;quot;&amp;quot;],&amp;quot;key&amp;quot;:{&amp;quot;algo&amp;quot;:&amp;quot;rsa&amp;quot;,&amp;quot;size&amp;quot;:2048}}&amp;apos; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes -hostname=&amp;quot;127.0.0.1,172.16.7.151,172.16.7.152,172.16.7.153,172.16.206.32,10.254.0.1,kubernetes,kubernetes.default&amp;quot; - | cfssljson -bare kubernetes
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;创建-Admin-证书&quot;&gt;&lt;a href=&quot;#创建-Admin-证书&quot; class=&quot;headerlink&quot; title=&quot;创建 Admin 证书&quot;&gt;&lt;/a&gt;创建 Admin 证书&lt;/h3&gt;&lt;p&gt;1.创建 admin 证书签名请求&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# vim admin-csr.json
{
  &amp;quot;CN&amp;quot;: &amp;quot;admin&amp;quot;,
  &amp;quot;hosts&amp;quot;: [],
  &amp;quot;key&amp;quot;: {
    &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;,
    &amp;quot;size&amp;quot;: 2048
  },
  &amp;quot;names&amp;quot;: [
    {
      &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;,
      &amp;quot;ST&amp;quot;: &amp;quot;BeiJing&amp;quot;,
      &amp;quot;L&amp;quot;: &amp;quot;BeiJing&amp;quot;,
      &amp;quot;O&amp;quot;: &amp;quot;system:masters&amp;quot;,
      &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;说明：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;后续 kube-apiserver 使用 RBAC 对客户端(如 kubelet、kube-proxy、Pod)请求进行授权；&lt;/li&gt;
&lt;li&gt;kube-apiserver 预定义了一些 RBAC 使用的 RoleBindings，如 cluster-admin 将 Groupsystem:masters 与 Role cluster-admin 绑定，该 Role 授予了调用kube-apiserver 的所有 API的权限；&lt;/li&gt;
&lt;li&gt;OU 指定该证书的 Group 为 system:masters，kubelet 使用该证书访问 kube-apiserver时 ，由于证书被 CA 签名，所以认证通过，同时由于证书用户组为经过预授权的system:masters，所以被授予访问所有 API 的权限；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2.生成 admin 证书和私钥&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin
2017/09/10 20:01:05 [INFO] generate received request
2017/09/10 20:01:05 [INFO] received CSR
2017/09/10 20:01:05 [INFO] generating key: rsa-2048
2017/09/10 20:01:05 [INFO] encoded CSR
2017/09/10 20:01:05 [INFO] signed certificate with serial number 580169825175224945071583937498159721917720511011
2017/09/10 20:01:05 [WARNING] This certificate lacks a &amp;quot;hosts&amp;quot; field. This makes it unsuitable for websites. For more information see the Baseline Requirements for the Issuance and Management of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 (&amp;quot;Information Requirements&amp;quot;).
[root@node1 ssl]# ls admin*
admin.csr  admin-csr.json  admin-key.pem  admin.pem
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;创建-Kube-Proxy-证书&quot;&gt;&lt;a href=&quot;#创建-Kube-Proxy-证书&quot; class=&quot;headerlink&quot; title=&quot;创建 Kube-Proxy 证书&quot;&gt;&lt;/a&gt;创建 Kube-Proxy 证书&lt;/h3&gt;&lt;p&gt;1.创建 kube-proxy 证书签名请求&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# vim kube-proxy-csr.json
{
  &amp;quot;CN&amp;quot;: &amp;quot;system:kube-proxy&amp;quot;,
  &amp;quot;hosts&amp;quot;: [],
  &amp;quot;key&amp;quot;: {
    &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;,
    &amp;quot;size&amp;quot;: 2048
  },
  &amp;quot;names&amp;quot;: [
    {
      &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;,
      &amp;quot;ST&amp;quot;: &amp;quot;BeiJing&amp;quot;,
      &amp;quot;L&amp;quot;: &amp;quot;BeiJing&amp;quot;,
      &amp;quot;O&amp;quot;: &amp;quot;k8s&amp;quot;,
      &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;说明：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CN 指定该证书的 User 为 system:kube-proxy；&lt;/li&gt;
&lt;li&gt;kube-apiserver 预定义的 RoleBinding cluster-admin 将User system:kube-proxy 与 Rolesystem:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2.生成 kube-proxy 客户端证书和私钥&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes  kube-proxy-csr.json | cfssljson -bare kube-proxy
2017/09/10 20:07:55 [INFO] generate received request
2017/09/10 20:07:55 [INFO] received CSR
2017/09/10 20:07:55 [INFO] generating key: rsa-2048
2017/09/10 20:07:55 [INFO] encoded CSR
2017/09/10 20:07:55 [INFO] signed certificate with serial number 655306618453852718922516297333812428130766975244
2017/09/10 20:07:55 [WARNING] This certificate lacks a &amp;quot;hosts&amp;quot; field. This makes it unsuitable for websites. For more information see the Baseline Requirements for the Issuance and Management of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 (&amp;quot;Information Requirements&amp;quot;).
[root@node1 ssl]# ls kube-proxy*
kube-proxy.csr  kube-proxy-csr.json  kube-proxy-key.pem  kube-proxy.pem
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;校验证书&quot;&gt;&lt;a href=&quot;#校验证书&quot; class=&quot;headerlink&quot; title=&quot;校验证书&quot;&gt;&lt;/a&gt;校验证书&lt;/h3&gt;&lt;p&gt;以校验Kubernetes证书为例。&lt;/p&gt;
&lt;h4 id=&quot;使用openssl命令校验证书&quot;&gt;&lt;a href=&quot;#使用openssl命令校验证书&quot; class=&quot;headerlink&quot; title=&quot;使用openssl命令校验证书&quot;&gt;&lt;/a&gt;使用openssl命令校验证书&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# openssl x509 -noout -text -in kubernetes.pem 
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number:
            79:ca:bb:84:73:15:b1:db:aa:24:d7:a3:60:65:b0:55:27:a7:e8:5a
    Signature Algorithm: sha256WithRSAEncryption
        Issuer: C=CN, ST=BeiJing, L=BeiJing, O=k8s, OU=System, CN=kubernetes
        Validity
            Not Before: Sep 10 11:39:00 2017 GMT
            Not After : Sep 10 11:39:00 2018 GMT
        Subject: C=CN, ST=BeiJing, L=BeiJing, O=k8s, OU=System, CN=kubernetes
...
        X509v3 extensions:
            X509v3 Key Usage: critical
                Digital Signature, Key Encipherment
            X509v3 Extended Key Usage: 
                TLS Web Server Authentication, TLS Web Client Authentication
            X509v3 Basic Constraints: critical
                CA:FALSE
            X509v3 Subject Key Identifier: 
                79:48:C1:1B:81:DD:9C:75:04:EC:B6:35:26:5E:82:AA:2E:45:F6:C5
            X509v3 Subject Alternative Name: 
                DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster, DNS:kubernetes.default.svc.cluster.local, IP Address:127.0.0.1, IP Address:172.16.7.151, IP Address:172.16.7.152, IP Address:172.16.7.153, IP Address:172.16.206.32, IP Address:10.254.0.1
...
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;确认 Issuer 字段的内容和 ca-csr.json 一致；&lt;/li&gt;
&lt;li&gt;确认 Subject 字段的内容和 kubernetes-csr.json 一致；&lt;/li&gt;
&lt;li&gt;确认 X509v3 Subject Alternative Name 字段的内容和 kubernetes-csr.json 一致；&lt;/li&gt;
&lt;li&gt;确认 X509v3 Key Usage、Extended Key Usage 字段的内容和 ca-config.json 中 kubernetesprofile 一致；&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;使用-Cfssl-Certinfo-命令校验&quot;&gt;&lt;a href=&quot;#使用-Cfssl-Certinfo-命令校验&quot; class=&quot;headerlink&quot; title=&quot;使用 Cfssl-Certinfo 命令校验&quot;&gt;&lt;/a&gt;使用 Cfssl-Certinfo 命令校验&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# cfssl-certinfo -cert kubernetes.pem
{
  &amp;quot;subject&amp;quot;: {
    &amp;quot;common_name&amp;quot;: &amp;quot;kubernetes&amp;quot;,
    &amp;quot;country&amp;quot;: &amp;quot;CN&amp;quot;,
    &amp;quot;organization&amp;quot;: &amp;quot;k8s&amp;quot;,
    &amp;quot;organizational_unit&amp;quot;: &amp;quot;System&amp;quot;,
    &amp;quot;locality&amp;quot;: &amp;quot;BeiJing&amp;quot;,
    &amp;quot;province&amp;quot;: &amp;quot;BeiJing&amp;quot;,
    &amp;quot;names&amp;quot;: [
      &amp;quot;CN&amp;quot;,
      &amp;quot;BeiJing&amp;quot;,
      &amp;quot;BeiJing&amp;quot;,
      &amp;quot;k8s&amp;quot;,
      &amp;quot;System&amp;quot;,
      &amp;quot;kubernetes&amp;quot;
    ]
  },
  &amp;quot;issuer&amp;quot;: {
    &amp;quot;common_name&amp;quot;: &amp;quot;kubernetes&amp;quot;,
    &amp;quot;country&amp;quot;: &amp;quot;CN&amp;quot;,
    &amp;quot;organization&amp;quot;: &amp;quot;k8s&amp;quot;,
    &amp;quot;organizational_unit&amp;quot;: &amp;quot;System&amp;quot;,
    &amp;quot;locality&amp;quot;: &amp;quot;BeiJing&amp;quot;,
    &amp;quot;province&amp;quot;: &amp;quot;BeiJing&amp;quot;,
    &amp;quot;names&amp;quot;: [
      &amp;quot;CN&amp;quot;,
      &amp;quot;BeiJing&amp;quot;,
      &amp;quot;BeiJing&amp;quot;,
      &amp;quot;k8s&amp;quot;,
      &amp;quot;System&amp;quot;,
      &amp;quot;kubernetes&amp;quot;
    ]
  },
  &amp;quot;serial_number&amp;quot;: &amp;quot;695308968867503306176219705194671734841389082714&amp;quot;,
  &amp;quot;sans&amp;quot;: [
    &amp;quot;kubernetes&amp;quot;,
    &amp;quot;kubernetes.default&amp;quot;,
    &amp;quot;kubernetes.default.svc&amp;quot;,
    &amp;quot;kubernetes.default.svc.cluster&amp;quot;,
    &amp;quot;kubernetes.default.svc.cluster.local&amp;quot;,
    &amp;quot;127.0.0.1&amp;quot;,
    &amp;quot;172.16.7.151&amp;quot;,
    &amp;quot;172.16.7.152&amp;quot;,
    &amp;quot;172.16.7.153&amp;quot;,
    &amp;quot;172.16.206.32&amp;quot;,
    &amp;quot;10.254.0.1&amp;quot;
  ],
  &amp;quot;not_before&amp;quot;: &amp;quot;2017-09-10T11:39:00Z&amp;quot;,
  &amp;quot;not_after&amp;quot;: &amp;quot;2018-09-10T11:39:00Z&amp;quot;,
  &amp;quot;sigalg&amp;quot;: &amp;quot;SHA256WithRSA&amp;quot;,
  &amp;quot;authority_key_id&amp;quot;: &amp;quot;&amp;quot;,
  &amp;quot;subject_key_id&amp;quot;: &amp;quot;79:48:C1:1B:81:DD:9C:75:4:EC:B6:35:26:5E:82:AA:2E:45:F6:C5&amp;quot;,
...
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;分发证书&quot;&gt;&lt;a href=&quot;#分发证书&quot; class=&quot;headerlink&quot; title=&quot;分发证书&quot;&gt;&lt;/a&gt;分发证书&lt;/h3&gt;&lt;p&gt;将生成的证书和秘钥文件（后缀名为.pem）拷贝到所有机器的 /etc/kubernetes/ssl 目录下备用:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# mkdir -p /etc/kubernetes/ssl
[root@node1 ssl]# cp *.pem /etc/kubernetes/ssl
[root@node1 ssl]# scp -p *.pem root@172.16.7.152:/etc/kubernetes/ssl/
[root@node1 ssl]# scp -p *.pem root@172.16.7.153:/etc/kubernetes/ssl/
[root@node1 ssl]# scp -p *.pem root@172.16.206.32:/etc/kubernetes/ssl/
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;下载和配置-kubectl-kubecontrol-命令行工具&quot;&gt;&lt;a href=&quot;#下载和配置-kubectl-kubecontrol-命令行工具&quot; class=&quot;headerlink&quot; title=&quot;下载和配置 kubectl(kubecontrol) 命令行工具&quot;&gt;&lt;/a&gt;下载和配置 kubectl(kubecontrol) 命令行工具&lt;/h2&gt;&lt;h3 id=&quot;下载kubectl&quot;&gt;&lt;a href=&quot;#下载kubectl&quot; class=&quot;headerlink&quot; title=&quot;下载kubectl&quot;&gt;&lt;/a&gt;下载kubectl&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@node1 local]# wget https://dl.k8s.io/v1.6.0/kubernetes-client-linux-amd64.tar.gz
[root@node1 local]# tar zxf kubernetes-client-linux-amd64.tar.gz
[root@node1 local]# cp kubernetes/client/bin/kube* /usr/bin/
[root@node1 local]# chmod +x /usr/bin/kube*
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;创建-kubectl-kubeconfig-文件&quot;&gt;&lt;a href=&quot;#创建-kubectl-kubeconfig-文件&quot; class=&quot;headerlink&quot; title=&quot;创建 kubectl kubeconfig 文件&quot;&gt;&lt;/a&gt;创建 kubectl kubeconfig 文件&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@node1 local]# cd /etc/kubernetes/
[root@node1 kubernetes]# export KUBE_APISERVER=&amp;quot;https://172.16.7.151:6443&amp;quot;
# 设置集群参数
[root@node1 kubernetes]# kubectl config set-cluster kubernetes \
&amp;gt; --certificate-authority=/etc/kubernetes/ssl/ca.pem \
&amp;gt; --embed-certs=true \
&amp;gt; --server=${KUBE_APISERVER}
Cluster &amp;quot;kubernetes&amp;quot; set.
# 设置客户端认证参数
[root@node1 kubernetes]# kubectl config set-credentials admin \
&amp;gt; --client-certificate=/etc/kubernetes/ssl/admin.pem \
&amp;gt; --embed-certs=true \
&amp;gt; --client-key=/etc/kubernetes/ssl/admin-key.pem
User &amp;quot;admin&amp;quot; set.
# 设置上下文参数
[root@node1 kubernetes]# kubectl config set-context kubernetes \
&amp;gt; --cluster=kubernetes \
&amp;gt; --user=admin
Context &amp;quot;kubernetes&amp;quot; set
# 设置默认上下文
[root@node1 kubernetes]# kubectl config use-context kubernetes
Switched to context &amp;quot;kubernetes&amp;quot;.
[root@node1 kubernetes]# ls ~/.kube/config 
/root/.kube/config
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;admin.pem 证书 OU 字段值为 system:masters，kube-apiserver 预定义的 RoleBinding cluster-admin 将 Groupsystem:masters 与 Role cluster admin 绑定，该 Role 授予了调用kube-apiserver 相关 API 的权限；&lt;/li&gt;
&lt;li&gt;生成的 kubeconfig 被保存到 ~/.kube/config 文件；&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;创建-kubeconfig-文件&quot;&gt;&lt;a href=&quot;#创建-kubeconfig-文件&quot; class=&quot;headerlink&quot; title=&quot;创建 kubeconfig 文件&quot;&gt;&lt;/a&gt;创建 kubeconfig 文件&lt;/h2&gt;&lt;p&gt;kubelet、kube-proxy 等 Node 机器上的进程与 Master 机器的 kube-apiserver 进程通信时需要认证和授权.&lt;br&gt;kubernetes 1.4 开始支持由 kube-apiserver 为客户端生成 TLS 证书的 TLS Bootstrapping 功能，这样就不需要为每个客户端生成证书了；该功能当前仅支持为 kubelet 生成证书。&lt;/p&gt;
&lt;h3 id=&quot;创建-TLS-Bootstrapping-Token&quot;&gt;&lt;a href=&quot;#创建-TLS-Bootstrapping-Token&quot; class=&quot;headerlink&quot; title=&quot;创建 TLS Bootstrapping Token&quot;&gt;&lt;/a&gt;创建 TLS Bootstrapping Token&lt;/h3&gt;&lt;h4 id=&quot;Token-auth-file&quot;&gt;&lt;a href=&quot;#Token-auth-file&quot; class=&quot;headerlink&quot; title=&quot;Token auth file&quot;&gt;&lt;/a&gt;Token auth file&lt;/h4&gt;&lt;p&gt;Token可以是任意的包涵128 bit的字符串，可以使用安全的随机数发生器生成。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d &amp;apos; &amp;apos;)
[root@node1 ssl]# cat &amp;gt; token.csv &amp;lt;&amp;lt;EOF
&amp;gt; ${BOOTSTRAP_TOKEN},kubelet-bootstrap,10001,&amp;quot;system:kubelet-bootstrap&amp;quot;
&amp;gt; EOF
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;将token.csv发到所有机器（Master 和 Node）的 /etc/kubernetes/ 目录。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# cp token.csv /etc/kubernetes/
[root@node1 ssl]# scp -p token.csv root@172.16.7.152:/etc/kubernetes/
[root@node1 ssl]# scp -p token.csv root@172.16.7.153:/etc/kubernetes/
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;创建-kubelet-bootstrapping-kubeconfig-文件&quot;&gt;&lt;a href=&quot;#创建-kubelet-bootstrapping-kubeconfig-文件&quot; class=&quot;headerlink&quot; title=&quot;创建 kubelet bootstrapping kubeconfig 文件&quot;&gt;&lt;/a&gt;创建 kubelet bootstrapping kubeconfig 文件&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# cd /etc/kubernetes
[root@node1 kubernetes]# export KUBE_APISERVER=&amp;quot;https://172.16.7.151:6443&amp;quot;
# 设置集群参数
[root@node1 kubernetes]# kubectl config set-cluster kubernetes \
&amp;gt; --certificate-authority=/etc/kubernetes/ssl/ca.pem \
&amp;gt; --embed-certs=true \
&amp;gt; --server=${KUBE_APISERVER} \
&amp;gt; --kubeconfig=bootstrap.kubeconfig
Cluster &amp;quot;kubernetes&amp;quot; set.
# 设置客户端认证参数
[root@node1 kubernetes]# kubectl config set-credentials kubelet-bootstrap \
&amp;gt; --token=${BOOTSTRAP_TOKEN} \
&amp;gt; --kubeconfig=bootstrap.kubeconfig
User &amp;quot;kubelet-bootstrap&amp;quot; set.
# 设置上下文参数
[root@node1 kubernetes]# kubectl config set-context default \
&amp;gt; --cluster=kubernetes \
&amp;gt; --user=kubelet-bootstrap \
&amp;gt; --kubeconfig=bootstrap.kubeconfig
Context &amp;quot;default&amp;quot; created.
# 设置默认上下文
[root@node1 kubernetes]# kubectl config use-context default --kubeconfig=bootstrap.kubeconfig
Switched to context &amp;quot;default&amp;quot;.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;说明：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;–embed-certs 为 true 时表示将 certificate-authority 证书写入到生成的 bootstrap.kubeconfig 文件中；&lt;/li&gt;
&lt;li&gt;设置客户端认证参数时没有指定秘钥和证书，后续由 kube-apiserver 自动生成；&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;创建-kube-proxy-kubeconfig-文件&quot;&gt;&lt;a href=&quot;#创建-kube-proxy-kubeconfig-文件&quot; class=&quot;headerlink&quot; title=&quot;创建 kube-proxy kubeconfig 文件&quot;&gt;&lt;/a&gt;创建 kube-proxy kubeconfig 文件&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@node1 kubernetes]# export KUBE_APISERVER=&amp;quot;https://172.16.7.151:6443&amp;quot;
# 设置集群参数
[root@node1 kubernetes]# kubectl config set-cluster kubernetes \
&amp;gt; --certificate-authority=/etc/kubernetes/ssl/ca.pem \
&amp;gt; --embed-certs=true \
&amp;gt; --server=${KUBE_APISERVER} \
&amp;gt; --kubeconfig=kube-proxy.kubeconfig
Cluster &amp;quot;kubernetes&amp;quot; set.
# 设置客户端认证参数
[root@node1 kubernetes]# kubectl config set-credentials kube-proxy \
&amp;gt; --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \
&amp;gt; --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \
&amp;gt; --embed-certs=true \
&amp;gt; --kubeconfig=kube-proxy.kubeconfig
User &amp;quot;kube-proxy&amp;quot; set.
# 设置上下文参数
[root@node1 kubernetes]# kubectl config set-context default \
&amp;gt; --cluster=kubernetes \
&amp;gt; --user=kube-proxy \
&amp;gt; --kubeconfig=kube-proxy.kubeconfig
Context &amp;quot;default&amp;quot; created.
# 设置默认上下文
[root@node1 kubernetes]# kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
Switched to context &amp;quot;default&amp;quot;.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;说明：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;设置集群参数和客户端认证参数时 –embed-certs 都为 true，这会将 certificate-authority、client-certificate 和client-key 指向的证书文件内容写入到生成的 kube-proxy.kubeconfig 文件中；&lt;/li&gt;
&lt;li&gt;kube-proxy.pem 证书中 CN 为 system:kube-proxy，kube-apiserver 预定义的 RoleBinding cluster-admin 将Usersystem:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限；&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;分发-kubeconfig-文件&quot;&gt;&lt;a href=&quot;#分发-kubeconfig-文件&quot; class=&quot;headerlink&quot; title=&quot;分发 kubeconfig 文件&quot;&gt;&lt;/a&gt;分发 kubeconfig 文件&lt;/h3&gt;&lt;p&gt;将两个 kubeconfig 文件分发到所有 Node 机器的 /etc/kubernetes/ 目录。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 kubernetes]# scp -p bootstrap.kubeconfig root@172.16.7.152:/etc/kubernetes/
[root@node1 kubernetes]# scp -p kube-proxy.kubeconfig root@172.16.7.152:/etc/kubernetes/
[root@node1 kubernetes]# scp -p bootstrap.kubeconfig root@172.16.7.153:/etc/kubernetes/
[root@node1 kubernetes]# scp -p kube-proxy.kubeconfig root@172.16.7.153:/etc/kubernetes/
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;创建高可用-etcd-集群&quot;&gt;&lt;a href=&quot;#创建高可用-etcd-集群&quot; class=&quot;headerlink&quot; title=&quot;创建高可用 etcd 集群&quot;&gt;&lt;/a&gt;创建高可用 etcd 集群&lt;/h2&gt;&lt;p&gt;etcd 是 CoreOS 团队发起的开源项目，基于 Go 语言实现，做为一个分布式键值对存储，通过分布式锁，leader选举和写屏障(write barriers)来实现可靠的分布式协作。&lt;br&gt;kubernetes系统使用etcd存储所有数据。&lt;br&gt;CoreOS官方推荐集群规模5个为宜，我这里使用了3个节点。&lt;/p&gt;
&lt;h3 id=&quot;安装配置etcd集群&quot;&gt;&lt;a href=&quot;#安装配置etcd集群&quot; class=&quot;headerlink&quot; title=&quot;安装配置etcd集群&quot;&gt;&lt;/a&gt;安装配置etcd集群&lt;/h3&gt;&lt;p&gt;搭建etcd集群有3种方式，分别为Static, etcd Discovery, DNS Discovery。Discovery请参见&lt;a href=&quot;https://coreos.com/etcd/docs/latest/op-guide/clustering.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;官网&lt;/a&gt;。这里仅以Static方式展示一次集群搭建过程。&lt;/p&gt;
&lt;p&gt;首先请做好3个节点的时间同步。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.TLS 认证文件&lt;/strong&gt;&lt;br&gt;需要为 etcd 集群创建加密通信的 TLS 证书，这里复用以前创建的 kubernetes 证书。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ssl]# cp ca.pem kubernetes-key.pem kubernetes.pem /etc/kubernetes/ssl
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这步在之前做过，可以忽略。【注意】:kubernetes 证书的 hosts 字段列表中包含上面三台机器的 IP，否则后续证书校验会失败。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.下载二进制文件&lt;/strong&gt;&lt;br&gt;到 &lt;a href=&quot;https://github.com/coreos/etcd/releases&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/coreos/etcd/releases&lt;/a&gt; 页面下载最新版本的二进制文件，并上传到/usr/local/目录下。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 local]# tar xf etcd-v3.2.7-linux-amd64.tar
[root@node1 local]# mv etcd-v3.2.7-linux-amd64/etcd* /usr/local/bin/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;etcd集群中另外两台机器也需要如上操作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3.创建 etcd 的 systemd unit 文件&lt;/strong&gt;&lt;br&gt;配置文件模板如下，注意替换 ETCD_NAME 和 INTERNAL_IP 变量的值；&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 system]# vim etcd.service
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
EnvironmentFile=-/etc/etcd/etcd.conf
ExecStart=/usr/local/bin/etcd \
  --name ${ETCD_NAME} \
  --cert-file=/etc/kubernetes/ssl/kubernetes.pem \
  --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
  --peer-cert-file=/etc/kubernetes/ssl/kubernetes.pem \
  --peer-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
  --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \
  --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \
  --initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \
  --listen-peer-urls https://${INTERNAL_IP}:2380 \
  --listen-client-urls https://${INTERNAL_IP}:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://${INTERNAL_IP}:2379 \
  --initial-cluster-token etcd-cluster-0 \
  --initial-cluster node1=https://172.16.7.151:2380,node2=https://172.16.7.152:2380,node3=https://172.16.7.153:2380 \
  --initial-cluster-state new \
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;针对上面几个配置参数做下简单的解释：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;–name：方便理解的节点名称，默认为default，在集群中应该保持唯一，可以使用 hostname&lt;/li&gt;
&lt;li&gt;–data-dir：服务运行数据保存的路径，默认为 ${name}.etcd&lt;/li&gt;
&lt;li&gt;–snapshot-count：指定有多少事务（transaction）被提交时，触发截取快照保存到磁盘&lt;/li&gt;
&lt;li&gt;–heartbeat-interval：leader 多久发送一次心跳到 followers。默认值是 100ms&lt;/li&gt;
&lt;li&gt;–eletion-timeout：重新投票的超时时间，如果 follow 在该时间间隔没有收到心跳包，会触发重新投票，默认为 1000 ms&lt;/li&gt;
&lt;li&gt;–listen-peer-urls：和同伴通信的地址，比如 &lt;a href=&quot;http://ip:2380&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://ip:2380&lt;/a&gt;&lt;br&gt;如果有多个，使用逗号分隔。需要所有节点都能够访问，所以不要使用 localhost！&lt;/li&gt;
&lt;li&gt;–listen-client-urls：对外提供服务的地址：比如 &lt;a href=&quot;http://ip:2379,http://127.0.0.1:2379，客户端会连接到这里和&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://ip:2379,http://127.0.0.1:2379，客户端会连接到这里和&lt;/a&gt; etcd 交互&lt;/li&gt;
&lt;li&gt;–advertise-client-urls：对外公告的该节点客户端监听地址，这个值会告诉集群中其他节点&lt;/li&gt;
&lt;li&gt;–initial-advertise-peer-urls：该节点同伴监听地址，这个值会告诉集群中其他节点&lt;/li&gt;
&lt;li&gt;–initial-cluster：集群中所有节点的信息，格式为 node1=&lt;a href=&quot;http://ip1:2380,node2=http://ip2:2380,…。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://ip1:2380,node2=http://ip2:2380,…。&lt;/a&gt;&lt;br&gt;注意：这里的 node1 是节点的 –name 指定的名字；后面的 ip1:2380 是 –initial-advertise-peer-urls 指定的值&lt;/li&gt;
&lt;li&gt;–initial-cluster-state：新建集群的时候，这个值为new；假如已经存在的集群，这个值为 existing&lt;/li&gt;
&lt;li&gt;–initial-cluster-token：创建集群的token，这个值每个集群保持唯一。这样的话，如果你要重新创建集群，即使配置和之前一样，也会再次生成新的集群和节点 uuid；否则会导致多个集群之间的冲突，造成未知的错误&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所有以–init开头的配置都是在bootstrap集群的时候才会用到，后续节点的重启会被忽略。&lt;/p&gt;
&lt;p&gt;node1主机：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 local]# mkdir -p /var/lib/etcd
[root@node1 local]# cd /etc/systemd/system/
[root@node1 system]# vim etcd.service
...
ExecStart=/usr/local/bin/etcd \
  --name node1 \
...
  --initial-advertise-peer-urls https://172.16.7.151:2380 \
  --listen-peer-urls https://172.16.7.151:2380 \
  --listen-client-urls https://172.16.7.151:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://172.16.7.151:2379 \
  --initial-cluster-token etcd-cluster-0 \
...

[root@node1 system]# scp -p etcd.service root@172.16.7.152:/etc/systemd/system/
[root@node1 system]# scp -p etcd.service root@172.16.7.153:/etc/systemd/system/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;node2主机：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node2 ~]# mkdir -p /var/lib/etcd /var/lib/etcd
[root@node2 ~]# vim /etc/systemd/system/etcd.service
...
ExecStart=/usr/local/bin/etcd \
  --name node2 \
...
  --initial-advertise-peer-urls https://172.16.7.152:2380 \
  --listen-peer-urls https://172.16.7.152:2380 \
  --listen-client-urls https://172.16.7.152:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://172.16.7.152:2379 \
  --initial-cluster-token etcd-cluster-0 \
...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;node3主机：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node3 ~]# mkdir -p /var/lib/etcd /var/lib/etcd
[root@node3 ~]# vim /etc/systemd/system/etcd.service
...
ExecStart=/usr/local/bin/etcd \
  --name node2 \
...
  --initial-advertise-peer-urls https://172.16.7.153:2380 \
  --listen-peer-urls https://172.16.7.153:2380 \
  --listen-client-urls https://172.16.7.153:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://172.16.7.153:2379 \
  --initial-cluster-token etcd-cluster-0 \
...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;指定 etcd 的工作目录为 /var/lib/etcd，数据目录为 /var/lib/etcd，需在启动服务前创建这两个目录；&lt;/li&gt;
&lt;li&gt;为了保证通信安全，需要指定 etcd 的公私钥(cert-file和key-file)、Peers 通信的公私钥和 CA 证书(peer-cert-file、peer-key-file、peer-trusted-ca-file)、客户端的CA证书（trusted-ca-file）；&lt;/li&gt;
&lt;li&gt;创建 kubernetes.pem 证书时使用的 kubernetes-csr.json 文件的 hosts 字段包含所有 etcd 节点的 INTERNAL_IP，否则证书校验会出错；&lt;/li&gt;
&lt;li&gt;–initial-cluster-state 值为 new 时，–name 的参数值必须位于 –initial-cluster 列表中。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;启动-etcd-服务&quot;&gt;&lt;a href=&quot;#启动-etcd-服务&quot; class=&quot;headerlink&quot; title=&quot;启动 etcd 服务&quot;&gt;&lt;/a&gt;启动 etcd 服务&lt;/h3&gt;&lt;p&gt;集群中的节点都需要执行：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable etcd
# systemctl start etcd
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;验证服务&quot;&gt;&lt;a href=&quot;#验证服务&quot; class=&quot;headerlink&quot; title=&quot;验证服务&quot;&gt;&lt;/a&gt;验证服务&lt;/h3&gt;&lt;p&gt;etcdctl 是一个命令行客户端，它能提供一些简洁的命令，供用户直接跟 etcd 服务打交道，而无需基于 HTTP API 方式。这在某些情况下将很方便，例如用户对服务进行测试或者手动修改数据库内容。我们也推荐在刚接触 etcd 时通过 etcdctl 命令来熟悉相关的操作，这些操作跟 HTTP API 实际上是对应的。&lt;br&gt;在etcd集群任意一台机器上执行如下命令：&lt;/p&gt;
&lt;p&gt;1.查看集群健康状态&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 system]# etcdctl --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/kubernetes/ssl/kubernetes.pem --key-file=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints &amp;quot;https://172.16.7.151:2379&amp;quot; cluster-health            
member 31800ab6b566b2b is healthy: got healthy result from https://172.16.7.151:2379
member 9a0745d96695eec6 is healthy: got healthy result from https://172.16.7.153:2379
member e64edc68e5e81b55 is healthy: got healthy result from https://172.16.7.152:2379
cluster is healthy
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;结果最后一行为 cluster is healthy 时表示集群服务正常。&lt;/p&gt;
&lt;p&gt;2.查看集群成员，并能看出哪个是leader节点&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 system]# etcdctl --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/kubernetes/ssl/kubernetes.pem --key-file=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints &amp;quot;https://172.16.7.151:2379&amp;quot; member list
31800ab6b566b2b: name=node1 peerURLs=https://172.16.7.151:2380 clientURLs=https://172.16.7.151:2379 isLeader=false
9a0745d96695eec6: name=node3 peerURLs=https://172.16.7.153:2380 clientURLs=https://172.16.7.153:2379 isLeader=false
e64edc68e5e81b55: name=node2 peerURLs=https://172.16.7.152:2380 clientURLs=https://172.16.7.152:2379 isLeader=true
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3.删除一个节点&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 如果你想更新一个节点的IP(peerURLS)，首先你需要知道那个节点的ID
# etcdctl --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/kubernetes/ssl/kubernetes.pem --key-file=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints &amp;quot;https://172.16.7.151:2379&amp;quot; member list
31800ab6b566b2b: name=node1 peerURLs=https://172.16.7.151:2380 clientURLs=https://172.16.7.151:2379 isLeader=false
9a0745d96695eec6: name=node3 peerURLs=https://172.16.7.153:2380 clientURLs=https://172.16.7.153:2379 isLeader=false
e64edc68e5e81b55: name=node2 peerURLs=https://172.16.7.152:2380 clientURLs=https://172.16.7.152:2379 isLeader=true
# etcdctl --endpoints &amp;quot;http://192.168.2.210:2379&amp;quot; member remove 9a0745d96695eec6
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;部署高可用-kubernetes-master-集群&quot;&gt;&lt;a href=&quot;#部署高可用-kubernetes-master-集群&quot; class=&quot;headerlink&quot; title=&quot;部署高可用 kubernetes master 集群&quot;&gt;&lt;/a&gt;部署高可用 kubernetes master 集群&lt;/h2&gt;&lt;p&gt;kubernetes master 节点包含的组件：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kube-apiserver&lt;/li&gt;
&lt;li&gt;kube-scheduler&lt;/li&gt;
&lt;li&gt;kube-controller-manager&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;目前这三个组件需要部署在同一台机器上。&lt;br&gt;kube-scheduler、kube-controller-manager 和 kube-apiserver 三者的功能紧密相关；&lt;br&gt;同时只能有一个 kube-scheduler、kube-controller-manager 进程处于工作状态，如果运行多个，则需要通过选举产生一个 leader；&lt;br&gt;本文档记录部署一个三个节点的高可用 kubernetes master 集群步骤。（后续创建一个 load balancer 来代理访问 kube-apiserver 的请求）&lt;/p&gt;
&lt;h3 id=&quot;TLS-证书文件&quot;&gt;&lt;a href=&quot;#TLS-证书文件&quot; class=&quot;headerlink&quot; title=&quot;TLS 证书文件&quot;&gt;&lt;/a&gt;TLS 证书文件&lt;/h3&gt;&lt;p&gt;检查之前生成的证书。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 kubernetes]# ls /etc/kubernetes/ssl
admin-key.pem  admin.pem  ca-key.pem  ca.pem  kube-proxy-key.pem  kube-proxy.pem  kubernetes-key.pem  kubernetes.pem
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;下载二进制文件&quot;&gt;&lt;a href=&quot;#下载二进制文件&quot; class=&quot;headerlink&quot; title=&quot;下载二进制文件&quot;&gt;&lt;/a&gt;下载二进制文件&lt;/h3&gt;&lt;p&gt;有两种下载方式：&lt;br&gt;方式一：从 &lt;a href=&quot;https://github.com/kubernetes/kubernetes/releases&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;github release&lt;/a&gt; 页面 下载发布版 tarball，解压后再执行下载脚本。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 local]# cd /opt/
[root@node1 opt]# wget https://github.com/kubernetes/kubernetes/releases/download/v1.6.0/kubernetes.tar.gz
[root@node1 opt]# tar zxf kubernetes.tar.gz 
[root@node1 opt]# cd kubernetes/
[root@node1 kubernetes]# ./cluster/get-kube-binaries.sh
Kubernetes release: v1.6.0
Server: linux/amd64  (to override, set KUBERNETES_SERVER_ARCH)
Client: linux/amd64  (autodetected)

Will download kubernetes-server-linux-amd64.tar.gz from https://storage.googleapis.com/kubernetes-release/release/v1.6.0
Will download and extract kubernetes-client-linux-amd64.tar.gz from https://storage.googleapis.com/kubernetes-release/release/v1.6.0
Is this ok? [Y]/n
y
...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;方式二：从 CHANGELOG页面 下载 client 或 server tarball 文件&lt;br&gt;server 的 tarball kubernetes-server-linux-amd64.tar.gz 已经包含了 client(kubectl) 二进制文件，所以不用单独下载kubernetes-client-linux-amd64.tar.gz文件；&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;wget https://dl.k8s.io/v1.6.0/kubernetes-server-linux-amd64.tar.gz
tar -xzvf kubernetes-server-linux-amd64.tar.gz
...
cd kubernetes
tar -xzvf  kubernetes-src.tar.gz
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;将二进制文件拷贝到指定路径：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 kubernetes]# pwd
/opt/kubernetes
[root@node1 kubernetes]# cd server/
[root@node1 server]# tar zxf kubernetes-server-linux-amd64.tar.gz
[root@node1 server]# cp -r kubernetes/server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet} /usr/local/bin/
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;配置和启动-kube-apiserver&quot;&gt;&lt;a href=&quot;#配置和启动-kube-apiserver&quot; class=&quot;headerlink&quot; title=&quot;配置和启动 kube-apiserver&quot;&gt;&lt;/a&gt;配置和启动 kube-apiserver&lt;/h3&gt;&lt;h4 id=&quot;创建-kube-apiserver的service配置文件&quot;&gt;&lt;a href=&quot;#创建-kube-apiserver的service配置文件&quot; class=&quot;headerlink&quot; title=&quot;创建 kube-apiserver的service配置文件&quot;&gt;&lt;/a&gt;创建 kube-apiserver的service配置文件&lt;/h4&gt;&lt;p&gt;在/usr/lib/systemd/system/下创建kube-apiserver.service，内容如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cd /usr/lib/systemd/system/
# vim kube-apiserver.service
[Unit]
Description=Kubernetes API Service
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
After=etcd.service

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/apiserver
ExecStart=/usr/local/bin/kube-apiserver \
        $KUBE_LOGTOSTDERR \
        $KUBE_LOG_LEVEL \
        $KUBE_ETCD_SERVERS \
        $KUBE_API_ADDRESS \
        $KUBE_API_PORT \
        $KUBELET_PORT \
        $KUBE_ALLOW_PRIV \
        $KUBE_SERVICE_ADDRESSES \
        $KUBE_ADMISSION_CONTROL \
        $KUBE_API_ARGS
Restart=on-failure
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;/etc/kubernetes/config文件的内容为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/kubernetes/config
###
# kubernetes system config
#
# The following values are used to configure various aspects of all
# kubernetes services, including
#
#   kube-apiserver.service
#   kube-controller-manager.service
#   kube-scheduler.service
#   kubelet.service
#   kube-proxy.service
# logging to stderr means we get it in the systemd journal
KUBE_LOGTOSTDERR=&amp;quot;--logtostderr=true&amp;quot;

# journal message level, 0 is debug
KUBE_LOG_LEVEL=&amp;quot;--v=0&amp;quot;

# Should this cluster be allowed to run privileged docker containers
KUBE_ALLOW_PRIV=&amp;quot;--allow-privileged=true&amp;quot;

# How the controller-manager, scheduler, and proxy find the apiserver
#KUBE_MASTER=&amp;quot;--master=http://domainName:8080&amp;quot;                             
KUBE_MASTER=&amp;quot;--master=http://172.16.7.151:8080&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;该配置文件同时被kube-apiserver、kube-controller-manager、kube-scheduler、kubelet、kube-proxy使用。&lt;/p&gt;
&lt;p&gt;创建apiserver配置文件/etc/kubernetes/apiserver：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/kubernetes/apiserver
###
## kubernetes system config
##
## The following values are used to configure the kube-apiserver
##
#
## The address on the local server to listen to.
#KUBE_API_ADDRESS=&amp;quot;--insecure-bind-address=sz-pg-oam-docker-test-001.tendcloud.com&amp;quot;
KUBE_API_ADDRESS=&amp;quot;--advertise-address=172.16.7.151 --bind-address=172.16.7.151 --insecure-bind-address=172.16.7.151&amp;quot;
#
## The port on the local server to listen on.
#KUBE_API_PORT=&amp;quot;--port=8080&amp;quot;
#
## Port minions listen on
#KUBELET_PORT=&amp;quot;--kubelet-port=10250&amp;quot;
#
## Comma separated list of nodes in the etcd cluster
KUBE_ETCD_SERVERS=&amp;quot;--etcd-servers=https://172.16.7.151:2379,https://172.16.7.152:2379,https://172.16.7.153:2379&amp;quot;
#
## Address range to use for services
KUBE_SERVICE_ADDRESSES=&amp;quot;--service-cluster-ip-range=10.254.0.0/16&amp;quot;
#
## default admission control policies
KUBE_ADMISSION_CONTROL=&amp;quot;--admission-control=ServiceAccount,NamespaceLifecycle,NamespaceExists,LimitRanger,ResourceQuota&amp;quot;
#
## Add your own!
KUBE_API_ARGS=&amp;quot;--authorization-mode=RBAC --runtime-config=rbac.authorization.k8s.io/v1beta1 --kubelet-https=true --experimental-bootstrap-token-auth --token-auth-file=/etc/kubernetes/token.csv --service-node-port-range=30000-32767 --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem --client-ca-file=/etc/kubernetes/ssl/ca.pem --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem --etcd-cafile=/etc/kubernetes/ssl/ca.pem --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem --enable-swagger-ui=true --apiserver-count=3 --audit-log-maxage=30 --audit-log-maxbackup=3 --audit-log-maxsize=100 --audit-log-path=/var/lib/audit.log --event-ttl=1h&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;–authorization-mode=RBAC 指定在安全端口使用 RBAC 授权模式，拒绝未通过授权的请求；&lt;/li&gt;
&lt;li&gt;kube-scheduler、kube-controller-manager 一般和 kube-apiserver 部署在同一台机器上，它们使用非安全端口和 kube-apiserver通信;&lt;/li&gt;
&lt;li&gt;kubelet、kube-proxy、kubectl 部署在其它 Node 节点上，如果通过安全端口访问 kube-apiserver，则必须先通过 TLS 证书认证，再通过 RBAC 授权；&lt;/li&gt;
&lt;li&gt;kube-proxy、kubectl 通过在使用的证书里指定相关的 User、Group 来达到通过 RBAC 授权的目的；&lt;/li&gt;
&lt;li&gt;如果使用了 kubelet TLS Boostrap 机制，则不能再指定 –kubelet-certificate-authority、–kubelet-client-certificate 和 –kubelet-client-key 选项，否则后续 kube-apiserver 校验 kubelet 证书时出现 ”x509: certificate signed by unknown authority“ 错误；&lt;/li&gt;
&lt;li&gt;–admission-control 值必须包含 ServiceAccount；&lt;/li&gt;
&lt;li&gt;–bind-address 不能为 127.0.0.1；&lt;/li&gt;
&lt;li&gt;runtime-config配置为rbac.authorization.k8s.io/v1beta1，表示运行时的apiVersion；&lt;/li&gt;
&lt;li&gt;–service-cluster-ip-range 指定 Service Cluster IP 地址段，该地址段不能路由可达；&lt;/li&gt;
&lt;li&gt;缺省情况下 kubernetes 对象保存在 etcd /registry 路径下，可以通过 –etcd-prefix 参数进行调整；&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;启动kube-apiserver&quot;&gt;&lt;a href=&quot;#启动kube-apiserver&quot; class=&quot;headerlink&quot; title=&quot;启动kube-apiserver&quot;&gt;&lt;/a&gt;启动kube-apiserver&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable kube-apiserver
# systemctl start kube-apiserver
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;启动过程中可以观察日志：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# tail -f /var/log/message
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;配置和启动-kube-controller-manager&quot;&gt;&lt;a href=&quot;#配置和启动-kube-controller-manager&quot; class=&quot;headerlink&quot; title=&quot;配置和启动 kube-controller-manager&quot;&gt;&lt;/a&gt;配置和启动 kube-controller-manager&lt;/h3&gt;&lt;h4 id=&quot;创建-kube-controller-manager-的service配置文件&quot;&gt;&lt;a href=&quot;#创建-kube-controller-manager-的service配置文件&quot; class=&quot;headerlink&quot; title=&quot;创建 kube-controller-manager 的service配置文件&quot;&gt;&lt;/a&gt;创建 kube-controller-manager 的service配置文件&lt;/h4&gt;&lt;p&gt;在/usr/lib/systemd/system/下创建kube-controller-manager.service，内容如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# pwd
/usr/lib/systemd/system
# vim kube-controller-manager.service
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/controller-manager
ExecStart=/usr/local/bin/kube-controller-manager \
            $KUBE_LOGTOSTDERR \
            $KUBE_LOG_LEVEL \
            $KUBE_MASTER \
            $KUBE_CONTROLLER_MANAGER_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;创建kube-controller-manager配置文件/etc/kubernetes/controller-manager：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/kubernetes/controller-manager
###
# The following values are used to configure the kubernetes controller-manager

# defaults from config and apiserver should be adequate

# Add your own!
KUBE_CONTROLLER_MANAGER_ARGS=&amp;quot;--address=127.0.0.1 --service-cluster-ip-range=10.254.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem  --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem --root-ca-file=/etc/kubernetes/ssl/ca.pem --leader-elect=true&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;–service-cluster-ip-range 参数指定 Cluster 中 Service 的CIDR范围，该网络在各 Node 间必须路由不可达，必须和 kube-apiserver 中的参数一致；&lt;/li&gt;
&lt;li&gt;–cluster-signing-* 指定的证书和私钥文件用来签名为 TLS BootStrap 创建的证书和私钥；&lt;/li&gt;
&lt;li&gt;–root-ca-file 用来对 kube-apiserver 证书进行校验，指定该参数后，才会在Pod 容器的 ServiceAccount 中放置该 CA 证书文件；&lt;/li&gt;
&lt;li&gt;&lt;p&gt;–address 值必须为 127.0.0.1，因为当前 kube-apiserver 期望 scheduler 和 controller-manager 在同一台机器，否则：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get componentstatuses
NAME                 STATUS      MESSAGE                                                                                        ERROR
scheduler            Unhealthy   Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: getsockopt: connection refused   
controller-manager   Healthy     ok                                                                                             
etcd-2               Unhealthy   Get http://172.20.0.113:2379/health: malformed HTTP response &amp;quot;\x15\x03\x01\x00\x02\x02&amp;quot;        
etcd-0               Healthy     {&amp;quot;health&amp;quot;: &amp;quot;true&amp;quot;}                                                                             
etcd-1               Healthy     {&amp;quot;health&amp;quot;: &amp;quot;true&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;参考：&lt;a href=&quot;https://github.com/kubernetes-incubator/bootkube/issues/64&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/kubernetes-incubator/bootkube/issues/64&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;启动-kube-controller-manager&quot;&gt;&lt;a href=&quot;#启动-kube-controller-manager&quot; class=&quot;headerlink&quot; title=&quot;启动 kube-controller-manager&quot;&gt;&lt;/a&gt;启动 kube-controller-manager&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable kube-controller-manager
# systemctl start kube-controller-manager
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;配置和启动-kube-scheduler&quot;&gt;&lt;a href=&quot;#配置和启动-kube-scheduler&quot; class=&quot;headerlink&quot; title=&quot;配置和启动 kube-scheduler&quot;&gt;&lt;/a&gt;配置和启动 kube-scheduler&lt;/h3&gt;&lt;h4 id=&quot;创建-kube-scheduler的serivce配置文件&quot;&gt;&lt;a href=&quot;#创建-kube-scheduler的serivce配置文件&quot; class=&quot;headerlink&quot; title=&quot;创建 kube-scheduler的serivce配置文件&quot;&gt;&lt;/a&gt;创建 kube-scheduler的serivce配置文件&lt;/h4&gt;&lt;p&gt;在/usr/lib/systemd/system/下创建kube-scheduler.serivce，内容如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# pwd
/usr/lib/systemd/system
# vim kube-scheduler.service
[Unit]
Description=Kubernetes Scheduler Plugin
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/scheduler
ExecStart=/usr/local/bin/kube-scheduler \
            $KUBE_LOGTOSTDERR \
            $KUBE_LOG_LEVEL \
            $KUBE_MASTER \
            $KUBE_SCHEDULER_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;创建kube-scheduler配置文件/etc/kubernetes/scheduler：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/kubernetes/scheduler
###
# kubernetes scheduler config

# default config should be adequate

# Add your own!
KUBE_SCHEDULER_ARGS=&amp;quot;--leader-elect=true --address=127.0.0.1&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;–address 值必须为 127.0.0.1，因为当前 kube-apiserver 期望 scheduler 和 controller-manager 在同一台机器。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;启动-kube-scheduler&quot;&gt;&lt;a href=&quot;#启动-kube-scheduler&quot; class=&quot;headerlink&quot; title=&quot;启动 kube-scheduler&quot;&gt;&lt;/a&gt;启动 kube-scheduler&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable kube-scheduler
# systemctl start kube-scheduler
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;验证-master-节点功能&quot;&gt;&lt;a href=&quot;#验证-master-节点功能&quot; class=&quot;headerlink&quot; title=&quot;验证 master 节点功能&quot;&gt;&lt;/a&gt;验证 master 节点功能&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;# kubectl get componentstatuses
NAME                 STATUS    MESSAGE              ERROR
scheduler            Healthy   ok                   
controller-manager   Healthy   ok                   
etcd-0               Healthy   {&amp;quot;health&amp;quot;: &amp;quot;true&amp;quot;}   
etcd-1               Healthy   {&amp;quot;health&amp;quot;: &amp;quot;true&amp;quot;}   
etcd-2               Healthy   {&amp;quot;health&amp;quot;: &amp;quot;true&amp;quot;}              
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;部署kubernetes-node节点&quot;&gt;&lt;a href=&quot;#部署kubernetes-node节点&quot; class=&quot;headerlink&quot; title=&quot;部署kubernetes node节点&quot;&gt;&lt;/a&gt;部署kubernetes node节点&lt;/h2&gt;&lt;p&gt;kubernetes node 节点包含如下组件：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Docker1.12.6&lt;/li&gt;
&lt;li&gt;Flanneld&lt;/li&gt;
&lt;li&gt;kubelet&lt;/li&gt;
&lt;li&gt;kube-proxy&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;安装Docker&quot;&gt;&lt;a href=&quot;#安装Docker&quot; class=&quot;headerlink&quot; title=&quot;安装Docker&quot;&gt;&lt;/a&gt;安装Docker&lt;/h3&gt;&lt;p&gt;参见之前的文章《Docker镜像和容器》。&lt;/p&gt;
&lt;h3 id=&quot;安装配置Flanneld&quot;&gt;&lt;a href=&quot;#安装配置Flanneld&quot; class=&quot;headerlink&quot; title=&quot;安装配置Flanneld&quot;&gt;&lt;/a&gt;安装配置Flanneld&lt;/h3&gt;&lt;h4 id=&quot;Flannel介绍&quot;&gt;&lt;a href=&quot;#Flannel介绍&quot; class=&quot;headerlink&quot; title=&quot;Flannel介绍&quot;&gt;&lt;/a&gt;Flannel介绍&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Flannel是CoreOS团队针对Kubernetes设计的一个网络规划服务，简单来说，它的功能是让集群中的不同节点主机创建的Docker容器都具有全集群唯一的虚拟IP地址。&lt;/li&gt;
&lt;li&gt;在默认的Docker配置中，每个节点上的Docker服务会分别负责所在节点容器的IP分配。这样导致的一个问题是，不同节点上容器可能获得相同的内外IP地址。&lt;/li&gt;
&lt;li&gt;Flannel的设计目的就是为集群中的所有节点重新规划IP地址的使用规则，从而使得不同节点上的容器能够获得“同属一个内网”且”不重复的”IP地址，并让属于不同节点上的容器能够直接通过内网IP通信。&lt;/li&gt;
&lt;li&gt;Flannel实质上是一种“覆盖网络(overlay network)”，也就是将TCP数据包装在另一种网络包里面进行路由转发和通信，目前已经支持udp、vxlan、host-gw、aws-vpc、gce和alloc路由等数据转发方式，默认的节点间数据通信方式是UDP转发。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在Flannel的GitHub页面有如下的一张原理图：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数据从源容器中发出后，经由所在主机的docker0虚拟网卡转发到flannel0虚拟网卡，这是个P2P的虚拟网卡，flanneld服务监听在网卡的另外一端。（Flannel通过ETCD服务维护了一张节点间的路由表）；&lt;/li&gt;
&lt;li&gt;源主机的flanneld服务将原本的数据内容UDP封装后根据自己的路由表投递给目的节点的flanneld服务，数据到达后被解包，然后直接进入目的节点的flannel0虚拟网卡，然后被转发到目的主机的docker0虚拟网卡；&lt;/li&gt;
&lt;li&gt;最后就像本机容器通信一样由docker0路由到目标容器，这样整个数据包的传递就完成了。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;安装配置flannel&quot;&gt;&lt;a href=&quot;#安装配置flannel&quot; class=&quot;headerlink&quot; title=&quot;安装配置flannel&quot;&gt;&lt;/a&gt;安装配置flannel&lt;/h4&gt;&lt;p&gt;我这里使用yum安装，安装的版本是0.7.1。集群中的3台node都需要安装配置flannel。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# yum install -y flannel
# rpm -ql flannel
/etc/sysconfig/flanneld
/run/flannel
/usr/bin/flanneld
/usr/bin/flanneld-start
/usr/lib/systemd/system/docker.service.d/flannel.conf
/usr/lib/systemd/system/flanneld.service
/usr/lib/tmpfiles.d/flannel.conf
/usr/libexec/flannel
/usr/libexec/flannel/mk-docker-opts.sh
...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;修改flannel配置文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/sysconfig/flanneld 
# Flanneld configuration options  

# etcd url location.  Point this to the server where etcd runs
FLANNEL_ETCD_ENDPOINTS=&amp;quot;https://172.16.7.151:2379,https://172.16.7.152:2379,https://172.16.7.153:2379&amp;quot;

# etcd config key.  This is the configuration key that flannel queries
# For address range assignment
FLANNEL_ETCD_PREFIX=&amp;quot;/kube-centos/network&amp;quot;

# Any additional options that you want to pass
#FLANNEL_OPTIONS=&amp;quot;&amp;quot;
FLANNEL_OPTIONS=&amp;quot;-etcd-cafile=/etc/kubernetes/ssl/ca.pem -etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem -etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：        &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;etcd的地址FLANNEL_ETCD_ENDPOINT&lt;/li&gt;
&lt;li&gt;etcd查询的目录，包含docker的IP地址段配置。FLANNEL_ETCD_PREFIX        &lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;在etcd中初始化flannel网络数据&quot;&gt;&lt;a href=&quot;#在etcd中初始化flannel网络数据&quot; class=&quot;headerlink&quot; title=&quot;在etcd中初始化flannel网络数据&quot;&gt;&lt;/a&gt;在etcd中初始化flannel网络数据&lt;/h4&gt;&lt;p&gt;多个node上的Flanneld依赖一个etcd cluster来做集中配置服务，etcd保证了所有node上flanned所看到的配置是一致的。同时每个node上的flanned监听etcd上的数据变化，实时感知集群中node的变化。&lt;/p&gt;
&lt;p&gt;执行下面的命令为docker分配IP地址段：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# etcdctl --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/kubernetes/ssl/kubernetes.pem --key-file=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints &amp;quot;https://172.16.7.151:2379&amp;quot; mkdir /kube-centos/network
# etcdctl --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/kubernetes/ssl/kubernetes.pem --key-file=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints &amp;quot;https://172.16.7.151:2379&amp;quot; mk /kube-centos/network/config &amp;apos;{&amp;quot;Network&amp;quot;: &amp;quot;172.30.0.0/16&amp;quot;, &amp;quot;SubnetLen&amp;quot;: 24, &amp;quot;Backend&amp;quot;: { &amp;quot;Type&amp;quot;: &amp;quot;vxlan&amp;quot; }}&amp;apos;
{&amp;quot;Network&amp;quot;: &amp;quot;172.30.0.0/16&amp;quot;, &amp;quot;SubnetLen&amp;quot;: 24, &amp;quot;Backend&amp;quot;: { &amp;quot;Type&amp;quot;: &amp;quot;vxlan&amp;quot; }}
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;启动flannel&quot;&gt;&lt;a href=&quot;#启动flannel&quot; class=&quot;headerlink&quot; title=&quot;启动flannel&quot;&gt;&lt;/a&gt;启动flannel&lt;/h4&gt;&lt;p&gt;集群中的3台node都启动flannel：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl start flanneld
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;启动完成后，会在/run/flannel/目录下生成两个文件，以node1为例：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# ls /run/flannel/         
docker  subnet.env
# cd /run/flannel/
[root@node1 flannel]# cat docker 
DOCKER_OPT_BIP=&amp;quot;--bip=172.30.51.1/24&amp;quot;
DOCKER_OPT_IPMASQ=&amp;quot;--ip-masq=true&amp;quot;
DOCKER_OPT_MTU=&amp;quot;--mtu=1450&amp;quot;
DOCKER_NETWORK_OPTIONS=&amp;quot; --bip=172.30.51.1/24 --ip-masq=true --mtu=1450&amp;quot;
# cat subnet.env 
FLANNEL_NETWORK=172.30.0.0/16
FLANNEL_SUBNET=172.30.51.1/24
FLANNEL_MTU=1450
FLANNEL_IPMASQ=false
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;现在查询etcd中的内容可以看到：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# etcdctl --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/kubernetes/ssl/kubernetes.pem --key-file=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints &amp;quot;https://172.16.7.151:2379&amp;quot; ls /kube-centos/network/subnets
/kube-centos/network/subnets/172.30.51.0-24
/kube-centos/network/subnets/172.30.29.0-24
/kube-centos/network/subnets/172.30.19.0-24
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;设置docker0网桥的IP地址(集群中node节点都需要设置)：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# source /run/flannel/subnet.env
# ifconfig docker0 $FLANNEL_SUBNET
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这样docker0和flannel网桥会在同一个子网中，查看node1主机网卡：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker0: flags=4099&amp;lt;UP,BROADCAST,MULTICAST&amp;gt;  mtu 1500
        inet 172.30.51.1  netmask 255.255.255.0  broadcast 172.30.51.255
flannel.1: flags=4163&amp;lt;UP,BROADCAST,RUNNING,MULTICAST&amp;gt;  mtu 1450
        inet 172.30.51.0  netmask 255.255.255.255  broadcast 0.0.0.0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;重启docker：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;【注意】:经过测试，docker 17.06.1-ce版本重启后，docker0网桥又会被重置为172.17.0.1，docker 1.12.6版本测试是不会有问题的。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果想重新设置flannel，先停止flanneld，清理etcd里的数据，然后 ifconfig flannel.1 down，然后启动flanneld，会重新生成子网，并up flannel.1网桥设备。&lt;/p&gt;
&lt;h4 id=&quot;测试跨主机容器通信&quot;&gt;&lt;a href=&quot;#测试跨主机容器通信&quot; class=&quot;headerlink&quot; title=&quot;测试跨主机容器通信&quot;&gt;&lt;/a&gt;测试跨主机容器通信&lt;/h4&gt;&lt;p&gt;分别在node1和node2上启动一个容器，然后ping对方容器的地址：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 flannel]# docker run -i -t centos /bin/bash
[root@38be151deb71 /]# yum install net-tools -y
[root@38be151deb71 /]# ifconfig
eth0: flags=4163&amp;lt;UP,BROADCAST,RUNNING,MULTICAST&amp;gt;  mtu 1450
        inet 172.30.51.2  netmask 255.255.255.0  broadcast 0.0.0.0

[root@node2 flannel]# docker run -i -t centos /bin/bash
[root@90e85c215fda /]# yum install net-tools -y
[root@90e85c215fda /]# ifconfig
eth0: flags=4163&amp;lt;UP,BROADCAST,RUNNING,MULTICAST&amp;gt;  mtu 1450
        inet 172.30.29.2  netmask 255.255.255.0  broadcast 0.0.0.0
[root@90e85c215fda /]# ping 172.16.51.2  
PING 172.16.51.2 (172.16.51.2) 56(84) bytes of data.
64 bytes from 172.16.51.2: icmp_seq=1 ttl=254 time=1.00 ms
64 bytes from 172.16.51.2: icmp_seq=2 ttl=254 time=1.29 ms
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;补充：下载二进制包安装flannel&quot;&gt;&lt;a href=&quot;#补充：下载二进制包安装flannel&quot; class=&quot;headerlink&quot; title=&quot;补充：下载二进制包安装flannel&quot;&gt;&lt;/a&gt;补充：下载二进制包安装flannel&lt;/h4&gt;&lt;p&gt;从官网 &lt;a href=&quot;https://github.com/coreos/flannel/releases&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/coreos/flannel/releases&lt;/a&gt; 下载的flannel release 0.7.1,并将下载的文件上传到服务器的/opt/flannel/目录下。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# mkdir flannel
# cd flannel/
# tar xf flannel-v0.7.1-linux-amd64.tar  
# ls
flanneld  flannel-v0.7.1-linux-amd64.tar  mk-docker-opts.sh  README.md
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;mk-docker-opts.sh是用来Generate Docker daemon options based on flannel env file。&lt;br&gt;执行 ./mk-docker-opts.sh -i 将会生成如下两个文件环境变量文件。&lt;/p&gt;
&lt;p&gt;Flannel的&lt;a href=&quot;https://github.com/coreos/flannel/blob/master/Documentation/running.md&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;文档&lt;/a&gt;中有写Docker Integration：&lt;br&gt;Docker daemon accepts –bip argument to configure the subnet of the docker0 bridge. It also accepts –mtu to set the MTU for docker0 and veth devices that it will be creating.&lt;br&gt;Because flannel writes out the acquired subnet and MTU values into a file, the script starting Docker can source in the values and pass them to Docker daemon:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;source /run/flannel/subnet.env
docker daemon --bip=${FLANNEL_SUBNET} --mtu=${FLANNEL_MTU} &amp;amp;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Systemd users can use EnvironmentFile directive in the .service file to pull in /run/flannel/subnet.env&lt;/p&gt;
&lt;h3 id=&quot;安装和配置-kubelet&quot;&gt;&lt;a href=&quot;#安装和配置-kubelet&quot; class=&quot;headerlink&quot; title=&quot;安装和配置 kubelet&quot;&gt;&lt;/a&gt;安装和配置 kubelet&lt;/h3&gt;&lt;p&gt;kubelet 启动时向 kube-apiserver 发送 TLS bootstrapping 请求，需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper cluster 角色(role)， 然后 kubelet 才能有权限创建认证请求(certificate signing requests)：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cd /etc/kubernetes
[root@node1 kubernetes]# kubectl create clusterrolebinding kubelet-bootstrap \
&amp;gt; --clusterrole=system:node-bootstrapper \
&amp;gt; --user=kubelet-bootstrap
clusterrolebinding &amp;quot;kubelet-bootstrap&amp;quot; created  
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;【注意】：以上这步只需要在kubernetes node集群中的一台执行一次就可以了。&lt;/strong&gt;&lt;br&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;–user=kubelet-bootstrap 是在 /etc/kubernetes/token.csv 文件中指定的用户名，同时也写入了/etc/kubernetes/bootstrap.kubeconfig 文件；&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;下载最新的-kubelet-和-kube-proxy-二进制文件&quot;&gt;&lt;a href=&quot;#下载最新的-kubelet-和-kube-proxy-二进制文件&quot; class=&quot;headerlink&quot; title=&quot;下载最新的 kubelet 和 kube-proxy 二进制文件&quot;&gt;&lt;/a&gt;下载最新的 kubelet 和 kube-proxy 二进制文件&lt;/h4&gt;&lt;p&gt;这个在之前安装kubernetes master时已经下载好了二进制文件，只需要复制到相应目录即可。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 kubernetes]# cd /opt/kubernetes/server/kubernetes/server/bin/
[root@node1 bin]# scp -p kubelet root@172.16.7.152:/usr/local/bin/
[root@node1 bin]# scp -p kube-proxy root@172.16.7.152:/usr/local/bin/
[root@node1 bin]# scp -p kubelet root@172.16.7.153:/usr/local/bin/
[root@node1 bin]# scp -p kube-proxy root@172.16.7.153:/usr/local/bin/
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;配置kubelet&quot;&gt;&lt;a href=&quot;#配置kubelet&quot; class=&quot;headerlink&quot; title=&quot;配置kubelet&quot;&gt;&lt;/a&gt;配置kubelet&lt;/h4&gt;&lt;p&gt;以下操作需要在集群的kubernetes node节点上都要运行，下面以node1服务器为例：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.创建 kubelet 的service配置文件：&lt;/strong&gt;&lt;br&gt;在/usr/lib/systemd/system/下创建文件kubelet.serivce：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /usr/lib/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/kubelet
ExecStart=/usr/local/bin/kubelet \
            $KUBE_LOGTOSTDERR \
            $KUBE_LOG_LEVEL \
            $KUBELET_API_SERVER \
            $KUBELET_ADDRESS \
            $KUBELET_PORT \
            $KUBELET_HOSTNAME \
            $KUBE_ALLOW_PRIV \
            $KUBELET_POD_INFRA_CONTAINER \
            $KUBELET_ARGS
Restart=on-failure

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;2.创建kubelet配置文件&lt;/strong&gt;&lt;br&gt;创建kubelet工作目录（必须创建，否则kubelet启动不了）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# mkdir /var/lib/kubelet
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;创建kubelet配置文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/kubernetes/kubelet
###
## kubernetes kubelet (minion) config
#
## The address for the info server to serve on (set to 0.0.0.0 or &amp;quot;&amp;quot; for all interfaces)
KUBELET_ADDRESS=&amp;quot;--address=172.16.7.151&amp;quot;
#
## The port for the info server to serve on
#KUBELET_PORT=&amp;quot;--port=10250&amp;quot;
#
## You may leave this blank to use the actual hostname
KUBELET_HOSTNAME=&amp;quot;--hostname-override=172.16.7.151&amp;quot;
#
## location of the api-server
KUBELET_API_SERVER=&amp;quot;--api-servers=http://172.16.7.151:8080&amp;quot;
#
## pod infrastructure container
#KUBELET_POD_INFRA_CONTAINER=&amp;quot;--pod-infra-container-image=sz-pg-oam-docker-hub-001.tendcloud.com/library/pod-infrastructure:rhel7&amp;quot;
KUBELET_POD_INFRA_CONTAINER=&amp;quot;--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure&amp;quot;
#
## Add your own!
KUBELET_ARGS=&amp;quot;--cgroup-driver=systemd --cluster-dns=10.254.0.2 --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig --require-kubeconfig --cert-dir=/etc/kubernetes/ssl --cluster-domain=cluster.local. --hairpin-mode promiscuous-bridge --serialize-image-pulls=false&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【注意】：将配置文件中的IP地址更改为你的每台node节点的IP地址（除了–api-servers=&lt;a href=&quot;http://172.16.7.151:8080这个ip地址是不用改的）。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.151:8080这个ip地址是不用改的）。&lt;/a&gt;&lt;br&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;–address 不能设置为 127.0.0.1，否则后续 Pods 访问 kubelet 的 API 接口时会失败，因为 Pods 访问的 127.0.0.1 指向自己而不是 kubelet；&lt;br&gt;如果设置了 –hostname-override 选项，则 kube-proxy 也需要设置该选项，否则会出现找不到 Node 的情况；&lt;/li&gt;
&lt;li&gt;KUBELET_POD_INFRA_CONTAINER=”–pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure”，这个是一个基础容器，每一个Pod启动的时候都会启动一个这样的容器。如果你的本地没有这个镜像，kubelet会连接外网把这个镜像下载下来。最开始的时候是在Google的registry上，因此国内因为GFW都下载不了导致Pod运行不起来。现在每个版本的Kubernetes都把这个镜像打包，你可以提前传到自己的registry上，然后再用这个参数指定。&lt;/li&gt;
&lt;li&gt;–experimental-bootstrap-kubeconfig 指向 bootstrap kubeconfig 文件，kubelet 使用该文件中的用户名和 token 向 kube-apiserver 发送 TLS Bootstrapping 请求；&lt;/li&gt;
&lt;li&gt;管理员通过了 CSR 请求后，kubelet 自动在 –cert-dir 目录创建证书和私钥文件(kubelet-client.crt 和 kubelet-client.key)，然后写入 –kubeconfig 文件；&lt;/li&gt;
&lt;li&gt;建议在 –kubeconfig 配置文件中指定 kube-apiserver 地址，如果未指定 –api-servers 选项，则必须指定 –require-kubeconfig 选项后才从配置文件中读取 kube-apiserver 的地址，否则 kubelet 启动后将找不到 kube-apiserver (日志中提示未找到 API Server），kubectl get nodes 不会返回对应的 Node 信息;&lt;/li&gt;
&lt;li&gt;–cluster-dns 指定 kubedns 的 Service IP(可以先分配，后续创建 kubedns 服务时指定该 IP)，–cluster-domain 指定域名后缀，这两个参数同时指定后才会生效；&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;启动kublet&quot;&gt;&lt;a href=&quot;#启动kublet&quot; class=&quot;headerlink&quot; title=&quot;启动kublet&quot;&gt;&lt;/a&gt;启动kublet&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable kubelet
# systemctl start kubelet
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;通过-kublet-的-TLS-证书请求&quot;&gt;&lt;a href=&quot;#通过-kublet-的-TLS-证书请求&quot; class=&quot;headerlink&quot; title=&quot;通过 kublet 的 TLS 证书请求&quot;&gt;&lt;/a&gt;通过 kublet 的 TLS 证书请求&lt;/h4&gt;&lt;p&gt;kubelet 首次启动时向 kube-apiserver 发送证书签名请求，必须通过后 kubernetes 系统才会将该 Node 加入到集群。&lt;/p&gt;
&lt;p&gt;1.查看未授权的 CSR 请求&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get csr 
NAME        AGE       REQUESTOR           CONDITION
csr-fv3bj   49s       kubelet-bootstrap   Pending
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.通过 CSR 请求&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl certificate approve csr-fv3bj
certificatesigningrequest &amp;quot;csr-fv3bj&amp;quot; approved
[root@node1 kubernetes]# kubectl get csr
NAME        AGE       REQUESTOR           CONDITION
csr-fv3bj   42m       kubelet-bootstrap   Approved,Issued
# kubectl get nodes
NAME           STATUS    AGE       VERSION
172.16.7.151   Ready     18s       v1.6.0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3.查看自动生成的 kubelet kubeconfig 文件和公私钥&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 kubernetes]# ls -l /etc/kubernetes/kubelet.kubeconfig
-rw-------. 1 root root 2215 Sep 13 09:04 /etc/kubernetes/kubelet.kubeconfig
[root@node1 kubernetes]# ls -l /etc/kubernetes/ssl/kubelet*
-rw-r--r--. 1 root root 1046 Sep 13 09:04 /etc/kubernetes/ssl/kubelet-client.crt
-rw-------. 1 root root  227 Sep 13 09:02 /etc/kubernetes/ssl/kubelet-client.key
-rw-r--r--. 1 root root 1111 Sep 13 09:04 /etc/kubernetes/ssl/kubelet.crt
-rw-------. 1 root root 1675 Sep 13 09:04 /etc/kubernetes/ssl/kubelet.key
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在集群中其它的kubernetes node节点上操作完成后，查看集群kubernetes node情况如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get csr
NAME        AGE       REQUESTOR           CONDITION
csr-5n72m   3m        kubelet-bootstrap   Approved,Issued
csr-clwzj   16m       kubelet-bootstrap   Approved,Issued
csr-fv3bj   4h        kubelet-bootstrap   Approved,Issued
# kubectl get nodes
NAME           STATUS    AGE       VERSION
172.16.7.151   Ready     4h        v1.6.0
172.16.7.152   Ready     6m        v1.6.0
172.16.7.153   Ready     12s       v1.6.0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【问题】：切记每台node节点上的kubelet配置文件/etc/kubernetes/kubelet中的ip地址要改正确，否则会出现加入不了的情况。我在将node1节点的/etc/kubernetes/kubelet远程复制到node2节点上，没有修改ip，直接启动了，配置文件中写的ip地址是node1的ip地址，这就造成了node2节点并没有加入进来。采取的恢复操作是：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node2 ~]# systemctl stop kubelet
[root@node2 ~]# cd /etc/kubernetes
[root@node2 kubernetes]# rm -f kubelet.kubeconfig
[root@node2 kubernetes]# rm -rf ~/.kube/cache
# 修改/etc/kubernetes/kubelet中的ip地址
[root@node2 kubernetes]# vim /etc/kubernetes/kubelet
[root@node2 ~]# systemctl start kubelet
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这样，再次启动kubelet时，kube-apiserver才收到证书签名请求。&lt;/p&gt;
&lt;h3 id=&quot;配置-kube-proxy&quot;&gt;&lt;a href=&quot;#配置-kube-proxy&quot; class=&quot;headerlink&quot; title=&quot;配置 kube-proxy&quot;&gt;&lt;/a&gt;配置 kube-proxy&lt;/h3&gt;&lt;p&gt;上面已经把kube-proxy复制到了kubernetes node节点的/usr/local/bin/目录下了，下面开始做配置。每台kubernetes node节点都需要做如下的操作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.创建 kube-proxy 的service配置文件&lt;/strong&gt;&lt;br&gt;在/usr/lib/systemd/system/目录下创建kube-proxy.service：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /usr/lib/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/proxy
ExecStart=/usr/local/bin/kube-proxy \
            $KUBE_LOGTOSTDERR \
            $KUBE_LOG_LEVEL \
            $KUBE_MASTER \
            $KUBE_PROXY_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;2.创建kube-proxy配置文件/etc/kubernetes/proxy&lt;/strong&gt;&lt;br&gt;【注意】：需要修改每台kubernetes node的ip地址。以下以node1主机为例：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/kubernetes/proxy
###
# kubernetes proxy config

# default config should be adequate

# Add your own!
KUBE_PROXY_ARGS=&amp;quot;--bind-address=172.16.7.151 --hostname-override=172.16.7.151 --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig --cluster-cidr=10.254.0.0/16&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;–hostname-override 参数值必须与 kubelet 的值一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 iptables 规则；&lt;/li&gt;
&lt;li&gt;kube-proxy 根据 –cluster-cidr 判断集群内部和外部流量，指定 –cluster-cidr 或 –masquerade-all 选项后 kube-proxy 才会对访问 Service IP 的请求做 SNAT；&lt;/li&gt;
&lt;li&gt;–kubeconfig 指定的配置文件嵌入了 kube-apiserver 的地址、用户名、证书、秘钥等请求和认证信息；&lt;/li&gt;
&lt;li&gt;预定义的 RoleBinding cluster-admin 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;3.启动 kube-proxy&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable kube-proxy
# systemctl start kube-proxy
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;验证测试&quot;&gt;&lt;a href=&quot;#验证测试&quot; class=&quot;headerlink&quot; title=&quot;验证测试&quot;&gt;&lt;/a&gt;验证测试&lt;/h3&gt;&lt;p&gt;创建一个niginx的service试一下集群是否可用。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl run nginx --replicas=2 --labels=&amp;quot;run=load-balancer-example&amp;quot; --image=docker.io/nginx:latest --port=80
deployment &amp;quot;nginx&amp;quot; created                   
# kubectl expose deployment nginx --type=NodePort --name=example-service     
service &amp;quot;example-service&amp;quot; exposed
# kubectl describe svc example-service
Name:                   example-service
Namespace:              default
Labels:                 run=load-balancer-example
Annotations:            &amp;lt;none&amp;gt;
Selector:               run=load-balancer-example
Type:                   NodePort
IP:                     10.254.67.61
Port:                   &amp;lt;unset&amp;gt; 80/TCP
NodePort:               &amp;lt;unset&amp;gt; 32201/TCP
Endpoints:              172.30.32.2:80,172.30.87.2:80
Session Affinity:       None
Events:                 &amp;lt;none&amp;gt;
# kubectl get all
NAME                        READY     STATUS    RESTARTS   AGE
po/nginx-1931613429-nlsj1   1/1       Running   0          5m
po/nginx-1931613429-xr7zk   1/1       Running   0          5m

NAME                  CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
svc/example-service   10.254.67.61   &amp;lt;nodes&amp;gt;       80:32201/TCP   1m
svc/kubernetes        10.254.0.1     &amp;lt;none&amp;gt;        443/TCP        5h

NAME           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deploy/nginx   2         2         2            2           5m

NAME                  DESIRED   CURRENT   READY     AGE
rs/nginx-1931613429   2         2         2         5m

# curl &amp;quot;10.254.67.61:80&amp;quot; 
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&amp;lt;style&amp;gt;
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
&amp;lt;/style&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;p&amp;gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;For online documentation and support please refer to
&amp;lt;a href=&amp;quot;http://nginx.org/&amp;quot;&amp;gt;nginx.org&amp;lt;/a&amp;gt;.&amp;lt;br/&amp;gt;
Commercial support is available at
&amp;lt;a href=&amp;quot;http://nginx.com/&amp;quot;&amp;gt;nginx.com&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;&amp;lt;em&amp;gt;Thank you for using nginx.&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;浏览器输入172.16.7.151:32201或172.16.7.152:32201或者172.16.7.153:32201都可以得到nginx的页面。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;查看运行的容器（在node1和node2上分别运行了一个pod）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# docker ps
CONTAINER ID        IMAGE                                                                                     COMMAND                  CREATED             STATUS              PORTS               NAMES
7d2ef8e34e43        docker.io/nginx@sha256:fc6d2ef47e674a9ffb718b7ac361ec4e421e3a0ef2c93df79abbe4e9ffb5fa08   &amp;quot;nginx -g &amp;apos;daemon off&amp;quot;   40 minutes ago      Up 40 minutes                           k8s_nginx_nginx-1931613429-xr7zk_default_c628f12f-9912-11e7-9acc-005056b7609a_0
5bbb98fba623        registry.access.redhat.com/rhel7/pod-infrastructure                                       &amp;quot;/usr/bin/pod&amp;quot;           42 minutes ago      Up 42 minutes                           k8s_POD_nginx-1931613429-xr7zk_default_c628f12f-9912-11e7-9acc-005056b7609a_0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果想删除刚才创建的deployment：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get deployments
NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx     2         2         2            0           2m
# kubectl delete deployment nginx
deployment &amp;quot;nginx&amp;quot; deleted
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;安装和配置-kube-dns-插件&quot;&gt;&lt;a href=&quot;#安装和配置-kube-dns-插件&quot; class=&quot;headerlink&quot; title=&quot;安装和配置 kube-dns 插件&quot;&gt;&lt;/a&gt;安装和配置 kube-dns 插件&lt;/h2&gt;&lt;h3 id=&quot;kube-dns是什么&quot;&gt;&lt;a href=&quot;#kube-dns是什么&quot; class=&quot;headerlink&quot; title=&quot;kube-dns是什么&quot;&gt;&lt;/a&gt;kube-dns是什么&lt;/h3&gt;&lt;p&gt;刚才在上一步中创建了个Nginx deployment，得到了两个运行nginx服务的Pod。待Pod运行之后查看一下它们的IP，并在k8s集群内通过podIP和containerPort来访问Nginx服务。&lt;br&gt;获取Pod IP：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get pod -o yaml -l run=load-balancer-example|grep podIP 
    podIP: 172.30.32.2
    podIP: 172.30.87.2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后就可以通过podIP在k8s集群内访问Nginx服务了。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# curl &amp;quot;172.30.32.2:80&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;但是这样存在几个问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每次收到获取podIP太扯了，总不能每次都要手动改程序或者配置才能访问服务吧，要怎么提前知道podIP呢？&lt;/li&gt;
&lt;li&gt;Pod在运行中可能会重建，Pod的IP地址会随着Pod的重启而变化,并 不建议直接拿Pod的IP来交互&lt;/li&gt;
&lt;li&gt;如何在多个Pod中实现负载均衡嘞？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;使用k8s Service就可以解决。Service为一组Pod(通过labels来选择)提供一个统一的入口,并为它们提供负载均衡 和自动服务发现。&lt;br&gt;所以紧接着就创建了个service：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl expose deployment nginx --type=NodePort --name=example-service
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;创建之后，仍需要获取Service的Cluster-IP，再结合Port访问Nginx服务。&lt;br&gt;获取IP：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get service example-service
NAME              CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
example-service   10.254.67.61   &amp;lt;nodes&amp;gt;       80:32201/TCP   1h
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在集群内访问Service：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# curl &amp;quot;10.254.67.61:80&amp;quot; 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;而在cluster外面，则只能通过&lt;a href=&quot;http://node-ip:32201来访问。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://node-ip:32201来访问。&lt;/a&gt;&lt;br&gt;虽然Service解决了Pod的服务发现和负载均衡问题，但存在着类似的问题：不提前知道Service的IP，还是需要改程序或配置啊。kube-dns就是用来解决上面这个问题的。&lt;br&gt;kube-dns可以解决Service的发现问题，&lt;br&gt;k8s将Service的名称当做域名注册到kube-dns中，通过Service的名称就可以访问其提供的服务。也就是说其他应用能够直接使用服务的名字，不需要关心它实际的 ip 地址，中间的转换能够自动完成。名字和 ip 之间的转换就是 DNS 系统的功能。&lt;br&gt;kubu-dns 服务不是独立的系统服务，而是一种 addon ，作为插件来安装的，不是 kubernetes 集群必须的（但是非常推荐安装）。可以把它看做运行在集群上的应用，只不过这个应用比较特殊而已。&lt;/p&gt;
&lt;h3 id=&quot;安装配置kube-dns&quot;&gt;&lt;a href=&quot;#安装配置kube-dns&quot; class=&quot;headerlink&quot; title=&quot;安装配置kube-dns&quot;&gt;&lt;/a&gt;安装配置kube-dns&lt;/h3&gt;&lt;p&gt;官方的yaml文件目录：&lt;a href=&quot;https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns。&lt;/a&gt;&lt;br&gt;kube-dns 有两种配置方式，在 1.3 之前使用 etcd + kube2sky + skydns 的方式，在 1.3 之后可以使用 kubedns + dnsmasq 的方式。&lt;br&gt;该插件直接使用kubernetes部署，实际上kube-dns插件只是运行在kube-system命名空间下的Pod，完全可以手动创建它。官方的配置文件中包含以下镜像：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.1
gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.1
gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.1
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;下载yaml文件&quot;&gt;&lt;a href=&quot;#下载yaml文件&quot; class=&quot;headerlink&quot; title=&quot;下载yaml文件&quot;&gt;&lt;/a&gt;下载yaml文件&lt;/h4&gt;&lt;p&gt;从 &lt;a href=&quot;https://github.com/rootsongjc/follow-me-install-kubernetes-cluster/tree/master/manifests/kubedns&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/rootsongjc/follow-me-install-kubernetes-cluster/tree/master/manifests/kubedns&lt;/a&gt; 下载 kubedns-cm.yaml、kubedns-sa.yaml、kubedns-controller.yaml和kubedns-svc.yaml这4个文件下来，并上传到/opt/kube-dns/目录下。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# mkdir /opt/kube-dns
# cd /opt/kube-dns/
# ls kubedns-*
kubedns-cm.yaml  kubedns-controller.yaml  kubedns-sa.yaml  kubedns-svc.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;修改kubedns-controller.yaml文件，将其中的镜像地址改为时速云的地址：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;index.tenxcloud.com/jimmy/k8s-dns-dnsmasq-nanny-amd64:1.14.1
index.tenxcloud.com/jimmy/k8s-dns-kube-dns-amd64:1.14.1
index.tenxcloud.com/jimmy/k8s-dns-sidecar-amd64:1.14.1
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;kubeDNS：提供了原来 kube2sky + etcd + skyDNS 的功能，可以单独对外提供 DNS 查询服务&lt;/li&gt;
&lt;li&gt;dnsmasq： 一个轻量级的 DNS 服务软件，可以提供 DNS 缓存功能。kubeDNS 模式下，dnsmasq 在内存中预留一块大小（默认是 1G）的地方，保存当前最常用的 DNS 查询记录，如果缓存中没有要查找的记录，它会到 kubeDNS 中查询，并把结果缓存起来&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;系统预定义的-RoleBinding&quot;&gt;&lt;a href=&quot;#系统预定义的-RoleBinding&quot; class=&quot;headerlink&quot; title=&quot;系统预定义的 RoleBinding&quot;&gt;&lt;/a&gt;系统预定义的 RoleBinding&lt;/h4&gt;&lt;p&gt;预定义的 RoleBinding system:kube-dns 将 kube-system 命名空间的 kube-dns ServiceAccount 与 system:kube-dns Role 绑定， 该 Role 具有访问 kube-apiserver DNS 相关 API 的权限；&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get clusterrolebindings system:kube-dns -o yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: &amp;quot;true&amp;quot;
  creationTimestamp: 2017-09-14T00:46:08Z
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kube-dns
  resourceVersion: &amp;quot;56&amp;quot;
  selfLink: /apis/rbac.authorization.k8s.io/v1beta1/clusterrolebindingssystem%3Akube-dns
  uid: 18fa2aff-98e6-11e7-a153-005056b7609a
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kube-dns
subjects:
- kind: ServiceAccount
  name: kube-dns
  namespace: kube-system
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;kubedns-controller.yaml 中定义的 Pods 时使用了 kubedns-sa.yaml 文件定义的 kube-dns ServiceAccount，所以具有访问 kube-apiserver DNS 相关 API 的权限。&lt;/p&gt;
&lt;h4 id=&quot;配置-kube-dns-ServiceAccount&quot;&gt;&lt;a href=&quot;#配置-kube-dns-ServiceAccount&quot; class=&quot;headerlink&quot; title=&quot;配置 kube-dns ServiceAccount&quot;&gt;&lt;/a&gt;配置 kube-dns ServiceAccount&lt;/h4&gt;&lt;p&gt;无需修改。&lt;/p&gt;
&lt;h4 id=&quot;配置-kube-dns-服务&quot;&gt;&lt;a href=&quot;#配置-kube-dns-服务&quot; class=&quot;headerlink&quot; title=&quot;配置 kube-dns 服务&quot;&gt;&lt;/a&gt;配置 kube-dns 服务&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# diff kubedns-svc.yaml.base kubedns-svc.yaml
30c30
&amp;lt;   clusterIP: __PILLAR__DNS__SERVER__
---
&amp;gt;   clusterIP: 10.254.0.2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;spec.clusterIP = 10.254.0.2，即明确指定了 kube-dns Service IP，这个 IP 需要和 kubelet 的 –cluster-dns 参数值一致；&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;配置-kube-dns-Deployment&quot;&gt;&lt;a href=&quot;#配置-kube-dns-Deployment&quot; class=&quot;headerlink&quot; title=&quot;配置 kube-dns Deployment&quot;&gt;&lt;/a&gt;配置 kube-dns Deployment&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# diff kubedns-controller.yaml.base kubedns-controller.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【说明】：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用系统已经做了 RoleBinding 的 kube-dns ServiceAccount，该账户具有访问 kube-apiserver DNS 相关 API 的权限；&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;执行所有定义文件&quot;&gt;&lt;a href=&quot;#执行所有定义文件&quot; class=&quot;headerlink&quot; title=&quot;执行所有定义文件&quot;&gt;&lt;/a&gt;执行所有定义文件&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# pwd
/opt/kube-dns
# ls
kubedns-cm.yaml  kubedns-controller.yaml  kubedns-sa.yaml  kubedns-svc.yaml
# kubectl create -f .
configmap &amp;quot;kube-dns&amp;quot; created
deployment &amp;quot;kube-dns&amp;quot; created
serviceaccount &amp;quot;kube-dns&amp;quot; created
service &amp;quot;kube-dns&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在3台node节点上查看生成的kube-dns相关pod和container：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node2 ~]# docker ps
CONTAINER ID        IMAGE                                                                                                                           COMMAND                  CREATED             STATUS              PORTS               NAMES
9b1dbfde7eac        index.tenxcloud.com/jimmy/k8s-dns-sidecar-amd64@sha256:947271f3e08b1fd61c4b26478f08d3a8f10bbca90d4dec067e3b33be08066970         &amp;quot;/sidecar --v=2 --log&amp;quot;   4 hours ago         Up 4 hours                              k8s_sidecar_kube-dns-351402727-6vnsj_kube-system_efb96c05-9928-11e7-9acc-005056b7609a_0
a455dc0a9b55        index.tenxcloud.com/jimmy/k8s-dns-dnsmasq-nanny-amd64@sha256:b253876345427dbd626b145897be51d87bfd535e2cd5d7d166deb97ea37701f8   &amp;quot;/dnsmasq-nanny -v=2 &amp;quot;   4 hours ago         Up 4 hours                              k8s_dnsmasq_kube-dns-351402727-6vnsj_kube-system_efb96c05-9928-11e7-9acc-005056b7609a_0
7f18c10c8d60        index.tenxcloud.com/jimmy/k8s-dns-kube-dns-amd64@sha256:94426e872d1a4a0cf88e6c5cd928a1acbe1687871ae5fe91ed751593aa6052d3        &amp;quot;/kube-dns --domain=c&amp;quot;   4 hours ago         Up 4 hours                              k8s_kubedns_kube-dns-351402727-6vnsj_kube-system_efb96c05-9928-11e7-9acc-005056b7609a_0
a6feb213296b        registry.access.redhat.com/rhel7/pod-infrastructure                                                                             &amp;quot;/usr/bin/pod&amp;quot;           4 hours ago         Up 4 hours                              k8s_POD_kube-dns-351402727-6vnsj_kube-system_efb96c05-9928-11e7-9acc-005056b7609a_0
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;检查-kube-dns-功能&quot;&gt;&lt;a href=&quot;#检查-kube-dns-功能&quot; class=&quot;headerlink&quot; title=&quot;检查 kube-dns 功能&quot;&gt;&lt;/a&gt;检查 kube-dns 功能&lt;/h3&gt;&lt;p&gt;上面是通过 kubectl run 来启动了第一个Pod，但是并不支持所有的功能。使用kubectl run在设定很复杂的时候需要非常长的一条语句，敲半天也很容易出错，也没法保存，在碰到转义字符的时候也经常会很抓狂，所以更多场景下会使用yaml或者json文件，而使用kubectl create或者delete就可以利用这些yaml文件。通过 kubectl create -f file.yaml 来创建资源。kubectl run 并不是直接创建一个Pod，而是先创建一个Deployment资源 (replicas=1)，再由Deployment来自动创建Pod。&lt;/p&gt;
&lt;p&gt;新建一个 Deployment：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 kube-dns]# vim my-nginx.yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: my-nginx
spec:
  replicas: 2
  template:
    metadata:
      labels:
        run: my-nginx
    spec:
      containers:
      - name: my-nginx
        image: docker.io/nginx:latest                                  
        ports:
        - containerPort: 80
# kubectl create -f my-nginx.yaml
deployment &amp;quot;my-nginx&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Export 该 Deployment，生成 my-nginx 服务:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl expose deploy my-nginx 
service &amp;quot;my-nginx&amp;quot; exposed
# kubectl get services --all-namespaces |grep my-nginx
default       my-nginx          10.254.34.181   &amp;lt;none&amp;gt;        80/TCP          26s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;创建另一个 Pod，查看 /etc/resolv.conf 是否包含 kubelet 配置的 –cluster-dns 和 –cluster-domain，是否能够将服务my-nginx 解析到 Cluster IP 10.254.34.181。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 kube-dns]# vim dns-test-busybox.yaml
apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: default
spec:
  containers:
  - image: busybox
    command:
      - sleep
      - &amp;quot;3600&amp;quot;
    imagePullPolicy: IfNotPresent
    name: busybox
  restartPolicy: Always
[root@node1 kube-dns]# kubectl create -f dns-test-busybox.yaml
pod &amp;quot;busybox&amp;quot; created
[root@node1 kube-dns]# kubectl exec -ti busybox -- nslookup kubernetes.default
Server:    10.254.0.2
Address 1: 10.254.0.2 kube-dns.kube-system.svc.cluster.local

Name:      kubernetes.default
Address 1: 10.254.0.1 kubernetes.default.svc.cluster.local

kubectl exec -ti busybox -- ping my-nginx
PING my-nginx (10.254.34.181): 56 data bytes

kubectl exec -ti busybox -- ping kubernetes
PING kubernetes (10.254.0.1): 56 data bytes

kubectl exec -ti busybox -- ping kube-dns.kube-system.svc.cluster.local
PING kube-dns.kube-system.svc.cluster.local (10.254.0.2): 56 data bytes
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;从结果来看，service名称可以正常解析。&lt;br&gt;使用kubernetes的时候建议不要再用docker命令操作。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Kubernetes简介&quot;&gt;&lt;a href=&quot;#Kubernetes简介&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes简介&quot;&gt;&lt;/a&gt;Kubernetes简介&lt;/h2&gt;&lt;p&gt;Kubernetes是谷歌开源的容器集群管理系统，是Google多年大规模容器管理技术Borg的 开源版本,主要功能包括:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于容器的应用部署、维护和滚动升级&lt;/li&gt;
&lt;li&gt;负载均衡和服务发现&lt;/li&gt;
&lt;li&gt;跨机器和跨地区的集群调度
    
    </summary>
    
      <category term="Kubernetes" scheme="http://yoursite.com/categories/Kubernetes/"/>
    
    
      <category term="容器编排 Docker Kubernetes" scheme="http://yoursite.com/tags/%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92-Docker-Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>企业级Docker Registry —— Harbor搭建和使用</title>
    <link href="http://yoursite.com/2017/09/08/%E4%BC%81%E4%B8%9A%E7%BA%A7Docker-Registry-%E2%80%94%E2%80%94-Harbor%E6%90%AD%E5%BB%BA%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    <id>http://yoursite.com/2017/09/08/企业级Docker-Registry-——-Harbor搭建和使用/</id>
    <published>2017-09-08T08:38:52.000Z</published>
    <updated>2017-09-09T01:41:42.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;Harbor介绍&quot;&gt;&lt;a href=&quot;#Harbor介绍&quot; class=&quot;headerlink&quot; title=&quot;Harbor介绍&quot;&gt;&lt;/a&gt;Harbor介绍&lt;/h2&gt;&lt;p&gt;Harbor是由VMWare公司开源的容器镜像仓库。事实上，Habor是在Docker Registry上进行了相应的企业级扩展，从而获得了更加广泛的应用，这些新的企业级特性包括：管理用户界面，基于角色的访问控制 ，AD/LDAP集成以及审计日志等。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;安装部署Harbor&quot;&gt;&lt;a href=&quot;#安装部署Harbor&quot; class=&quot;headerlink&quot; title=&quot;安装部署Harbor&quot;&gt;&lt;/a&gt;安装部署Harbor&lt;/h2&gt;&lt;p&gt;官方安装文档：&lt;br&gt;&lt;a href=&quot;https://github.com/vmware/harbor/blob/master/docs/installation_guide.md&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/vmware/harbor/blob/master/docs/installation_guide.md&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;环境要求&quot;&gt;&lt;a href=&quot;#环境要求&quot; class=&quot;headerlink&quot; title=&quot;环境要求&quot;&gt;&lt;/a&gt;环境要求&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;python 2.7+&lt;/li&gt;
&lt;li&gt;docker 1.10+&lt;/li&gt;
&lt;li&gt;docker-compose 1.6.0+&lt;br&gt;Docker的安装见文章《Docker镜像和容器》和docker-compose的安装见文章《Registry私有仓库搭建及认证》。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;环境信息&quot;&gt;&lt;a href=&quot;#环境信息&quot; class=&quot;headerlink&quot; title=&quot;环境信息&quot;&gt;&lt;/a&gt;环境信息&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;操作系统版本&lt;/th&gt;
&lt;th&gt;IP地址&lt;/th&gt;
&lt;th&gt;安装软件&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;spark32&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.206.32&lt;/td&gt;
&lt;td&gt;docker-ce 17.06.1、docker-compose 1.15.0、harbor-online-installer-v1.1.2.tar&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&quot;安装部署harbor&quot;&gt;&lt;a href=&quot;#安装部署harbor&quot; class=&quot;headerlink&quot; title=&quot;安装部署harbor&quot;&gt;&lt;/a&gt;安装部署harbor&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# cd /opt/
[root@spark32 opt]# mkdir harbor
[root@spark32 opt]# cd harbor/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;下载地址：&lt;a href=&quot;https://github.com/vmware/harbor/releases&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/vmware/harbor/releases&lt;/a&gt;&lt;br&gt;由于我这里服务器可以联网，离线版本又很大，所以下载的在线安装版本的1.1.2版本，将软件上传到/opt/harbor/目录下。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 harbor]# tar xvf harbor-online-installer-v1.1.2.tar 
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;配置harbor&quot;&gt;&lt;a href=&quot;#配置harbor&quot; class=&quot;headerlink&quot; title=&quot;配置harbor&quot;&gt;&lt;/a&gt;配置harbor&lt;/h3&gt;&lt;p&gt;可配置的参数在文件harbor.cfg中。在该文件中，有两种参数，必须配置的和可选的。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;必须的：这些参数会在用户更新配置文件和执行install.sh脚本重新安装harbor起作用。你至少得设置 hostname 参数。&lt;/li&gt;
&lt;li&gt;可选的：用户可以就使用默认值，或者在启动后通过web ui进行修改。如果这些可选参数被设置在harbor.cfg文件中，它们只会在第一次启动时起作用，以后在harbor.cfg文件中修改会被忽略。也就是说在 Harbor 初次启动时，Admin Server 从 harbor.cfg 文件读取配置并记录下来。之后重新启动Harbor的过程中，只有必需的配置会从 harbor.cfg 文件读取；其他可选的配置将不再生效，需要通过 Admin Server 的管理界面来修改。&lt;br&gt;&lt;strong&gt;【注意】:如果你通过web ui设置这些参数，请在harbor启动后立即设置。尤其是，必须先设置 auth_mode 在你注册或创建任何用户之前。&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些具体参数的名字和意义详见&lt;a href=&quot;https://github.com/vmware/harbor/blob/master/docs/installation_guide.md&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/vmware/harbor/blob/master/docs/installation_guide.md&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 harbor]# cd harbor/
[root@spark32 harbor]# vim harbor.cfg
# 指定 hostname，为IP或者域名，用于登录 Web UI 界面
hostname = 172.16.206.32
# mysql 数据库 root 账户密码
db_password = wisedu123
# 邮件相关信息配置，如忘记密码发送邮件
email_server = smtp.exmail.qq.com
email_server_port = 465
email_username = 01115004@wisedu.com
email_password = zjk230640
email_from = admin &amp;lt;01115004@wisedu.com&amp;gt;
email_ssl = on
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;配置存储&quot;&gt;&lt;a href=&quot;#配置存储&quot; class=&quot;headerlink&quot; title=&quot;配置存储&quot;&gt;&lt;/a&gt;配置存储&lt;/h3&gt;&lt;p&gt;默认情况下，harbor把镜像存储在本地文件系统，在生产环境中，你可能考虑用其他的存储替代本地存储，比如S3, Openstack Swift, Ceph等等。你需要修改 common/templates/registry/config.yml 中的 storage 段。比如使用Openstack Swift，storage段的配置类似如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;storage:
  swift:
    username: admin
    password: ADMIN_PASS
    authurl: http://keystone_addr:35357/v3/auth
    tenant: admin
    domain: default
    region: regionOne
    container: docker_images
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;更详细的存储配置，见 &lt;a href=&quot;https://docs.docker.com/registry/configuration/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Registry Configuration Reference&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;完成安装和启动harbor&quot;&gt;&lt;a href=&quot;#完成安装和启动harbor&quot; class=&quot;headerlink&quot; title=&quot;完成安装和启动harbor&quot;&gt;&lt;/a&gt;完成安装和启动harbor&lt;/h3&gt;&lt;p&gt;一旦harbor.cfg和后端存储(可选的)配置完成，就可以使用install.sh脚本安装和启动harbor了。注意在线安装可能需要等待一些时间从dockerhubs下载harbor镜像。&lt;br&gt;&lt;strong&gt;【注意】：请确保主机上80和443端口没被占用。如果想修改端口，需要去修改docker-compose.yml文件&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 harbor]# pwd
/opt/harbor/harbor
[root@spark32 harbor]# ./install.sh 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我这里配置了阿里云容器加速，所以在线版安装没有问题，如果有网络问题可以选择离线包安装。&lt;/p&gt;
&lt;h2 id=&quot;访问Harbor&quot;&gt;&lt;a href=&quot;#访问Harbor&quot; class=&quot;headerlink&quot; title=&quot;访问Harbor&quot;&gt;&lt;/a&gt;访问Harbor&lt;/h2&gt;&lt;p&gt;浏览器输入&lt;a href=&quot;http://172.16.206.32&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.206.32&lt;/a&gt;&lt;br&gt;默认账号密码是admin/Harbor12345。默认是80端口，如果端口占用，我们可以去修改docker-compose.yml文件中，对应服务的端口映射。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/36.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/37.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;我们可以看到系统各个模块如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;项目：新增/删除项目，查看镜像仓库，给项目添加成员、查看操作日志、复制项目等&lt;/li&gt;
&lt;li&gt;日志：仓库各个镜像create、push、pull等操作日志&lt;/li&gt;
&lt;li&gt;系统管理 &lt;ul&gt;
&lt;li&gt;用户管理：新增/删除用户、设置管理员等&lt;/li&gt;
&lt;li&gt;复制管理：新增/删除从库目标、新建/删除/启停复制规则等&lt;/li&gt;
&lt;li&gt;配置管理：认证模式、复制、邮箱设置、系统设置等&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;其他设置 &lt;ul&gt;
&lt;li&gt;用户设置：修改用户名、邮箱、名称信息&lt;/li&gt;
&lt;li&gt;修改密码：修改用户密码&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;注意：非系统管理员用户登录，只能看到有权限的项目和日志，其他模块不可见。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;修改管理员密码&quot;&gt;&lt;a href=&quot;#修改管理员密码&quot; class=&quot;headerlink&quot; title=&quot;修改管理员密码&quot;&gt;&lt;/a&gt;修改管理员密码&lt;/h3&gt;&lt;p&gt;点击右上角的admin，点击修改密码。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/38.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;启动后相关容器&quot;&gt;&lt;a href=&quot;#启动后相关容器&quot; class=&quot;headerlink&quot; title=&quot;启动后相关容器&quot;&gt;&lt;/a&gt;启动后相关容器&lt;/h3&gt;&lt;p&gt;Harbor的所有服务组件都是在Docker中部署的。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 harbor]# ls
common  docker-compose.notary.yml  docker-compose.yml  harbor_1_1_0_template  harbor.cfg  install.sh  LICENSE  NOTICE  prepare  upgrade
[root@spark32 harbor]# docker-compose ps
       Name                     Command               State                                Ports                               
------------------------------------------------------------------------------------------------------------------------------
harbor-adminserver   /harbor/harbor_adminserver       Up                                                                       
harbor-db            docker-entrypoint.sh mysqld      Up      3306/tcp                                                         
harbor-jobservice    /harbor/harbor_jobservice        Up                                                                       
harbor-log           /bin/sh -c crond &amp;amp;&amp;amp; rm -f  ...   Up      127.0.0.1:1514-&amp;gt;514/tcp                                          
harbor-ui            /harbor/harbor_ui                Up                                                                       
nginx                nginx -g daemon off;             Up      0.0.0.0:443-&amp;gt;443/tcp, 0.0.0.0:4443-&amp;gt;4443/tcp, 0.0.0.0:80-&amp;gt;80/tcp 
registry             /entrypoint.sh serve /etc/ ...   Up      5000/tcp 
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;harbor-adminserver：用来管理系统配置，并提供了相应的 WEB 页面和 API 来供用户操作，改进了之前需用户手动修改配置文件并手动重启系统的用户体验。&lt;/li&gt;
&lt;li&gt;harbor-db : 由官方MySql镜像构成的数据库容器&lt;/li&gt;
&lt;li&gt;harbor-jobservice：是harbor的job管理模块，job在harbor里面主要是为了镜像仓库同步使用的。&lt;/li&gt;
&lt;li&gt;harbor-log : 运行着rsyslogd的容器，通过log-driver的形式收集其他容器的日志&lt;/li&gt;
&lt;li&gt;harbor-ui : 即架构中的core services, 构成此容器的代码是Harbor项目的主体&lt;/li&gt;
&lt;li&gt;nginx : 由 nginx 服务器构成的反向代理&lt;/li&gt;
&lt;li&gt;registry : 由Docker官方的开源 registry 镜像构成的容器实例&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这几个 Contianer 通过 Docker link 的形式连接在一起，在容器之间通过容器名字互相访问。对终端用户而言，只需要暴露 proxy（即Nginx）的服务端口。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;【注意】:之前的版本更新配置，需要修改harbor.cfg，然后停止并删除现有Harbor实例，再重新运行Harbor，比较繁琐。新版本的adminconsole可以使用户很方便地通过WEB界面配置认证、同步、邮件和系统等信息，修改立即生效，无需重启整个系统。&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&quot;Harbor持久化数据和日志&quot;&gt;&lt;a href=&quot;#Harbor持久化数据和日志&quot; class=&quot;headerlink&quot; title=&quot;Harbor持久化数据和日志&quot;&gt;&lt;/a&gt;Harbor持久化数据和日志&lt;/h2&gt;&lt;p&gt;默认情况下，registrys数据被持久化在宿主机的/data/目录下，甚至你删除harbor容器或者重新被创建，这部分数据也不会改变。&lt;br&gt;另外，harbor使用rsyslog来收集每一个容器日志，默认情况下，这些日志存放在宿主机的/var/log/harbor/目录下。&lt;/p&gt;
&lt;h2 id=&quot;管理Harbor的生命&quot;&gt;&lt;a href=&quot;#管理Harbor的生命&quot; class=&quot;headerlink&quot; title=&quot;管理Harbor的生命&quot;&gt;&lt;/a&gt;管理Harbor的生命&lt;/h2&gt;&lt;p&gt;可以使用docker-compose来管理harbor的启动、停止和销毁。但是注意必须切换到docker-compose.yml同级目录运行以下的命令。&lt;/p&gt;
&lt;h3 id=&quot;停止harbor&quot;&gt;&lt;a href=&quot;#停止harbor&quot; class=&quot;headerlink&quot; title=&quot;停止harbor&quot;&gt;&lt;/a&gt;停止harbor&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;# docker-compose stop
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;启动harbor&quot;&gt;&lt;a href=&quot;#启动harbor&quot; class=&quot;headerlink&quot; title=&quot;启动harbor&quot;&gt;&lt;/a&gt;启动harbor&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;# docker-compose start
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;修改配置后启动&quot;&gt;&lt;a href=&quot;#修改配置后启动&quot; class=&quot;headerlink&quot; title=&quot;修改配置后启动&quot;&gt;&lt;/a&gt;修改配置后启动&lt;/h3&gt;&lt;p&gt;先停止harbor，在修改配置文件harbor.cfg，然后运行prepare脚本应用配置，最后重新创建harbor并运行它。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# docker-compose down -v
# vim harbor.cfg
# prepare
# docker-compose up -d
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;清除harbor容器，保留镜像和数据&quot;&gt;&lt;a href=&quot;#清除harbor容器，保留镜像和数据&quot; class=&quot;headerlink&quot; title=&quot;清除harbor容器，保留镜像和数据&quot;&gt;&lt;/a&gt;清除harbor容器，保留镜像和数据&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;# docker-compose down -v
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;删除harbors数据库和镜像-用于干净的重新安装&quot;&gt;&lt;a href=&quot;#删除harbors数据库和镜像-用于干净的重新安装&quot; class=&quot;headerlink&quot; title=&quot;删除harbors数据库和镜像(用于干净的重新安装)&quot;&gt;&lt;/a&gt;删除harbors数据库和镜像(用于干净的重新安装)&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;# rm -r /data/database
# rm -r /data/registry
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;Harbor的安全机制&quot;&gt;&lt;a href=&quot;#Harbor的安全机制&quot; class=&quot;headerlink&quot; title=&quot;Harbor的安全机制&quot;&gt;&lt;/a&gt;Harbor的安全机制&lt;/h2&gt;&lt;p&gt;企业中的软件研发团队往往划分为诸多角色，如项目经理、产品经理、测试、运维等。在实际的软件开发和运维过程中，这些角色对于镜像的使用需求是不一样的。比如：开发人员需要拥有对镜像的读写(PULL/PUSH)权限以更新和改正代码；测试人员中需要读取(PULL)权限；而项目经理需要对上述的角色进行管理。&lt;br&gt;Harbor为这种需求提供了用户和成员两种管理概念。&lt;/p&gt;
&lt;h3 id=&quot;用户&quot;&gt;&lt;a href=&quot;#用户&quot; class=&quot;headerlink&quot; title=&quot;用户&quot;&gt;&lt;/a&gt;用户&lt;/h3&gt;&lt;p&gt;用户主要分两类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;管理员&lt;/li&gt;
&lt;li&gt;普通用户。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;两类用户都可以成为项目的成员。而管理员可以对用户进行管理。&lt;/p&gt;
&lt;h3 id=&quot;成员&quot;&gt;&lt;a href=&quot;#成员&quot; class=&quot;headerlink&quot; title=&quot;成员&quot;&gt;&lt;/a&gt;成员&lt;/h3&gt;&lt;p&gt;成员是对应于项目的概念，分为三类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;管理员&lt;/li&gt;
&lt;li&gt;开发者&lt;/li&gt;
&lt;li&gt;访客&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;管理员可以对开发者和访客作权限的配置和管理。测试和运维人员可以访客身份读取项目镜像，或者公共镜像库中的文件。&lt;br&gt;从项目的角度出发，项目管理员拥有最大的项目权限，如果要对用户进行禁用或限权等，可以通过修改用户在项目中的成员角色来实现，甚至将用户移除出这个项目。&lt;br&gt;下面以实际操作来演示。&lt;/p&gt;
&lt;h2 id=&quot;Harbor使用&quot;&gt;&lt;a href=&quot;#Harbor使用&quot; class=&quot;headerlink&quot; title=&quot;Harbor使用&quot;&gt;&lt;/a&gt;Harbor使用&lt;/h2&gt;&lt;p&gt;官方使用文档：&lt;a href=&quot;https://github.com/vmware/harbor/blob/master/docs/user_guide.md&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/vmware/harbor/blob/master/docs/user_guide.md&lt;/a&gt;&lt;br&gt;注意：当项目设为公开后，任何人都有此项目下镜像的读权限。命令行用户不需要“docker login”就可以拉取此项目下的镜像。所以一般需要建立私有项目。&lt;br&gt;1.登录harbor，点击“+项目”&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/39.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;2.点击左侧菜单“用户管理”，点击“+用户”&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/40.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;3.点击左侧菜单项目，选择刚才创建的项目“godseye”，在点击右侧正文中的选项卡“成员”，点击“+成员”，输入刚才创建的用户，并设置其为管理员。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/41.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;对于权限(角色)，项目管理员和开发人员可以有 push 的权限，而访客只能查看和 pull&lt;/p&gt;
&lt;p&gt;4.测试&lt;br&gt;我这里找了另外一台机器，安装了docker 1.12.6。由于这里harbor采用了默认的 http 方式连接，而 Docker 认为这是不安全的，所以在 push 之前需要调整一下 docker 配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node3 ~]# vim /etc/docker/daemon.json 
{
   &amp;quot;insecure-registries&amp;quot;: [&amp;quot;172.16.206.32&amp;quot;]
}
[root@node3 ~]# systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;登录harbor：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node3 ~]# docker login 172.16.206.32
Username: jkzhao
Password: 
Login Succeeded
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后 tag 一个 image，名称一定要标准( registryAddress[:端口]/项目/imageName[:tag] )，最后将其 push 即可&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node3 ~]# docker tag centos:centos7 172.16.206.32/godseye/centos:latest
[root@node3 ~]# docker push 172.16.206.32/godseye/centos:latest
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后到web ui上查看刚才push的镜像是否成功了：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/42.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;镜像删除和空间回收&quot;&gt;&lt;a href=&quot;#镜像删除和空间回收&quot; class=&quot;headerlink&quot; title=&quot;镜像删除和空间回收&quot;&gt;&lt;/a&gt;镜像删除和空间回收&lt;/h2&gt;&lt;p&gt;Docker命令没有提供Registry镜像删除功能，日积月累，将会产生许多无用的镜像，占用大量存储空间。若要删除镜像并回收空间，需要调用docker registry API来完成，比较麻烦。Harbor提供了可视化的镜像删除界面，可以逻辑删除镜像。在维护状态下可以回收垃圾镜像的空间。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node3 ~]# docker images
REPOSITORY                           TAG                    IMAGE ID            CREATED             SIZE
java                                 openjdk-8-jre-alpine   d61ff40a5bf6        16 months ago       108.3 MB
[root@node3 ~]# docker login 172.16.206.32
Username: jkzhao
Password: 
Login Succeeded
[root@node3 ~]# docker tag java:openjdk-8-jre-alpine 172.16.206.32/godseye/jdk:openjdk-8-jre-alpine
[root@node3 ~]# docker push 172.16.206.32/godseye/jdk:openjdk-8-jre-alpine
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们先查看下宿主机上存放镜像的目录大小：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 2017-09-08]# du -sh /data/registry/docker/registry/v2/
110M    /data/registry/docker/registry/v2/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;登录harbor界面，点击godseye项目，删除刚才上传的镜像：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/43.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;但是实际上这只是逻辑删除，我们可以查看此时宿主机上存放镜像的目录大小，仍然是110M：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 2017-09-08]# du -sh /data/registry/docker/registry/v2/
110M    /data/registry/docker/registry/v2/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;此时你完全可以再次上传这个镜像，会显示这些镜像层已经存在了：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node3 ~]# docker push 172.16.206.32/godseye/jdk:openjdk-8-jre-alpine
The push refers to a repository [172.16.206.32/godseye/jdk]
2b4866cc0048: Layer already exists 
5f70bf18a086: Layer already exists 
82a47053c51a: Layer already exists 
8f01a53880b9: Layer already exists 
openjdk-8-jre-alpine: digest:sha256:56b1ffe13af2ee1c5e2c9a3d3cd8c377b5f1bc6130a87648d48ba3fffab0d5eb size: 1977
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;那么如何彻底删除这个镜像呢？&lt;/strong&gt;&lt;br&gt;1.首先去界面删除这个镜像&lt;br&gt;2.在harbor宿主机上执行如下的命令：&lt;br&gt;先找到当前的registry版本：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 harbor]# docker images vmware/registry
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
vmware/registry     2.6.1-photon        0f6c96580032        3 months ago        150MB
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;列出要删除的镜像：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 harbor]# docker run -it --name gc --rm --volumes-from registry vmware/registry:2.6.1-photon garbage-collect --dry-run /etc/registry/config.yml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;选项 –dry-run 只是在最后打印出界面删除了的但是实际上并未删除的镜像层，但是这条命令不会删除这些镜像层。&lt;br&gt;运行下面的命令删除镜像：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 harbor]# docker run -it --name gc --rm --volumes-from registry vmware/registry:2.6.1-photon garbage-collect  /etc/registry/config.yml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;再次查看存放镜像的目录大小：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 2017-09-08]# du -sh /data/registry/docker/registry/v2/                                              
70M     /data/registry/docker/registry/v2/
&lt;/code&gt;&lt;/pre&gt;</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Harbor介绍&quot;&gt;&lt;a href=&quot;#Harbor介绍&quot; class=&quot;headerlink&quot; title=&quot;Harbor介绍&quot;&gt;&lt;/a&gt;Harbor介绍&lt;/h2&gt;&lt;p&gt;Harbor是由VMWare公司开源的容器镜像仓库。事实上，Habor是在Docker Registry上进行了相应的企业级扩展，从而获得了更加广泛的应用，这些新的企业级特性包括：管理用户界面，基于角色的访问控制 ，AD/LDAP集成以及审计日志等。&lt;br&gt;
    
    </summary>
    
      <category term="Docker" scheme="http://yoursite.com/categories/Docker/"/>
    
    
      <category term="容器 Docker Registry" scheme="http://yoursite.com/tags/%E5%AE%B9%E5%99%A8-Docker-Registry/"/>
    
  </entry>
  
  <entry>
    <title>overlay实现容器跨主机通信</title>
    <link href="http://yoursite.com/2017/09/05/overlay%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B7%A8%E4%B8%BB%E6%9C%BA%E9%80%9A%E4%BF%A1/"/>
    <id>http://yoursite.com/2017/09/05/overlay实现容器跨主机通信/</id>
    <published>2017-09-05T05:42:06.000Z</published>
    <updated>2017-09-07T01:44:59.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;Docker容器跨主机通信方案&quot;&gt;&lt;a href=&quot;#Docker容器跨主机通信方案&quot; class=&quot;headerlink&quot; title=&quot;Docker容器跨主机通信方案&quot;&gt;&lt;/a&gt;Docker容器跨主机通信方案&lt;/h2&gt;&lt;p&gt;实现跨主机的容器通信有很多种方案，需要看实际的网络状况，是云上环境，私有云环境，还是混合云环境；是否有SDN对网络做特殊控制等等。网络状况不一样，适用的方案也会不一样。比如有的环境可以使用路由的方案，有的却不能使用。不考虑网络模型的话，基本是两个派别：overlay和路由方案。&lt;br&gt;Docker 1.12中把swarmkit集成到了docker中，本篇博客使用的版本是docker 1.11版本，这是我以前做的一个方案，现整理出来。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;环境信息&quot;&gt;&lt;a href=&quot;#环境信息&quot; class=&quot;headerlink&quot; title=&quot;环境信息&quot;&gt;&lt;/a&gt;环境信息&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;操作系统版本&lt;/th&gt;
&lt;th&gt;IP地址&lt;/th&gt;
&lt;th&gt;Docker版本&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;node1&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.7.151&lt;/td&gt;
&lt;td&gt;1.11.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;node2&lt;/td&gt;
&lt;td&gt;CentOS 7.0&lt;/td&gt;
&lt;td&gt;172.16.7.152&lt;/td&gt;
&lt;td&gt;1.11.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&quot;升级内核&quot;&gt;&lt;a href=&quot;#升级内核&quot; class=&quot;headerlink&quot; title=&quot;升级内核&quot;&gt;&lt;/a&gt;升级内核&lt;/h2&gt;&lt;p&gt;默认内核：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# uname -r
3.10.0-229.el7.x86_64
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;1.升级内核需要使用 elrepo 的yum 源&lt;br&gt;首先我们导入 elrepo 的key&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.安装 elrepo 源&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3.在yum的ELRepo源中，mainline 为最新版本的内核&lt;br&gt;安装 ml 的内核&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# yum --enablerepo=elrepo-kernel install  kernel-ml-devel kernel-ml -y
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;4.修改内核启动顺序，默认启动的顺序应该为1,升级以后内核是往前面插入，为0&lt;br&gt;由于CentOS 7使用grub2作为引导程序 ，所以和CentOS 6有所不同，并不是修改/etc/grub.conf来修改启动项，需要如下操作：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# cat /boot/grub2/grub.cfg |grep menuentry   #查看有哪些内核选项
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/21.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;        [root@node1 ~]# grub2-editenv list&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/22.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;        [root@node1 ~]# grub2-set-default 0&lt;/p&gt;
&lt;p&gt;5.重启系统&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# shutdown -r now
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;6.查看 内核版本&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# uname -r
4.5.2-1.el7.elrepo.x86_64
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;安装docker&quot;&gt;&lt;a href=&quot;#安装docker&quot; class=&quot;headerlink&quot; title=&quot;安装docker&quot;&gt;&lt;/a&gt;安装docker&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;[root@node1 ~]# vim /etc/yum.repos.d/docker.repo
[dockerrepo]
name=Docker Repository
baseurl=https://yum.dockerproject.org/repo/main/centos/$releasever/
enabled=1
gpgcheck=1
gpgkey=https://yum.dockerproject.org/gpg

[root@node1 ~]# yum install -y docker-engine
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果后面升级：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# yum update docker-engine
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;卸载：yum remove  &lt;/p&gt;
&lt;h2 id=&quot;防火墙设置和开启内核转发&quot;&gt;&lt;a href=&quot;#防火墙设置和开启内核转发&quot; class=&quot;headerlink&quot; title=&quot;防火墙设置和开启内核转发&quot;&gt;&lt;/a&gt;防火墙设置和开启内核转发&lt;/h2&gt;&lt;p&gt;停止firewalld，安装iptables-services&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# systemctl stop firewalld.service
[root@node1 ~]# systemctl disable firewalld.service
[root@node1 ~]# yum install -y iptables-services
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;修改防火墙策略：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# vim /etc/sysconfig/iptables
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/23.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;        [root@node1 ~]# systemctl start iptables.service&lt;br&gt;        [root@node1 ~]# systemctl enable iptables.service&lt;/p&gt;
&lt;p&gt;开启内核转发，在/etc/sysctl.conf中添加一行配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# vim /etc/sysctl.conf 
net.ipv4.ip_forward=1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;运行下面的命令使内核修改生效：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# sysctl -p
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;安装启动consul&quot;&gt;&lt;a href=&quot;#安装启动consul&quot; class=&quot;headerlink&quot; title=&quot;安装启动consul&quot;&gt;&lt;/a&gt;安装启动consul&lt;/h2&gt;&lt;p&gt;overlay一般需要一个全局的KV存储（sdn controller、etcd、consul）来存储各个主机节点在overlay网络中的配置信息。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# wget https://releases.hashicorp.com/consul/0.6.4/consul_0.6.4_linux_amd64.zip
# unzip -oq consul_0.6.4_linux_amd64.zip
# mv consul /usr/local/bin/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;启动consul：&lt;br&gt;host-1 Start Consul as a server in bootstrap mode:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# nohup consul agent -server -bootstrap -data-dir /tmp/consul -bind=172.16.7.151 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;host-2 Start the Consul agent:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node2 ~]# nohup consul agent -data-dir /tmp/consul -bind=172.16.7.152 &amp;amp;
[root@node2 ~]# consul join 172.16.7.151
Successfully joined cluster by contacting 1 nodes.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/24.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;启动Docker&quot;&gt;&lt;a href=&quot;#启动Docker&quot; class=&quot;headerlink&quot; title=&quot;启动Docker&quot;&gt;&lt;/a&gt;启动Docker&lt;/h2&gt;&lt;h3 id=&quot;修改docker-daemon配置&quot;&gt;&lt;a href=&quot;#修改docker-daemon配置&quot; class=&quot;headerlink&quot; title=&quot;修改docker daemon配置&quot;&gt;&lt;/a&gt;修改docker daemon配置&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;# cp /usr/lib/systemd/system/docker.service /etc/systemd/system/
# vim /etc/systemd/system/docker.service
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在ExecStart那行加上如下的选项，其中ens32是网卡名字：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--cluster-store=consul://localhost:8500 --cluster-advertise=ens32:2376
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/25.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;其中–cluster-store是指向key-value存储的地址，我这里就是consul的地址，consul里保存着整个overlay网络配置和节点信息。–cluster-advertise中是Host1和Host2互通的端口。&lt;/p&gt;
&lt;h3 id=&quot;启动Docker-1&quot;&gt;&lt;a href=&quot;#启动Docker-1&quot; class=&quot;headerlink&quot; title=&quot;启动Docker&quot;&gt;&lt;/a&gt;启动Docker&lt;/h3&gt;&lt;p&gt;执行systemctl daemon-reload使配置生效，然后执行systemctl start docker.service启动docker服务。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl start docker.service
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;加入开机自启动：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl enable docker.service
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;创建overlay-network&quot;&gt;&lt;a href=&quot;#创建overlay-network&quot; class=&quot;headerlink&quot; title=&quot;创建overlay network&quot;&gt;&lt;/a&gt;创建overlay network&lt;/h2&gt;&lt;h3 id=&quot;vxlan简介&quot;&gt;&lt;a href=&quot;#vxlan简介&quot; class=&quot;headerlink&quot; title=&quot;vxlan简介&quot;&gt;&lt;/a&gt;vxlan简介&lt;/h3&gt;&lt;p&gt;overlay network这种方式一般也是只需要三层可达，容器就能互通。overlay模式容器有独立IP，不同overlay方案之间的性能差别也是很大的。我这里采用的的vxlan技术。&lt;br&gt;vxlan(virtual Extensible LAN)虚拟可扩展局域网，是一种overlay的网络技术，使用MAC in UDP的方法进 行封装，共50字节的封装报文头。&lt;br&gt;用于对VXLAN报文进行封装/解封装，包括ARP请求报文和正常的VXLAN数据报文，在一段封装报文 后通过隧道向另一端VTEP发送封装报文，另一端VTEP接收到封装的报文解封装后根据封装的MAC地址进行转发。VTEP可由支持VXLAN的硬件设备或软件来实现。&lt;br&gt;从封装的结构上来看，VXLAN提供了将二层网络overlay在三层网络上的能力。&lt;/p&gt;
&lt;h3 id=&quot;创建overlay-network-1&quot;&gt;&lt;a href=&quot;#创建overlay-network-1&quot; class=&quot;headerlink&quot; title=&quot;创建overlay network&quot;&gt;&lt;/a&gt;创建overlay network&lt;/h3&gt;&lt;p&gt;默认情况下，docker启动后初始化3种网络，这3种都是不能删除的。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# docker network ls
NETWORK ID          NAME                DRIVER
5944745e7d6d             bridge                   bridge              
ce5d1ba0be32             host                       host                
244bb9a34016             none                      null  
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在node1主机上创建overlay network:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker network create -d overlay --subnet=10.10.10.0/24 net1
ca0c50dd3a49e028c3323024b9d6e8f837f4b76889b8d5848046ec0a5948ee2d
[root@node1 ~]# docker network ls
NETWORK ID          NAME                DRIVER
5944745e7d6d             bridge                  bridge              
ce5d1ba0be32             host                     host                
ca0c50dd3a49             net1                    overlay             
244bb9a34016             none                    null
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在其他主机上执行docker network ls，也会看到新建的这个名字叫net1的overlay网络。&lt;/p&gt;
&lt;h2 id=&quot;创建容器&quot;&gt;&lt;a href=&quot;#创建容器&quot; class=&quot;headerlink&quot; title=&quot;创建容器&quot;&gt;&lt;/a&gt;创建容器&lt;/h2&gt;&lt;p&gt;node1主机创建容器：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker run -it --net=net1 --name=contain1 --hostname=test1 --ip=10.10.10.3 --add-host test2:10.10.10.4 centos:centos7
[root@test1 /]# yum install -y iproute net-tools
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/26.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/27.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;node2上创建容器：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node2 ~]# docker run -it --net=net1 --name=contain2 --hostname=test2 --ip=10.10.10.4 --add-host test1:10.10.10.3 centos:centos7
[root@test2 /]# yum install -y iproute net-tools
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/28.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/29.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;测试容器跨主机通信&quot;&gt;&lt;a href=&quot;#测试容器跨主机通信&quot; class=&quot;headerlink&quot; title=&quot;测试容器跨主机通信&quot;&gt;&lt;/a&gt;测试容器跨主机通信&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/30.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/31.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;网络拓扑&quot;&gt;&lt;a href=&quot;#网络拓扑&quot; class=&quot;headerlink&quot; title=&quot;网络拓扑&quot;&gt;&lt;/a&gt;网络拓扑&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/32.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;容器内部有两个网络接口eth0、eth1。实际上，eth1连接到docker_gwbridge，这可以从ip就能看出。eth0即为overlay network的接口。&lt;/p&gt;
&lt;h2 id=&quot;抓包分析&quot;&gt;&lt;a href=&quot;#抓包分析&quot; class=&quot;headerlink&quot; title=&quot;抓包分析&quot;&gt;&lt;/a&gt;抓包分析&lt;/h2&gt;&lt;p&gt;在node2主机上使用tcpdump抓包，然后在windows上用wireshark分析。&lt;br&gt;1.container1容器里ping container2的ip地址：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/33.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;2.node2主机上抓包&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node2 ~]# tcpdump -i ens32 -s 0 -X -nnn -vvv -w /tmp/package.pcap
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3.把package.pcap传下来放到wireshark上分析&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/34.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/35.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;strong&gt;以在container1中ping container2，分析数据包流向：&lt;/strong&gt;&lt;br&gt;①container1(10.10.10.3)中ping container2(10.10.10.4)，根据container1的路由表，数据包可通过直连网络到达container2。于是arp请求获取container2的MAC地址(在xvlan上的arp这里不详述)，得到mac地址后，封包，从eth0发出；&lt;br&gt;②eth0桥接在net ns 1-ca0c50dd3a中的br0上，这个br0是个网桥(交换机)虚拟设备，需要将来自eth0的包转发出去，于是包转给了vxlan设备；这个可以通过arp -a看到一些端倪：&lt;br&gt;[root@node1 ~]# ip netns exec 1-ca0c50dd3a arp -a&lt;br&gt;③vxlan是个特殊设备，收到包后，由vxlan设备创建时注册的设备处理程序对包进行处理，即进行VXLAN封包（这期间会查询consul中存储的net1信息），将ICMP包整体作为UDP包的payload封装起来，并将UDP包通过宿主机的eth0发送出去。&lt;br&gt;④152宿主机收到UDP包后，发现是VXLAN包，根据VXLAN包中的相关信息（比如Vxlan Network Identifier，VNI=256)找到vxlan设备，并转给该vxlan设备处理。vxlan设备的处理程序进行解包，并将UDP中的payload取出，整体通过br0转给veth口，net1c2从eth0收到ICMP数据包，回复icmp reply。&lt;br&gt;&lt;strong&gt;从这个通信过程中来看，跨主机通信过程中的步骤如下：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;容器的网络命名空间与overlay网络的网络命名空间通过一对veth pair连接起来，当容器对外通信时，veth pair起到网线的作用，将流量发送到overlay网络的网络命名空间中。 &lt;/li&gt;
&lt;li&gt;容器的veth pair对端eth2与vxlan设备通过br0这个Linux bridge桥接在一起，br0在同一宿主机上起到虚拟机交换机的作用，如果目标地址在同一宿主机上，则直接通信，如果不再则通过设置在vxlan1这个vxlan设备进行跨主机通信。 &lt;/li&gt;
&lt;li&gt;vxlan1设备上会在创建时，由docker daemon为其分配vxlan隧道ID，起到网络隔离的作用。 &lt;/li&gt;
&lt;li&gt;docker主机集群通过key/value存储共享数据，在7946端口上，相互之间通过gossip协议学习各个宿主机上运行了哪些容器。守护进程根据这些数据来在vxlan1设备上生成静态MAC转发表。 &lt;/li&gt;
&lt;li&gt;根据静态MAC转发表的设置，通过UDP端口4789，将流量转发到对端宿主机的网卡上。&lt;br&gt;根据流量包中的vxlan隧道ID，将流量转发到对端宿主机的overlay网络的网络命名空间中。 &lt;/li&gt;
&lt;li&gt;对端宿主机的overlay网络的网络命名空间中br0网桥，起到虚拟交换机的作用，将流量根据MAC地址转发到对应容器内部。&lt;/li&gt;
&lt;/ul&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Docker容器跨主机通信方案&quot;&gt;&lt;a href=&quot;#Docker容器跨主机通信方案&quot; class=&quot;headerlink&quot; title=&quot;Docker容器跨主机通信方案&quot;&gt;&lt;/a&gt;Docker容器跨主机通信方案&lt;/h2&gt;&lt;p&gt;实现跨主机的容器通信有很多种方案，需要看实际的网络状况，是云上环境，私有云环境，还是混合云环境；是否有SDN对网络做特殊控制等等。网络状况不一样，适用的方案也会不一样。比如有的环境可以使用路由的方案，有的却不能使用。不考虑网络模型的话，基本是两个派别：overlay和路由方案。&lt;br&gt;Docker 1.12中把swarmkit集成到了docker中，本篇博客使用的版本是docker 1.11版本，这是我以前做的一个方案，现整理出来。&lt;br&gt;
    
    </summary>
    
      <category term="Docker" scheme="http://yoursite.com/categories/Docker/"/>
    
    
      <category term="容器 Docker" scheme="http://yoursite.com/tags/%E5%AE%B9%E5%99%A8-Docker/"/>
    
  </entry>
  
  <entry>
    <title>Registry私有仓库搭建及认证</title>
    <link href="http://yoursite.com/2017/09/01/Registry%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93%E6%90%AD%E5%BB%BA%E5%8F%8A%E8%AE%A4%E8%AF%81/"/>
    <id>http://yoursite.com/2017/09/01/Registry私有仓库搭建及认证/</id>
    <published>2017-09-01T09:44:19.000Z</published>
    <updated>2017-09-08T08:39:54.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;Registry相关概念&quot;&gt;&lt;a href=&quot;#Registry相关概念&quot; class=&quot;headerlink&quot; title=&quot;Registry相关概念&quot;&gt;&lt;/a&gt;Registry相关概念&lt;/h2&gt;&lt;p&gt;前面的文章讲过Docker的组成部分，我们一般在使用Docker的过程中更为常用的是pull image、run image、build image和push image。主要是围绕image展开的。&lt;br&gt;image和Registry的关系可以想象成自己机器上的源码和远端SVN或者Git服务的关系。Registry是一个几种存放image并对外提供上传下载以及一系列API的服务。可以很容易和本地源代码以及远端Git服务的关系相对应。&lt;br&gt;Docker hub是Docker公司提供的一些存储镜像的空间，这部分空间是有限的。我们一般会自主建设Docker私有仓库Registry。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Registry-V1和V2&quot;&gt;&lt;a href=&quot;#Registry-V1和V2&quot; class=&quot;headerlink&quot; title=&quot;Registry V1和V2&quot;&gt;&lt;/a&gt;Registry V1和V2&lt;/h2&gt;&lt;p&gt;Docker Registry 2.0版本在安全性和性能上做了诸多优化，并重新设计了镜像的存储的格式。Docker目前1.6之后支持V2。&lt;/p&gt;
&lt;h2 id=&quot;安装Docker&quot;&gt;&lt;a href=&quot;#安装Docker&quot; class=&quot;headerlink&quot; title=&quot;安装Docker&quot;&gt;&lt;/a&gt;安装Docker&lt;/h2&gt;&lt;p&gt;见前面发布的文章《CentOS安装Docker CE》。注意，在docker 1.12版本上测试registry有问题。&lt;/p&gt;
&lt;h2 id=&quot;搭建本地registry-v2&quot;&gt;&lt;a href=&quot;#搭建本地registry-v2&quot; class=&quot;headerlink&quot; title=&quot;搭建本地registry v2&quot;&gt;&lt;/a&gt;搭建本地registry v2&lt;/h2&gt;&lt;p&gt;环境：172.16.7.151 CentOS 7.0&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker run -d -p 5000:5000 --name wisedu_registry registry:2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;本地上传:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker pull ubuntu:16.04
[root@node1 ~]# docker tag ubuntu:16.04 localhost:5000/my-ubuntu
[root@node1 ~]# docker push localhost:5000/my-ubuntu
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;删除本地的ubuntu:16.04和localhost:5000/my-ubuntu&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker image remove ubuntu:16.04
[root@node1 ~]# docker image remove localhost:5000/my-ubuntu
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;从本地registry中拉取 localhost:5000/my-ubuntu 镜像：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker pull localhost:5000/my-ubuntu
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;但是这种registry只是本地能使用，我们找另外一台主机172.16.7.152来push镜像：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node2 ~]# docker pull ubuntu:16.04
[root@node2 docker]# docker tag ubuntu:16.04 172.16.7.151:5000/ubuntu:v1
[root@node2 docker]# docker push 172.16.7.151:5000/ubuntu:v1
The push refers to a repository [172.16.7.151:5000/ubuntu]
Get https://172.16.7.151:5000/v2/: http: server gave HTTP response to HTTPS client
[root@node2 ~]# echo $?
1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这是因为从docker1.13.2版本开始，使用registry时，必须使用TLS保证其安全。&lt;/p&gt;
&lt;p&gt;停止并删除本地registry：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker stop wisedu_registry         
wisedu_registry
[root@node1 ~]# docker rm -v wisedu_registry
wisedu_registry
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;搭建外部可访问的Registry&quot;&gt;&lt;a href=&quot;#搭建外部可访问的Registry&quot; class=&quot;headerlink&quot; title=&quot;搭建外部可访问的Registry&quot;&gt;&lt;/a&gt;搭建外部可访问的Registry&lt;/h2&gt;&lt;p&gt;官方文档：&lt;a href=&quot;https://docs.docker.com/registry/deploying/#run-an-externally-accessible-registry&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://docs.docker.com/registry/deploying/#run-an-externally-accessible-registry&lt;/a&gt;&lt;br&gt;Running a registry only accessible on localhost has limited usefulness. In order to make your registry accessible to external hosts, you must first secure it using TLS.&lt;/p&gt;
&lt;p&gt;使用TLS认证registry容器时，必须有证书。一般情况下，是要去认证机构购买签名证书。这里使用openssl生成自签名的证书。&lt;br&gt;1.生成自签名证书&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# mkdir -p /opt/docker/registry/certs
[root@spark32 ~]# openssl req -newkey rsa:4096 -nodes -sha256 -keyout /opt/docker/registry/certs/domain.key -x509 -days 365 -out /opt/docker/registry/certs/domain.crt
Generating a 4096 bit RSA private key
...
Country Name (2 letter code) [XX]:CN
State or Province Name (full name) []:JiangSu
Locality Name (eg, city) [Default City]:NanJing
Organization Name (eg, company) [Default Company Ltd]:wisedu
Organizational Unit Name (eg, section) []:edu
Common Name (eg, your name or your server&amp;apos;s hostname) []:registry.docker.com
Email Address []:01115004@wisedu.com
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.创建带有TLS认证的registry容器&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# docker run -d --name registry2 -p 5000:5000 -v /opt/docker/registry/certs:/certs -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key registry:2 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3.在每一个docker客户端宿主机上配置/etc/hosts，以使客户端宿主机可以解析域名”registry.docker.com”。并创建与这个registry服务器域名一致的目录（因为我这里的域名是假的）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/hosts
172.16.206.32 registry.docker.com
[root@node1 ~]# cd /etc/docker/certs.d/
[root@node1 certs.d]# mkdir registry.docker.com:5000
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;4.将证书 domain.crt 复制到每一个docker客户端宿主机/etc/docker/certs.d/registry.docker.com:5000/ca.crt，不需要重启docker&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# scp -p /opt/docker/registry/certs/domain.crt root@172.16.7.151:/etc/docker/certs.d/registry.docker.com\:5000/ca.crt
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;5.push镜像到registry&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 certs.d]# docker tag ubuntu:16.04 registry.docker.com:5000/my-ubuntu:v1
[root@node1 certs.d]# docker push registry.docker.com:5000/my-ubuntu:v1
The push refers to a repository [registry.docker.com:5000/my-ubuntu]
a09947e71dc0: Pushed 
9c42c2077cde: Pushed 
625c7a2a783b: Pushed 
25e0901a71b8: Pushed 
8aa4fcad5eeb: Pushed 
v1: digest: sha256:634a341aa83f32b48949ef428db8fefcd897dbacfdac26f044b60c14d1b5e972 size: 1357
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;6.列出私有仓库中的所有镜像&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 certs.d]# curl -X GET https://registry.docker.com:5000/v2/_catalog -k
{&amp;quot;repositories&amp;quot;:[&amp;quot;my-ubuntu&amp;quot;]}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;7.查看存储在registry:2宿主机上的镜像&lt;br&gt;在registry:2创建的私有仓库中，上传的镜像保存在容器的/var/lib/registry目录下。创建registry:2的容器时，会自动创建一个数据卷(Data Volumes)，数据卷对应的宿主机下的目录一般为：/var/lib/docker/volumes/XXX/_data。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# ls /var/lib/docker/volumes/91a0091963fa6d107dc988a60b61790bba843a115573e331db967921d5e83372/_data/docker/registry/v2/repositories/my-ubuntu/
_layers  _manifests  _uploads
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可以在创建registry:2的容器时，通过-v参数，修改这种数据卷关系：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;–v /opt/docker/registry/data:/var/lib/registry
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;除了可以将数据保存在当前主机的文件系统上，registry也支持其他基于云的存储系统，比如S3，Microsoft Azure, Ceph Rados, OpenStack Swift and Aliyun OSS等。可以在配置文件中进行配置：&lt;a href=&quot;https://github.com/docker/distribution/blob/master/docs/configuration.md#storage&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/docker/distribution/blob/master/docs/configuration.md#storage&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;【补充】：&lt;br&gt;一般情况下，证书只支持域名访问，要使其支持IP地址访问，需要修改配置文件openssl.cnf。&lt;br&gt;在Redhat7系统中，文件所在位置是/etc/pki/tls/openssl.cnf。在其中的[ v3_ca]部分，添加subjectAltName选项：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[ v3_ca ]  
subjectAltName = IP:192.168.1.104 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;生成证书：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;...  
Country Name (2 letter code) [XX]:  
State or Province Name (full name) []:  
Locality Name (eg, city) [Default City]:  
Organization Name (eg, company) [Default Company Ltd]:  
Organizational Unit Name (eg, section) []:  
Common Name (eg, your name or your server&amp;apos;s hostname) []:172.16.206.32:5000  
Email Address []:  
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;添加认证&quot;&gt;&lt;a href=&quot;#添加认证&quot; class=&quot;headerlink&quot; title=&quot;添加认证&quot;&gt;&lt;/a&gt;添加认证&lt;/h2&gt;&lt;h3 id=&quot;Native-basic-auth&quot;&gt;&lt;a href=&quot;#Native-basic-auth&quot; class=&quot;headerlink&quot; title=&quot;Native basic auth&quot;&gt;&lt;/a&gt;Native basic auth&lt;/h3&gt;&lt;p&gt;The simplest way to achieve access restriction is through basic authentication (this is very similar to other web servers’ basic authentication mechanism). This example uses native basic authentication using htpasswd to store the secrets.&lt;/p&gt;
&lt;p&gt;1.创建用户密码文件，testuser，testpassword&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# mkdir /opt/docker/registry/auth
[root@spark32 ~]# docker run --entrypoint htpasswd registry:2 -Bbn testuser testpassword &amp;gt; /opt/docker/registry/auth/htpasswd
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.运行registry容器&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# docker run -d --name registry_native_auth -p 5000:5000 -v /opt/docker/registry/auth:/auth -e &amp;quot;REGISTRY_AUTH=htpasswd&amp;quot; -e &amp;quot;REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm&amp;quot; -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd -v /opt/docker/registry/certs:/certs -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key registry:2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3.现在尝试拉取镜像：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker pull registry.docker.com:5000/my-ubuntu:v1
Error response from daemon: Get https://registry.docker.com:5000/v2/my-ubuntu/manifests/v1: no basic auth credentials
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;4.登录registry，push镜像：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker login registry.docker.com:5000
Username: testuser
Password: 
Login Succeeded
[root@node1 ~]# docker tag ubuntu:16.04 registry.docker.com:5000/my-ubuntu:v1
[root@node1 ~]# docker push registry.docker.com:5000/my-ubuntu:v1 
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;更高级的认证&quot;&gt;&lt;a href=&quot;#更高级的认证&quot; class=&quot;headerlink&quot; title=&quot;更高级的认证&quot;&gt;&lt;/a&gt;更高级的认证&lt;/h2&gt;&lt;p&gt;更好的方式是在registry前使用代理，利用代理提供https的ssl的认证和basic authentication。&lt;a href=&quot;https://docs.docker.com/registry/recipes/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://docs.docker.com/registry/recipes/&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;配置Nginx作为认证代理&quot;&gt;&lt;a href=&quot;#配置Nginx作为认证代理&quot; class=&quot;headerlink&quot; title=&quot;配置Nginx作为认证代理&quot;&gt;&lt;/a&gt;配置Nginx作为认证代理&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://docs.docker.com/registry/recipes/nginx/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://docs.docker.com/registry/recipes/nginx/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;1.创建需要的目录&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# mkdir -p /opt/nginx_proxy_registry/{auth,data}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.创建Nginx主配置文件&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# vim /opt/nginx_proxy_registry/auth/nginx.conf
events {
    worker_connections  1024;
}

http {

  upstream docker-registry {
    server registry:5000;
  }

  ## Set a variable to help us decide if we need to add the
  ## &amp;apos;Docker-Distribution-Api-Version&amp;apos; header.
  ## The registry always sets this header.
  ## In the case of nginx performing auth, the header will be unset
  ## since nginx is auth-ing before proxying.
  map $upstream_http_docker_distribution_api_version $docker_distribution_api_version {
    &amp;apos;&amp;apos; &amp;apos;registry/2.0&amp;apos;;
  }

  server {
    listen 443 ssl;
    server_name registry.docker.com;

    # SSL
    ssl_certificate /etc/nginx/conf.d/domain.crt;
    ssl_certificate_key /etc/nginx/conf.d/domain.key;

    # Recommendations from https://raymii.org/s/tutorials/Strong_SSL_Security_On_nginx.html
    ssl_protocols TLSv1.1 TLSv1.2;
    ssl_ciphers &amp;apos;EECDH+AESGCM:EDH+AESGCM:AES256+EECDH:AES256+EDH&amp;apos;;
    ssl_prefer_server_ciphers on;
    ssl_session_cache shared:SSL:10m;

    # disable any limits to avoid HTTP 413 for large image uploads
    client_max_body_size 0;

    # required to avoid HTTP 411: see Issue #1486 (https://github.com/moby/moby/issues/1486)
    chunked_transfer_encoding on;

    location /v2/ {
      # Do not allow connections from docker 1.5 and earlier
      # docker pre-1.6.0 did not properly set the user agent on ping, catch &amp;quot;Go *&amp;quot; user agents
      if ($http_user_agent ~ &amp;quot;^(docker\/1\.(3|4|5(?!\.[0-9]-dev))|Go ).*$&amp;quot; ) {
        return 404;
      }

      # To add basic authentication to v2 use auth_basic setting.
      auth_basic &amp;quot;Registry realm&amp;quot;;
      auth_basic_user_file /etc/nginx/conf.d/nginx.htpasswd;

      ## If $docker_distribution_api_version is empty, the header will not be added.
      ## See the map directive above where this variable is defined.
      add_header &amp;apos;Docker-Distribution-Api-Version&amp;apos; $docker_distribution_api_version always;

      proxy_pass                          http://docker-registry;
      proxy_set_header  Host              $http_host;   # required for docker client&amp;apos;s sake
      proxy_set_header  X-Real-IP         $remote_addr; # pass on real client&amp;apos;s IP
      proxy_set_header  X-Forwarded-For   $proxy_add_x_forwarded_for;
      proxy_set_header  X-Forwarded-Proto $scheme;
      proxy_read_timeout                  900;
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3.创建密码认证文件&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# docker run --rm --entrypoint htpasswd registry:2 -bn testuser testpassword &amp;gt; /opt/nginx_proxy_registry/auth/nginx.htpasswd
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;4.拷贝之前生成的证书和key到auth目录下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# cp /opt/docker/registry/certs/domain.crt /opt/nginx_proxy_registry/auth/
[root@spark32 ~]# cp /opt/docker/registry/certs/domain.key /opt/nginx_proxy_registry/auth/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;5.创建compose文件 &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# cd /opt/nginx_proxy_registry/
[root@spark32 nginx_proxy_registry]# vim docker-compose.yml
nginx:
  image: &amp;quot;nginx:1.9&amp;quot;
  ports:
    - 5043:443
  links:
    - registry:registry
  volumes:
    - ./auth:/etc/nginx/conf.d
    - ./auth/nginx.conf:/etc/nginx/nginx.conf:ro

registry:
  image: registry:2
  ports:
    - 127.0.0.1:5000:5000
  volumes:
    - ./data:/var/lib/registry
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;6.启动&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 nginx_proxy_registry]# docker-compose up -d
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;7.验证启动的服务&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 nginx_proxy_registry]# docker-compose ps
[root@spark32 ~]# docker ps 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;找一台docker客户端机器登录：&lt;br&gt;创建需要的目录：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# mkdir /etc/docker/certs.d/registry.docker.com:5043
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;把 domain.crt 传到上一步生成的目录里：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# scp -p /opt/nginx_proxy_registry/auth/domain.crt root@172.16.7.151:/etc/docker/certs.d/registry.docker.com:5043/ca.crt
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;登录：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker login -u=testuser -p=testpassword registry.docker.com:5043
Login Succeeded
[root@node1 ~]# docker tag ubuntu:16.04 registry.docker.com:5043/ubuntu-test:v1
[root@node1 ~]# docker push registry.docker.com:5043/ubuntu-test:v1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;8.停止服务&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# cd /opt/nginx_proxy_registry/
[root@spark32 nginx_proxy_registry]# docker-compose stop
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;9.查看日志&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 nginx_proxy_registry]# docker-compose logs
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;补充Docker-compose&quot;&gt;&lt;a href=&quot;#补充Docker-compose&quot; class=&quot;headerlink&quot; title=&quot;补充Docker compose&quot;&gt;&lt;/a&gt;补充Docker compose&lt;/h3&gt;&lt;h4 id=&quot;Docker-compose是什么&quot;&gt;&lt;a href=&quot;#Docker-compose是什么&quot; class=&quot;headerlink&quot; title=&quot;Docker compose是什么&quot;&gt;&lt;/a&gt;Docker compose是什么&lt;/h4&gt;&lt;p&gt;Docker Compose是一个用来定义和运行复杂应用的Docker工具。使用Compose，你可以在一个文件中定义一个多容器应用，然后使用一条命令来启动你的应用，完成一切准备工作。&lt;br&gt;一个使用Docker容器的应用，通常由多个容器组成。使用Docker Compose，不再需要使用shell脚本来启动容器。&lt;br&gt;Docker Compose将所管理的容器分为三层，工程(project)、服务(service)以及容器(contaienr)。一个工程当中可包含多个服务，每个服务中定义了容器运行的镜像，参数，依赖。一个服务当中可包括多个容器实例，Docker Compose并没有解决负载均衡的问题，因此需要借助其他工具实现服务发现及负载均衡。&lt;/p&gt;
&lt;h4 id=&quot;安装docker-compose&quot;&gt;&lt;a href=&quot;#安装docker-compose&quot; class=&quot;headerlink&quot; title=&quot;安装docker compose&quot;&gt;&lt;/a&gt;安装docker compose&lt;/h4&gt;&lt;p&gt;将变量 $dockerComposeVersion 换成指定的&lt;a href=&quot;https://github.com/docker/compose/releases&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;版本&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# curl -L https://github.com/docker/compose/releases/download/$dockerComposeVersion/docker-compose-`uname -m` -o /usr/local/bin/docker-compose
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;比如下载1.15.0版本：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# curl -L https://github.com/docker/compose/releases/download/1.15.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可能会下载失败，多试几次。实在不行就需要翻墙去下载。&lt;/p&gt;
&lt;p&gt;赋予执行权限：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# chmod +x /usr/local/bin/docker-compose
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查看docker compose版本：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# docker-compose --version
docker-compose version 1.15.0, build e12f3b9
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;卸载docker-compose&quot;&gt;&lt;a href=&quot;#卸载docker-compose&quot; class=&quot;headerlink&quot; title=&quot;卸载docker compose&quot;&gt;&lt;/a&gt;卸载docker compose&lt;/h4&gt;&lt;p&gt;如果docker compose是通过curl安装的:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;rm /usr/local/bin/docker-compose
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果docker compose是通过pip安装的:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip uninstall docker-compose
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;registry-web-ui&quot;&gt;&lt;a href=&quot;#registry-web-ui&quot; class=&quot;headerlink&quot; title=&quot;registry web ui&quot;&gt;&lt;/a&gt;registry web ui&lt;/h2&gt;&lt;p&gt;搭建完了docker registry，我们可以使用 docker 命令行工具对我们搭建的 registry 做各种操作了，如 push / pull。但是不够方便，比如不能直观的查看 registry 中的资源情况，如果有一个 ui 工具，能够看到仓库中有哪些镜像、每个镜像的版本是多少。&lt;br&gt;registry web ui主要有3个，一个是 &lt;a href=&quot;https://github.com/kwk/docker-registry-frontend&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;docker-registry-frontend&lt;/a&gt;，一个是 &lt;a href=&quot;https://hub.docker.com/r/hyper/docker-registry-web/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;hyper/docker-registry-web&lt;/a&gt;，还有一个是&lt;a href=&quot;http://port.us.org/documentation.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Portus&lt;/a&gt;。&lt;br&gt;关于registry ui的搭建会在后面的文章中介绍。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Registry相关概念&quot;&gt;&lt;a href=&quot;#Registry相关概念&quot; class=&quot;headerlink&quot; title=&quot;Registry相关概念&quot;&gt;&lt;/a&gt;Registry相关概念&lt;/h2&gt;&lt;p&gt;前面的文章讲过Docker的组成部分，我们一般在使用Docker的过程中更为常用的是pull image、run image、build image和push image。主要是围绕image展开的。&lt;br&gt;image和Registry的关系可以想象成自己机器上的源码和远端SVN或者Git服务的关系。Registry是一个几种存放image并对外提供上传下载以及一系列API的服务。可以很容易和本地源代码以及远端Git服务的关系相对应。&lt;br&gt;Docker hub是Docker公司提供的一些存储镜像的空间，这部分空间是有限的。我们一般会自主建设Docker私有仓库Registry。&lt;br&gt;
    
    </summary>
    
      <category term="Docker" scheme="http://yoursite.com/categories/Docker/"/>
    
    
      <category term="容器 Docker Registry" scheme="http://yoursite.com/tags/%E5%AE%B9%E5%99%A8-Docker-Registry/"/>
    
  </entry>
  
  <entry>
    <title>CentOS安装Docker CE</title>
    <link href="http://yoursite.com/2017/08/28/CentOS%E5%AE%89%E8%A3%85Docker-CE/"/>
    <id>http://yoursite.com/2017/08/28/CentOS安装Docker-CE/</id>
    <published>2017-08-28T05:29:47.000Z</published>
    <updated>2017-09-12T07:54:59.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;在搭建Registry的过程中，发现使用Docker 1.12版本，在push镜像到Registry时会报错误，几经查询解决之道无果后，去github的docker项目上提问，得到的回答是”Also note you appear running an unsupported version of docker which has changes specifically around how registries are handled.”，并且建议我尝试较新的版本。在2017年4月份的DockerCon会议上，Docker公司直接将 Github 上原隶属于 Docker 组织的 Docker 项目，直接 transfer 到了一个新的、名叫 Moby 的组织下，并将其重命名为 Moby 项目。由此先来了解下这个Moby项目。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Moby项目&quot;&gt;&lt;a href=&quot;#Moby项目&quot; class=&quot;headerlink&quot; title=&quot;Moby项目&quot;&gt;&lt;/a&gt;Moby项目&lt;/h2&gt;&lt;p&gt;为什么会产生Moby这个项目呢？可以看到最近几年里面，在2015年之后的Docker 1.17版本之后，引入了很多的新特性，比如Network、runC等，Docker的组件越来越多，提供支持的场景也越来越复杂，比如微服务、机器学习、物联网等，所以Docker镜像的下载量也呈现指数型上升，最终Docker运行的环境，也是越来越复杂，越来越多，比如在Linux，Windows，还有在嵌入式设备上。&lt;br&gt;所以Docker原本的发行版本已经不能适配于这些越来越复杂的场景了，举个例子，其实在IoT里面，树莓派的性能是比较好的，但是它去运行Docker的话，Docker就会占整个性能的一大部分，这样它上面就已经无法运行其他的一些自己的应用了，那用户要怎么去解决这些事情，难道自己去再写一个Docker容器引擎吗？这个成本是很高的，需要自己去造轮子。所以为了提供更加开放的生态，Docker公司把Docker项目中现在的一些组件抽象成Moby项目，这样系统构建者就可以通过Moby项目把现有的组件去进行组装，然后组装成自己所需要的一个容器引擎。&lt;br&gt;Moby项目现在有80多个组件，通过这些组件，用户可以避免重复地去造轮子。用户可以按照自己的需要去组装组件，做出自己的一个容器系统。这些组件有一个标准化的调用方式，他们之间通过gRPC通信，它的语言也是可以去定制的，不会像之前一样必须用Go语言去写。通过标准化的方式，通过Moby这个项目就可以把这些组件进行组合，成为自己所依赖的容器系统。&lt;br&gt;Docker项目现在改名成Moby项目，但是Docker会逐渐的从Moby项目中去抽象和剥离出来，作为Moby的一种组装方式，比如Docker依赖于这里面的一些库，它就特化成一种组装，组装成自己的Docker CE的版本，也就是Docker的社区版。Docker社区版后面也会继续做开源，所以用户和开发者不需要担心以后用Docker就会收费了，对于Docker用户来说，他也无须感知用户接口的变化，使用的命令还是Docker不是Moby，需要更多运维支持的可以选择Docker EE的版本，让Docker公司的工程师去替你去做运维和更复杂的线上支持，如果自己开发可以继续选择Docker社区版。对于架构师而言，现在就可以不强依赖于Docker项目，而是通过这些组件去拼装出来一个容器引擎去满足自己的需求。&lt;/p&gt;
&lt;h2 id=&quot;安装Docker-CE&quot;&gt;&lt;a href=&quot;#安装Docker-CE&quot; class=&quot;headerlink&quot; title=&quot;安装Docker CE&quot;&gt;&lt;/a&gt;安装Docker CE&lt;/h2&gt;&lt;h3 id=&quot;版本说明&quot;&gt;&lt;a href=&quot;#版本说明&quot; class=&quot;headerlink&quot; title=&quot;版本说明&quot;&gt;&lt;/a&gt;版本说明&lt;/h3&gt;&lt;p&gt;2017年2月份，Docker公司发布了全新的Docker版本：V1.13.0。从2017年3月1号开始，Docker的版本命名发生如下变化：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;项目&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;版本格式&lt;/td&gt;
&lt;td&gt;YY.MM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;stable版本&lt;/td&gt;
&lt;td&gt;每个季度发行&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;edge版本&lt;/td&gt;
&lt;td&gt;每个月发行&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;同时将Docker分成CE和EE 2个版本。CE版本即社区版（免费，支持周期三个月），EE即企业版，强调安全，付费使用。&lt;br&gt;Docker 会每月发布一个 edge 版本(17.03, 17.04, 17.05…)，每三个月发布一个 stable 版本(17.03, 17.06, 17.09…)，企业版(EE) 和 stable 版本号保持一致，但每个版本提供一年维护。&lt;br&gt;Docker 的 Linux 发行版的软件仓库也从以前的&lt;a href=&quot;https://apt.dockerproject.org和https://yum.dockerproject.org&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://apt.dockerproject.org和https://yum.dockerproject.org&lt;/a&gt; 变更为目前的 &lt;a href=&quot;https://download.docker.com&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://download.docker.com&lt;/a&gt; 。软件包名变更为 docker-ce(社区版) 和 docker-ee(企业版)。&lt;br&gt;当前的CE版本为17.03.0，基于V1.13.0。主要修复错误，没有重大功能增加，API亦保持不变。本文以此版本安装。&lt;br&gt;此版本的发行说明，请参考：&lt;a href=&quot;https://github.com/docker/docker/releases&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/docker/docker/releases&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;安装Docker&quot;&gt;&lt;a href=&quot;#安装Docker&quot; class=&quot;headerlink&quot; title=&quot;安装Docker&quot;&gt;&lt;/a&gt;安装Docker&lt;/h3&gt;&lt;p&gt;官方安装文档：&lt;a href=&quot;https://docs.docker.com/engine/installation/linux/docker-ce/centos/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://docs.docker.com/engine/installation/linux/docker-ce/centos/&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;环境要求&quot;&gt;&lt;a href=&quot;#环境要求&quot; class=&quot;headerlink&quot; title=&quot;环境要求&quot;&gt;&lt;/a&gt;环境要求&lt;/h4&gt;&lt;p&gt;To install Docker CE, you need the 64-bit version of CentOS 7.&lt;br&gt;The centos-extras repository must be enabled. This repository is enabled by default, but if you have disabled it, you need to re-enable it.&lt;/p&gt;
&lt;h4 id=&quot;卸载安装的所有Docker组件&quot;&gt;&lt;a href=&quot;#卸载安装的所有Docker组件&quot; class=&quot;headerlink&quot; title=&quot;卸载安装的所有Docker组件&quot;&gt;&lt;/a&gt;卸载安装的所有Docker组件&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@spark32 lib]# systemctl stop docker.service
[root@spark32 lib]# yum remove docker docker-common docker-selinux docker-engine container-selinux
[root@spark32 lib]# rm -rf docker/
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;通过仓库安装Docker-CE&quot;&gt;&lt;a href=&quot;#通过仓库安装Docker-CE&quot; class=&quot;headerlink&quot; title=&quot;通过仓库安装Docker CE&quot;&gt;&lt;/a&gt;通过仓库安装Docker CE&lt;/h4&gt;&lt;p&gt;1.安装依赖包&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# yum install -y yum-utils device-mapper-persistent-data lvm2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.下载docker yum源&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3.更新软件缓存&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# yum makecache fast
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;4.安装Docker CE&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# yum install docker-ce -y
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;5.启动docker&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# systemctl start docker.service
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;6.查看docker版本信息&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# docker info
Containers: 0
 Running: 0
 Paused: 0
 Stopped: 0
Images: 0
Server Version: 17.06.1-ce
Storage Driver: overlay
 Backing Filesystem: extfs
 Supports d_type: true
Logging Driver: json-file
Cgroup Driver: cgroupfs
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【注意】:在生产环境中，可能需要使用某一个特定版本的Docker，而不是使用最新的版本，我们可以先列出版本，然后安装指定版本：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# yum list docker-ce.x86_64  --showduplicates | sort -r
[root@spark32 ~]# yum install docker-ce-&amp;lt;VERSION&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;配置镜像加速&quot;&gt;&lt;a href=&quot;#配置镜像加速&quot; class=&quot;headerlink&quot; title=&quot;配置镜像加速&quot;&gt;&lt;/a&gt;配置镜像加速&lt;/h3&gt;&lt;p&gt;国内访问 Docker Hub 有时会遇到困难，此时可以配置镜像加速器。国内很多云服务商都提供了加速器服务，例如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;阿里云加速器&lt;/li&gt;
&lt;li&gt;DaoCloud 加速器&lt;/li&gt;
&lt;li&gt;&lt;p&gt;灵雀云加速器&lt;br&gt;以阿里云加速器为例&lt;br&gt;1.首先进入&lt;a href=&quot;https://dev.aliyun.com/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;阿里云docker库首页&lt;/a&gt;&lt;br&gt;注册用户并登录，点击右上角“管理中心”。如果第一次，会提示“您还没有开通服务”，点击“确定”，然后会出现弹窗“初始化设置”，设置docker登录时使用的密码。&lt;br&gt;2.点击左侧菜单“Docker Hub镜像站点”，可以看到“您的专属加速器地址：&lt;a href=&quot;https://6101v8g5.mirror.aliyuncs.com”，我们需要将其配置到Docker&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://6101v8g5.mirror.aliyuncs.com”，我们需要将其配置到Docker&lt;/a&gt; 引擎。&lt;br&gt;3.修改daemon配置文件/etc/docker/daemon.json来使用加速器：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark32 ~]# vim /etc/docker/daemon.json
{
  &amp;quot;registry-mirrors&amp;quot;: [&amp;quot;https://6101v8g5.mirror.aliyuncs.com&amp;quot;]
}
[root@spark32 ~]# systemctl daemon-reload
[root@spark32 ~]# systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;卸载Docker-CE&quot;&gt;&lt;a href=&quot;#卸载Docker-CE&quot; class=&quot;headerlink&quot; title=&quot;卸载Docker-CE&quot;&gt;&lt;/a&gt;卸载Docker-CE&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;# yum remove docker-ce
# rm -rf /var/lib/docker
&lt;/code&gt;&lt;/pre&gt;</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;在搭建Registry的过程中，发现使用Docker 1.12版本，在push镜像到Registry时会报错误，几经查询解决之道无果后，去github的docker项目上提问，得到的回答是”Also note you appear running an unsupported version of docker which has changes specifically around how registries are handled.”，并且建议我尝试较新的版本。在2017年4月份的DockerCon会议上，Docker公司直接将 Github 上原隶属于 Docker 组织的 Docker 项目，直接 transfer 到了一个新的、名叫 Moby 的组织下，并将其重命名为 Moby 项目。由此先来了解下这个Moby项目。&lt;br&gt;
    
    </summary>
    
      <category term="Docker" scheme="http://yoursite.com/categories/Docker/"/>
    
    
      <category term="容器 Docker" scheme="http://yoursite.com/tags/%E5%AE%B9%E5%99%A8-Docker/"/>
    
  </entry>
  
  <entry>
    <title>Dockerfile构建镜像</title>
    <link href="http://yoursite.com/2017/08/25/Dockerfile%E6%9E%84%E5%BB%BA%E9%95%9C%E5%83%8F/"/>
    <id>http://yoursite.com/2017/08/25/Dockerfile构建镜像/</id>
    <published>2017-08-25T02:55:41.000Z</published>
    <updated>2017-08-28T03:38:51.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;Dockerfile介绍&quot;&gt;&lt;a href=&quot;#Dockerfile介绍&quot; class=&quot;headerlink&quot; title=&quot;Dockerfile介绍&quot;&gt;&lt;/a&gt;Dockerfile介绍&lt;/h2&gt;&lt;p&gt;dockerfile是构建镜像的说明书。dockerfile提供了一种基于DSL语法的指令来构建镜像，通过代码化，镜像构建过程一目了然，我们能看出镜像构建的每一步都在干什么。&lt;br&gt;若要共享镜像，我们只需要共享dockerfile就可以了。共享dockerfile文件，具有以下优点：&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可重现&lt;/li&gt;
&lt;li&gt;dockerfile可以加入版本控制，这样可以追踪文件的变化和回滚错误&lt;/li&gt;
&lt;li&gt;dockerfile很轻量，我们不需要copy几百M甚至上G的docker镜像&lt;br&gt;使用Dockerfile构建的流程：&lt;/li&gt;
&lt;li&gt;编写Dockerfile&lt;/li&gt;
&lt;li&gt;执行docker build命令&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Dockerfile指令简单介绍&quot;&gt;&lt;a href=&quot;#Dockerfile指令简单介绍&quot; class=&quot;headerlink&quot; title=&quot;Dockerfile指令简单介绍&quot;&gt;&lt;/a&gt;Dockerfile指令简单介绍&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;FROM：基础镜像。&lt;/li&gt;
&lt;li&gt;MAINTAINER：维护者信息。&lt;/li&gt;
&lt;li&gt;RUN：在需要执行的命令前加RUN。&lt;/li&gt;
&lt;li&gt;ADD：ADD的文件和Dockerfile必须在同一个路径下。&lt;/li&gt;
&lt;li&gt;ENV：设置环境变量。&lt;/li&gt;
&lt;li&gt;WORKDIR：相当于cd。&lt;/li&gt;
&lt;li&gt;VOLUME：目录挂载。&lt;/li&gt;
&lt;li&gt;EXPOSE：映射端口。&lt;/li&gt;
&lt;li&gt;ENTRYPOINT&lt;/li&gt;
&lt;li&gt;CMD&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;RUN指令&quot;&gt;&lt;a href=&quot;#RUN指令&quot; class=&quot;headerlink&quot; title=&quot;RUN指令&quot;&gt;&lt;/a&gt;RUN指令&lt;/h3&gt;&lt;p&gt;会在前一条命令创建的镜像的基础上启动一个容器，并在容器中运行指令的命令。在该命令结束后，会提交容器为新的镜像。新镜像会被dockerfile中的下一条指令所使用。&lt;br&gt;默认情况下，RUN指令会以shell的形式去执行命令。当然我们也可以使用exec格式的RUN指令。在exec方式中，我们使用一个数组来指定要运行的命令和传递给该命令的参数。exec中的参数会当成什么数组被docker解析，因此必须使用双引号，而不能使用单引号。&lt;br&gt;&lt;strong&gt;shell形式&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;RUN yum install -y nginx
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;exec形式&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;RUN [&amp;quot;yum&amp;quot;, &amp;quot;install&amp;quot;, &amp;quot;-y&amp;quot;, &amp;quot;nginx&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;EXPOSE指令&quot;&gt;&lt;a href=&quot;#EXPOSE指令&quot; class=&quot;headerlink&quot; title=&quot;EXPOSE指令&quot;&gt;&lt;/a&gt;EXPOSE指令&lt;/h3&gt;&lt;p&gt;告诉docker该容器内的应用程序将会指定端口，但是这并不意味着我们可以自动访问该容器内服务的端口。出于安全的原因，docker并不会自动打开该端口。而是需要我们在使用docker run的时候通过-p参数指定需要打开哪些端口。我们可以使用多个EXPOSE指令向外部暴露多个端口。注意不可以在dockerfile中指定端口映射关系。比如EXPOSE 80，这条指令是对的，而EXPOSE 8080:80这条指令就是错的。&lt;br&gt;【注意】:这会影响docker的可移植性。我们应该在docker run命令中使用-p参数实现端口映射。&lt;/p&gt;
&lt;h3 id=&quot;CMD指令&quot;&gt;&lt;a href=&quot;#CMD指令&quot; class=&quot;headerlink&quot; title=&quot;CMD指令&quot;&gt;&lt;/a&gt;CMD指令&lt;/h3&gt;&lt;p&gt;CMD 指令允许用户指定容器启动的默认执行的命令。此命令会在容器启动且 docker run 没有指定其他命令时运行。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果 docker run 指定了其他命令，CMD 指定的默认命令将被忽略。&lt;/li&gt;
&lt;li&gt;&lt;p&gt;如果 Dockerfile 中有多个 CMD 指令，只有最后一个 CMD 有效。&lt;br&gt;CMD 有三种格式：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Exec 格式：CMD [&amp;quot;executable&amp;quot;,&amp;quot;param1&amp;quot;,&amp;quot;param2&amp;quot;] 这是 CMD 的推荐格式。
CMD [&amp;quot;param1&amp;quot;,&amp;quot;param2&amp;quot;] 为 ENTRYPOINT 提供额外的参数，此时 ENTRYPOINT 必须使用 Exec 格式。
Shell 格式：CMD command param1 param2
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;第一种格式：运行一个可执行的文件并提供参数。&lt;br&gt;第二种格式 CMD [“param1”,”param2”] 要与 Exec 格式 的 ENTRYPOINT 指令配合使用，其用途是为 ENTRYPOINT 设置默认的参数。&lt;br&gt;第三种格式：是以”/bin/sh -c”的方法执行的命令。&lt;/p&gt;
&lt;p&gt;下面看看 CMD 是如何工作的。Dockerfile 片段如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CMD echo &amp;quot;Hello world&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;运行容器 docker run -it [image] 将输出：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Hello world
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;但当后面加上一个命令，比如 docker run -it [image] /bin/bash，CMD 会被忽略掉，命令 bash 将被执行：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@76a5fd777648 /]# 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【注意】:使用CMD指令时，最好使用数组语法。&lt;/p&gt;
&lt;h3 id=&quot;ENTRYPOIN指令&quot;&gt;&lt;a href=&quot;#ENTRYPOIN指令&quot; class=&quot;headerlink&quot; title=&quot;ENTRYPOIN指令&quot;&gt;&lt;/a&gt;ENTRYPOIN指令&lt;/h3&gt;&lt;p&gt;ENTRYPOINT 看上去与 CMD 很像，它们都可以指定要执行的命令及其参数。不同的地方在于 ENTRYPOINT 不会被忽略，一定会被执行，即使运行 docker run 时指定了其他命令。&lt;br&gt;实际上docker run命令行中指定的任何参数都会被当作参数再次传递给ENTRYPOINT指令中指定的命令。&lt;br&gt;ENTRYPOINT 有两种格式：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Exec 格式：ENTRYPOINT [&amp;quot;executable&amp;quot;, &amp;quot;param1&amp;quot;, &amp;quot;param2&amp;quot;] 这是 ENTRYPOINT 的推荐格式。
Shell 格式：ENTRYPOINT command param1 param2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在为 ENTRYPOINT 选择格式时必须小心，因为这两种格式的效果差别很大。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ENTRYPOINT [&amp;quot;/usr/sbin/nginx&amp;quot;, &amp;quot;-g&amp;quot;, &amp;quot;daemon off;&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;Exec-格式&quot;&gt;&lt;a href=&quot;#Exec-格式&quot; class=&quot;headerlink&quot; title=&quot;Exec 格式&quot;&gt;&lt;/a&gt;Exec 格式&lt;/h4&gt;&lt;p&gt;ENTRYPOINT 的 Exec 格式用于设置要执行的命令及其参数，同时可通过 CMD 提供额外的参数。&lt;br&gt;ENTRYPOINT 中的参数始终会被使用，而 CMD 的额外参数可以在容器启动时动态替换掉。&lt;br&gt;比如下面的 Dockerfile 片段：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ENTRYPOINT [&amp;quot;/bin/echo&amp;quot;, &amp;quot;Hello&amp;quot;]  
CMD [&amp;quot;world&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;当容器通过 docker run -it [image] 启动时，输出为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Hello world
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;而如果通过 docker run -it [image] CloudMan 启动，则输出为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Hello CloudMan
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;Shell-格式&quot;&gt;&lt;a href=&quot;#Shell-格式&quot; class=&quot;headerlink&quot; title=&quot;Shell 格式&quot;&gt;&lt;/a&gt;Shell 格式&lt;/h4&gt;&lt;p&gt;ENTRYPOINT 的 Shell 格式会忽略任何 CMD 或 docker run 提供的参数。&lt;br&gt;&lt;strong&gt;【注意】:使用CMD和ENTRYPOINT时，请务必使用数组语法。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;【总结】：&lt;/strong&gt;&lt;br&gt;1.使用 RUN 指令安装应用和软件包，构建镜像。&lt;br&gt;2.如果想为容器设置默认的启动命令，可使用 CMD 指令。用户可在 docker run 命令行中替换此默认命令。&lt;br&gt;3.如果 Docker 镜像的用途是运行应用程序或服务，比如运行一个 MySQL，应该优先使用 Exec 格式的 ENTRYPOINT 指令。CMD 可为 ENTRYPOINT 提供额外的默认参数，同时可利用 docker run 命令行替换默认参数。&lt;/p&gt;
&lt;h2 id=&quot;示例1：构建Nginx镜像&quot;&gt;&lt;a href=&quot;#示例1：构建Nginx镜像&quot; class=&quot;headerlink&quot; title=&quot;示例1：构建Nginx镜像&quot;&gt;&lt;/a&gt;示例1：构建Nginx镜像&lt;/h2&gt;&lt;p&gt;创建一个目录单独存放各种Dockerfile：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# mkdir /opt/docker-file
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;创建Nginx目录存放Nginx Dockerfile和相关文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# mkdir nginx
[root@node1 ~]# cd nginx
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;上传openresty-1.9.7.3.tar.gz到Nginx目录下，编写Dockerfile文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 nginx]# vim Dockerfile
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;该Dockerfile内容如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# This dockerfile is for openresty
# Version 1.0

# Base images. 
FROM centos:centos7

# Author.
MAINTAINER jkzhao &amp;lt;01115004@wisedu.com&amp;gt;

# Add openresty software.
ADD openresty-1.9.7.3.tar.gz /usr/local

# Define working directory.
WORKDIR /usr/local/openresty-1.9.7.3

# Install epel repo
RUN rpm -ivh http://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpm

# Install openresty.
RUN yum install -y readline-devel pcre-devel openssl-devel gcc perl make 
RUN ./configure &amp;amp;&amp;amp; gmake &amp;amp;&amp;amp; gmake install
RUN sed -i &amp;apos;1 i\daemon off;&amp;apos; /usr/local/openresty/nginx/conf/nginx.conf
RUN sed -i &amp;apos;s@#error_log  logs\/error.log;@error_log logs\/error.log debug;@&amp;apos; /usr/local/openresty/nginx/conf/nginx.conf

# Define environment variables.
ENV PATH /usr/local/openresty/nginx/sbin:$PATH

# Define default command. 
CMD [&amp;quot;nginx&amp;quot;]

# Expose ports.
EXPOSE 80
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;利用该Dockerfile构建镜像：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 nginx]# docker build -t jkzhao/mynginx:v2 /opt/docker-file/nginx/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查看构建的镜像：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 nginx]# docker images
REPOSITORY                              TAG                 IMAGE ID            CREATED             SIZE
jkzhao/mynginx                          v2                  f61afc8ce858        17 minutes ago      581.6 MB
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;基于镜像启动容器：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 nginx]# docker run -d -p 84:80 jkzhao/mynginx:v2   
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;访问Nginx服务：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/20.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;如果dockerfile中某条指令执行失败了，没有正常结束，那么我们也将得到一个可以使用的镜像。这对调试非常有帮助，我们可以基于该镜像，运行一个具有交互功能的容器，使用最后创建的镜像对为什么我们的指令会失败进行调试。&lt;/p&gt;
&lt;h2 id=&quot;示例2：构建一个Ruby运行环境&quot;&gt;&lt;a href=&quot;#示例2：构建一个Ruby运行环境&quot; class=&quot;headerlink&quot; title=&quot;示例2：构建一个Ruby运行环境&quot;&gt;&lt;/a&gt;示例2：构建一个Ruby运行环境&lt;/h2&gt;&lt;p&gt;创建Ruby目录存放Ruby Dockerfile和相关文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# mkdir ruby
[root@node1 ~]# cd ruby
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;该Dockerfile内容如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;FROM centos:7  

MAINTAINER jkzhao &amp;lt;01115004@wisedu.com&amp;gt;

# 为镜像添加备注信息
LABEL version=&amp;quot;2.2.2&amp;quot; lang=&amp;quot;ruby&amp;quot;

ENV RUBY_MAJOR 2.2
ENV RUBY_VERSION 2.2.2

RUN yum install -y wget tar gcc g++ make automake autoconf curl-devel openssl-devel zlib-devel httpd-devel apr-devel apr-util-devel sqlite-devel

RUN cd /tmp \
    &amp;amp;&amp;amp; wget http://cache.ruby-lang.org/pub/ruby/2.2/ruby-2.2.2.tar.gz \
    &amp;amp;&amp;amp; tar zxvf ruby-2.2.2.tar.gz \
    &amp;amp;&amp;amp; cd ruby-2.2.2 \
    &amp;amp;&amp;amp; autoconf \
    &amp;amp;&amp;amp; ./configure --disable-install-doc \
    &amp;amp;&amp;amp; make \
    &amp;amp;&amp;amp; make install \
    &amp;amp;&amp;amp; rm -rf /tmp/ruby-2.2.2*

# skip installing gem documentation
RUN echo -e &amp;apos;install: --no-document\nupdate: --no-document&amp;apos; &amp;gt;&amp;gt; &amp;quot;$HOME/.gemrc&amp;quot;

ENV GEM_HOME /usr/local/bundle
ENV PATH $GEM_HOME/bin:$PATH

ENV BUNDLER_VERSION 1.10.6

RUN gem install bundler --version &amp;quot;$BUNDLER_VERSION&amp;quot; \
 &amp;amp;&amp;amp; bundle config --global path &amp;quot;$GEM_HOME&amp;quot; \
 &amp;amp;&amp;amp; bundle config --global bin &amp;quot;$GEM_HOME/bin&amp;quot;

# don&amp;apos;t create &amp;quot;.bundle&amp;quot; in all our apps
ENV BUNDLE_APP_CONFIG $GEM_HOME

CMD [ &amp;quot;irb&amp;quot; ]
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;构建缓存&quot;&gt;&lt;a href=&quot;#构建缓存&quot; class=&quot;headerlink&quot; title=&quot;构建缓存&quot;&gt;&lt;/a&gt;构建缓存&lt;/h2&gt;&lt;p&gt;dockerfile构建镜像过程非常聪明，它会将每一步的构建结果提交为一个镜像，我们可以将之前指令创建的镜像层看做缓存，比如第5条指令由于命令书写错误而导致构建失败，当我们修改它之后，前面的4条指令不会再执行，而是直接使用之前构建过程中保存的缓存。这样速度会提高很多，大大减少了构建的时间。&lt;br&gt;然而有时候我们要确保构建过程中不会使用缓存，比如对yum update等操作，要想跳过缓存，&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker build no-cache
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;缓存还有个好处，就是调试方便。比如第5条指令失败了，那么我们可以基于第4条指令创建的镜像启动一个新的容器。在容器里手工执行第5条指令所要的操作，调查指令失败的原因。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Dockerfile介绍&quot;&gt;&lt;a href=&quot;#Dockerfile介绍&quot; class=&quot;headerlink&quot; title=&quot;Dockerfile介绍&quot;&gt;&lt;/a&gt;Dockerfile介绍&lt;/h2&gt;&lt;p&gt;dockerfile是构建镜像的说明书。dockerfile提供了一种基于DSL语法的指令来构建镜像，通过代码化，镜像构建过程一目了然，我们能看出镜像构建的每一步都在干什么。&lt;br&gt;若要共享镜像，我们只需要共享dockerfile就可以了。共享dockerfile文件，具有以下优点：&lt;br&gt;
    
    </summary>
    
      <category term="Docker" scheme="http://yoursite.com/categories/Docker/"/>
    
    
      <category term="容器 Docker" scheme="http://yoursite.com/tags/%E5%AE%B9%E5%99%A8-Docker/"/>
    
  </entry>
  
  <entry>
    <title>手动构建镜像</title>
    <link href="http://yoursite.com/2017/08/23/%E6%89%8B%E5%8A%A8%E6%9E%84%E5%BB%BA%E9%95%9C%E5%83%8F/"/>
    <id>http://yoursite.com/2017/08/23/手动构建镜像/</id>
    <published>2017-08-23T03:15:08.000Z</published>
    <updated>2017-08-23T06:22:18.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;创建docker镜像的方法&quot;&gt;&lt;a href=&quot;#创建docker镜像的方法&quot; class=&quot;headerlink&quot; title=&quot;创建docker镜像的方法&quot;&gt;&lt;/a&gt;创建docker镜像的方法&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;使用”docker commit”命令&lt;/li&gt;
&lt;li&gt;使用”docker build”命令+”Dockerfile”文件&lt;br&gt;不推荐使用docker commit命令，而应该使用更灵活、更强大的dockerfile来构建docker镜像。&lt;br&gt;本篇文章先介绍docker commit来构建镜像。&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;手动构建镜像&quot;&gt;&lt;a href=&quot;#手动构建镜像&quot; class=&quot;headerlink&quot; title=&quot;手动构建镜像&quot;&gt;&lt;/a&gt;手动构建镜像&lt;/h2&gt;&lt;p&gt;一般来说，我们并不是真正从0开始构建镜像，而是基于一个已经存在的镜像，比如centos，然后进行一些安装和配置，构建自己新的镜像。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;【示例】:手动构建Nginx镜像。&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker pull centos
[root@node1 ~]# docker run -it --name mynginx centos 
[root@eadfe0c0903d /]# rpm -ivh http://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpm
[root@eadfe0c0903d /]# yum install nginx -y
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;安装完成后，我们需要将nginx程序设置为前台运行模式，这样容器启动后nginx进程会一直在前台运行而不会退出。因为如果启动容器时的进程退出，容器也就结束了。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@eadfe0c0903d /]# vi /etc/nginx/nginx.conf
daemon off;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/17.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@eadfe0c0903d /]# exit
exit
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;基于上面的容器制作一个镜像：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker ps -a
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                     PORTS                NAMES
eadfe0c0903d        centos              &amp;quot;/bin/bash&amp;quot;              12 minutes ago      Exited (0) 2 minutes ago                        mynginx
[root@node1 ~]# docker commit -m &amp;quot;nginx test mirror&amp;quot; eadfe0c0903d jkzhao/mynginx:v1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/18.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;基于这个镜像启动容器：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker images
REPOSITORY                              TAG                 IMAGE ID            CREATED             SIZE
jkzhao/mynginx                          v1                  bdf7d4fda6fb        4 minutes ago       381.6 MB
docker.io/centos                        latest              328edcd84f1b        2 weeks ago         192.5 MB
registry.docker-cn.com/library/centos   latest              328edcd84f1b        2 weeks ago         192.5 MB
docker.io/nginx                         latest              b8efb18f159b        3 weeks ago         107.5 MB
[root@node1 ~]# docker run -d -p 82:80 jkzhao/mynginx:v1 nginx
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【注意】:&lt;br&gt;1.必须加tag v1，否则会去仓库中找latest标签的镜像；&lt;br&gt;2.最后 nginx 是命令，容器启动时运行的命令，我这里是yum安装，不是yum安装的要写绝对路径。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/19.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;创建docker镜像的方法&quot;&gt;&lt;a href=&quot;#创建docker镜像的方法&quot; class=&quot;headerlink&quot; title=&quot;创建docker镜像的方法&quot;&gt;&lt;/a&gt;创建docker镜像的方法&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;使用”docker commit”命令&lt;/li&gt;
&lt;li&gt;使用”docker build”命令+”Dockerfile”文件&lt;br&gt;不推荐使用docker commit命令，而应该使用更灵活、更强大的dockerfile来构建docker镜像。&lt;br&gt;本篇文章先介绍docker commit来构建镜像。
    
    </summary>
    
      <category term="Docker" scheme="http://yoursite.com/categories/Docker/"/>
    
    
      <category term="容器 Docker" scheme="http://yoursite.com/tags/%E5%AE%B9%E5%99%A8-Docker/"/>
    
  </entry>
  
  <entry>
    <title>Docker网络和存储</title>
    <link href="http://yoursite.com/2017/08/18/Docker%E7%BD%91%E7%BB%9C%E5%92%8C%E5%AD%98%E5%82%A8/"/>
    <id>http://yoursite.com/2017/08/18/Docker网络和存储/</id>
    <published>2017-08-18T09:17:19.000Z</published>
    <updated>2017-08-23T03:44:51.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;Docker网络访问&quot;&gt;&lt;a href=&quot;#Docker网络访问&quot; class=&quot;headerlink&quot; title=&quot;Docker网络访问&quot;&gt;&lt;/a&gt;Docker网络访问&lt;/h2&gt;&lt;h3 id=&quot;docker自带的网络&quot;&gt;&lt;a href=&quot;#docker自带的网络&quot; class=&quot;headerlink&quot; title=&quot;docker自带的网络&quot;&gt;&lt;/a&gt;docker自带的网络&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;docker network ls：列出当前docker中已有的网络&lt;/li&gt;
&lt;li&gt;docker network inspect：查看网络详情，比如查看网络中有哪些容器&lt;/li&gt;
&lt;li&gt;docker run –network=&lt;network&gt;：指定运行容器时使用哪个网络&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;/network&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;host网络&quot;&gt;&lt;a href=&quot;#host网络&quot; class=&quot;headerlink&quot; title=&quot;host网络&quot;&gt;&lt;/a&gt;host网络&lt;/h3&gt;&lt;p&gt;容器的网络接口跟主机一样。&lt;/p&gt;
&lt;h3 id=&quot;none网络&quot;&gt;&lt;a href=&quot;#none网络&quot; class=&quot;headerlink&quot; title=&quot;none网络&quot;&gt;&lt;/a&gt;none网络&lt;/h3&gt;&lt;p&gt;除了lo接口外，容器没有任何其他网络接口。&lt;/p&gt;
&lt;h3 id=&quot;bridge网络&quot;&gt;&lt;a href=&quot;#bridge网络&quot; class=&quot;headerlink&quot; title=&quot;bridge网络&quot;&gt;&lt;/a&gt;bridge网络&lt;/h3&gt;&lt;p&gt;这是新建容器时默认使用的网络，也是使用得最多的网络。网络中的所有容器可以通过IP互相访问。&lt;br&gt;bridge网络通过网络接口docker0跟主机桥接，可以在主机上通过ifconfig docker0查看到该网络接口的信息。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/11.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/12.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;端口映射&quot;&gt;&lt;a href=&quot;#端口映射&quot; class=&quot;headerlink&quot; title=&quot;端口映射&quot;&gt;&lt;/a&gt;端口映射&lt;/h2&gt;&lt;p&gt;docker使用端口映射来让容器里面某一个端口对外。一种是随机映射，一种是指定端口映射。&lt;/p&gt;
&lt;h3 id=&quot;随机映射&quot;&gt;&lt;a href=&quot;#随机映射&quot; class=&quot;headerlink&quot; title=&quot;随机映射&quot;&gt;&lt;/a&gt;随机映射&lt;/h3&gt;&lt;p&gt;docker run -P&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;-P参数 随机映射端口。随机端口的好处是不会冲突。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;举个例子：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker pull nginx
[root@node1 ~]# docker run -d -P nginx
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/13.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/14.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;可以访问Docker日志：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker logs 1822a2ea3ff3
172.16.4.81 - - [22/Aug/2017:09:20:25 +0000] &amp;quot;GET / HTTP/1.1&amp;quot; 200 612 &amp;quot;-&amp;quot; &amp;quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.101 Safari/537.36&amp;quot; &amp;quot;-&amp;quot;
2017/08/22 09:20:25 [error] 5#5: *1 open() &amp;quot;/usr/share/nginx/html/favicon.ico&amp;quot; failed (2: No such file or directory), client: 172.16.4.81, server: localhost, request: &amp;quot;GET /favicon.ico HTTP/1.1&amp;quot;, host: &amp;quot;172.16.7.151:32768&amp;quot;, referrer: &amp;quot;http://172.16.7.151:32768/&amp;quot;
172.16.4.81 - - [22/Aug/2017:09:20:25 +0000] &amp;quot;GET /favicon.ico HTTP/1.1&amp;quot; 404 571 &amp;quot;http://172.16.7.151:32768/&amp;quot; &amp;quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.101 Safari/537.36&amp;quot; &amp;quot;-&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;指定端口映射&quot;&gt;&lt;a href=&quot;#指定端口映射&quot; class=&quot;headerlink&quot; title=&quot;指定端口映射&quot;&gt;&lt;/a&gt;指定端口映射&lt;/h3&gt;&lt;p&gt;docker run -p hostPort:containerPort&lt;br&gt;docker run -p ip:hostPort:containerPort&lt;br&gt;docker run -p ip::containerPort&lt;br&gt;docker run -p hostPort:containerPort:udp&lt;br&gt;-p 指定端口映射，前面的是主机端口，后面的是容器端口。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker run -d -p 81:80 nginx
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/15.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;数据管理&quot;&gt;&lt;a href=&quot;#数据管理&quot; class=&quot;headerlink&quot; title=&quot;数据管理&quot;&gt;&lt;/a&gt;数据管理&lt;/h2&gt;&lt;p&gt;两种方式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数据卷&lt;/li&gt;
&lt;li&gt;数据卷容器&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;数据卷&quot;&gt;&lt;a href=&quot;#数据卷&quot; class=&quot;headerlink&quot; title=&quot;数据卷&quot;&gt;&lt;/a&gt;数据卷&lt;/h3&gt;&lt;p&gt;启动一个容器，把宿主机的某个目录挂载到容器目录上。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;-v /data&lt;/li&gt;
&lt;li&gt;-v src:dst  指定一个目录挂载进容器。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;【示例1】:未指定宿主机目录挂载&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker run -it --name volume-test1 -v /data centos    
[root@5d485979254b /]# ls -l /data 
total 0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这个容器里的/data 肯定在宿主机的某个位置。打开一个新的终端，查看这个位置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                   NAMES
5d485979254b        centos              &amp;quot;/bin/bash&amp;quot;              2 minutes ago       Up 2 minutes                                volume-test1                                 test
[root@node1 ~]# docker inspect 5d485979254b
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/16.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;在宿主机上的目录中创建文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# cd /var/lib/docker/volumes/7f37e83849f079275545b7d1f3381903795e946d113e1c43ff24fc1a7717e00f/_data
[root@node1 _data]# ls -l
total 0
[root@node1 _data]# touch test.txt
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;回到容器中，查看/data目录下是否有刚才创建的文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@5d485979254b /]# ls -l /data 
total 0
-rw-r--r--. 1 root root 0 Aug 23 02:07 test.txt
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;【示例2】:指定宿主机目录挂载&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker run -it -v /opt:/opt centos
[root@850031902bb7 /]# ls /opt/
ansible_playbooks  rh
[root@850031902bb7 /]# exit
exit
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;宿主机的/opt:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ls /opt
ansible_playbooks  rh
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;挂载的时候可以指定权限：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker run -it -v /opt:/opt:rw centos
rw: 读写
ro: 只读
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;挂载单个文件到容器：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker run -it -v ~/.bash_history:/.bash_history centos
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;数据卷容器&quot;&gt;&lt;a href=&quot;#数据卷容器&quot; class=&quot;headerlink&quot; title=&quot;数据卷容器&quot;&gt;&lt;/a&gt;数据卷容器&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;--volumes-from  让一个容器访问另外一个容器的卷
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;【示例3】:创建一个容器作为数据卷，启动其他容器访问这个容器的卷。&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker run -it --name nfs -v /data centos /bin/bash 
[root@72bb5caa54db /]#
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Ctrl+P Ctrl +Q退出交互式容器的bash，这样容器就会在后台运行。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                NAMES
72bb5caa54db        centos              &amp;quot;/bin/bash&amp;quot;              37 seconds ago      Up 36 seconds                            nfs
[root@node1 ~]# docker inspect 72bb5caa54db
[root@node1 ~]# cd /var/lib/docker/volumes/8f1e1da837d5d86f79ac14e5d57c0e47243e3c68d5ca822d4f7a498386c07a59/_data
[root@node1 _data]# touch test2.txt
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;启动其他容器，访问这个容器的卷：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 _data]# docker run -it --name test1 --volumes-from nfs centos
[root@9097c1af8e86 /]# ls /data/
test2.txt
[root@9097c1af8e86 /]# 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这个数据卷容器可以用来作收容器间数据共享，哪怕这个数据卷容器nfs停了，依然可以通过其在宿主机上真实的目录访问里面的数据。这个数据卷容器创建后可以什么都不干，直接关闭，其他容器挂载过来。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Docker网络访问&quot;&gt;&lt;a href=&quot;#Docker网络访问&quot; class=&quot;headerlink&quot; title=&quot;Docker网络访问&quot;&gt;&lt;/a&gt;Docker网络访问&lt;/h2&gt;&lt;h3 id=&quot;docker自带的网络&quot;&gt;&lt;a href=&quot;#docker自带的网络&quot; class=&quot;headerlink&quot; title=&quot;docker自带的网络&quot;&gt;&lt;/a&gt;docker自带的网络&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;docker network ls：列出当前docker中已有的网络&lt;/li&gt;
&lt;li&gt;docker network inspect：查看网络详情，比如查看网络中有哪些容器&lt;/li&gt;
&lt;li&gt;docker run –network=&lt;network&gt;：指定运行容器时使用哪个网络
    
    </summary>
    
      <category term="Docker" scheme="http://yoursite.com/categories/Docker/"/>
    
    
      <category term="容器 Docker" scheme="http://yoursite.com/tags/%E5%AE%B9%E5%99%A8-Docker/"/>
    
  </entry>
  
  <entry>
    <title>Docker镜像和容器</title>
    <link href="http://yoursite.com/2017/08/16/Docker%E9%95%9C%E5%83%8F%E5%92%8C%E5%AE%B9%E5%99%A8/"/>
    <id>http://yoursite.com/2017/08/16/Docker镜像和容器/</id>
    <published>2017-08-16T01:03:56.000Z</published>
    <updated>2017-09-12T07:59:45.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;安装Docker&quot;&gt;&lt;a href=&quot;#安装Docker&quot; class=&quot;headerlink&quot; title=&quot;安装Docker&quot;&gt;&lt;/a&gt;安装Docker&lt;/h2&gt;&lt;p&gt;Docker 对 Linux 内核版本的最低要求是3.10，如果内核版本低于 3.10 会缺少一些运行 Docker 容器的功能。这些比较旧的内核，在一定条件下会导致数据丢失和频繁恐慌错误。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;br&gt;CentOS 6：&lt;br&gt;操作系统需要升级下内核，升级内核后安装Docker：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# yum install -y docker-io
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;CentOS 7：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# yum install -y docker
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我这里用的CentOS 7。启动docker：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl start docker.service
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;卸载docker&quot;&gt;&lt;a href=&quot;#卸载docker&quot; class=&quot;headerlink&quot; title=&quot;卸载docker&quot;&gt;&lt;/a&gt;卸载docker&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;# systemctl stop docker.service
# yum remove docker docker-common docker-selinux docker-engine container-selinux
# rm -rf docker/
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;镜像基本操作&quot;&gt;&lt;a href=&quot;#镜像基本操作&quot; class=&quot;headerlink&quot; title=&quot;镜像基本操作&quot;&gt;&lt;/a&gt;镜像基本操作&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;搜索镜像 docker search&lt;/li&gt;
&lt;li&gt;获取镜像 docker pull&lt;/li&gt;
&lt;li&gt;查看镜像 docker images&lt;/li&gt;
&lt;li&gt;删除镜像 docker rmi&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;1.获取镜像&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# docker pull centos
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;默认是去 docker.io/library/centos 仓库中拉取centos镜像，镜像资源都在国外，所以在国内要获取docker镜像，速度有时会很慢。为了快速访问 Docker 官方镜像都会配置三方加速器，目前常用三方加速器有：网易、USTC、DaoCloud、阿里云。&lt;br&gt;现在 Docker 官方针对中国区推出了镜像加速服务。通过 Docker 官方镜像加速，国内用户能够以更快的下载速度和更强的稳定性访问最流行的 Docker 镜像。&lt;br&gt;Docker 中国官方镜像加速可通过 registry.docker-cn.com 访问。目前该镜像库只包含流行的公有镜像，而私有镜像仍需要从美国镜像库中拉取。&lt;br&gt;可以使用以下命令直接从该镜像加速地址进行拉取&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ docker pull registry.docker-cn.com/myname/myrepo:mytag
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;例如:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# docker pull registry.docker-cn.com/library/centos:latest
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;注意: 除非修改了 Docker 守护进程的 –registry-mirror 参数 , 否则您将需要完整地指定官方镜像的名称。例如，library/ubuntu、library/redis、library/nginx。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;给Docker守护进程配置加速器&lt;/strong&gt;&lt;br&gt;如果要配置 Docker 守护进程默认使用 Docker 官方镜像加速。您可以在 Docker 守护进程启动时配置 –registry-mirror参数。&lt;br&gt;（1）通过命令行启动Docker&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ docker --registry-mirror=https://registry.docker-cn.com daemon
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;（2）通过配置文件启动Docker&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Docker版本在 1.12 或更高&lt;br&gt;修改 /etc/docker/daemon.json 文件并添加上 registry-mirrors 键值。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
  &amp;quot;registry-mirror&amp;quot;: [&amp;quot;https://registry.docker-cn.com&amp;quot;]
}
# systemctl restart docker.service
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Docker版本在 1.8 与 1.11 之间&lt;br&gt;找到 Docker 配置文件，在配置文件中的 DOCKER_OPTS 加入。不同的 Linux 发行版的配置路径不同。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;2.查看镜像&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# docker images
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/8.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;REPOSITORY：镜像所属的仓库名。&lt;br&gt;TAG：镜像的标签名&lt;br&gt;IMAGE ID：镜像的唯一ID，实际上和磁盘上存储的镜像的文件名是对应的。这是文件名截断显示的结果。&lt;br&gt;CREATED：镜像建立的时间。&lt;br&gt;SIZE：镜像的大小。&lt;/p&gt;
&lt;p&gt;查看镜像的详细信息：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# docker inspect 328edcd84f1b
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;3.镜像标签和仓库&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;REPOSITORY和REGISTRY&lt;br&gt;一系列镜像的集合。包含了一系列关联的镜像，比如上图中的Ubuntu，就是个大的仓库，下面的不同镜像就对应这个操作系统的不同版本。这和之前讲到的docker组件中的仓库有很大的区别。REGISTRY提供的是docker镜像的存储服务。所以在REGISTRY仓库中包含了很多REPOSITORY之类的仓库。而在REPOSITORY仓库中包含的是一个一个独立的镜像。&lt;/li&gt;
&lt;li&gt;TAG 镜像标签&lt;br&gt;在仓库中，不同的镜像是以标签来区分的。一个REPOSITORY仓库名和一个标签名就构成了一个完整的镜像名字。我们之前在演示的时候只是指定镜像名Ubuntu，并没有指定标签名。那么在docker中会默认使用latest标签所对应的镜像。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;4.删除镜像&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# docker rmi 328edcd84f1b
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;5.推送镜像&lt;/strong&gt;&lt;br&gt;pull镜像到本地后，我们在具体的业务场景中，可以会自己修改镜像，制作镜像，完成后可以把镜像push到&lt;a href=&quot;https://registry.hub.docker.com&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Docker Hub&lt;/a&gt;，或者自建的registry中。这个在后面的博客中会写到。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;6.导出镜像&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker images
REPOSITORY                              TAG                 IMAGE ID            CREATED             SIZE
registry.docker-cn.com/library/centos   latest              328edcd84f1b        13 days ago         192.5 MB
[root@node1 ~]# docker save 328edcd84f1b &amp;gt; /opt/centos.tar.gz
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;7.导入镜像&lt;/strong&gt;&lt;br&gt;比如有些镜像很难下载，别人下载好了，或者之前下载过了，可以导入。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker load &amp;lt; /opt/centos.tar.gz
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;容器基本操作&quot;&gt;&lt;a href=&quot;#容器基本操作&quot; class=&quot;headerlink&quot; title=&quot;容器基本操作&quot;&gt;&lt;/a&gt;容器基本操作&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;启动容器 docker run &lt;/li&gt;
&lt;li&gt;停止容器 docker stop&lt;/li&gt;
&lt;li&gt;查看容器 docker ps&lt;/li&gt;
&lt;li&gt;进入容器 docker exec | docker attach&lt;/li&gt;
&lt;li&gt;删除容器 docker rm&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;启动容器&quot;&gt;&lt;a href=&quot;#启动容器&quot; class=&quot;headerlink&quot; title=&quot;启动容器&quot;&gt;&lt;/a&gt;启动容器&lt;/h3&gt;&lt;p&gt;docker run IMAGE [COMMOND] [ARG…]&lt;br&gt;   run 启动一个容器并在容器中执行命令&lt;br&gt;   IMAGE 启动容器所使用的操作系统镜像&lt;br&gt;当运行docker run这个命令的时候，docker会检测本地是否有相应名字的镜像，如果没有，会去公共的仓库下载。然后利用该镜像启动一个容器，包括分配文件系统，ip地址等等。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker run centos echo &amp;quot;Hello world&amp;quot;
Hello world
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;但是在启动容器并完成命令后，实际上这个容器已经停止了。这只是执行单次命令的容器。这是docker中最基本的容器运行方式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;启动交互式容器&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -i -t IMAGE /bin/bash
    -i --interface=true|false 默认是false      ——告诉docker的守护进程为容器始终打开标准输入
    -t --tty=true|false 默认是false   ——告诉docker要为创建的容器分配一个伪tty终端
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这样新创建的容器就可以提供一个交互式的shell。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker run -i -t centos /bin/bash
[root@fb316d4c0719 /]# exit;
exit
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;一退出，容器就停止了。&lt;/p&gt;
&lt;p&gt;重新启动已经停止的容器：&lt;br&gt;docker start [-i] 容器名&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;-i 以交互的方式来重新启动停止的容器&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker start -i fb316d4c0719
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;查看容器&quot;&gt;&lt;a href=&quot;#查看容器&quot; class=&quot;headerlink&quot; title=&quot;查看容器&quot;&gt;&lt;/a&gt;查看容器&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;docker ps [-a]
    -a 列出所有的容器
    不给参数，ps命令返回的是正在运行中的docker容器

[root@node1 ~]# docker ps 
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
[root@node1 ~]# docker ps  -a
CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS                      PORTS               NAMES
fb316d4c0719        centos              &amp;quot;/bin/bash&amp;quot;            17 minutes ago      Exited (0) 17 minutes ago                       silly_bell
762db863e04a        centos              &amp;quot;echo &amp;apos;Hello world&amp;apos;&amp;quot;   20 minutes ago      Exited (0) 20 minutes ago                       romantic_murdock
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;CONTAINER ID：docker的守护进程在启动容器时为容器分配的唯一ID。&lt;br&gt;NAMES：docker守护进程为容器自动分配的名字。也可以在启动容器时手动指定，比如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker run -i -t --name test centos /bin/bash   
[root@d985fe8622f7 /]# exit
exit
[root@node1 ~]# docker ps  -a
CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS                      PORTS               NAMES
d985fe8622f7        centos              &amp;quot;/bin/bash&amp;quot;            7 seconds ago       Exited (0) 4 seconds ago                        test
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;查看某个容器的详细信息，包括名称、网络配置等信息：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker inspect fb316d4c0719 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/9.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;停止容器&quot;&gt;&lt;a href=&quot;#停止容器&quot; class=&quot;headerlink&quot; title=&quot;停止容器&quot;&gt;&lt;/a&gt;停止容器&lt;/h3&gt;&lt;p&gt;docker stop 容器名&lt;br&gt;docker kill 容器名&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;stop命令是发送一个信号给容器，等待容器的停止。&lt;/li&gt;
&lt;li&gt;kill命令会直接停止容器。&lt;br&gt;容器名可以是container id，也可以是names &lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;删除停止的容器&quot;&gt;&lt;a href=&quot;#删除停止的容器&quot; class=&quot;headerlink&quot; title=&quot;删除停止的容器&quot;&gt;&lt;/a&gt;删除停止的容器&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;注意不能删除运行中的容器&lt;/strong&gt;&lt;br&gt;docker rm 容器名&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker ps -a
CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS                         PORTS               NAMES
d985fe8622f7        centos              &amp;quot;/bin/bash&amp;quot;            About an hour ago   Exited (0) About an hour ago                       test
fb316d4c0719        centos              &amp;quot;/bin/bash&amp;quot;            2 hours ago         Exited (0) 2 hours ago                             silly_bell
762db863e04a        centos              &amp;quot;echo &amp;apos;Hello world&amp;apos;&amp;quot;   2 hours ago         Exited (0) 2 hours ago                             romantic_murdock
[root@node1 ~]# docker rm 762db863e04a
762db863e04a
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;Docker守护式容器&quot;&gt;&lt;a href=&quot;#Docker守护式容器&quot; class=&quot;headerlink&quot; title=&quot;Docker守护式容器&quot;&gt;&lt;/a&gt;Docker守护式容器&lt;/h3&gt;&lt;p&gt;守护式容器：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;能够长期运行&lt;/li&gt;
&lt;li&gt;没有交互式会话&lt;/li&gt;
&lt;li&gt;适合运行应用程序和服务&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;方式一&quot;&gt;&lt;a href=&quot;#方式一&quot; class=&quot;headerlink&quot; title=&quot;方式一&quot;&gt;&lt;/a&gt;方式一&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# docker run -i -t IMAGE /bin/bash
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Ctrl+P Ctrl +Q退出交互式容器的bash，这样容器就会在后台运行&lt;/p&gt;
&lt;h4 id=&quot;方式二&quot;&gt;&lt;a href=&quot;#方式二&quot; class=&quot;headerlink&quot; title=&quot;方式二&quot;&gt;&lt;/a&gt;方式二&lt;/h4&gt;&lt;p&gt;docker run -d 镜像名 [COMMAND] [ARG…]&lt;br&gt;     -d 启动容器时用后台的方式&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker run -d -i -t --name=test2 centos /bin/bash -c &amp;quot;while true; do echo hello world; sleep 5; done&amp;quot;
6678e5462d023e2ecc90f5bac887326b0b10ed034e51884176028ddcee24d249
[root@node1 ~]# docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES
6678e5462d02        centos              &amp;quot;/bin/bash -c &amp;apos;while &amp;quot;   5 seconds ago       Up 4 seconds                            test2
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;进入容器&quot;&gt;&lt;a href=&quot;#进入容器&quot; class=&quot;headerlink&quot; title=&quot;进入容器&quot;&gt;&lt;/a&gt;进入容器&lt;/h4&gt;&lt;p&gt;docker attach 容器名  &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES
6678e5462d02        centos              &amp;quot;/bin/bash -c &amp;apos;while &amp;quot;   5 seconds ago       Up 4 seconds                            test2
[root@node1 ~]# docker attach 6678e5462d02
hello world
hello world
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;使用 attach 命令有时候并不方便。当多个窗口同时 attach 到同一个容器的时候，所有窗口都会同步显示，当某个窗口因命令阻塞时，其他窗口也无法执行操作了。nsenter  (namespace enter)，nsenter 可以访问进程的另外一个名字空间。建议使用nsenter。&lt;br&gt;【注意】:nsenter 要正常工作需要有 root 权限。而且nsenter 工具在 util-linux 包2.23版本后才包含。安装方式：yum install util-linux&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker start d985fe8622f7
d985fe8622f7
[root@node1 ~]# docker inspect --format &amp;quot;{{.State.Pid}}&amp;quot; d985fe8622f7
14526
[root@node1 ~]# nsenter -t 14526 -u -i -n -p
[root@d985fe8622f7 ~]# exit
logout
[root@node1 ~]# docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
d985fe8622f7        centos              &amp;quot;/bin/bash&amp;quot;         3 hours ago         Up 4 minutes                            test
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/10.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;可以把这个命令写成脚本，以后用起来也方便：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim access_docker.sh
#!/bin/bash
Cname=$1
Cpid=$(docker inspect --format &amp;quot;{{.State.Pid}}&amp;quot; $Cname)

if ! ps ax | awk &amp;apos;{print $1}&amp;apos; | grep -e &amp;quot;^${Cpid}$&amp;quot; &amp;amp;&amp;gt; /dev/null; then
        echo &amp;quot;$Cname is not exist,you cant&amp;apos;s enter it.!!!!!!!!!!!!!!!!!!!!!&amp;quot;
else
        echo &amp;quot;You will enter the docker containter:$Cname.&amp;quot;
        nsenter --target $Cpid --mount --uts --ipc --net --pid
fi
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;授予脚本执行权限：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# chmod +x access_docker.sh
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果需要进去docker的某个容器，直接/path/to/access_docker.sh 容器名称或是容器id：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# ./access_docker.sh d985fe8622f7
You will enter the docker containter: d985fe8622f7.
[root@d985fe8622f7 /]#
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;查看容器日志&quot;&gt;&lt;a href=&quot;#查看容器日志&quot; class=&quot;headerlink&quot; title=&quot;查看容器日志&quot;&gt;&lt;/a&gt;查看容器日志&lt;/h3&gt;&lt;p&gt;docker logs [-f] [-t] [–tail] 容器名&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;-f –follows=true|false  默认是false    一直跟踪日志的变化，并返回结果&lt;/li&gt;
&lt;li&gt;-t –timestamps=true|false  默认是false  在返回的结果上加上时间戳&lt;/li&gt;
&lt;li&gt;&lt;p&gt;–tail=”all”  返回结尾处多少数量的日志&lt;br&gt;如果不指定，logs将会返回所有的日志&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
d985fe8622f7        centos              &amp;quot;/bin/bash&amp;quot;         3 hours ago         Up 4 minutes                            test
[root@node1 ~]# docker logs -tf d985fe8622f7
2017-08-17T03:50:39.127155000Z [root@d985fe8622f7 /]# exit
2017-08-17T03:50:39.127493000Z exit
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;查看容器内的进程&quot;&gt;&lt;a href=&quot;#查看容器内的进程&quot; class=&quot;headerlink&quot; title=&quot;查看容器内的进程&quot;&gt;&lt;/a&gt;查看容器内的进程&lt;/h3&gt;&lt;p&gt;docker top 容器名&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker top d985fe8622f7
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                14526               14506               0                   03:12               pts/1               00:00:00            /bin/bash
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;在运行中的容器内启动新进程&quot;&gt;&lt;a href=&quot;#在运行中的容器内启动新进程&quot; class=&quot;headerlink&quot; title=&quot;在运行中的容器内启动新进程&quot;&gt;&lt;/a&gt;在运行中的容器内启动新进程&lt;/h3&gt;&lt;p&gt;虽然docker的理念是一个容器运行一个服务，我们仍就需要在docker中运行多个进程。比如需要对运行中的容器进行维护、监控，或者执行一些管理任务。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# docker exec [-d] [-i] [-t] 容器名 [COMMAND] [ARG...]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果我们在学习时做实验，可能会启动很多容器，长期下来，残留的容器就会有很多，有个技巧，可以在启动的时候加一个参数，让在退出容器后，自动删除容器。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# docker run --rm centos echo &amp;quot;Hello world&amp;quot;
Hello world
&lt;/code&gt;&lt;/pre&gt;</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;安装Docker&quot;&gt;&lt;a href=&quot;#安装Docker&quot; class=&quot;headerlink&quot; title=&quot;安装Docker&quot;&gt;&lt;/a&gt;安装Docker&lt;/h2&gt;&lt;p&gt;Docker 对 Linux 内核版本的最低要求是3.10，如果内核版本低于 3.10 会缺少一些运行 Docker 容器的功能。这些比较旧的内核，在一定条件下会导致数据丢失和频繁恐慌错误。&lt;br&gt;
    
    </summary>
    
      <category term="Docker" scheme="http://yoursite.com/categories/Docker/"/>
    
    
      <category term="容器 Docker" scheme="http://yoursite.com/tags/%E5%AE%B9%E5%99%A8-Docker/"/>
    
  </entry>
  
  <entry>
    <title>Docker介绍</title>
    <link href="http://yoursite.com/2017/08/14/Docker%E4%BB%8B%E7%BB%8D/"/>
    <id>http://yoursite.com/2017/08/14/Docker介绍/</id>
    <published>2017-08-14T01:17:58.000Z</published>
    <updated>2017-09-05T11:25:17.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;Docker的诞生&quot;&gt;&lt;a href=&quot;#Docker的诞生&quot; class=&quot;headerlink&quot; title=&quot;Docker的诞生&quot;&gt;&lt;/a&gt;Docker的诞生&lt;/h2&gt;&lt;p&gt;dotCloud公司(PaaS)自身业务发展并不如愿，创始人Solomon Hykes决定投向容器技术，并从一开始就在Github上开源，2013年项目命名为Docker，并取得广泛的关注。&lt;br&gt;Docker是基于LXC技术之上构建的Container容器引擎，基于Go语言并遵循Apache 2.0协议开源。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;br&gt;容器技术其实早在2000年早起的UNIX系统上就出现了，直到2007年才加入Linux。容器的行为表现与虚拟机非常类似。现在人们提起容器，基本上指的都是Docker容器了。Docker极大地简化了容器的使用方法，将基于Linux内核隔离机制的容器实现细节统统隐藏在一条简单的命令：docker run之后。另外，Docker提供了一种容器格式，允许将应用和服务打包成一个理论上可以运行在任何安装了Docker Daemon的Linux系统上的容器。&lt;/p&gt;
&lt;h2 id=&quot;Hypervisor与Docker&quot;&gt;&lt;a href=&quot;#Hypervisor与Docker&quot; class=&quot;headerlink&quot; title=&quot;Hypervisor与Docker&quot;&gt;&lt;/a&gt;Hypervisor与Docker&lt;/h2&gt;&lt;h3 id=&quot;Hypervisor&quot;&gt;&lt;a href=&quot;#Hypervisor&quot; class=&quot;headerlink&quot; title=&quot;Hypervisor&quot;&gt;&lt;/a&gt;Hypervisor&lt;/h3&gt;&lt;p&gt;Hypervisor是一种运行在物理服务器和操作系统之间的中间软件层,可允许多个操作系统和应用共享一套基础物理硬件，因此也可以看作是虚拟环境中的“元”操作系统，它可以协调访问服务器上的所有物理设备和虚拟机，也叫虚拟机监视器（Virtual Machine Monitor）。Hypervisor是所有虚拟化技术的核心。&lt;br&gt;Hypervisor是一种运行在基础物理服务器和操作系统之间的中间软件层,可允许多个操作系统和应用共享硬件。也可叫做VMM（ virtual machine monitor ），即虚拟机监视器。 Hypervisors是一种在虚拟环境中的“元”操作系统。他们可以访问服务器上包括磁盘和内存在内的所有物理设备。Hypervisors不但协调着这些硬件资源的访问，也同时在各个虚拟机之间施加防护。当服务器启动并执行Hypervisor时，它会加载所有虚拟机客户端的操作系统同时会分配给每一台虚拟机适量的内存，CPU，网络和磁盘。&lt;/p&gt;
&lt;h3 id=&quot;Docker&quot;&gt;&lt;a href=&quot;#Docker&quot; class=&quot;headerlink&quot; title=&quot;Docker&quot;&gt;&lt;/a&gt;Docker&lt;/h3&gt;&lt;p&gt;Docker也是虚拟化的一种，是通过内核虚拟化技术(namespaces和cgroups等)来提供容器的资源隔离与安全保障等。由于Docker通过操作系统层虚拟化实现隔离，所以Docker容器在运行时，不需要类似虚拟机(VM)额外的操作系统开销，提高资源利用率。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;Docker要解决的问题&quot;&gt;&lt;a href=&quot;#Docker要解决的问题&quot; class=&quot;headerlink&quot; title=&quot;Docker要解决的问题&quot;&gt;&lt;/a&gt;Docker要解决的问题&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;消除环境的不一致&lt;/li&gt;
&lt;li&gt;隔离运行环境&lt;/li&gt;
&lt;li&gt;应用的打包和分发&lt;/li&gt;
&lt;li&gt;简单便捷的操作（统一的API）&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/2.png&quot; alt=&quot;&quot;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Docker配合运维的优势&quot;&gt;&lt;a href=&quot;#Docker配合运维的优势&quot; class=&quot;headerlink&quot; title=&quot;Docker配合运维的优势&quot;&gt;&lt;/a&gt;Docker配合运维的优势&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;环境一致性——减少环境切换带来的问题&lt;/li&gt;
&lt;li&gt;交付标准化——统一的交付件规格便于管理&lt;/li&gt;
&lt;li&gt;自动化运维——提高运维效率，Docker提供了API&lt;/li&gt;
&lt;li&gt;资源利用率——降低基础设施成本&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Docker配合运维的劣势&quot;&gt;&lt;a href=&quot;#Docker配合运维的劣势&quot; class=&quot;headerlink&quot; title=&quot;Docker配合运维的劣势&quot;&gt;&lt;/a&gt;Docker配合运维的劣势&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;操作习惯的变化——ssh/cronjob等变动&lt;/li&gt;
&lt;li&gt;监控方式的变化——系统指标/日志等方面&lt;/li&gt;
&lt;li&gt;网络/存储的变化——学习成本&lt;/li&gt;
&lt;li&gt;周边系统的变化——架构成本&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Docker的架构及工作流&quot;&gt;&lt;a href=&quot;#Docker的架构及工作流&quot; class=&quot;headerlink&quot; title=&quot;Docker的架构及工作流&quot;&gt;&lt;/a&gt;Docker的架构及工作流&lt;/h2&gt;&lt;p&gt;Docker的基本组成：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Docker Client 客户端&lt;/li&gt;
&lt;li&gt;Docker Daemon 守护进程&lt;/li&gt;
&lt;li&gt;Docker Image 镜像&lt;/li&gt;
&lt;li&gt;Docker Container 容器&lt;/li&gt;
&lt;li&gt;Docker Registry 仓库&lt;br&gt;docker是C/S架构的程序。客户端对服务器端的访问既可以是在本地，也可以通过远程来访问。&lt;br&gt;我们通过docker客户端来执行各种命令，docker客户端会将这些命令发送给守护进程，从而操作docker的容器，而容器是通过镜像来创建，而镜像是保存在仓库中。守护进程执行的结果还会传回给客户端。这样我们就可以通过客户端来查看命令运行的结果。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/3.png&quot; alt=&quot;&quot;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;Docker-Image镜像&quot;&gt;&lt;a href=&quot;#Docker-Image镜像&quot; class=&quot;headerlink&quot; title=&quot;Docker Image镜像&quot;&gt;&lt;/a&gt;Docker Image镜像&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/4.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;镜像是docker容器的基石，容器基于镜像启动和运行。镜像就好比容器的源代码，保存了用于启动容器的各种条件。&lt;br&gt;深层次的理解镜像：docker的镜像是个层叠的只读文件系统，它的最底端是个引导文件系统，即bootfs。这很像典型的Linux文件系统，docker用户几乎永远不可能和底层引导文件系统有交互。实际上，当一个容器启动后，它将会被移到内存中，而引导文件系统则会被卸载。&lt;br&gt;docker镜像的第二层是root文件系统，即rootfs。root文件系统可以是一种或多种的文件系统，比如Ubuntu或者CentOS等。在传统的linux引导过程中，root文件系统会最先以只读的方式加载，但引导结束并完成了完整性检查后，它才会切换为读写模式。但是在docker里，root文件系统永远只能是只读的。并且docker利用联合加载技术又会在root文件系统之上加载更多的只读文件系统。联合加载：一次加载多个文件系统，但是在外面看起来只能看见一个文件系统。联合加载会将各层文件系统叠加到一起。docker将这样的文件系统称为镜像。一个镜像可以放到另一个镜像的顶部，位于下面的镜像称为父镜像，以此类推，最底部的镜像称为基础镜像。也就是图中的root文件系统。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/5.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;bootfs (boot file system) 主要包含 bootloader 和 kernel, bootloader主要是引导加载kernel, 当boot成功后 kernel 被加载到内存中后 bootfs就被umount了. rootfs (root file system) 包含的就是典型 Linux 系统中的 /dev, /proc,/bin, /etc 等标准目录和文件。&lt;br&gt;对于不同的linux发行版, bootfs基本是一致的, 但rootfs会有差别, 因此不同的发行版可以公用bootfs 如下图:&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/6.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;【补充】:BusyBox就是一个工具，使用它可以对linux进行定制，可以制作非常小的系统。BusyBox生成的系统映像可以独立安装在硬盘上。&lt;/p&gt;
&lt;h3 id=&quot;Docker-Container容器&quot;&gt;&lt;a href=&quot;#Docker-Container容器&quot; class=&quot;headerlink&quot; title=&quot;Docker Container容器&quot;&gt;&lt;/a&gt;Docker Container容器&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Docker/7.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;容器通过镜像来启动，docker的容器是docker的执行单元，容器中可以运行客户的一个或多个进程。如果说镜像是docker生命周期中的构建和打包阶段，那么容器则是启动和执行阶段。&lt;br&gt;那么容器是怎样通过镜像来启动的呢？当一个容器启动时，会在该镜像的最顶层加载一个读写文件系统，也就是一个可写的文件层。我们在docker中运行的程序就是在这层中运行的。当docker第一次启动一个容器时，初始的读写层是空的，当文件系统发生变化时，这些变化都会应用到这一层上，比如如果想修改一个文件，这个文件首先会从该读写层下面的只读层复制到该读写层，该文件的只读版本依然存在，但是已经被读写层中的文件副本所隐藏。这就是docker中的一个重要技术，“写时复制”。每个只读镜像层都是只读的，并且以后永远不会变化。&lt;br&gt;当创建一个新容器时，docker会构建一个镜像站，就像右图所示。在站的最顶层添加可写层，这个读写层加上下面的镜像层以及一些配置数据就构成了一个容器。&lt;/p&gt;
&lt;h3 id=&quot;Docker-Registry仓库&quot;&gt;&lt;a href=&quot;#Docker-Registry仓库&quot; class=&quot;headerlink&quot; title=&quot;Docker Registry仓库&quot;&gt;&lt;/a&gt;Docker Registry仓库&lt;/h3&gt;&lt;p&gt;docker用仓库保存用户构建的镜像。仓库分为公有和私有两种。docker公司自己提供了一个公有的仓库，叫Docker Hub。我们可以在docker hub上注册账号，分享并保存自己的镜像。目前docker上已经有了非常丰富的镜像。所以我们也可以通过docker hub来查找我们需要的镜像。当然我们也可以架设自己私有的仓库。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Docker的诞生&quot;&gt;&lt;a href=&quot;#Docker的诞生&quot; class=&quot;headerlink&quot; title=&quot;Docker的诞生&quot;&gt;&lt;/a&gt;Docker的诞生&lt;/h2&gt;&lt;p&gt;dotCloud公司(PaaS)自身业务发展并不如愿，创始人Solomon Hykes决定投向容器技术，并从一开始就在Github上开源，2013年项目命名为Docker，并取得广泛的关注。&lt;br&gt;Docker是基于LXC技术之上构建的Container容器引擎，基于Go语言并遵循Apache 2.0协议开源。&lt;br&gt;
    
    </summary>
    
      <category term="Docker" scheme="http://yoursite.com/categories/Docker/"/>
    
    
      <category term="容器 Docker" scheme="http://yoursite.com/tags/%E5%AE%B9%E5%99%A8-Docker/"/>
    
  </entry>
  
  <entry>
    <title>Spark介绍及安装部署</title>
    <link href="http://yoursite.com/2017/08/11/Spark%E4%BB%8B%E7%BB%8D%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
    <id>http://yoursite.com/2017/08/11/Spark介绍及安装部署/</id>
    <published>2017-08-11T00:11:26.000Z</published>
    <updated>2017-09-20T12:39:19.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;Spark介绍&quot;&gt;&lt;a href=&quot;#Spark介绍&quot; class=&quot;headerlink&quot; title=&quot;Spark介绍&quot;&gt;&lt;/a&gt;Spark介绍&lt;/h2&gt;&lt;h3 id=&quot;Apache-Spark&quot;&gt;&lt;a href=&quot;#Apache-Spark&quot; class=&quot;headerlink&quot; title=&quot;Apache Spark&quot;&gt;&lt;/a&gt;Apache Spark&lt;/h3&gt;&lt;p&gt;Apache Spark是一个围绕速度、易用性和复杂分析构建的大数据处理框架(没有数据存储)。最初在2009年由加州大学伯克利分校的AMPLab开发，并于2010年成为Apache的开源项目之一。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;Hadoop和Spark&quot;&gt;&lt;a href=&quot;#Hadoop和Spark&quot; class=&quot;headerlink&quot; title=&quot;Hadoop和Spark&quot;&gt;&lt;/a&gt;Hadoop和Spark&lt;/h3&gt;&lt;p&gt;Hadoop常用于解决高吞吐、批量处理的业务场景，例如离线计算结果用于浏览量统计。如果需要实时查看浏览量统计信息，Hadoop显然不符合这样的要求。Spark通过内存计算能力极大地提高了大数据处理速度，满足了以上场景的需要。&lt;br&gt;与Hadoop和Storm等其他大数据和MapReduce技术相比，Spark有以下特点：&lt;br&gt;&lt;strong&gt;1.快速处理能力&lt;/strong&gt;&lt;br&gt;随着实时大数据应用越来越多，Hadoop作为离线的高吞吐、低响应框架已不能满足这类需求。Hadoop MapReduce的Job将中间输出和结果存储在HDFS中，读写HDFS造成磁盘I/O称为瓶颈。Spark允许将中间输出和结果存储在内存中，避免了大量的磁盘I/O。同时Spark自身的DAG执行引擎也支持数据在内存中的计算。Spark官网声称性能比Hadoop快100倍，如图所示。即便是内存不足，需要磁盘I/O，其速度也是Hadoop的10倍以上。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Spark/1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.易于使用&lt;/strong&gt;&lt;br&gt;Spark现在支持Java、Scala、Python和R等语言编写应用程序，大大降低了使用者的门槛。自带了80多个高等级操作符，允许在Scala、Python、R的shell中进行交互式查询。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3.通用性&lt;/strong&gt;&lt;br&gt;Spark支持SQL及Hive SQL对数据查询，支持流式计算、支持机器学习和图计算。而且除了Spark core以外，建立在其上的这些功能都是一些库，安装好Spark后，这些库就可以使用了。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Spark/2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4.可用性高&lt;/strong&gt;&lt;br&gt;Spark自身实现了Standalone部署模式，还可以跑在Hadoop、Mesos、或者云上。此外，Spark还有丰富的数据源支持。Spark除了可以访问操作系统自身的文件系统和HDFS，还可以访问Cassandra、HBase、Hive、Tachyon以及任何Hadoop的数据源。&lt;/p&gt;
&lt;h3 id=&quot;Spark中的概念&quot;&gt;&lt;a href=&quot;#Spark中的概念&quot; class=&quot;headerlink&quot; title=&quot;Spark中的概念&quot;&gt;&lt;/a&gt;Spark中的概念&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;RDD：&lt;/strong&gt;弹性分布式数据集。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Task：&lt;/strong&gt;具体执行任务。Task分为ShuffleMapTask和ResultTask两种。ShuffleMapTask和ResultTask分别类似于Hadoop中的Map和Reduce。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Job：&lt;/strong&gt;用户提交的作业。一个Job可能由一到多个Task组成。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stage：&lt;/strong&gt;Job分成的阶段。一个Job可能被划分为一到多个Stage。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Partition：&lt;/strong&gt;数据分区。即一个RDD的数据可以划分为多少个分区。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NarrowDependency：&lt;/strong&gt;窄依赖，即子RDD依赖于父RDD中固定的Partition。NarrowDependency分为OneToOneDependency和RangeDependency两种。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ShuffleDependency：&lt;/strong&gt;shuffle依赖，也称为宽依赖，即子RDD对父RDD中的所有Partition都有依赖。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DAG：&lt;/strong&gt;有向无环图。用于反映各RDD之间的依赖关系。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;Spark生态系统&quot;&gt;&lt;a href=&quot;#Spark生态系统&quot; class=&quot;headerlink&quot; title=&quot;Spark生态系统&quot;&gt;&lt;/a&gt;Spark生态系统&lt;/h3&gt;&lt;p&gt;整个Spark主要由以下模块组成：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Spark Core：&lt;/strong&gt;Spark的核心功能实现，包括：SparkContext的初始化(Driver Application通过SparkContext提交)、部署模式、存储体系、任务提交与执行、计算引擎等。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Spark SQL：&lt;/strong&gt;提供SQL处理能力，便于熟悉关系型数据库操作的工程师进行交互查询。此外，还为熟悉Hadoop的用户提供Hive SQL处理能力。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Spark Streaming：&lt;/strong&gt;提供流式计算处理能力，目前支持Kafka、Flume、Twitter、MQTT、ZeroMQ、Kinesis和简单的TCP套接字等数据源。此外，还提供窗口操作。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GraphX：&lt;/strong&gt;提供图计算处理能力，支持分布式。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MLlib：&lt;/strong&gt;提供机器学习相关的统计、分类、回归等领域的多种算法实现。其一致的API接口大大降低了用户的学习成本。&lt;br&gt;Spark SQL、Spark Streaming、GraphX、MLlib的能力都是建立在核心引擎之上。如下图所示。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Spark/2.png&quot; alt=&quot;&quot;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;Spark核心功能&quot;&gt;&lt;a href=&quot;#Spark核心功能&quot; class=&quot;headerlink&quot; title=&quot;Spark核心功能&quot;&gt;&lt;/a&gt;Spark核心功能&lt;/h4&gt;&lt;p&gt;Spark Core提供Spark最基础与最核心的功能，主要包括以下功能：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;SparkContext：&lt;/strong&gt;通常而言，Driver Application的执行和输出都是通过SparkContent来完成的，在正式提交Application之前，首先需要初始化SparkContent。SparkContent隐藏了网络通信、分布式部署、消息通信、存储能力、计算能力、缓存、测量系统、文件服务、Web服务等内容，应用程序开发者只需要使用SparkContent提供的API完成功能开发。SparkContent内置的DAGScheduler负责创建Job，将DAG中的RDD划分到不同的Stage，提交Stage等功能。内置的TaskScheduler负责资源的申请、任务的提交及请求集群对任务的调度等工作。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;存储体系：&lt;/strong&gt;Spark优先考虑使用各节点的内存作为存储，当内存不足时才会考虑使用磁盘，这极大地减少了磁盘I/O，提升了任务执行效率，使得Spark适用于实时计算、流式计算等场景。此外，Spark还提供了以内存为中心的高容错的分布式文件系统Tachyon供用户进行选择。Tachyon能够为Spark提供可靠的内存级的文件共享服务。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;计算引擎：&lt;/strong&gt;计算引擎由SparkContent中的DAGScheduler、RDD以及具体节点上的Executor负责执行的Map和Reduce任务组成。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;部署模式：&lt;/strong&gt;由于单节点不足以提供足够的存储和计算能力，所以作为大数据处理的Spark在SparkContext的TaskScheduler组件中提供了对Standalone部署模式的实现和Yarn、Mesos等分布式资源管理系统的支持。通过使用Standallone、Yarn、Mesos等部署模式为Task分配计算资源，提高任务的并发执行效率。除了可用于实际生产环境的Standalone、Yarn、Mesos等部署模式外，Spark还提供了Local模式和local-cluster模式便于开发和调试。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;Spark扩展功能&quot;&gt;&lt;a href=&quot;#Spark扩展功能&quot; class=&quot;headerlink&quot; title=&quot;Spark扩展功能&quot;&gt;&lt;/a&gt;Spark扩展功能&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Spark SQL&lt;/li&gt;
&lt;li&gt;Spark Streaming&lt;/li&gt;
&lt;li&gt;GraphX&lt;/li&gt;
&lt;li&gt;MLlib&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;Spark部署架构&quot;&gt;&lt;a href=&quot;#Spark部署架构&quot; class=&quot;headerlink&quot; title=&quot;Spark部署架构&quot;&gt;&lt;/a&gt;Spark部署架构&lt;/h3&gt;&lt;h4 id=&quot;集群架构&quot;&gt;&lt;a href=&quot;#集群架构&quot; class=&quot;headerlink&quot; title=&quot;集群架构&quot;&gt;&lt;/a&gt;集群架构&lt;/h4&gt;&lt;p&gt;从集群部署的角度看，Spark集群由以下部分组成：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cluster Manager：&lt;/strong&gt;Spark的集群管理器，主要负责资源的分配与管理。集群管理器分配的资源属于一级分配，它将各个Worker上内存、CPU等资源分配给应用程序，但是并不负责对Executor的资源分配。目前Standalone、YARN、Mesos、EC2等都可以作为Spark的集群管理器。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Worker：&lt;/strong&gt;Spark的工作节点。对Spark应用程序来说，由集群管理器分配得到资源的Worker节点主要负责以下工作：创建Executor，将资源和任务进一步分配给Executor，同步资源信息给Cluster Manager。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Executor：&lt;/strong&gt;执行计算任务的一线进程。主要负责任务的执行以及与Worker、Driver App的信息同步。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Driver App：&lt;/strong&gt;客户端驱动程序，也可以理解问客户端应用程序，用于将任务程序转换为RDD和DAG，并与Cluster Manager进行通信与调度。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Spark/3.jpg&quot; alt=&quot;&quot;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;Spark部署模式&quot;&gt;&lt;a href=&quot;#Spark部署模式&quot; class=&quot;headerlink&quot; title=&quot;Spark部署模式&quot;&gt;&lt;/a&gt;Spark部署模式&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;1.一些概念&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Driver：&lt;/strong&gt;应用驱动程序，可以理解为是老板的客户。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Master：&lt;/strong&gt;Spark的主控节点，可以理解为集群的老板。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Worker：&lt;/strong&gt;Spark的工作节点，可以理解为集群的各个主管。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Executor：&lt;/strong&gt;Spark的工作进程，由Worker监管，负责具体任务的执行。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;2.Spark目前支持的部署方式&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;本地部署模式：&lt;/strong&gt;local、local[N]或者local[N, maxRetries]。主要用于代码调试和跟踪。不具备容错能力，所以不适用于生产环境。local部署模式只有Driver，没有Master和Worker，执行任务的Executor与Driver在同一个JVM进程内。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;本地集群部署模式：&lt;/strong&gt;local-cluster[N, cores, memory]。也主要用于代码调试，是源码学习常用的模式。不具备容错能力，不能用于生产环境。local-cluster模式是一种伪分布式集群部署模式，Driver、Master和Worker在同一个JVM内，可以存在多个Worker，每个Worker会有多个Executor，但这些Executor都独自存在于一个JVM进程内。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Standalone部署模式：&lt;/strong&gt;spark://。具备容错能力并且支持分布式部署，所以可用于实际的生产。Driver在集群之外，可以是任意的客户端应用程序。Master部署于单独的进程，甚至应该在单独的机器节点上。Master有多个，但同时最多有只有一个处于激活状态。Worker部署于单独的进程，也推荐在单独的节点上部署。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;第三方部署模式：&lt;/strong&gt;yarn-standalone、yarn-cluster、mesos://、zk://、simr://等。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;安装部署spark&quot;&gt;&lt;a href=&quot;#安装部署spark&quot; class=&quot;headerlink&quot; title=&quot;安装部署spark&quot;&gt;&lt;/a&gt;安装部署spark&lt;/h2&gt;&lt;p&gt;Spark runs on Java 7+, Python 2.6+ and R 3.1+. For the Scala API, Spark 1.6.2 uses Scala 2.10. You will need to use a compatible Scala version (2.10.x).&lt;/p&gt;
&lt;h3 id=&quot;本地部署模式&quot;&gt;&lt;a href=&quot;#本地部署模式&quot; class=&quot;headerlink&quot; title=&quot;本地部署模式&quot;&gt;&lt;/a&gt;本地部署模式&lt;/h3&gt;&lt;h4 id=&quot;下载安装JDK8&quot;&gt;&lt;a href=&quot;#下载安装JDK8&quot; class=&quot;headerlink&quot; title=&quot;下载安装JDK8&quot;&gt;&lt;/a&gt;下载安装JDK8&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# mkdir /usr/java
# tar zxf /usr/local/jdk-8u73-linux-x64.gz -C /usr/java/
# vim /etc/profile
export JAVA_HOME=/usr/java/jdk1.8.0_73
export PATH=$JAVA_HOME/bin:$PATH
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
# source /etc/profile
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;下载安装scala-2-10-6&quot;&gt;&lt;a href=&quot;#下载安装scala-2-10-6&quot; class=&quot;headerlink&quot; title=&quot;下载安装scala-2.10.6&quot;&gt;&lt;/a&gt;下载安装scala-2.10.6&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@care ~]# cd /usr/local/
[root@care local]# tar zxf scala-2.10.6.tgz
[root@care local]# vim /etc/profile
# Scala environment
export SCALA_HOME=/usr/local/scala-2.10.6
export PATH=$SCALA_HOME/bin:$PATH
[root@care local]# source /etc/profile
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查看是否成功：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@care local]# scala -version
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;配置登录自己不需要输入密码&quot;&gt;&lt;a href=&quot;#配置登录自己不需要输入密码&quot; class=&quot;headerlink&quot; title=&quot;配置登录自己不需要输入密码&quot;&gt;&lt;/a&gt;配置登录自己不需要输入密码&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@care ~]# ssh-keygen -t rsa -P &amp;apos;&amp;apos;
[root@care ~]# ssh-copy-id -i ~/.ssh/id_rsa.pub root@localhost
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;下载安装spark&quot;&gt;&lt;a href=&quot;#下载安装spark&quot; class=&quot;headerlink&quot; title=&quot;下载安装spark&quot;&gt;&lt;/a&gt;下载安装spark&lt;/h4&gt;&lt;p&gt;&lt;a href=&quot;http://spark.apache.org/downloads.html，我这里选择编译好的二进制版本1.6.1&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://spark.apache.org/downloads.html，我这里选择编译好的二进制版本1.6.1&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Spark/4.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@care local]# tar zxf spark-1.6.1-bin-hadoop2.6.tgz
[root@care local]# mv spark-1.6.1-bin-hadoop2.6 spark-1.6.1
[root@care local]# vim /etc/profile
# Spark environment
export SPARK_HOME=/usr/local/spark-1.6.1
export PATH=$SPARK_HOME/bin:$PATH
[root@care local]# source /etc/profile

[root@care local]# cd spark-1.6.1/conf/
[root@care conf]# cp spark-env.sh.template spark-env.sh
[root@care conf]# vim spark-env.sh
export JAVA_HOME=/usr/java/jdk1.8.0_73
export SCALA_HOME=/usr/local/scala-2.10.6
export SPARK_MASTER_IP=172.16.7.119
export SPARK_WORKER_MEMORY=4G
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果要选择源码编译安装，Build方法网址：&lt;br&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/building-spark.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://spark.apache.org/docs/latest/building-spark.html&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;启动spark&quot;&gt;&lt;a href=&quot;#启动spark&quot; class=&quot;headerlink&quot; title=&quot;启动spark&quot;&gt;&lt;/a&gt;启动spark&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@care ~]# /usr/local/spark-1.6.1/sbin/start-all.sh
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查看启动的进程：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@care ~]# jps
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;查看Web-UI&quot;&gt;&lt;a href=&quot;#查看Web-UI&quot; class=&quot;headerlink&quot; title=&quot;查看Web UI&quot;&gt;&lt;/a&gt;查看Web UI&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;Master UI：&lt;/strong&gt;&lt;a href=&quot;http://172.16.7.119:8080/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.119:8080/&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Spark/5.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;strong&gt;Worker UI：&lt;/strong&gt;&lt;a href=&quot;http://172.16.7.119:8081/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://172.16.7.119:8081/&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Spark/6.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h4 id=&quot;停止spark&quot;&gt;&lt;a href=&quot;#停止spark&quot; class=&quot;headerlink&quot; title=&quot;停止spark&quot;&gt;&lt;/a&gt;停止spark&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@care ~]# /usr/local/spark-1.6.1/sbin/stop-all.sh
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;Standalone模式部署spark-无HA&quot;&gt;&lt;a href=&quot;#Standalone模式部署spark-无HA&quot; class=&quot;headerlink&quot; title=&quot;Standalone模式部署spark (无HA)&quot;&gt;&lt;/a&gt;Standalone模式部署spark (无HA)&lt;/h3&gt;&lt;p&gt;Spark Standalone采用了Master/Slaves架构的集群模式，因此，存在着Master单点故障。&lt;br&gt;Spark提供了两种单点故障的解决方案：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于文件系统的单点恢复&lt;/li&gt;
&lt;li&gt;基于ZooKeeper的Standby Masters&lt;br&gt;此模式主要用来做开发，因为开发时应用运行频率高，而且对Master故障的影响不大，最主要的是出现故障重新运行便可，不需要恢复。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;环境信息&quot;&gt;&lt;a href=&quot;#环境信息&quot; class=&quot;headerlink&quot; title=&quot;环境信息&quot;&gt;&lt;/a&gt;环境信息&lt;/h4&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;IP地址&lt;/th&gt;
&lt;th&gt;操作系统版本&lt;/th&gt;
&lt;th&gt;安装软件&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;spark17&lt;/td&gt;
&lt;td&gt;172.16.206.17&lt;/td&gt;
&lt;td&gt;CentOS 7.1&lt;/td&gt;
&lt;td&gt;JDK8、scala-2.10.6、spark-1.6.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;spark31&lt;/td&gt;
&lt;td&gt;172.16.206.31&lt;/td&gt;
&lt;td&gt;CentOS 7.1&lt;/td&gt;
&lt;td&gt;JDK8、scala-2.10.6、spark-1.6.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;spark132&lt;/td&gt;
&lt;td&gt;172.16.206.32&lt;/td&gt;
&lt;td&gt;CentOS 7.1&lt;/td&gt;
&lt;td&gt;JDK8、scala-2.10.6、spark-1.6.1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;spark17作为Mater节点，其他两台作为Worker节点。&lt;/p&gt;
&lt;h4 id=&quot;节点时间同步&quot;&gt;&lt;a href=&quot;#节点时间同步&quot; class=&quot;headerlink&quot; title=&quot;节点时间同步&quot;&gt;&lt;/a&gt;节点时间同步&lt;/h4&gt;&lt;p&gt;采用NTP(Network Time Protocol)方式来实现, 选择一台机器, 作为集群的时间同步服务器, 然后分别配置服务端和集群其他机器。我这里以spark17机器(Hadoop集群机器)时间为准，其他机器同这台机器时间做同步。&lt;/p&gt;
&lt;h5 id=&quot;NTP服务端&quot;&gt;&lt;a href=&quot;#NTP服务端&quot; class=&quot;headerlink&quot; title=&quot;NTP服务端&quot;&gt;&lt;/a&gt;NTP服务端&lt;/h5&gt;&lt;p&gt;1.安装ntp服务&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# yum install ntp -y
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.配置/etc/ntp.conf，这边采用本地机器作为时间的原点&lt;br&gt;注释server列表：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;server 0.centos.pool.ntp.org iburst
server 1.centos.pool.ntp.org iburst
server 2.centos.pool.ntp.org iburst
server 3.centos.pool.ntp.org iburst
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;添加如下内容：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;server 127.127.1.0 prefer
fudge 127.127.1.0 stratum 8
logfile /var/log/ntp.log
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3.启动ntpd服务&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl start ntpd
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;4.查看ntp服务状态&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl status ntpd
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;5.加入开机启动&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl enable ntpd
&lt;/code&gt;&lt;/pre&gt;&lt;h5 id=&quot;NTP客户端&quot;&gt;&lt;a href=&quot;#NTP客户端&quot; class=&quot;headerlink&quot; title=&quot;NTP客户端&quot;&gt;&lt;/a&gt;NTP客户端&lt;/h5&gt;&lt;p&gt;1.安装ntp&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# yum install ntpdate -y
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.配置crontab任务主动同步&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# crontab -e
*/10 * * * * /usr/sbin/ntpdate 172.16.206.17;hwclock -w
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;各节点配置hosts文件&quot;&gt;&lt;a href=&quot;#各节点配置hosts文件&quot; class=&quot;headerlink&quot; title=&quot;各节点配置hosts文件&quot;&gt;&lt;/a&gt;各节点配置hosts文件&lt;/h4&gt;&lt;p&gt;集群各主机都要配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/hosts
172.16.206.17 spark17
172.16.206.31 spark31
172.16.206.32 spark32
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;下载安装JDK8-1&quot;&gt;&lt;a href=&quot;#下载安装JDK8-1&quot; class=&quot;headerlink&quot; title=&quot;下载安装JDK8&quot;&gt;&lt;/a&gt;下载安装JDK8&lt;/h4&gt;&lt;p&gt;集群每台机器都要安装JDK8。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# mkdir /usr/java
# tar zxf /usr/local/jdk-8u73-linux-x64.gz -C /usr/java/
# vim /etc/profile
export JAVA_HOME=/usr/java/jdk1.8.0_73
export PATH=$JAVA_HOME/bin:$PATH
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
# source /etc/profile
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;下载安装scala-2-10-6-1&quot;&gt;&lt;a href=&quot;#下载安装scala-2-10-6-1&quot; class=&quot;headerlink&quot; title=&quot;下载安装scala-2.10.6&quot;&gt;&lt;/a&gt;下载安装scala-2.10.6&lt;/h4&gt;&lt;p&gt;集群每个节点都需要安装scala。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cd /usr/local/
# tar zxf scala-2.10.6.tgz
# vim /etc/profile
# Scala environment
export SCALA_HOME=/usr/local/scala-2.10.6
export PATH=$SCALA_HOME/bin:$PATH
# source /etc/profile
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查看是否成功：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# scala -version
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;配置主节点登录自己和其他节点不需要输入密码&quot;&gt;&lt;a href=&quot;#配置主节点登录自己和其他节点不需要输入密码&quot; class=&quot;headerlink&quot; title=&quot;配置主节点登录自己和其他节点不需要输入密码&quot;&gt;&lt;/a&gt;配置主节点登录自己和其他节点不需要输入密码&lt;/h4&gt;&lt;p&gt;生成一对密钥：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark17 ~]# ssh-keygen -t rsa -P &amp;apos;&amp;apos;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;拷贝公钥到自己和其他节点：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark17 ~]# ssh-copy-id -i ~/.ssh/id_rsa.pub root@spark17
[root@spark17 ~]# ssh-copy-id -i ~/.ssh/id_rsa.pub root@spark31
[root@spark17 ~]# ssh-copy-id -i ~/.ssh/id_rsa.pub root@spark32
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;安装配置spark&quot;&gt;&lt;a href=&quot;#安装配置spark&quot; class=&quot;headerlink&quot; title=&quot;安装配置spark&quot;&gt;&lt;/a&gt;安装配置spark&lt;/h4&gt;&lt;p&gt;我这里下载的是Spark的编译版本spark-1.6.1-bin-hadoop2.6.tgz，否则需要自己事先自行编译。&lt;br&gt;&lt;strong&gt;先在master机器上(172.16.206.17)安装spark：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark17 ~]# cd /usr/local/
[root@spark17 local]# tar zxf spark-1.6.1-bin-hadoop2.6.tgz
[root@spark17 local]# mv spark-1.6.1-bin-hadoop2.6 spark-1.6.1
[root@spark17 local]# vim /etc/profile
# Spark environment
export SPARK_HOME=/usr/local/spark-1.6.1
export PATH=$SPARK_HOME/bin:$PATH
[root@spark17 local]# source /etc/profile
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;配置spark：&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;修改spark-env.sh文件：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark17 local]# cd spark-1.6.1/conf/
[root@ spark17 conf]# cp spark-env.sh.template spark-env.sh
[root@ spark17 conf]# vim spark-env.sh
export JAVA_HOME=/usr/java/jdk1.8.0_73
export SCALA_HOME=/usr/local/scala-2.10.6
export SPARK_MASTER_IP=spark17
export SPARK_MASTER_PORT=7077
export SPARK_WORKER_CORES=1
export SPARK_WORDER_INSTANCES=1
export SPARK_WORKER_MEMORY=4G
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;修改slave文件：&lt;/strong&gt;只需要在slave文件中写入各节点的主机名即可，包括master的主机名。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark17 conf]# cp slaves.template slaves
[root@spark17 conf]# vim slaves
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Spark/7.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;一旦创建好文件， 你就可以使用下面的shell脚本启动或者停止你的集群了。 这些脚本基于Hadoop的发布脚本， 可以在SPARK_HOME/bin找到:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sbin/start-master.sh：在脚本执行的机器上启动master.&lt;/li&gt;
&lt;li&gt;sbin/start-slaves.sh：启动conf/slaves 文件中配置的所有的slave.&lt;/li&gt;
&lt;li&gt;sbin/start-all.sh：启动上面描述的master和salve.&lt;/li&gt;
&lt;li&gt;sbin/stop-master.sh：停止bin/start-master.sh 脚本启动的master.&lt;/li&gt;
&lt;li&gt;sbin/stop-slaves.sh：停止conf/slaves 文件中配置的slave.&lt;/li&gt;
&lt;li&gt;sbin/stop-all.sh：停止上面描述的master和slave.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;【注意】:这些脚本必须在你想运行的master机器上执行，而不是你的本地机。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;将master上配置好的spark通过scp复制到其他各个节点上（注意其他节点上的profile文件也要一致）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark17 ~]# scp -r /usr/local/spark-1.6.1 root@spark31:/usr/local/
[root@spark17 ~]# scp -r /usr/local/spark-1.6.1 root@spark32:/usr/local/
[root@spark31 local]# vim /etc/profile
# Spark environment
export SPARK_HOME=/usr/local/spark-1.6.1
export PATH=$SPARK_HOME/bin:$PATH
[root@spark31 local]# source /etc/profile
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;启动spark-1&quot;&gt;&lt;a href=&quot;#启动spark-1&quot; class=&quot;headerlink&quot; title=&quot;启动spark&quot;&gt;&lt;/a&gt;启动spark&lt;/h4&gt;&lt;p&gt;在master上一次性启动集群：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@spark17 ~]# cd /usr/local/spark-1.6.1/sbin/
[root@spark17 sbin]# ./start-all.sh
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;停止spark-1&quot;&gt;&lt;a href=&quot;#停止spark-1&quot; class=&quot;headerlink&quot; title=&quot;停止spark&quot;&gt;&lt;/a&gt;停止spark&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@spark17 ~]# cd /usr/local/spark-1.6.1/sbin/
[root@spark17 sbin]# ./stop-all.sh
&lt;/code&gt;&lt;/pre&gt;</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Spark介绍&quot;&gt;&lt;a href=&quot;#Spark介绍&quot; class=&quot;headerlink&quot; title=&quot;Spark介绍&quot;&gt;&lt;/a&gt;Spark介绍&lt;/h2&gt;&lt;h3 id=&quot;Apache-Spark&quot;&gt;&lt;a href=&quot;#Apache-Spark&quot; class=&quot;headerlink&quot; title=&quot;Apache Spark&quot;&gt;&lt;/a&gt;Apache Spark&lt;/h3&gt;&lt;p&gt;Apache Spark是一个围绕速度、易用性和复杂分析构建的大数据处理框架(没有数据存储)。最初在2009年由加州大学伯克利分校的AMPLab开发，并于2010年成为Apache的开源项目之一。&lt;br&gt;
    
    </summary>
    
      <category term="实时处理" scheme="http://yoursite.com/categories/%E5%AE%9E%E6%97%B6%E5%A4%84%E7%90%86/"/>
    
    
      <category term="大数据 实时处理 Spark" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE-%E5%AE%9E%E6%97%B6%E5%A4%84%E7%90%86-Spark/"/>
    
  </entry>
  
  <entry>
    <title>Ansible实战：部署分布式日志系统</title>
    <link href="http://yoursite.com/2017/07/27/Ansible%E5%AE%9E%E6%88%98%EF%BC%9A%E9%83%A8%E7%BD%B2%E5%88%86%E5%B8%83%E5%BC%8F%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F/"/>
    <id>http://yoursite.com/2017/07/27/Ansible实战：部署分布式日志系统/</id>
    <published>2017-07-27T12:30:05.000Z</published>
    <updated>2017-07-28T09:11:52.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;产品组在开发一个分布式日志系统，用的组件较多，单独手工部署一各个个软件比较繁琐，花的时间比较长，于是就想到了使用ansible playbook + roles进行部署，效率大大提高。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;分布式日志系统架构图&quot;&gt;&lt;a href=&quot;#分布式日志系统架构图&quot; class=&quot;headerlink&quot; title=&quot;分布式日志系统架构图&quot;&gt;&lt;/a&gt;分布式日志系统架构图&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/48.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;创建roles&quot;&gt;&lt;a href=&quot;#创建roles&quot; class=&quot;headerlink&quot; title=&quot;创建roles&quot;&gt;&lt;/a&gt;创建roles&lt;/h2&gt;&lt;p&gt;每一个软件或集群都创建一个单独的角色。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# mkdir -pv ansible_playbooks/roles/{db_server,web_server,redis_server,zk_server,kafka_server,es_server,tomcat_server,flume_agent,hadoop,spark,hbase,hive,jdk7,jdk8}/{tasks,files,templates,meta,handlers,vars} 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/49.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;JDK7-role&quot;&gt;&lt;a href=&quot;#JDK7-role&quot; class=&quot;headerlink&quot; title=&quot;JDK7 role&quot;&gt;&lt;/a&gt;JDK7 role&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@node1 jdk7]# pwd
/root/ansible_playbooks/roles/jdk7
[root@node1 jdk7]# ls
files  handlers  meta  tasks  templates  vars
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;上传安装包&quot;&gt;&lt;a href=&quot;#上传安装包&quot; class=&quot;headerlink&quot; title=&quot;上传安装包&quot;&gt;&lt;/a&gt;上传安装包&lt;/h4&gt;&lt;p&gt;将jdk-7u80-linux-x64.gz上传到files目录下。&lt;/p&gt;
&lt;h4 id=&quot;编写tasks&quot;&gt;&lt;a href=&quot;#编写tasks&quot; class=&quot;headerlink&quot; title=&quot;编写tasks&quot;&gt;&lt;/a&gt;编写tasks&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@node1 jdk7]# vim tasks/main.yml 
- name: mkdir necessary catalog                                                                                                               
  file: path=/usr/java state=directory mode=0755
- name: copy and unzip jdk 
  unarchive: src={{jdk_package_name}} dest=/usr/java/
- name: set env 
  lineinfile: dest={{env_file}} insertafter=&amp;quot;{{item.position}}&amp;quot; line=&amp;quot;{{item.value}}&amp;quot; state=present
  with_items:
  - {position: EOF, value: &amp;quot;\n&amp;quot;}
  - {position: EOF, value: &amp;quot;export JAVA_HOME=/usr/java/{{jdk_version}}&amp;quot;}
  - {position: EOF, value: &amp;quot;export PATH=$JAVA_HOME/bin:$PATH&amp;quot;}
  - {position: EOF, value: &amp;quot;export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar&amp;quot;}
- name: enforce env 
  shell: source {{env_file}}
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;编写vars&quot;&gt;&lt;a href=&quot;#编写vars&quot; class=&quot;headerlink&quot; title=&quot;编写vars&quot;&gt;&lt;/a&gt;编写vars&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@node1 jdk7]# vim vars/main.yml 
jdk_package_name: jdk-7u80-linux-x64.gz                                                                                                       
env_file: /etc/profile
jdk_version: jdk1.7.0_80
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;使用角色&quot;&gt;&lt;a href=&quot;#使用角色&quot; class=&quot;headerlink&quot; title=&quot;使用角色&quot;&gt;&lt;/a&gt;使用角色&lt;/h4&gt;&lt;p&gt;在roles同级目录，创建一个jdk.yml文件，里面定义好你的playbook。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ansible_playbooks]# vim jdk.yml 
- hosts: jdk
  remote_user: root
  roles:
  - jdk7
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;运行playbook安装JDK7：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ansible_playbooks]# ansible-playbook jdk.yml 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;使用jdk7 role可以需要根据实际环境修改vars/main.yml里的变量以及/etc/ansible/hosts文件里定义的主机。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;JDK8-role&quot;&gt;&lt;a href=&quot;#JDK8-role&quot; class=&quot;headerlink&quot; title=&quot;JDK8 role&quot;&gt;&lt;/a&gt;JDK8 role&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@node1 jdk8]# pwd
/root/ansible_playbooks/roles/jdk8
[root@node1 jdk8]# ls
files  handlers  meta  tasks  templates  vars
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;上传安装包-1&quot;&gt;&lt;a href=&quot;#上传安装包-1&quot; class=&quot;headerlink&quot; title=&quot;上传安装包&quot;&gt;&lt;/a&gt;上传安装包&lt;/h4&gt;&lt;p&gt;将jdk-8u73-linux-x64.gz上传到files目录下。&lt;/p&gt;
&lt;h4 id=&quot;编写tasks-1&quot;&gt;&lt;a href=&quot;#编写tasks-1&quot; class=&quot;headerlink&quot; title=&quot;编写tasks&quot;&gt;&lt;/a&gt;编写tasks&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@node1 jdk8]# vim tasks/main.yml 
- name: mkdir necessary catalog                                                                                                               
  file: path=/usr/java state=directory mode=0755
- name: copy and unzip jdk 
  unarchive: src={{jdk_package_name}} dest=/usr/java/
- name: set env 
  lineinfile: dest={{env_file}} insertafter=&amp;quot;{{item.position}}&amp;quot; line=&amp;quot;{{item.value}}&amp;quot; state=present
  with_items:
  - {position: EOF, value: &amp;quot;\n&amp;quot;}
  - {position: EOF, value: &amp;quot;export JAVA_HOME=/usr/java/{{jdk_version}}&amp;quot;}
  - {position: EOF, value: &amp;quot;export PATH=$JAVA_HOME/bin:$PATH&amp;quot;}
  - {position: EOF, value: &amp;quot;export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar&amp;quot;}
- name: enforce env 
  shell: source {{env_file}}
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;编写vars-1&quot;&gt;&lt;a href=&quot;#编写vars-1&quot; class=&quot;headerlink&quot; title=&quot;编写vars&quot;&gt;&lt;/a&gt;编写vars&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@node1 jdk8]# vim vars/main.yml 
jdk_package_name: jdk-8u73-linux-x64.gz                                                                                                       
env_file: /etc/profile
jdk_version: jdk1.8.0_73
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;使用角色-1&quot;&gt;&lt;a href=&quot;#使用角色-1&quot; class=&quot;headerlink&quot; title=&quot;使用角色&quot;&gt;&lt;/a&gt;使用角色&lt;/h4&gt;&lt;p&gt;在roles同级目录，创建一个jdk.yml文件，里面定义好你的playbook。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ansible_playbooks]# vim jdk.yml 
- hosts: jdk
  remote_user: root
  roles:
  - jdk8
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;运行playbook安装JDK8：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ansible_playbooks]# ansible-playbook jdk.yml 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;使用jdk8 role可以需要根据实际环境修改vars/main.yml里的变量以及/etc/ansible/hosts文件里定义的主机。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;Zookeeper-role&quot;&gt;&lt;a href=&quot;#Zookeeper-role&quot; class=&quot;headerlink&quot; title=&quot;Zookeeper role&quot;&gt;&lt;/a&gt;Zookeeper role&lt;/h3&gt;&lt;p&gt;Zookeeper集群节点配置好/etc/hosts文件，配置集群各节点主机名和ip地址的对应关系。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 zk_server]# pwd
/root/ansible_playbooks/roles/zk_server
[root@node1 zk_server]# ls
files  handlers  meta  tasks  templates  vars
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;上传安装包-2&quot;&gt;&lt;a href=&quot;#上传安装包-2&quot; class=&quot;headerlink&quot; title=&quot;上传安装包&quot;&gt;&lt;/a&gt;上传安装包&lt;/h4&gt;&lt;p&gt;将zookeeper-3.4.6.tar.gz和clean_zklog.sh上传到files目录。clean_zklog.sh是清理Zookeeper日志的脚本。&lt;/p&gt;
&lt;h4 id=&quot;编写tasks-2&quot;&gt;&lt;a href=&quot;#编写tasks-2&quot; class=&quot;headerlink&quot; title=&quot;编写tasks&quot;&gt;&lt;/a&gt;编写tasks&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@node1 zk_server]# vim tasks/main.yml 
- name: install zookeeper                                                                                                                     
  unarchive: src=zookeeper-3.4.6.tar.gz dest=/usr/local/
- name: install configuration file for zookeeper
  template: src=zoo.cfg.j2 dest=/usr/local/zookeeper-3.4.6/conf/zoo.cfg
- name: add myid file
  shell: echo {{ myid }} &amp;gt; /usr/local/zookeeper-3.4.6/dataDir/myid
- name: copy script to clear zookeeper logs.
  copy: src=clean_zklog.sh dest=/usr/local/zookeeper-3.4.6/clean_zklog.sh mode=755
- name: crontab task
  cron: name=&amp;quot;clear zk logs&amp;quot; weekday=&amp;quot;0&amp;quot; hour=&amp;quot;0&amp;quot; minute=&amp;quot;0&amp;quot; job=&amp;quot;/usr/local/zookeeper-3.4.6/clean_zklog.sh&amp;quot;
- name: start zookeeper
  shell: /usr/local/zookeeper-3.4.6/bin/zkServer.sh start
  tags:
  - start
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;编写templates&quot;&gt;&lt;a href=&quot;#编写templates&quot; class=&quot;headerlink&quot; title=&quot;编写templates&quot;&gt;&lt;/a&gt;编写templates&lt;/h4&gt;&lt;p&gt;将zookeeper-3.4.6.tar.gz包中的默认配置文件重命名为zoo.cfg.j2，并修改其中的内容。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ansible_playbooks]# vim roles/zk_server/templates/zoo.cfg.j2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;配置文件内容过多，具体见github，地址是&lt;a href=&quot;https://github.com/jkzhao/ansible-godseye。配置文件内容也不在解释，在前面博客中的文章中都已写明。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/jkzhao/ansible-godseye。配置文件内容也不在解释，在前面博客中的文章中都已写明。&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;编写vars-2&quot;&gt;&lt;a href=&quot;#编写vars-2&quot; class=&quot;headerlink&quot; title=&quot;编写vars&quot;&gt;&lt;/a&gt;编写vars&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@node1 zk_server]# vim vars/main.yml 
server1_hostname: hadoop27                                                                                                                    
server2_hostname: hadoop28
server3_hostname: hadoop29
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;另外在tasks中还使用了个变量，该变量每台主机的值是不一样的，所以定义在了/etc/ansible/hosts文件中:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[zk_servers]
172.16.206.27 myid=1
172.16.206.28 myid=2
172.16.206.29 myid=3
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;设置主机组&quot;&gt;&lt;a href=&quot;#设置主机组&quot; class=&quot;headerlink&quot; title=&quot;设置主机组&quot;&gt;&lt;/a&gt;设置主机组&lt;/h4&gt;&lt;p&gt;/etc/ansible/hosts文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[zk_servers]
172.16.206.27 myid=1
172.16.206.28 myid=2
172.16.206.29 myid=3
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;使用角色-2&quot;&gt;&lt;a href=&quot;#使用角色-2&quot; class=&quot;headerlink&quot; title=&quot;使用角色&quot;&gt;&lt;/a&gt;使用角色&lt;/h4&gt;&lt;p&gt;在roles同级目录，创建一个zk.yml文件，里面定义好你的playbook。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ansible_playbooks]# vim zk.yml 
- hosts: zk_servers
  remote_user: root
  roles:
  - zk_server
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;运行playbook安装Zookeeper集群：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ansible_playbooks]# ansible-playbook zk.yml 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;使用zk_server role需要根据实际环境修改vars/main.yml里的变量以及/etc/ansible/hosts文件里定义的主机。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;Kafka-role&quot;&gt;&lt;a href=&quot;#Kafka-role&quot; class=&quot;headerlink&quot; title=&quot;Kafka role&quot;&gt;&lt;/a&gt;Kafka role&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@node1 kafka_server]# pwd
/root/ansible_playbooks/roles/kafka_server
[root@node1 kafka_server]# ls
files  handlers  meta  tasks  templates  vars
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;上传安装包-3&quot;&gt;&lt;a href=&quot;#上传安装包-3&quot; class=&quot;headerlink&quot; title=&quot;上传安装包&quot;&gt;&lt;/a&gt;上传安装包&lt;/h4&gt;&lt;p&gt;将kafka_2.11-0.9.0.1.tar.gz、kafka-manager-1.3.0.6.zip和clean_kafkalog.sh上传到files目录。clean_kafkalog.sh是清理kafka日志的脚本。&lt;/p&gt;
&lt;h4 id=&quot;编写tasks-3&quot;&gt;&lt;a href=&quot;#编写tasks-3&quot; class=&quot;headerlink&quot; title=&quot;编写tasks&quot;&gt;&lt;/a&gt;编写tasks&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@node1 kafka_server]# vim tasks/main.yml 
- name: copy and unzip kafka
  unarchive: src=kafka_2.11-0.9.0.1.tgz dest=/usr/local/
- name: install configuration file for kafka
  template: src=server.properties.j2 dest=/usr/local/kafka_2.11-0.9.0.1/config/server.properties
- name: copy script to clear kafka logs.
  copy: src=clean_kafkalog.sh dest=/usr/local/kafka_2.11-0.9.0.1/clean_kafkalog.sh mode=755                                                   
- name: crontab task                                                         
  cron: name=&amp;quot;clear kafka logs&amp;quot; weekday=&amp;quot;0&amp;quot; hour=&amp;quot;0&amp;quot; minute=&amp;quot;0&amp;quot; job=&amp;quot;/usr/local/kafka_2.11-0.9.0.1/clean_kafkalog.sh&amp;quot;
- name: start kafka                            
  shell: JMX_PORT=9997 /usr/local/kafka_2.11-0.9.0.1/bin/kafka-server-start.sh -daemon /usr/local/kafka_2.11-0.9.0.1/config/server.properties &amp;amp;                                              
  tags:                                        
  - start                                      
- name: copy and unizp kafka-manager           
  unarchive: src=kafka-manager-1.3.0.6.zip dest=/usr/local/
  when: ansible_default_ipv4[&amp;apos;address&amp;apos;] == &amp;quot;{{kafka_manager_ip}}&amp;quot;
- name: install configuration file for kafka-manager
  template: src=application.conf.j2 dest=/usr/local/kafka-manager-1.3.0.6/conf/application.conf
  when: ansible_default_ipv4[&amp;apos;address&amp;apos;] == &amp;quot;{{kafka_manager_ip}}&amp;quot;
- name: start kafka-manager                    
  shell: nohup /usr/local/kafka-manager-1.3.0.6/bin/kafka-manager &amp;amp;
  when: ansible_default_ipv4[&amp;apos;address&amp;apos;] == &amp;quot;{{kafka_manager_ip}}&amp;quot;
  tags:                                        
  - kafkaManagerStart
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;编写templates-1&quot;&gt;&lt;a href=&quot;#编写templates-1&quot; class=&quot;headerlink&quot; title=&quot;编写templates&quot;&gt;&lt;/a&gt;编写templates&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@node1 kafka_server]# vim templates/server.properties.j2 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;配置文件内容过多，具体见github，地址是&lt;a href=&quot;https://github.com/jkzhao/ansible-godseye。配置文件内容也不再解释，在前面博客中的文章中都已写明。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/jkzhao/ansible-godseye。配置文件内容也不再解释，在前面博客中的文章中都已写明。&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;编写vars-3&quot;&gt;&lt;a href=&quot;#编写vars-3&quot; class=&quot;headerlink&quot; title=&quot;编写vars&quot;&gt;&lt;/a&gt;编写vars&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@node1 kafka_server]# vim vars/main.yml
zk_cluster: 172.16.7.151:2181,172.16.7.152:2181,172.16.7.153:2181
kafka_manager_ip: 172.16.7.151 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;另外在template的文件中还使用了个变量，该变量每台主机的值是不一样的，所以定义在了/etc/ansible/hosts文件中:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[kafka_servers]
172.16.206.17 broker_id=0
172.16.206.31 broker_id=1
172.16.206.32 broker_id=2
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;设置主机组-1&quot;&gt;&lt;a href=&quot;#设置主机组-1&quot; class=&quot;headerlink&quot; title=&quot;设置主机组&quot;&gt;&lt;/a&gt;设置主机组&lt;/h4&gt;&lt;p&gt;/etc/ansible/hosts文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[kafka_servers]
172.16.206.17 broker_id=0
172.16.206.31 broker_id=1
172.16.206.32 broker_id=2
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;使用角色-3&quot;&gt;&lt;a href=&quot;#使用角色-3&quot; class=&quot;headerlink&quot; title=&quot;使用角色&quot;&gt;&lt;/a&gt;使用角色&lt;/h4&gt;&lt;p&gt;在roles同级目录，创建一个kafka.yml文件，里面定义好你的playbook。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ansible_playbooks]# vim kafka.yml 
- hosts: kafka_servers
  remote_user: root
  roles:
  - kafka_server
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;运行playbook安装kafka集群：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ansible_playbooks]# ansible-playbook kafka.yml 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;使用kafka_server role需要根据实际环境修改vars/main.yml里的变量以及/etc/ansible/hosts文件里定义的主机。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;Elasticsearch-role&quot;&gt;&lt;a href=&quot;#Elasticsearch-role&quot; class=&quot;headerlink&quot; title=&quot;Elasticsearch role&quot;&gt;&lt;/a&gt;Elasticsearch role&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@node1 es_server]# pwd
/root/ansible_playbooks/roles/es_server
[root@node1 es_server]# ls
files  handlers  meta  tasks  templates  vars
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;上传安装包-4&quot;&gt;&lt;a href=&quot;#上传安装包-4&quot; class=&quot;headerlink&quot; title=&quot;上传安装包&quot;&gt;&lt;/a&gt;上传安装包&lt;/h4&gt;&lt;p&gt;将elasticsearch-2.3.3.tar.gz  elasticsearch-analysis-ik-1.9.3.zip上传到files目录。&lt;/p&gt;
&lt;h4 id=&quot;编写tasks-4&quot;&gt;&lt;a href=&quot;#编写tasks-4&quot; class=&quot;headerlink&quot; title=&quot;编写tasks&quot;&gt;&lt;/a&gt;编写tasks&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@node1 es_server]# vim tasks/main.yml
- name: create es user
  user: name=es password={{password}}
  vars:
    # created with:
    # python -c &amp;apos;import crypt; print crypt.crypt(&amp;quot;This is my Password&amp;quot;, &amp;quot;$1$SomeSalt$&amp;quot;)&amp;apos;
    # &amp;gt;&amp;gt;&amp;gt; import crypt
    # &amp;gt;&amp;gt;&amp;gt; crypt.crypt(&amp;apos;wisedu123&amp;apos;, &amp;apos;$1$bigrandomsalt$&amp;apos;)
    # &amp;apos;$1$bigrando$wzfZ2ifoHJPvaMuAelsBq0&amp;apos;
    password: $1$bigrando$wzfZ2ifoHJPvaMuAelsBq0
- name: mkdir directory for elasticsearch data
  file: dest=/esdata mode=0755 state=directory owner=es group=es
- name: copy and unzip es
  #unarchive module owner and group only effect on directory.
  unarchive: src=elasticsearch-2.3.3.tar.gz dest=/usr/local/
- name: install memory configuration file for es
  template: src=elasticsearch.in.sh.j2 dest=/usr/local/elasticsearch-2.3.3/bin/elasticsearch.in.sh owner=es group=es
- name: install configuration file for es
  template: src=elasticsearch.yml.j2 dest=/usr/local/elasticsearch-2.3.3/config/elasticsearch.yml owner=es group=es
- name: mkdir directory for elasticsearch-analysis-ik plugin
  file: dest=/usr/local/elasticsearch-2.3.3/plugins/ik mode=0755 state=directory owner=es group=es
- name: copy and unizp elasticsearch-analysis-ik plugin
  unarchive: src=elasticsearch-analysis-ik-1.9.3.zip dest=/usr/local/elasticsearch-2.3.3/plugins/ik
- name: change owner and group
  #recurse=yes make all files in a directory changed.
  file: path=/usr/local/elasticsearch-2.3.3 owner=es group=es recurse=yes
- name: start es
  shell: su - es -c &amp;apos;/usr/local/elasticsearch-2.3.3/bin/elasticsearch -d&amp;apos;
  #command: /usr/local/elasticsearch-2.3.3/bin/elasticsearch -d
  #become: true
  #become_method: su
  #become_user: es
  tags:
  - start
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;编写templates-2&quot;&gt;&lt;a href=&quot;#编写templates-2&quot; class=&quot;headerlink&quot; title=&quot;编写templates&quot;&gt;&lt;/a&gt;编写templates&lt;/h4&gt;&lt;p&gt;将模板elasticsearch.in.sh.j2和elasticsearch.yml.j2放入templates目录下&lt;br&gt;&lt;strong&gt;注意模板里的变量名中间不能用。比如：这样的变量名是不合法的。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;配置文件内容过多，具体见github，地址是&lt;a href=&quot;https://github.com/jkzhao/ansible-godseye。配置文件内容也不再解释，在前面博客中的文章中都已写明。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/jkzhao/ansible-godseye。配置文件内容也不再解释，在前面博客中的文章中都已写明。&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;编写vars-4&quot;&gt;&lt;a href=&quot;#编写vars-4&quot; class=&quot;headerlink&quot; title=&quot;编写vars&quot;&gt;&lt;/a&gt;编写vars&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@node1 es_server]# vim vars/main.yml
ES_MEM: 2g
cluster_name: wisedu
master_ip: 172.16.7.151 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;另外在template的文件中还使用了个变量，该变量每台主机的值是不一样的，所以定义在了/etc/ansible/hosts文件中:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[es_servers]
172.16.7.151 node_master=true
172.16.7.152 node_master=false
172.16.7.153 node_master=false 
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;设置主机组-2&quot;&gt;&lt;a href=&quot;#设置主机组-2&quot; class=&quot;headerlink&quot; title=&quot;设置主机组&quot;&gt;&lt;/a&gt;设置主机组&lt;/h4&gt;&lt;p&gt;/etc/ansible/hosts文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[es_servers]
172.16.7.151 node_master=true
172.16.7.152 node_master=false
172.16.7.153 node_master=false 
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;使用角色-4&quot;&gt;&lt;a href=&quot;#使用角色-4&quot; class=&quot;headerlink&quot; title=&quot;使用角色&quot;&gt;&lt;/a&gt;使用角色&lt;/h4&gt;&lt;p&gt;在roles同级目录，创建一个es.yml文件，里面定义好你的playbook。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ansible_playbooks]# vim es.yml 
- hosts: es_servers
  remote_user: root
  roles:
  - es_server
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;运行playbook安装Elasticsearch集群：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ansible_playbooks]# ansible-playbook es.yml 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;使用es_server role需要根据实际环境修改vars/main.yml里的变量以及/etc/ansible/hosts文件里定义的主机。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;MySQL-role&quot;&gt;&lt;a href=&quot;#MySQL-role&quot; class=&quot;headerlink&quot; title=&quot;MySQL role&quot;&gt;&lt;/a&gt;MySQL role&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@node1 db_server]# pwd
/root/ansible_playbooks/roles/db_server
[root@node1 db_server]# ls
files  handlers  meta  tasks  templates  vars
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;上传安装包-5&quot;&gt;&lt;a href=&quot;#上传安装包-5&quot; class=&quot;headerlink&quot; title=&quot;上传安装包&quot;&gt;&lt;/a&gt;上传安装包&lt;/h4&gt;&lt;p&gt;将制作好的rpm包mysql-5.6.27-1.x86_64.rpm放到/root/ansible_playbooks/roles/db_server/files/目录下。&lt;br&gt;&lt;strong&gt;【注意】:这个rpm包是自己打包制作的，打包成rpm会使得部署的效率提高。关于如何打包成rpm见之前的博客《速成RPM包制作》。&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&quot;编写tasks-5&quot;&gt;&lt;a href=&quot;#编写tasks-5&quot; class=&quot;headerlink&quot; title=&quot;编写tasks&quot;&gt;&lt;/a&gt;编写tasks&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@node1 db_server]# vim tasks/main.yml
- name: install dependency package
  yum: name={{ item }} state=present
  with_items:
  - libaio
  - libaio-devel
- name: copy mysql rpm
  copy: src=mysql-5.6.27-1.x86_64.rpm dest=/tmp/
- name: install mysql
  yum: name=/tmp/mysql-5.6.27-1.x86_64.rpm state=present
- name: start mysql
  shell: /etc/init.d/mysqld start
  tags:
  - start
- name: set up root password
  shell: /usr/local/mysql/bin/mysql -uroot -e &amp;quot;UPDATE mysql.user SET Password=PASSWORD(&amp;apos;wisedu123&amp;apos;) where USER=&amp;apos;root&amp;apos;&amp;quot; &amp;amp;&amp;gt;/dev/null
- name: delete anonymous account1
  shell: /usr/local/mysql/bin/mysql -uroot -Dmysql -pwisedu123 -e &amp;quot;DROP USER &amp;apos;&amp;apos;@localhost&amp;quot; &amp;amp;&amp;gt;/dev/null
- name: delete anonymous account2
  shell: /usr/local/mysql/bin/mysql -uroot -Dmysql -pwisedu123 -e &amp;quot;grant all on *.* to root@&amp;apos;%.%.%.%&amp;apos; identified by &amp;apos;wisedu123&amp;apos;&amp;quot; &amp;amp;&amp;gt;/dev/null
- name: flush privileges
  shell: /usr/local/mysql/bin/mysql -uroot -Dmysql -pwisedu123 -e &amp;quot;flush privileges&amp;quot; &amp;amp;&amp;gt;/dev/null
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;设置主机组-3&quot;&gt;&lt;a href=&quot;#设置主机组-3&quot; class=&quot;headerlink&quot; title=&quot;设置主机组&quot;&gt;&lt;/a&gt;设置主机组&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;# vim /etc/ansible/hosts 
[db_servers]
172.16.7.152 
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;使用角色-5&quot;&gt;&lt;a href=&quot;#使用角色-5&quot; class=&quot;headerlink&quot; title=&quot;使用角色&quot;&gt;&lt;/a&gt;使用角色&lt;/h4&gt;&lt;p&gt;在roles同级目录，创建一个db.yml文件，里面定义好你的playbook。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ansible_playbooks]# vim db.yml 
- hosts: mysql_server
  remote_user: root
  roles:
  - db_server
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;运行playbook安装MySQL：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ansible_playbooks]# ansible-playbook db.yml 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;使用db_server role需要根据实际环境修改/etc/ansible/hosts文件里定义的主机。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;Nginx-role&quot;&gt;&lt;a href=&quot;#Nginx-role&quot; class=&quot;headerlink&quot; title=&quot;Nginx role&quot;&gt;&lt;/a&gt;Nginx role&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@node1 web_server]# pwd
/root/ansible_playbooks/roles/web_server
[root@node1 web_server]# ls
files  handlers  meta  tasks  templates  vars
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;上传安装包-6&quot;&gt;&lt;a href=&quot;#上传安装包-6&quot; class=&quot;headerlink&quot; title=&quot;上传安装包&quot;&gt;&lt;/a&gt;上传安装包&lt;/h4&gt;&lt;p&gt;将制作好的rpm包openresty-for-godseye-1.9.7.3-1.x86_64.rpm放到/root/ansible_playbooks/roles/web_server/files/目录下。&lt;br&gt;&lt;strong&gt;【注意】:做成rpm包，在安装时省去了编译nginx的过程，提升了部署效率。这个包里面打包了很多与我们系统相关的文件。&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&quot;编写tasks-6&quot;&gt;&lt;a href=&quot;#编写tasks-6&quot; class=&quot;headerlink&quot; title=&quot;编写tasks&quot;&gt;&lt;/a&gt;编写tasks&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@node1 web_server]# vim tasks/main.yml 
- name: install dependency package
  yum: name={{ item }} state=present
  with_items:
  - openssl-devel
  - readline-devel
  - pcre-devel
  - gcc
- name: copy nginx
  copy: src=openresty-for-godseye-1.9.7.3-1.x86_64.rpm dest=/tmp/
- name: install nginx
  yum: name=/tmp/openresty-for-godseye-1.9.7.3-1.x86_64.rpm state=present
- name: install configuration file for nginx
  template: src=nginx.conf.j2 dest=/usr/local/openresty/nginx/conf/nginx.conf
- name: crontab task
  cron: name=&amp;quot;clear nginx logs&amp;quot; weekday=&amp;quot;0&amp;quot; hour=&amp;quot;0&amp;quot; minute=&amp;quot;0&amp;quot; job=&amp;quot;/usr/local/openresty/clrnginxlog.sh&amp;quot;
- name: start nginx
  shell: systemctl start nginx.service
  tags:
  - start
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;编写templates-3&quot;&gt;&lt;a href=&quot;#编写templates-3&quot; class=&quot;headerlink&quot; title=&quot;编写templates&quot;&gt;&lt;/a&gt;编写templates&lt;/h4&gt;&lt;p&gt;将模板nginx.conf.j2放入templates目录下&lt;/p&gt;
&lt;p&gt;配置文件内容过多，具体见github，地址是&lt;a href=&quot;https://github.com/jkzhao/ansible-godseye。配置文件内容也不再解释，在前面博客中的文章中都已写明。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/jkzhao/ansible-godseye。配置文件内容也不再解释，在前面博客中的文章中都已写明。&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;编写vars-5&quot;&gt;&lt;a href=&quot;#编写vars-5&quot; class=&quot;headerlink&quot; title=&quot;编写vars&quot;&gt;&lt;/a&gt;编写vars&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@node1 web_server]# vim vars/main.yml 
elasticsearch_cluster: server 172.16.7.151:9200;server 172.16.7.152:9200;server 172.16.7.153:9200;
kafka_server1: 172.16.7.151
kafka_server2: 172.16.7.152
kafka_server3: 172.16.7.153   
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;经过测试，变量里面不能有逗号。 &lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&quot;设置主机组-4&quot;&gt;&lt;a href=&quot;#设置主机组-4&quot; class=&quot;headerlink&quot; title=&quot;设置主机组&quot;&gt;&lt;/a&gt;设置主机组&lt;/h4&gt;&lt;p&gt;/etc/ansible/hosts文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/ansible/hosts 
[nginx_servers]
172.16.7.153 
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;使用角色-6&quot;&gt;&lt;a href=&quot;#使用角色-6&quot; class=&quot;headerlink&quot; title=&quot;使用角色&quot;&gt;&lt;/a&gt;使用角色&lt;/h4&gt;&lt;p&gt;在roles同级目录，创建一个nginx.yml文件，里面定义好你的playbook。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ansible_playbooks]# vim nginx.yml 
- hosts: nginx_servers
  remote_user: root
  roles:
  - web_server  
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;运行playbook安装Nginx：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ansible_playbooks]# ansible-playbook nginx.yml 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;使用web_server role需要根据实际环境修改vars/main.yml里的变量以及/etc/ansible/hosts文件里定义的主机。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;Redis-role&quot;&gt;&lt;a href=&quot;#Redis-role&quot; class=&quot;headerlink&quot; title=&quot;Redis role&quot;&gt;&lt;/a&gt;Redis role&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@node1 redis_server]# pwd
/root/ansible_playbooks/roles/redis_server
[root@node1 redis_server]# ls
files  handlers  meta  tasks  templates  vars
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;上传安装包-7&quot;&gt;&lt;a href=&quot;#上传安装包-7&quot; class=&quot;headerlink&quot; title=&quot;上传安装包&quot;&gt;&lt;/a&gt;上传安装包&lt;/h4&gt;&lt;p&gt;将制作好的rpm包redis-3.2.2-1.x86_64.rpm放到/root/ansible_playbooks/roles/redis_server/files/目录下。&lt;/p&gt;
&lt;h4 id=&quot;编写tasks-7&quot;&gt;&lt;a href=&quot;#编写tasks-7&quot; class=&quot;headerlink&quot; title=&quot;编写tasks&quot;&gt;&lt;/a&gt;编写tasks&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@node1 redis_server]# vim tasks/main.yml
- name: install dependency package
  yum: name={{ item }} state=present
  with_items:
  - openssl-devel
  - readline-devel
  - pcre-devel
- name: copy redis
  copy: src=redis-3.2.2-1.x86_64.rpm dest=/tmp/
- name: install redis
  yum: name=/tmp/redis-3.2.2-1.x86_64.rpm state=present
- name: start redis
  shell: /usr/local/bin/redis-server /etc/redis.conf
  tags:
  - start
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;设置主机组-5&quot;&gt;&lt;a href=&quot;#设置主机组-5&quot; class=&quot;headerlink&quot; title=&quot;设置主机组&quot;&gt;&lt;/a&gt;设置主机组&lt;/h4&gt;&lt;p&gt;/etc/ansible/hosts文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/ansible/hosts 
[redis_servers]
172.16.7.152 
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;使用角色-7&quot;&gt;&lt;a href=&quot;#使用角色-7&quot; class=&quot;headerlink&quot; title=&quot;使用角色&quot;&gt;&lt;/a&gt;使用角色&lt;/h4&gt;&lt;p&gt;在roles同级目录，创建一个redis.yml文件，里面定义好你的playbook。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ansible_playbooks]# vim redis.yml 
- hosts: redis_servers
  remote_user: root
  roles:
  - redis_server 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;运行playbook安装redis：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ansible_playbooks]# ansible-playbook redis.yml 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;使用redis_server role需要根据实际环境修改vars/main.yml里的变量以及/etc/ansible/hosts文件里定义的主机。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;Hadoop-role&quot;&gt;&lt;a href=&quot;#Hadoop-role&quot; class=&quot;headerlink&quot; title=&quot;Hadoop role&quot;&gt;&lt;/a&gt;Hadoop role&lt;/h3&gt;&lt;p&gt;完全分布式集群部署，NameNode和ResourceManager高可用。&lt;br&gt;提前配置集群节点的/etc/hosts文件、节点时间同步、某些集群主节点登录其他节点不需要输入密码。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 hadoop]# pwd
/root/ansible_playbooks/roles/hadoop
[root@node1 hadoop]# ls
files  handlers  meta  tasks  templates  vars
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;上传安装包-8&quot;&gt;&lt;a href=&quot;#上传安装包-8&quot; class=&quot;headerlink&quot; title=&quot;上传安装包&quot;&gt;&lt;/a&gt;上传安装包&lt;/h4&gt;&lt;p&gt;将hadoop-2.7.2.tar.gz放到/root/ansible_playbooks/roles/hadoop/files/目录下。&lt;/p&gt;
&lt;h4 id=&quot;编写tasks-8&quot;&gt;&lt;a href=&quot;#编写tasks-8&quot; class=&quot;headerlink&quot; title=&quot;编写tasks&quot;&gt;&lt;/a&gt;编写tasks&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@node1 hadoop]# cat tasks/main.yml 
- name: install dependency package
  yum: name={{ item }} state=present
  with_items:
  - openssh
  - rsync
- name: create hadoop user
  user: name=hadoop password={{password}}
  vars:
    # created with:
    # python -c &amp;apos;import crypt; print crypt.crypt(&amp;quot;This is my Password&amp;quot;, &amp;quot;$1$SomeSalt$&amp;quot;)&amp;apos;
    # &amp;gt;&amp;gt;&amp;gt; import crypt
    # &amp;gt;&amp;gt;&amp;gt; crypt.crypt(&amp;apos;wisedu123&amp;apos;, &amp;apos;$1$bigrandomsalt$&amp;apos;)
    # &amp;apos;$1$bigrando$wzfZ2ifoHJPvaMuAelsBq0&amp;apos;
    password: $1$bigrando$wzfZ2ifoHJPvaMuAelsBq0
- name: copy and unzip hadoop
  #unarchive module owner and group only effect on directory.
  unarchive: src=hadoop-2.7.2.tar.gz dest=/usr/local/
- name: create hadoop soft link
  file: src=/usr/local/hadoop-2.7.2 dest=/usr/local/hadoop state=link
- name: create hadoop logs directory
  file: dest=/usr/local/hadoop/logs mode=0775 state=directory
- name: change hadoop soft link owner and group
  #recurse=yes make all files in a directory changed.
  file: path=/usr/local/hadoop owner=hadoop group=hadoop recurse=yes
- name: change hadoop-2.7.2 directory owner and group
  #recurse=yes make all files in a directory changed.
  file: path=/usr/local/hadoop-2.7.2 owner=hadoop group=hadoop recurse=yes
- name: set hadoop env
  lineinfile: dest={{env_file}} insertafter=&amp;quot;{{item.position}}&amp;quot; line=&amp;quot;{{item.value}}&amp;quot; state=present
  with_items:
  - {position: EOF, value: &amp;quot;\n&amp;quot;}
  - {position: EOF, value: &amp;quot;# Hadoop environment&amp;quot;}
  - {position: EOF, value: &amp;quot;export HADOOP_HOME=/usr/local/hadoop&amp;quot;}
  - {position: EOF, value: &amp;quot;export PATH=$PATH:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin&amp;quot;}
- name: enforce env
  shell: source {{env_file}}
- name: install configuration file hadoop-env.sh.j2 for hadoop
  template: src=hadoop-env.sh.j2 dest=/usr/local/hadoop/etc/hadoop/hadoop-env.sh owner=hadoop group=hadoop
- name: install configuration file core-site.xml.j2 for hadoop
  template: src=core-site.xml.j2 dest=/usr/local/hadoop/etc/hadoop/core-site.xml owner=hadoop group=hadoop
- name: install configuration file hdfs-site.xml.j2 for hadoop
  template: src=hdfs-site.xml.j2 dest=/usr/local/hadoop/etc/hadoop/hdfs-site.xml owner=hadoop group=hadoop
- name: install configuration file mapred-site.xml.j2 for hadoop
  template: src=mapred-site.xml.j2 dest=/usr/local/hadoop/etc/hadoop/mapred-site.xml owner=hadoop group=hadoop
- name: install configuration file yarn-site.xml.j2 for hadoop
  template: src=yarn-site.xml.j2 dest=/usr/local/hadoop/etc/hadoop/yarn-site.xml owner=hadoop group=hadoop
- name: install configuration file slaves.j2 for hadoop
  template: src=slaves.j2 dest=/usr/local/hadoop/etc/hadoop/slaves owner=hadoop group=hadoop
- name: install configuration file hadoop-daemon.sh.j2 for hadoop
  template: src=hadoop-daemon.sh.j2 dest=/usr/local/hadoop/sbin/hadoop-daemon.sh owner=hadoop group=hadoop
- name: install configuration file yarn-daemon.sh.j2 for hadoop
  template: src=yarn-daemon.sh.j2 dest=/usr/local/hadoop/sbin/yarn-daemon.sh owner=hadoop group=hadoop
# make sure zookeeper started, and then start hadoop.
# start journalnode
- name: start journalnode
  shell: /usr/local/hadoop/sbin/hadoop-daemon.sh start journalnode
  become: true
  become_method: su
  become_user: hadoop
  when: datanode == &amp;quot;true&amp;quot;
# format namenode
- name: format active namenode hdfs
  shell: /usr/local/hadoop/bin/hdfs namenode -format
  become: true
  become_method: su
  become_user: hadoop
  when: namenode_active == &amp;quot;true&amp;quot;
- name: start active namenode hdfs
  shell: /usr/local/hadoop/sbin/hadoop-daemon.sh start namenode
  become: true
  become_method: su
  become_user: hadoop
  when: namenode_active == &amp;quot;true&amp;quot;
- name: format standby namenode hdfs
  shell: /usr/local/hadoop/bin/hdfs namenode -bootstrapStandby
  become: true
  become_method: su
  become_user: hadoop
  when: namenode_standby == &amp;quot;true&amp;quot;
- name: stop active namenode hdfs
  shell: /usr/local/hadoop/sbin/hadoop-daemon.sh stop namenode
  become: true
  become_method: su
  become_user: hadoop
  when: namenode_active == &amp;quot;true&amp;quot;
# format ZKFC
- name: format ZKFC
  shell: /usr/local/hadoop/bin/hdfs zkfc -formatZK
  become: true
  become_method: su
  become_user: hadoop
  when: namenode_active == &amp;quot;true&amp;quot;
# start hadoop cluster
- name: start namenode
  shell: /usr/local/hadoop/sbin/start-dfs.sh
  become: true
  become_method: su
  become_user: hadoop
  when: namenode_active == &amp;quot;true&amp;quot;
- name: start yarn
  shell: /usr/local/hadoop/sbin/start-yarn.sh
  become: true
  become_method: su
  become_user: hadoop
  when: namenode_active == &amp;quot;true&amp;quot;
- name: start standby rm
  shell: /usr/local/hadoop/sbin/yarn-daemon.sh start resourcemanager
  become: true
  become_method: su
  become_user: hadoop
  when: namenode_standby == &amp;quot;true&amp;quot; 
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;编写templates-4&quot;&gt;&lt;a href=&quot;#编写templates-4&quot; class=&quot;headerlink&quot; title=&quot;编写templates&quot;&gt;&lt;/a&gt;编写templates&lt;/h4&gt;&lt;p&gt;将模板core-site.xml.j2、hadoop-daemon.sh.j2、hadoop-env.sh.j2、hdfs-site.xml.j2、mapred-site.xml.j2、slaves.j2、yarn-daemon.sh.j2、yarn-site.xml.j2放入templates目录下。&lt;/p&gt;
&lt;p&gt;配置文件内容过多，具体见github，地址是&lt;a href=&quot;https://github.com/jkzhao/ansible-godseye。配置文件内容也不再解释，在前面博客中的文章中都已写明。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/jkzhao/ansible-godseye。配置文件内容也不再解释，在前面博客中的文章中都已写明。&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;编写vars-6&quot;&gt;&lt;a href=&quot;#编写vars-6&quot; class=&quot;headerlink&quot; title=&quot;编写vars&quot;&gt;&lt;/a&gt;编写vars&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@node1 hadoop]# vim vars/main.yml 
env_file: /etc/profile
# hadoop-env.sh.j2 file variables.
JAVA_HOME: /usr/java/jdk1.8.0_73
# core-site.xml.j2 file variables.
ZK_NODE1: node1:2181
ZK_NODE2: node2:2181
ZK_NODE3: node3:2181
# hdfs-site.xml.j2 file variables.
NAMENODE1_HOSTNAME: node1
NAMENODE2_HOSTNAME: node2
DATANODE1_HOSTNAME: node3
DATANODE2_HOSTNAME: node4
DATANODE3_HOSTNAME: node5
# mapred-site.xml.j2 file variables.
MR_MODE: yarn
# yarn-site.xml.j2 file variables.
RM1_HOSTNAME: node1
RM2_HOSTNAME: node2
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;设置主机组-6&quot;&gt;&lt;a href=&quot;#设置主机组-6&quot; class=&quot;headerlink&quot; title=&quot;设置主机组&quot;&gt;&lt;/a&gt;设置主机组&lt;/h4&gt;&lt;p&gt;/etc/ansible/hosts文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/ansible/hosts 
[hadoop]
172.16.7.151 namenode_active=true namenode_standby=false datanode=false
172.16.7.152 namenode_active=false namenode_standby=true datanode=false
172.16.7.153 namenode_active=false namenode_standby=false datanode=true
172.16.7.154 namenode_active=false namenode_standby=false datanode=true
172.16.7.155 namenode_active=false namenode_standby=false datanode=true
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;使用角色-8&quot;&gt;&lt;a href=&quot;#使用角色-8&quot; class=&quot;headerlink&quot; title=&quot;使用角色&quot;&gt;&lt;/a&gt;使用角色&lt;/h4&gt;&lt;p&gt;在roles同级目录，创建一个hadoop.yml文件，里面定义好你的playbook。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ansible_playbooks]# vim hadoop.yml 
- hosts: hadoop
  remote_user: root
  roles:
  - jdk8
  - hadoop  
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;运行playbook安装hadoop集群：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ansible_playbooks]# ansible-playbook hadoop.yml 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;使用hadoop role需要根据实际环境修改vars/main.yml里的变量以及/etc/ansible/hosts文件里定义的主机。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;Spark-role&quot;&gt;&lt;a href=&quot;#Spark-role&quot; class=&quot;headerlink&quot; title=&quot;Spark role&quot;&gt;&lt;/a&gt;Spark role&lt;/h3&gt;&lt;p&gt;Standalone模式部署spark (无HA)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 spark]# pwd
/root/ansible_playbooks/roles/spark
[root@node1 spark]# ls
files  handlers  meta  tasks  templates  vars
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;上传安装包-9&quot;&gt;&lt;a href=&quot;#上传安装包-9&quot; class=&quot;headerlink&quot; title=&quot;上传安装包&quot;&gt;&lt;/a&gt;上传安装包&lt;/h4&gt;&lt;p&gt;将scala-2.10.6.tgz和spark-1.6.1-bin-hadoop2.6.tgz放到/root/ansible_playbooks/roles/hadoop/files/目录下。&lt;/p&gt;
&lt;h4 id=&quot;编写tasks-9&quot;&gt;&lt;a href=&quot;#编写tasks-9&quot; class=&quot;headerlink&quot; title=&quot;编写tasks&quot;&gt;&lt;/a&gt;编写tasks&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@node1 spark]# cat tasks/main.yml 
- name: copy and unzip scala
  unarchive: src=scala-2.10.6.tgz dest=/usr/local/
- name: set scala env
  lineinfile: dest={{env_file}} insertafter=&amp;quot;{{item.position}}&amp;quot; line=&amp;quot;{{item.value}}&amp;quot; state=present
  with_items:
  - {position: EOF, value: &amp;quot;\n&amp;quot;}
  - {position: EOF, value: &amp;quot;# Scala environment&amp;quot;}
  - {position: EOF, value: &amp;quot;export SCALA_HOME=/usr/local/scala-2.10.6&amp;quot;}
  - {position: EOF, value: &amp;quot;export PATH=$SCALA_HOME/bin:$PATH&amp;quot;}
- name: copy and unzip spark
  unarchive: src=spark-1.6.1-bin-hadoop2.6.tgz dest=/usr/local/
- name: rename spark directory
  command: mv /usr/local/spark-1.6.1-bin-hadoop2.6 /usr/local/spark-1.6.1
- name: set spark env
  lineinfile: dest={{env_file}} insertafter=&amp;quot;{{item.position}}&amp;quot; line=&amp;quot;{{item.value}}&amp;quot; state=present
  with_items:
  - {position: EOF, value: &amp;quot;\n&amp;quot;}
  - {position: EOF, value: &amp;quot;# Spark environment&amp;quot;}
  - {position: EOF, value: &amp;quot;export SPARK_HOME=/usr/local/spark-1.6.1&amp;quot;}
  - {position: EOF, value: &amp;quot;export PATH=$SPARK_HOME/bin:$PATH&amp;quot;}
- name: enforce env
  shell: source {{env_file}}
- name: install configuration file for spark
  template: src=slaves.j2 dest=/usr/local/spark-1.6.1/conf/slaves
- name: install configuration file for spark
  template: src=spark-env.sh.j2 dest=/usr/local/spark-1.6.1/conf/spark-env.sh
- name: start spark cluster
  shell: /usr/local/spark-1.6.1/sbin/start-all.sh
  tags:
  - start
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;编写templates-5&quot;&gt;&lt;a href=&quot;#编写templates-5&quot; class=&quot;headerlink&quot; title=&quot;编写templates&quot;&gt;&lt;/a&gt;编写templates&lt;/h4&gt;&lt;p&gt;将模板slaves.j2和spark-env.sh.j2放到/root/ansible_playbooks/roles/spark/templates/目录下。&lt;/p&gt;
&lt;p&gt;配置文件内容过多，具体见github，地址是&lt;a href=&quot;https://github.com/jkzhao/ansible-godseye。配置文件内容也不再解释，在前面博客中的文章中都已写明。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/jkzhao/ansible-godseye。配置文件内容也不再解释，在前面博客中的文章中都已写明。&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;编写vars-7&quot;&gt;&lt;a href=&quot;#编写vars-7&quot; class=&quot;headerlink&quot; title=&quot;编写vars&quot;&gt;&lt;/a&gt;编写vars&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@node1 spark]# vim vars/main.yml
env_file: /etc/profile
# spark-env.sh.j2 file variables
JAVA_HOME: /usr/java/jdk1.8.0_73
SCALA_HOME: /usr/local/scala-2.10.6
SPARK_MASTER_HOSTNAME: node1
SPARK_HOME: /usr/local/spark-1.6.1
SPARK_WORKER_MEMORY: 256M
HIVE_HOME: /usr/local/apache-hive-2.1.0-bin
HADOOP_CONF_DIR: /usr/local/hadoop/etc/hadoop/
# slave.j2 file variables
SLAVE1_HOSTNAME: node2
SLAVE2_HOSTNAME: node3
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;设置主机组-7&quot;&gt;&lt;a href=&quot;#设置主机组-7&quot; class=&quot;headerlink&quot; title=&quot;设置主机组&quot;&gt;&lt;/a&gt;设置主机组&lt;/h4&gt;&lt;p&gt;/etc/ansible/hosts文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/ansible/hosts 
[spark]
172.16.7.151
172.16.7.152
172.16.7.153 
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;使用角色-9&quot;&gt;&lt;a href=&quot;#使用角色-9&quot; class=&quot;headerlink&quot; title=&quot;使用角色&quot;&gt;&lt;/a&gt;使用角色&lt;/h4&gt;&lt;p&gt;在roles同级目录，创建一个spark.yml文件，里面定义好你的playbook。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ansible_playbooks]# vim spark.yml 
- hosts: spark
  remote_user: root
  roles:
  - spark  
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;运行playbook安装spark集群：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ansible_playbooks]# ansible-playbook spark.yml 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;使用spark role需要根据实际环境修改vars/main.yml里的变量以及/etc/ansible/hosts文件里定义的主机。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;所有的文件都在github上，&lt;a href=&quot;https://github.com/jkzhao/ansible-godseye。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/jkzhao/ansible-godseye。&lt;/a&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;产品组在开发一个分布式日志系统，用的组件较多，单独手工部署一各个个软件比较繁琐，花的时间比较长，于是就想到了使用ansible playbook + roles进行部署，效率大大提高。&lt;br&gt;
    
    </summary>
    
      <category term="运维自动化" scheme="http://yoursite.com/categories/%E8%BF%90%E7%BB%B4%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    
    
      <category term="Ansible 运维自动化" scheme="http://yoursite.com/tags/Ansible-%E8%BF%90%E7%BB%B4%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Ansible之roles介绍</title>
    <link href="http://yoursite.com/2017/07/24/Ansible%E4%B9%8Broles%E4%BB%8B%E7%BB%8D/"/>
    <id>http://yoursite.com/2017/07/24/Ansible之roles介绍/</id>
    <published>2017-07-24T01:09:22.000Z</published>
    <updated>2017-07-24T02:52:44.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;什么场景下会用roles？&quot;&gt;&lt;a href=&quot;#什么场景下会用roles？&quot; class=&quot;headerlink&quot; title=&quot;什么场景下会用roles？&quot;&gt;&lt;/a&gt;什么场景下会用roles？&lt;/h2&gt;&lt;p&gt;假如我们现在有3个被管理主机，第一个要配置成httpd，第二个要配置成php服务器，第三个要配置成MySQL服务器。我们如何来定义playbook？&lt;br&gt;第一个play用到第一个主机上，用来构建httpd，第二个play用到第二个主机上，用来构建php，第三个play用到第三个主机上，用来构建MySQL。这些个play定义在playbook中比较麻烦，将来也不利于模块化调用，不利于多次调。比如说后来又加进来一个主机，这个第4个主机既是httpd服务器，又是php服务器，我们只能写第4个play，上面写上安装httpd和php。这样playbook中的代码就重复了。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;br&gt;为了避免代码重复，roles能够实现代码重复被调用。定义一个角色叫websrvs，第二个角色叫phpappsrvs，第三个角色叫dbsrvs。那么调用时如下来调用：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;hosts: host1
role:
- websrvs

hosts: host2
role:
- phpappsrvs

hosts: host3
role:
- dbsrvs

hosts: host4
role:
- websrvs
- phpappsrvs
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这样代码就可以重复利用了,每个角色可以被独立重复调用。下面举例说明使用方式。&lt;/p&gt;
&lt;h2 id=&quot;roles示例&quot;&gt;&lt;a href=&quot;#roles示例&quot; class=&quot;headerlink&quot; title=&quot;roles示例&quot;&gt;&lt;/a&gt;roles示例&lt;/h2&gt;&lt;p&gt;假设有3台主机，172.16.7.151主机上安装MySQL，172.16.7.152上安装httpd，172.16.7.153上安装MySQL和httpd。我们建立两个角色websrvs和dbsrvs，然后应用到这几个主机上。&lt;/p&gt;
&lt;h3 id=&quot;创建roles的必需目录&quot;&gt;&lt;a href=&quot;#创建roles的必需目录&quot; class=&quot;headerlink&quot; title=&quot;创建roles的必需目录&quot;&gt;&lt;/a&gt;创建roles的必需目录&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;[root@node1 opt]# mkdir -pv ansible_playbooks/roles/{websrvs,dbsrvs}/{tasks,files,templates,meta,handlers,vars}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/44.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/45.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;每个role下面有个目录叫meta，在里面可以新建文件main.yml，在文件中可以设置该role和其它role之前的关联关系。&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/46.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;配置角色&quot;&gt;&lt;a href=&quot;#配置角色&quot; class=&quot;headerlink&quot; title=&quot;配置角色&quot;&gt;&lt;/a&gt;配置角色&lt;/h3&gt;&lt;h4 id=&quot;配置角色websrvs&quot;&gt;&lt;a href=&quot;#配置角色websrvs&quot; class=&quot;headerlink&quot; title=&quot;配置角色websrvs&quot;&gt;&lt;/a&gt;配置角色websrvs&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@node1 opt]# cd ansible_playbooks/roles/
[root@node1 roles]# cd websrvs/
[root@node1 websrvs]# ls
files  handlers  meta  tasks  templates  vars
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;1.将httpd配置文件上传到files目录下，我这里假设httpd.conf每台主机都是一样的，实际上应该用模板，先用一样的配置文件举例&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 websrvs]# cp /etc/httpd/conf/httpd.conf files/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;直接复制的静态文件都放在files目录下。打算用模板文件的都放在templates目录下。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;2.编写任务列表tasks&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 websrvs]# vim tasks/main.yml
- name: install httpd package
  yum: name=httpd
- name: install configuration file
  copy: src=httpd.conf dest=/etc/httpd/conf
  tags:
  - conf
  notify:
  - restart httpd
- name: start httpd
  service: name=httpd state=started
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/47.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;3.由于上面的tasks中定义了notify，所以要定义handlers&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 websrvs]# vim handlers/main.yml
- name: restart httpd
  service: name=httpd state=restarted
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果需要定义变量，则在vars目录下创建main.yml文件，在文件中写入变量，以key:value的形式定义，比如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;http_port: 8080
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;配置角色dbsrvs&quot;&gt;&lt;a href=&quot;#配置角色dbsrvs&quot; class=&quot;headerlink&quot; title=&quot;配置角色dbsrvs&quot;&gt;&lt;/a&gt;配置角色dbsrvs&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;[root@node1 roles]# cd dbsrvs/
[root@node1 dbsrvs]# ls
files  handlers  meta  tasks  templates  vars
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;1.将MySQL配置文件上传到files目录下。&lt;br&gt;2.编写任务列表tasks&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 dbsrvs]# vim tasks/main.yml
- name: install mysql-server package
  yum: name=mysql-server state=latest
- name: install configuration file
  copy: src=my.cnf dest/etc/my.cnf
  tags:
  - conf
  notify:
  - restart mysqld
- name:
  service: name=mysqld enabled=true state=started
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3.定义handlers&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 dbsrvs]# vim handlers/main.yml
- name: restart mysqld
  service: name=mysqld state=restarted
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;定义playbook&quot;&gt;&lt;a href=&quot;#定义playbook&quot; class=&quot;headerlink&quot; title=&quot;定义playbook&quot;&gt;&lt;/a&gt;定义playbook&lt;/h4&gt;&lt;p&gt;【注意】：要在roles目录同级创建playbook。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ansible_playbooks]# vim web.yml
- hosts: 172.16.7.152
  roles:
  - websrvs 

[root@node1 ansible_playbooks]# vim db.yml
- hosts: 172.16.7.151
  roles:
  - dbsrvs 

[root@node1 ansible_playbooks]# vim site.yml
- hosts: 172.16.7.153
  roles:
  - websrvs
  - dbsrvs 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;运行：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ansible_playbooks]# ansible-playbook web.yml
[root@node1 ansible_playbooks]# ansible-playbook db.yml
[root@node1 ansible_playbooks]# ansible-playbook site.yml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;当然也可以把这些内容写入同一个playbook中。playbook的名字可以自定义。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;什么场景下会用roles？&quot;&gt;&lt;a href=&quot;#什么场景下会用roles？&quot; class=&quot;headerlink&quot; title=&quot;什么场景下会用roles？&quot;&gt;&lt;/a&gt;什么场景下会用roles？&lt;/h2&gt;&lt;p&gt;假如我们现在有3个被管理主机，第一个要配置成httpd，第二个要配置成php服务器，第三个要配置成MySQL服务器。我们如何来定义playbook？&lt;br&gt;第一个play用到第一个主机上，用来构建httpd，第二个play用到第二个主机上，用来构建php，第三个play用到第三个主机上，用来构建MySQL。这些个play定义在playbook中比较麻烦，将来也不利于模块化调用，不利于多次调。比如说后来又加进来一个主机，这个第4个主机既是httpd服务器，又是php服务器，我们只能写第4个play，上面写上安装httpd和php。这样playbook中的代码就重复了。&lt;br&gt;
    
    </summary>
    
      <category term="运维自动化" scheme="http://yoursite.com/categories/%E8%BF%90%E7%BB%B4%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    
    
      <category term="Ansible 运维自动化" scheme="http://yoursite.com/tags/Ansible-%E8%BF%90%E7%BB%B4%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Ansible之tags介绍</title>
    <link href="http://yoursite.com/2017/07/22/Ansible%E4%B9%8Btags%E4%BB%8B%E7%BB%8D/"/>
    <id>http://yoursite.com/2017/07/22/Ansible之tags介绍/</id>
    <published>2017-07-22T12:37:16.000Z</published>
    <updated>2017-07-24T01:01:28.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;tags介绍&quot;&gt;&lt;a href=&quot;#tags介绍&quot; class=&quot;headerlink&quot; title=&quot;tags介绍&quot;&gt;&lt;/a&gt;tags介绍&lt;/h2&gt;&lt;p&gt;我们每次改完配置文件，比如上一篇博客中的的apache.yml，没必要把整个playbook都运行一遍，只需要运行改变了的task。我们可以给task一个标签，运行playbook时明确只运行这个标签对应的task就可以了。多个任务可以使用同一个tags。&lt;br&gt;如果在某次运行中，我们多次运行同一个playbook，第一次运行时我们期望所有的tasks都运行，第二次运行时我们期望只运行某一个task，你可以给这个task定义一个tags，例如：&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# vim apache.yml 
- hosts: nginx
  remote_user: root
  vars:
  - package: apache
  tasks:
  - name: install httpd package
    yum: name={{ package }} state=latest
  - name: install configuration file for httpd
    template: src=/root/conf/httpd.conf.j2 dest=/etc/httpd/conf/httpd.conf
    tags:
    - conf                                                                    
    notify:
    - restart httpd
  - name: start httpd service 
    service: enabled=true name=httpd state=started
  handlers:
  - name: restart httpd
    service: name=httpd state=restarted
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/43.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;运行：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ansible-playbook apache.yml --tags=&amp;quot;conf&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果一个playbook中有多个tags，但是有个tag你不想跑，可以使用–skip-tags。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;tags介绍&quot;&gt;&lt;a href=&quot;#tags介绍&quot; class=&quot;headerlink&quot; title=&quot;tags介绍&quot;&gt;&lt;/a&gt;tags介绍&lt;/h2&gt;&lt;p&gt;我们每次改完配置文件，比如上一篇博客中的的apache.yml，没必要把整个playbook都运行一遍，只需要运行改变了的task。我们可以给task一个标签，运行playbook时明确只运行这个标签对应的task就可以了。多个任务可以使用同一个tags。&lt;br&gt;如果在某次运行中，我们多次运行同一个playbook，第一次运行时我们期望所有的tasks都运行，第二次运行时我们期望只运行某一个task，你可以给这个task定义一个tags，例如：&lt;br&gt;
    
    </summary>
    
      <category term="运维自动化" scheme="http://yoursite.com/categories/%E8%BF%90%E7%BB%B4%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    
    
      <category term="Ansible 运维自动化" scheme="http://yoursite.com/tags/Ansible-%E8%BF%90%E7%BB%B4%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Ansible之迭代、模板</title>
    <link href="http://yoursite.com/2017/07/20/Ansible%E4%B9%8B%E8%BF%AD%E4%BB%A3%E3%80%81%E6%A8%A1%E6%9D%BF/"/>
    <id>http://yoursite.com/2017/07/20/Ansible之迭代、模板/</id>
    <published>2017-07-20T07:05:29.000Z</published>
    <updated>2017-07-20T09:13:27.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;迭代&quot;&gt;&lt;a href=&quot;#迭代&quot; class=&quot;headerlink&quot; title=&quot;迭代&quot;&gt;&lt;/a&gt;迭代&lt;/h2&gt;&lt;p&gt;当有需要重复性执行的任务时，可以使用迭代机制。其使用格式为将需要迭代的内容定义为item变量引用，并通过with_items语句来指明迭代的元素列表即可。例如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- name: add several users
  user: name={{ item }} state=present groups=wheel
  with_items:
        - testuser1
        - testuser2
&lt;/code&gt;&lt;/pre&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;上面语句的功能等同于下面的语句：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- name: add user testuser1
  user: name=testuser1 state=present groups=wheel
- name: add user testuser2
  user: name=testuser2 state=present groups=wheel
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;另外，with_items中使用的元素还可以是hashes，例如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- name: add several users
  user: name={{ item.name }} state=present groups={{ item.groups }}
  with_items:
        - { name: &amp;apos;testuser1&amp;apos;, groups: &amp;apos;wheel&amp;apos;}
        - { name: &amp;apos;testuser2&amp;apos;, groups: &amp;apos;root&amp;apos;}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;【注意】：item是固定变量名。&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&quot;模板-JInjia2相关&quot;&gt;&lt;a href=&quot;#模板-JInjia2相关&quot; class=&quot;headerlink&quot; title=&quot;模板(JInjia2相关)&quot;&gt;&lt;/a&gt;模板(JInjia2相关)&lt;/h2&gt;&lt;p&gt;假如为两台webserver安装httpd，而他们的配置文件，172.16.7.152上的httpd需要监听80端口，172.16.7.153需要监听8080端口，ServerName也是不一样的，所以我们就需要两个配置文件，这管理起来极为不便。&lt;br&gt;在这种情况下，我们可以考虑在配置文件中使用变量来定义。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# mkdir templates
[root@node1 ~]# cp conf/httpd.conf templates/
[root@node1 ~]# mv templates/httpd.conf templates/httpd.conf.j2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;后缀为j2表明是Jinja2模板。编辑这个模板：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# vim templates/httpd.conf.j2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/39.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/40.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;strong&gt;这个模板复制到每台主机上时都应该将这文件里的变量换成对应的值。这个模板就是Jinjia2模板。&lt;/strong&gt;&lt;br&gt;设置每台主机使用的变量值：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# vim /etc/ansible/hosts
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/41.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;当然这http_port和maxClients也可以在playbook中定义。但是那样我们没法区别每台主机使用不同的值了。因此我们要想让每个主机变量名相同但值不同时只能使用主机变量来定义。下面定义playbook：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# vim apache.yml 
- hosts: nginx
  remote_user: root
  vars:
  - package: apache
  tasks:
  - name: install httpd package
    yum: name={{ package }} state=latest
  - name: install configuration file for httpd
    template: src=/root/conf/httpd.conf.j2 dest=/etc/httpd/conf/httpd.conf                                                                    
    notify:
    - restart httpd
  - name: start httpd service 
    service: enabled=true name=httpd state=started
  handlers:
  - name: restart httpd
    service: name=httpd state=restarted
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/42.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ansible-playbook apache.yml 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;执行完成后，去查看两个节点的配置文件，发生变量都被替换了。&lt;/p&gt;
&lt;h2 id=&quot;Jinja2相关&quot;&gt;&lt;a href=&quot;#Jinja2相关&quot; class=&quot;headerlink&quot; title=&quot;Jinja2相关&quot;&gt;&lt;/a&gt;Jinja2相关&lt;/h2&gt;&lt;h3 id=&quot;字面量&quot;&gt;&lt;a href=&quot;#字面量&quot; class=&quot;headerlink&quot; title=&quot;字面量&quot;&gt;&lt;/a&gt;字面量&lt;/h3&gt;&lt;p&gt;表达式最简单的形式就是字面量。字面量表示诸如字符串和数值的Python对象。下面的字面量是可用的：&lt;br&gt;1.字符串：“Hello World”&lt;br&gt;双引号或单引号中间的一切都是字符串，无论何时你需要在模板中使用一个字符串（比如函数引用、过滤器或只是包含或继承一个模板的参数），它们都是有用的。&lt;/p&gt;
&lt;p&gt;2.整数和浮点数：42 / 42.23&lt;br&gt;直接写下数值就可以创建整数和浮点数。如果有小数点，则为浮点数，否则为整数。在Python里，42和42.0是不一样的。&lt;/p&gt;
&lt;p&gt;3.列表：[‘list’, ‘of’, ‘object’]&lt;br&gt;一对中括号括起来的东西是一个列表。列表用于存储和迭代序列化的数据。例如你可以容易地在for循环中用列表和元组创建一个链接的列表：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;ul&amp;gt;
{% for href, caption in [(&#39;index.html&#39;, &#39;Index&#39;), (&#39;about.html&#39;, &#39;About&#39;), (&#39;download.html&#39;, &#39;Downloads&#39;)] %}
            &lt;li&gt;&lt;a href=&quot;{{ href }}&quot;&gt;{{ caption }}&lt;/a&gt;&lt;/li&gt;
        {% end for %}
/ul&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;4.元组：(‘tuple’, ‘of’, ‘values’)&lt;br&gt;元组和列表类似，只是不能修改里面的元素。如果元组中只有一项，你需要使用逗号结尾它。元组通常用于表示两个或更多元素的项。&lt;/p&gt;
&lt;p&gt;5.字典：{‘dict’: ‘of’, ‘key’: ‘and’, ‘value’: ‘pairs’}&lt;br&gt;Python中的字典是一种关联键和值的结构。键必须是唯一的，并且键必须只有一个值。&lt;/p&gt;
&lt;p&gt;6.Boolen：true / false&lt;/p&gt;
&lt;h3 id=&quot;算术运算&quot;&gt;&lt;a href=&quot;#算术运算&quot; class=&quot;headerlink&quot; title=&quot;算术运算&quot;&gt;&lt;/a&gt;算术运算&lt;/h3&gt;&lt;p&gt;Jinja2允许你用计算值。这在模板中很少使用，但为了完整性允许其存在，支持下面的运算符：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+ 把两个对象加到一起，通常对象是整数或浮点数，但是如果两者是字符串或列表，你可以用这种方式来连接它们。无论如何这不是首选的连接字符串的方式。{{ 1 + 1 }}等于2。

- 用第一个数减去第二个数，{{ 3 - 2 }}等于1.

/ 对两个数做除法，返回值会是一个浮点数。{{ 1 / 2 }}等于{{ 0.5 }}。

// 对两个手做除法，返回整数商，{{ 20 / 7 }}等于2。

% 计算整数除法的余数。{{  11 % 7 }}等于4。

* 用右边的数乘左边的操作数。{{ 2 * 2 }}会返回4，也可以用于重复一个字符串多次，{{ &#39;=&#39; * 80 }}会打印80个等号的横条。

** 取左操作数的右操作数次幂，{{ 2**3 }}会返回8。
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;比较操作符&quot;&gt;&lt;a href=&quot;#比较操作符&quot; class=&quot;headerlink&quot; title=&quot;比较操作符&quot;&gt;&lt;/a&gt;比较操作符&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;== 比较两个对象是否相等。
!= 比较两个对象是否不等。
&amp;gt; 如果左边大于右边，返回true。
&amp;lt; 如果左边小于右边，返回true。
&amp;gt;= 如果左边大于等于右边，返回true。
&amp;lt;= 如果左边小于等于右边，返回true。
&lt;/code&gt;&lt;/pre&gt;</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;迭代&quot;&gt;&lt;a href=&quot;#迭代&quot; class=&quot;headerlink&quot; title=&quot;迭代&quot;&gt;&lt;/a&gt;迭代&lt;/h2&gt;&lt;p&gt;当有需要重复性执行的任务时，可以使用迭代机制。其使用格式为将需要迭代的内容定义为item变量引用，并通过with_items语句来指明迭代的元素列表即可。例如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- name: add several users
  user: name={{ item }} state=present groups=wheel
  with_items:
        - testuser1
        - testuser2
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="运维自动化" scheme="http://yoursite.com/categories/%E8%BF%90%E7%BB%B4%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    
    
      <category term="Ansible 运维自动化" scheme="http://yoursite.com/tags/Ansible-%E8%BF%90%E7%BB%B4%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Ansible条件测试</title>
    <link href="http://yoursite.com/2017/07/17/Ansible%E6%9D%A1%E4%BB%B6%E6%B5%8B%E8%AF%95/"/>
    <id>http://yoursite.com/2017/07/17/Ansible条件测试/</id>
    <published>2017-07-17T05:39:32.000Z</published>
    <updated>2017-07-17T06:10:14.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;Ansible条件测试&quot;&gt;&lt;a href=&quot;#Ansible条件测试&quot; class=&quot;headerlink&quot; title=&quot;Ansible条件测试&quot;&gt;&lt;/a&gt;Ansible条件测试&lt;/h2&gt;&lt;p&gt;在ansible中还可以进行条件测试。如果需要根据变量、facts或此前任务的执行结果来做为某task执行与否的前提时要用到条件测试。&lt;/p&gt;
&lt;h3 id=&quot;when语句&quot;&gt;&lt;a href=&quot;#when语句&quot; class=&quot;headerlink&quot; title=&quot;when语句&quot;&gt;&lt;/a&gt;when语句&lt;/h3&gt;&lt;p&gt;在task后添加when子句即可使用条件测试：when语句支持Jinja2表达式语法。例如：&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tasks:
- name: &amp;quot;shutdown Debian flavored systems&amp;quot;
  command: /sbin/shutdown -h now
  when: ansible_os_family == &amp;quot;Debian&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;when语句还可以使用Jinja2的大多“filter”，例如要忽略此前某语句额错误并基于其结果（failed或success）运行后面指定的语句，可使用类似如下形式：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tasks:
- command: /bin/false
  register: result
  ignore_errors: True
- command: /bin/sonmething
  when: result|failed
- command: /bin/something_else
  when: result|success
- command: /bin/still/something_else
  when: result|skipped
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;此外，when语句还可以使用facts或playbook中定义的变量。facts就是主机报告上来的变量。比如：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/37.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Ansible条件测试&quot;&gt;&lt;a href=&quot;#Ansible条件测试&quot; class=&quot;headerlink&quot; title=&quot;Ansible条件测试&quot;&gt;&lt;/a&gt;Ansible条件测试&lt;/h2&gt;&lt;p&gt;在ansible中还可以进行条件测试。如果需要根据变量、facts或此前任务的执行结果来做为某task执行与否的前提时要用到条件测试。&lt;/p&gt;
&lt;h3 id=&quot;when语句&quot;&gt;&lt;a href=&quot;#when语句&quot; class=&quot;headerlink&quot; title=&quot;when语句&quot;&gt;&lt;/a&gt;when语句&lt;/h3&gt;&lt;p&gt;在task后添加when子句即可使用条件测试：when语句支持Jinja2表达式语法。例如：&lt;br&gt;
    
    </summary>
    
      <category term="运维自动化" scheme="http://yoursite.com/categories/%E8%BF%90%E7%BB%B4%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    
    
      <category term="Ansible 运维自动化" scheme="http://yoursite.com/tags/Ansible-%E8%BF%90%E7%BB%B4%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>ansible playbook基础组件介绍</title>
    <link href="http://yoursite.com/2017/07/15/ansible-playbook%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6%E4%BB%8B%E7%BB%8D/"/>
    <id>http://yoursite.com/2017/07/15/ansible-playbook基础组件介绍/</id>
    <published>2017-07-15T07:24:31.000Z</published>
    <updated>2017-07-20T08:18:24.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;ansible-playbook介绍&quot;&gt;&lt;a href=&quot;#ansible-playbook介绍&quot; class=&quot;headerlink&quot; title=&quot;ansible playbook介绍&quot;&gt;&lt;/a&gt;ansible playbook介绍&lt;/h2&gt;&lt;p&gt;playbook是由一个或多个“play”组成的列表(剧本是由多出戏组成的)。play的主要功能在于将事先归并为一组的主机装扮成事先通过ansible中的task定义好的角色。从根本上来讲，所谓task无非是调用ansible的一个module。将多个play组织在一个playbook中，即可以让它们联同起来按事先编排的机制同唱一台大戏。下面是一个简单示例。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- hosts: webnodes
  vars:
  http_port: 80
  max_clients: 256
remote_user: root
tasks:          
- name: ensure apache is at the latest version
  yum: name=httpd state=latest
- name: ensure apache is running
  service: name=httpd state=started
handlers:
- name: restart apache
  service: name=httpd state=restarted
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其中，tasks是一个一个任务。&lt;/p&gt;
&lt;h2 id=&quot;ansible-playbook基础组件&quot;&gt;&lt;a href=&quot;#ansible-playbook基础组件&quot; class=&quot;headerlink&quot; title=&quot;ansible playbook基础组件&quot;&gt;&lt;/a&gt;ansible playbook基础组件&lt;/h2&gt;&lt;p&gt;Playbooks结构：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tasks：任务，即调用模块完成的某操作。&lt;/li&gt;
&lt;li&gt;Variables：变量&lt;/li&gt;
&lt;li&gt;Templates：模板&lt;/li&gt;
&lt;li&gt;Handlers：处理器，指的是在某条件满足时能够触发完成的功能，或者说是由某事件触发执行的操作&lt;/li&gt;
&lt;li&gt;Roles：角色。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;Hosts和Users&quot;&gt;&lt;a href=&quot;#Hosts和Users&quot; class=&quot;headerlink&quot; title=&quot;Hosts和Users&quot;&gt;&lt;/a&gt;Hosts和Users&lt;/h3&gt;&lt;p&gt;playbook中的每一个play的目的都是为了让某个或某些主机以某个指定的用户身份执行任务。hosts用于指定要执行指定任务的主机，其可以是一个或多个由冒号分隔主机组；remote_user则用于指定远程主机上的执行任务的用户。如上面示例中的&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;-hosts: webnodes
 remote_user: root
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;不过，remote_user也可用于各task中。也可以通过指定其通过sudo的方式在远程主机上执行任务，其可用于play全局或某任务；此外，甚至可以在sudo时使用sudo_user指定sudo时切换的用户。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- hosts: webnodes
 remote_user: mageedu
 tasks:
   - name: test connection
     ping:
     remote_user: mageedu
     sudo: yes
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;比如写一个最简单的playbook，里面写了两个play，一个play是在nginx组的主机上都创建一个nginx组，nginx用户，另一个play是复制一个文件到mysql组的主机上：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# vim nginx.yml
- hosts: nginx                                                                                                                                
  remote_user: root
  tasks:
  - name: create nginx group
    group: name=nginx system=yes gid=208
  - name: create nginx user
    user: name=nginx uid=208 group=nginx system=yes
- hosts: mysql
  remote_user: root
  tasks:
  - name: copy file to mysql hosts
    copy: src=/etc/inittab dest=/tmp/inittab.ans
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查看ansible-playbook的使用方法：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# man ansible-playbook
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;运行playbook：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ansible-playbook nginx.yml 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/31.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;任务列表-Tasks-和action&quot;&gt;&lt;a href=&quot;#任务列表-Tasks-和action&quot; class=&quot;headerlink&quot; title=&quot;任务列表(Tasks)和action&quot;&gt;&lt;/a&gt;任务列表(Tasks)和action&lt;/h3&gt;&lt;p&gt;play的主体部分是task list。task list中的各任务按次序逐个在hosts中指定的所有主机上执行，即在所有主机上完成第一个任务后再开始第二个。在运行自下而下某playbook时，如果中途发生错误，所有已执行任务都可能回滚，因此，在更正playbook后重新执行一次即可。(因为具有幂等性)&lt;br&gt;task的目的是使用指定的参数执行模块，而在模块参数中可以使用变量。模块执行是幂等的，这意味着多次执行是安全的，因为其结果均一致。&lt;br&gt;每个task都应该有其name，用于playbook的执行结果输出，建议其内容尽可能清晰地描述任务执行步骤。如果未提供name，则action的结果将用于输出。&lt;br&gt;定义task的可以使用“action: module options”(这个在较新版本上才能执行)或“module: options”的格式，推荐使用后者以实现向后兼容。如果action一行的内容过多，也可以使用在行首使用几个空白字符进行换行。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tasks:
- name: make sure apache is running
  service: name=httpd state=running
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在众多模块中，只有command和shell模块仅需要给定一个列表而无需使用“key=value”格式，例如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tasks:
- name: disable selinux
  command: /sbin/setenforce 0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果命令或脚本的退出码不为零，可能会阻止playbook继续往下执行可以使用如下方式替代：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tasks:
- name: run this command and ignore the result
  shell: /usr/bin/somecommand || /bin/true
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;也就是说这个命令失败也是要继续往下走的，就是失败了但不要影响下面的操作。或者使用ignore_errors来忽略错误信息：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tasks:
- name: run this command and ignore the result
  shell: /usr/bin/somecommand
  ignore_errors: True        
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;handlers&quot;&gt;&lt;a href=&quot;#handlers&quot; class=&quot;headerlink&quot; title=&quot;handlers&quot;&gt;&lt;/a&gt;handlers&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;用于当关注的资源发生变化时采取一定的操作。&lt;/strong&gt;&lt;br&gt;“notify”这个action可用于在每个play的最后被触发，这样可以避免多次有改变发生时每次都执行指定的操作，取而代之，仅在所有的变化发生完成后一次性地执行指定操作。在notify中列出的操作称为handler，也即notify中调用handler中定义的操作。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- name: template configuration file
  template: src=template.j2 dest=/etc/foo.conf
  notify:
  - restart memcached
  - restart apache    
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;handler是task列表，这些task与前述的task并没有本质上的不同。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;handlers:
- name: restart memcached
  service:  name=memcached state=restarted
- name: restart apache
  service: name=apache state=restarted
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;【举例】：比如有个配置nginx的playbook，然后利用这个来说明handlers。&lt;br&gt;1.先创建一个apache.yml，里面定义play安装启动apache&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# vim apache.yml
- hosts: mysql
  remote_user: root
  tasks:
  - name: install httpd package
    yum: name=httpd state=latest
  - name: install configuration file for httpd
    copy: src=/root/conf/httpd.conf dest=/etc/httpd/conf/httpd.conf 
  - name: start httpd service 
    service: enabled=true name=httpd state=started
[root@node1 ~]# ansible-playbook apache.yml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.假如说某个时刻httpd.conf发生改变了，比如说不再监听在80，而是监听在8080端口，其他没变。修改/root/conf/httpd.conf，把端口改成8080，再执行这个playbook：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ansible-playbook apache.yml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;到mysql组所在的主机172.16.7.153上查看端口，发现监听端口仍然是80：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node3 ~]# ss -tnlp
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/32.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;3.一个程序的配置文件发生了改变，那么程序应该重读配置文件才对。然而默认情况下，你多次唱同一个剧本，如果那个task此前执行过，为了保证幂等性，它是不会再被执行。handlers就是为了解决这种问题而生的。Handlers也是任务，但它不是上来就执行的，只有某个条件满足时才会执行。所以我们去修改apache.yml：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- hosts: mysql
  remote_user: root
  tasks:
  - name: install httpd package
    yum: name=httpd state=latest
  - name: install configuration file for httpd
    copy: src=/root/conf/httpd.conf dest=/etc/httpd/conf/httpd.conf 
    notify:
    - restart httpd
  - name: start httpd service 
    service: enabled=true name=httpd state=started
  handlers:
  - name: restart httpd
    service: name=httpd state=restarted
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/33.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;修改/root/conf/httpd.conf，把端口改成8090，再执行这个playbook：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ansible-playbook apache.yml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/34.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;到mysql组所在的主机172.16.7.153上查看端口，发现监听端口改变了，变成了8090：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/35.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;playbook中使用变量&quot;&gt;&lt;a href=&quot;#playbook中使用变量&quot; class=&quot;headerlink&quot; title=&quot;playbook中使用变量&quot;&gt;&lt;/a&gt;playbook中使用变量&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;- hosts: mysql
  remote_user: root
  vars:
  - package: httpd
  tasks:
  - name: install httpd package
    yum: name={{ package }} state=latest                                                                                                      
  - name: install configuration file for httpd
    copy: src=/root/conf/httpd.conf dest=/etc/httpd/conf/httpd.conf 
    notify:
    - restart httpd
  - name: start httpd service 
    service: enabled=true name=httpd state=started
  handlers:
  - name: restart httpd
    service: name=httpd state=restarted
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/36.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;strong&gt;【注意】：playbook中能使用的变量不仅仅是这里定义的变量，而是可以使用ansible中定义的所有变量。例如：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ansible 172.16.7.152 -m setup
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/37.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;另外，在inventory中定义的变量也可以在playbook中调用。例如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# vim /etc/ansible/hosts
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/38.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;ansible-playbook介绍&quot;&gt;&lt;a href=&quot;#ansible-playbook介绍&quot; class=&quot;headerlink&quot; title=&quot;ansible playbook介绍&quot;&gt;&lt;/a&gt;ansible playbook介绍&lt;/h2&gt;&lt;p&gt;playbook是由一个或多个“play”组成的列表(剧本是由多出戏组成的)。play的主要功能在于将事先归并为一组的主机装扮成事先通过ansible中的task定义好的角色。从根本上来讲，所谓task无非是调用ansible的一个module。将多个play组织在一个playbook中，即可以让它们联同起来按事先编排的机制同唱一台大戏。下面是一个简单示例。&lt;br&gt;
    
    </summary>
    
      <category term="运维自动化" scheme="http://yoursite.com/categories/%E8%BF%90%E7%BB%B4%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    
    
      <category term="Ansible 运维自动化" scheme="http://yoursite.com/tags/Ansible-%E8%BF%90%E7%BB%B4%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Ansible的基础元素和YAML介绍</title>
    <link href="http://yoursite.com/2017/07/11/Ansible%E7%9A%84%E5%9F%BA%E7%A1%80%E5%85%83%E7%B4%A0%E5%92%8CYAML%E4%BB%8B%E7%BB%8D/"/>
    <id>http://yoursite.com/2017/07/11/Ansible的基础元素和YAML介绍/</id>
    <published>2017-07-11T10:37:28.000Z</published>
    <updated>2017-07-14T08:03:58.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;YAML&quot;&gt;&lt;a href=&quot;#YAML&quot; class=&quot;headerlink&quot; title=&quot;YAML&quot;&gt;&lt;/a&gt;YAML&lt;/h2&gt;&lt;h3 id=&quot;YAML介绍&quot;&gt;&lt;a href=&quot;#YAML介绍&quot; class=&quot;headerlink&quot; title=&quot;YAML介绍&quot;&gt;&lt;/a&gt;YAML介绍&lt;/h3&gt;&lt;p&gt;YAML是一个可读性高的用来表达资料序列的格式。YAML参考了其他多种语言，包括：XML、C语言、Python、Perl以及电子邮件格式RFC2822等。Clark Evans在2001年在首次发表了这种语言，另外Ingy döt Net与Oren Ben-Kiki也是这语言的共同设计者。&lt;br&gt;YAML Ain’t Markup Language，即YAML不是XML。不过，在开发的这种语言时，YAML的意思其实是：”Yet Another Markup Language”（仍是一种标记语言）。其特性：&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;YAML的可读性好&lt;/li&gt;
&lt;li&gt;YAML和脚本语言的交互性好&lt;/li&gt;
&lt;li&gt;YAML使用实现语言的数据类型&lt;/li&gt;
&lt;li&gt;YAML有一个一致的信息模型&lt;/li&gt;
&lt;li&gt;YAML易于实现&lt;/li&gt;
&lt;li&gt;YAML可以基于流来处理&lt;/li&gt;
&lt;li&gt;YAML表达能力强，扩展性好&lt;br&gt;更多的内容及规范参见&lt;a href=&quot;http://www.yaml.org。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://www.yaml.org。&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;YAML语法&quot;&gt;&lt;a href=&quot;#YAML语法&quot; class=&quot;headerlink&quot; title=&quot;YAML语法&quot;&gt;&lt;/a&gt;YAML语法&lt;/h3&gt;&lt;p&gt;YAML的语法和其他高阶语言类似，并且可以简单表达清单、散列表、标量等数据结构。其结构（Structure）通过空格来展示，序列（Sequence）里的项用”-“来代表，Map里的键值对用”:”分隔。下面是一个示例。YAML是用键值对和缩进来表示的。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;name: John Smith
age: 41
gender: Male
spouse:                         
    name: Jane Smith
    age: 37
    gender: Female
children:                     
    - name: Jimmy Smith
      age: 17
      gender: Male
    - name: Jenny Smith
      age 13
      gender: Female
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;YAML文件扩展名通常为.yaml，如example.yaml。&lt;/p&gt;
&lt;h3 id=&quot;Ansible常用的数据类型&quot;&gt;&lt;a href=&quot;#Ansible常用的数据类型&quot; class=&quot;headerlink&quot; title=&quot;Ansible常用的数据类型&quot;&gt;&lt;/a&gt;Ansible常用的数据类型&lt;/h3&gt;&lt;p&gt;在ansible中常用的数据类型有序列(list)，也叫列表，还有字典，这些都很类似Python语言。&lt;/p&gt;
&lt;h4 id=&quot;list&quot;&gt;&lt;a href=&quot;#list&quot; class=&quot;headerlink&quot; title=&quot;list&quot;&gt;&lt;/a&gt;list&lt;/h4&gt;&lt;p&gt;列表中的所有元素都使用“-”打头，例如：A list of tasty fruits&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Apple&lt;/li&gt;
&lt;li&gt;Orange&lt;/li&gt;
&lt;li&gt;Strawberry&lt;/li&gt;
&lt;li&gt;Mango&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;dictionary&quot;&gt;&lt;a href=&quot;#dictionary&quot; class=&quot;headerlink&quot; title=&quot;dictionary&quot;&gt;&lt;/a&gt;dictionary&lt;/h4&gt;&lt;h2 id=&quot;字典通过key与value进行标识，例如：&quot;&gt;&lt;a href=&quot;#字典通过key与value进行标识，例如：&quot; class=&quot;headerlink&quot; title=&quot;字典通过key与value进行标识，例如：&quot;&gt;&lt;/a&gt;字典通过key与value进行标识，例如：&lt;/h2&gt;&lt;p&gt;An employee record：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;name: Example Developer
job: Developer
skill: Elite
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;也可以将key-value放置于-中进行表示，例如：&quot;&gt;&lt;a href=&quot;#也可以将key-value放置于-中进行表示，例如：&quot; class=&quot;headerlink&quot; title=&quot;也可以将key:value放置于{}中进行表示，例如：&quot;&gt;&lt;/a&gt;也可以将key:value放置于{}中进行表示，例如：&lt;/h2&gt;&lt;p&gt;An employ record：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{name: Example Developer, job: Developer, skill: Elite}
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;Ansible基础元素&quot;&gt;&lt;a href=&quot;#Ansible基础元素&quot; class=&quot;headerlink&quot; title=&quot;Ansible基础元素&quot;&gt;&lt;/a&gt;Ansible基础元素&lt;/h2&gt;&lt;h3 id=&quot;变量&quot;&gt;&lt;a href=&quot;#变量&quot; class=&quot;headerlink&quot; title=&quot;变量&quot;&gt;&lt;/a&gt;变量&lt;/h3&gt;&lt;h4 id=&quot;变量命名&quot;&gt;&lt;a href=&quot;#变量命名&quot; class=&quot;headerlink&quot; title=&quot;变量命名&quot;&gt;&lt;/a&gt;变量命名&lt;/h4&gt;&lt;p&gt;变量名仅能由字母、数字和下划线组成，而且只能以字母开头。&lt;/p&gt;
&lt;h4 id=&quot;facts&quot;&gt;&lt;a href=&quot;#facts&quot; class=&quot;headerlink&quot; title=&quot;facts&quot;&gt;&lt;/a&gt;facts&lt;/h4&gt;&lt;p&gt;facts是由正在通信的远程目标主机发回的信息，这些信息被保存在ansible变量中。要获取指定的远程主机所支持的所有facts，可使用如下命令进行：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# ansible hostname -m setup
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;register&quot;&gt;&lt;a href=&quot;#register&quot; class=&quot;headerlink&quot; title=&quot;register&quot;&gt;&lt;/a&gt;register&lt;/h4&gt;&lt;p&gt;把任务的输出定义为变量，然后用于其他任务，示例如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tasks:
  - shell: /usr/bin/foo
    register: foo_result
    ignore_errors: True
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;通过命令行传递变量&quot;&gt;&lt;a href=&quot;#通过命令行传递变量&quot; class=&quot;headerlink&quot; title=&quot;通过命令行传递变量&quot;&gt;&lt;/a&gt;通过命令行传递变量&lt;/h4&gt;&lt;p&gt;在运行playbook的时候也可以传递一些变量供playbook使用，示例如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ansible-playbook test.yml --extra-vars &amp;quot;hosts=www user=magedu&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;就是说hosts和user这两个变量可以在test.yml文件中直接调用&lt;/p&gt;
&lt;h4 id=&quot;通过roles传递变量&quot;&gt;&lt;a href=&quot;#通过roles传递变量&quot; class=&quot;headerlink&quot; title=&quot;通过roles传递变量&quot;&gt;&lt;/a&gt;通过roles传递变量&lt;/h4&gt;&lt;p&gt;当给一个主机应用角色的时候可以传递变量，然后在角色内使用这些变量，示例如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- hosts: webservers
  roles: 
      - common
      - { role: foo_app_instance, dir: &amp;apos;/web/htdocs/a.com&amp;apos;, port: 8080 }
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;注意:role、dir、port是变量名，冒号后面的是变量值。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;Inventory&quot;&gt;&lt;a href=&quot;#Inventory&quot; class=&quot;headerlink&quot; title=&quot;Inventory&quot;&gt;&lt;/a&gt;Inventory&lt;/h3&gt;&lt;p&gt;ansible的主要功能在于批量主机操作，为了便捷地使用其中的部分主机，可以在inventory file中将其分组命名。默认的inventory file为/etc/ansible/hosts。&lt;br&gt;inventory file可以有多个，且也可以通过Dynamic Inventory来动态生成。&lt;/p&gt;
&lt;h4 id=&quot;inventory文件格式&quot;&gt;&lt;a href=&quot;#inventory文件格式&quot; class=&quot;headerlink&quot; title=&quot;inventory文件格式&quot;&gt;&lt;/a&gt;inventory文件格式&lt;/h4&gt;&lt;p&gt;inventory文件遵循INI文件风格，中括号中的字符为组名。可以将同一个主机同时归并到多个不同的组中；此外，当如若目标主机使用了非默认的SSH端口，还可以在主机名称之后使用冒号加端口号标明。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[webservers]
www1.wisedu.com:8888
www2.wisedu.com
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果主机名称遵循相似的命名模式，还可以使用列表的方式标识各主机，例如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[webservers]
www[01:50].example.com

[databases]
db-[a:f].example.com
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;主机变量&quot;&gt;&lt;a href=&quot;#主机变量&quot; class=&quot;headerlink&quot; title=&quot;主机变量&quot;&gt;&lt;/a&gt;主机变量&lt;/h4&gt;&lt;p&gt;可以在inventory中定义主机时为其添加主机变量以便于在playbook中使用，例如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[webservers]
www1.wisedu.com http_port=80 maxRequestsPerChild=808
www2.wisedu.com http_port=8080 maxRequestsPerChild=909
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;组变量&quot;&gt;&lt;a href=&quot;#组变量&quot; class=&quot;headerlink&quot; title=&quot;组变量&quot;&gt;&lt;/a&gt;组变量&lt;/h4&gt;&lt;p&gt;组变量是指赋予给指定组内所有主机上的在playbook中可用的变量。例如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; [webservers]
 www1.wisedu.com
 www2.wisedu.com

[webservers:vars]   # 表示向webservers这组主机定义变量如下，回头这两台主机上都可以调用变量ntp_server和nfs_server
ntp_server=ntp.wisedu.com
nfs_server=ntp.wisedu.com
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;组嵌套&quot;&gt;&lt;a href=&quot;#组嵌套&quot; class=&quot;headerlink&quot; title=&quot;组嵌套&quot;&gt;&lt;/a&gt;组嵌套&lt;/h4&gt;&lt;p&gt;inventory中，组还可以包含其它的组，并且也可以向组中的主机指定变量。不过，这些变量只能在ansible-playbook中使用，而ansible不支持。例如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[apache]
httpd1.wisedu.com
httpd2.wisedu.com

[nginx]
ngx1.wisedu.com
ngx2.wisedu.com

[webservers:children]     # 注意:children是固定格式
apache
nginx

[webservers:vars]
ntp_server=ntp.wisedu.com
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;inventory参数&quot;&gt;&lt;a href=&quot;#inventory参数&quot; class=&quot;headerlink&quot; title=&quot;inventory参数&quot;&gt;&lt;/a&gt;inventory参数&lt;/h4&gt;&lt;p&gt;ansible基于ssh连接inventory中指定的远程主机时，还可以通过参数指定其交互方式，这些参数如下所示：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/29.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/30.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;YAML&quot;&gt;&lt;a href=&quot;#YAML&quot; class=&quot;headerlink&quot; title=&quot;YAML&quot;&gt;&lt;/a&gt;YAML&lt;/h2&gt;&lt;h3 id=&quot;YAML介绍&quot;&gt;&lt;a href=&quot;#YAML介绍&quot; class=&quot;headerlink&quot; title=&quot;YAML介绍&quot;&gt;&lt;/a&gt;YAML介绍&lt;/h3&gt;&lt;p&gt;YAML是一个可读性高的用来表达资料序列的格式。YAML参考了其他多种语言，包括：XML、C语言、Python、Perl以及电子邮件格式RFC2822等。Clark Evans在2001年在首次发表了这种语言，另外Ingy döt Net与Oren Ben-Kiki也是这语言的共同设计者。&lt;br&gt;YAML Ain’t Markup Language，即YAML不是XML。不过，在开发的这种语言时，YAML的意思其实是：”Yet Another Markup Language”（仍是一种标记语言）。其特性：&lt;br&gt;
    
    </summary>
    
      <category term="运维自动化" scheme="http://yoursite.com/categories/%E8%BF%90%E7%BB%B4%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    
    
      <category term="Ansible 运维自动化" scheme="http://yoursite.com/tags/Ansible-%E8%BF%90%E7%BB%B4%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Ansible常见模块介绍</title>
    <link href="http://yoursite.com/2017/07/07/Ansible%E5%B8%B8%E8%A7%81%E6%A8%A1%E5%9D%97%E4%BB%8B%E7%BB%8D/"/>
    <id>http://yoursite.com/2017/07/07/Ansible常见模块介绍/</id>
    <published>2017-07-07T00:48:26.000Z</published>
    <updated>2017-07-07T03:06:49.000Z</updated>
    
    <content type="html">&lt;h2 id=&quot;ansible命令基础&quot;&gt;&lt;a href=&quot;#ansible命令基础&quot; class=&quot;headerlink&quot; title=&quot;ansible命令基础&quot;&gt;&lt;/a&gt;ansible命令基础&lt;/h2&gt;&lt;p&gt;语法：ansible &lt;host-pattern&gt; [-f forks] [-m module_name] [-a args] [options] &lt;/host-pattern&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;host-pattern：这次的命令对哪些主机生效；&lt;/li&gt;
&lt;li&gt;-f forks：启动的并发线程数，就是一次并行处理多少主机；&lt;/li&gt;
&lt;li&gt;-m module_name：要使用的模块；&lt;/li&gt;
&lt;li&gt;-a args：模块特有的参数。&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常见的模块：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;user&lt;/li&gt;
&lt;li&gt;yum&lt;/li&gt;
&lt;li&gt;copy&lt;/li&gt;
&lt;li&gt;cron&lt;/li&gt;
&lt;li&gt;command：这是默认的模块，表示在被管理主机上运行一个命令。对于command模块，-a不再是指定参数，而是命令本身。&lt;/li&gt;
&lt;li&gt;shell&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;常见模块举例&quot;&gt;&lt;a href=&quot;#常见模块举例&quot; class=&quot;headerlink&quot; title=&quot;常见模块举例&quot;&gt;&lt;/a&gt;常见模块举例&lt;/h2&gt;&lt;h3 id=&quot;etc-ansible-hosts配置文件内容&quot;&gt;&lt;a href=&quot;#etc-ansible-hosts配置文件内容&quot; class=&quot;headerlink&quot; title=&quot;/etc/ansible/hosts配置文件内容&quot;&gt;&lt;/a&gt;/etc/ansible/hosts配置文件内容&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/7.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;command模块&quot;&gt;&lt;a href=&quot;#command模块&quot; class=&quot;headerlink&quot; title=&quot;command模块&quot;&gt;&lt;/a&gt;command模块&lt;/h3&gt;&lt;p&gt;command模块是默认的模块，表示在被管理主机上运行一个命令。对于command模块，-a不再是指定参数，而是命令本身。所以这个模块有个缺陷，运行的命令中不能使用变量或者参数。&lt;br&gt;&lt;strong&gt;示例：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ansible nginx -m command -a &amp;quot;date&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/8.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ansible mysql -m command -a &amp;quot;tail -3 /etc/passwd&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/9.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;cron模块&quot;&gt;&lt;a href=&quot;#cron模块&quot; class=&quot;headerlink&quot; title=&quot;cron模块&quot;&gt;&lt;/a&gt;cron模块&lt;/h3&gt;&lt;p&gt;cron模块可以让每一个被管理节点能够自动生成一个定期任务计划。查看cron模块的用法：&lt;br&gt;[root@node1 ~]# ansible-doc -s cron&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/10.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;几个主要参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;state：present表示安装crontab任务&lt;pre&gt;&lt;code&gt;absent表示移除crontab任务
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;job：指明运行的命令是什么&lt;/li&gt;
&lt;li&gt;&lt;p&gt;name：crontab任务的名字&lt;br&gt;&lt;strong&gt;示例：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ansible mysql -m cron -a &amp;apos;minute=&amp;quot;*/10&amp;quot; job=&amp;quot;/usr/bin/echo hello&amp;quot; name=&amp;quot;test cron job&amp;quot;&amp;apos;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/11.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;注意：&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;所有的参数可以用””包含起来&lt;/li&gt;
&lt;li&gt;day之类的参数没有指定，默认都是*&lt;/li&gt;
&lt;li&gt;默认state参数的值为present&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/12.png&quot; alt=&quot;&quot;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;user模块&quot;&gt;&lt;a href=&quot;#user模块&quot; class=&quot;headerlink&quot; title=&quot;user模块&quot;&gt;&lt;/a&gt;user模块&lt;/h3&gt;&lt;p&gt;user模块实现用户账号管理。查看user模块的用法：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ansible-doc -s user
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;几个主要参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;name=：用户名&lt;/li&gt;
&lt;li&gt;uid：用户的uid&lt;/li&gt;
&lt;li&gt;group：所属组，即私有组&lt;/li&gt;
&lt;li&gt;groups：附加组。&lt;/li&gt;
&lt;li&gt;&lt;p&gt;state：状态。&lt;br&gt;&lt;strong&gt;示例：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ansible mysql -m user -a &amp;apos;name=&amp;quot;jack&amp;quot;&amp;apos;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/13.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ansible mysql -m user -a &amp;apos;name=&amp;quot;jack&amp;quot; state=absent&amp;apos;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/14.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;group模块&quot;&gt;&lt;a href=&quot;#group模块&quot; class=&quot;headerlink&quot; title=&quot;group模块&quot;&gt;&lt;/a&gt;group模块&lt;/h3&gt;&lt;p&gt;group模块：组管理。查看group模块的用法：&lt;br&gt;&lt;strong&gt;示例：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ansible-doc -s group
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/15.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ansible mysql -m group -a &amp;apos;name=mysql gid=306 system=yes&amp;apos;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/16.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;copy模块&quot;&gt;&lt;a href=&quot;#copy模块&quot; class=&quot;headerlink&quot; title=&quot;copy模块&quot;&gt;&lt;/a&gt;copy模块&lt;/h3&gt;&lt;p&gt;copy模块实现文件复制。查看copy模块的用法：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ansible-doc -s copy
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;几个主要参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;src=：指明源文件本地路径。可以是绝对路径，也可以是相对路径。可以不使用src，使用content。就是说用content内容来生成文件。&lt;/li&gt;
&lt;li&gt;dest=：定义远程目标文件路径，只能使用绝对路径。&lt;/li&gt;
&lt;li&gt;&lt;p&gt;content=：可以不使用src，使用content。就是说用content内容来生成文件。&lt;br&gt;&lt;strong&gt;示例：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ansible mysql -m copy -a &amp;apos;src=/etc/fstab dest=/tmp/fstab.ansible owner=root mode=640&amp;apos;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/17.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ansible mysql -m copy -a &amp;apos;content=&amp;quot;Hello World\nGood boy&amp;quot; dest=/tmp/test.txt owner=root mode=640&amp;apos;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/18.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;file模块&quot;&gt;&lt;a href=&quot;#file模块&quot; class=&quot;headerlink&quot; title=&quot;file模块&quot;&gt;&lt;/a&gt;file模块&lt;/h3&gt;&lt;p&gt;file模块可以设定文件属性，还可以创建文件的符号链接。查看file模块的用法：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ansible-doc -s file
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;几个重要参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;path=：指明对哪个文件做管理。也可以使用dest和name。&lt;/li&gt;
&lt;li&gt;&lt;p&gt;创建文件的符号链接：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;src=：指明源文件&lt;/li&gt;
&lt;li&gt;&lt;p&gt;path=：指明符号链接文件路径&lt;br&gt;&lt;strong&gt;示例：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;  [root@node1 ~]# ansible mysql -m file -a ‘owner=root group=mysql mode=644 path=/tmp/test.txt’&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/19.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;  [root@node1 ~]# ansible mysql -m file -a ‘path=/tmp/test.link src=/tmp/test.txt state=link’&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/20.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;ping模块&quot;&gt;&lt;a href=&quot;#ping模块&quot; class=&quot;headerlink&quot; title=&quot;ping模块&quot;&gt;&lt;/a&gt;ping模块&lt;/h3&gt;&lt;p&gt;ping模块测试指定主机是否能连接。查看ping模块的用法：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ansible-doc -s ping
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;示例：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ansible nginx -m ping
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/21.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;service模块&quot;&gt;&lt;a href=&quot;#service模块&quot; class=&quot;headerlink&quot; title=&quot;service模块&quot;&gt;&lt;/a&gt;service模块&lt;/h3&gt;&lt;p&gt;service模块是管理服务的。查看service模块的用法：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ansible-doc -s service
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/22.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;几个重要参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;enabled=：是否开机自动启动，取值为true或false；&lt;/li&gt;
&lt;li&gt;name=：服务名字；&lt;/li&gt;
&lt;li&gt;&lt;p&gt;state=：状态，取值有started，stoped    ，restarted。&lt;br&gt;&lt;strong&gt;示例：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ansible 172.16.7.151 -m service -a &amp;apos;enabled=true name=httpd state=stopped&amp;apos;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/23.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;shell模块&quot;&gt;&lt;a href=&quot;#shell模块&quot; class=&quot;headerlink&quot; title=&quot;shell模块&quot;&gt;&lt;/a&gt;shell模块&lt;/h3&gt;&lt;p&gt;shell模块：和command模块类似，但是可以使用变量。用于执行一些复杂的命令。查看shell模块的使用方法：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ansible-doc -s shell
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;示例：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ansible mysql -m user -a &amp;apos;name=&amp;quot;test&amp;quot;&amp;apos;
[root@node1 ~]# ansible mysql -m command -a &amp;apos;echo wisedu | passwd --stdin test&amp;apos;
[root@node1 ~]# ansible mysql -m command -a &amp;apos;tail -1 /etc/passwd&amp;apos; 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/24.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ansible mysql -m shell -a &amp;apos;echo wisedu | passwd --stdin user1&amp;apos;
[root@node1 ~]# ansible mysql -m command -a &amp;apos;tail -1 /etc/shadow&amp;apos;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/25.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;strong&gt;所以一旦有管道、变量之类的，你最好使用shell模块，而不要用command模块。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;script模块&quot;&gt;&lt;a href=&quot;#script模块&quot; class=&quot;headerlink&quot; title=&quot;script模块&quot;&gt;&lt;/a&gt;script模块&lt;/h3&gt;&lt;p&gt;script模块将本地脚本复制到远程主机并运行之。查看script模块的用法：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ansible-doc -s script
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;示例：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# vim test.sh
#/bin/bash                                                                                                                                    
echo &amp;quot;hello world&amp;quot; &amp;gt;/tmp/nba.txt
 [root@node1 ~]# chmod +x test.sh 
 [root@node1 ~]# ansible mysql -m script -a &amp;apos;/root/test.sh&amp;apos;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/26.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;yum模块&quot;&gt;&lt;a href=&quot;#yum模块&quot; class=&quot;headerlink&quot; title=&quot;yum模块&quot;&gt;&lt;/a&gt;yum模块&lt;/h3&gt;&lt;p&gt;yum模块管理程序包。查看yum模块的用法：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ansible-doc -s yum        
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;几个重要参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;name=：指定要安装的程序包，可以带上版本号，否则安装最新版本；&lt;/li&gt;
&lt;li&gt;&lt;p&gt;state=：present表示安装，absent表示卸载。&lt;br&gt;&lt;strong&gt;示例：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ansible mysql -m yum -a &amp;apos;name=ksh&amp;apos;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/27.png&quot; alt=&quot;&quot;&gt; &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;setup模块&quot;&gt;&lt;a href=&quot;#setup模块&quot; class=&quot;headerlink&quot; title=&quot;setup模块&quot;&gt;&lt;/a&gt;setup模块&lt;/h3&gt;&lt;p&gt;setup模块：收集远程主机的facts。ansbile在管理每一个主机时，这些主机在被运行管理命令之前，会首先向ansible节点报告自己主机当前的各种可能被ansible主机用到的状态信息，如操作系统版本、ip地址等信息，这些信息都是以变量的形式，ansible主机可以在jinjia2中调用，为不同的服务器生成不同的配置文件。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ansible mysql -m setup
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Ansible/28.png&quot; alt=&quot;&quot;&gt;  &lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;ansible命令基础&quot;&gt;&lt;a href=&quot;#ansible命令基础&quot; class=&quot;headerlink&quot; title=&quot;ansible命令基础&quot;&gt;&lt;/a&gt;ansible命令基础&lt;/h2&gt;&lt;p&gt;语法：ansible &lt;host-pattern&gt; [-f forks] [-m module_name] [-a args] [options] &lt;/host-pattern&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;host-pattern：这次的命令对哪些主机生效；&lt;/li&gt;
&lt;li&gt;-f forks：启动的并发线程数，就是一次并行处理多少主机；&lt;/li&gt;
&lt;li&gt;-m module_name：要使用的模块；&lt;/li&gt;
&lt;li&gt;-a args：模块特有的参数。
    
    </summary>
    
      <category term="运维自动化" scheme="http://yoursite.com/categories/%E8%BF%90%E7%BB%B4%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    
    
      <category term="Ansible 运维自动化" scheme="http://yoursite.com/tags/Ansible-%E8%BF%90%E7%BB%B4%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    
  </entry>
  
</feed>
