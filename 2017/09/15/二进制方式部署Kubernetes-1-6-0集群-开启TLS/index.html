<!doctype html>



  


<html class="theme-next pisces use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Lobster Two:300,300italic,400,400italic,700,700italic|consolas:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Kubernetes," />





  <link rel="alternate" href="/atom.xml" title="jkzhao's blog" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico?v=5.0.1" />






<meta name="description" content="Kubernetes简介Kubernetes是谷歌开源的容器集群管理系统，是Google多年大规模容器管理技术Borg的 开源版本，主要功能包括:  基于容器的应用部署、维护和滚动升级 负载均衡和服务发现 跨机器和跨地区的集群调度">
<meta name="keywords" content="Kubernetes">
<meta property="og:type" content="article">
<meta property="og:title" content="二进制方式部署Kubernetes 1.6.0集群(开启TLS)">
<meta property="og:url" content="http://yoursite.com/2017/09/15/二进制方式部署Kubernetes-1-6-0集群-开启TLS/index.html">
<meta property="og:site_name" content="jkzhao&#39;s blog">
<meta property="og:description" content="Kubernetes简介Kubernetes是谷歌开源的容器集群管理系统，是Google多年大规模容器管理技术Borg的 开源版本，主要功能包括:  基于容器的应用部署、维护和滚动升级 负载均衡和服务发现 跨机器和跨地区的集群调度">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/1.png">
<meta property="og:image" content="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/2.png">
<meta property="og:updated_time" content="2017-11-19T06:01:35.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="二进制方式部署Kubernetes 1.6.0集群(开启TLS)">
<meta name="twitter:description" content="Kubernetes简介Kubernetes是谷歌开源的容器集群管理系统，是Google多年大规模容器管理技术Borg的 开源版本，主要功能包括:  基于容器的应用部署、维护和滚动升级 负载均衡和服务发现 跨机器和跨地区的集群调度">
<meta name="twitter:image" content="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/1.png">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 6287775856811050000,
      author: 'Author'
    }
  };
</script>

  <title> 二进制方式部署Kubernetes 1.6.0集群(开启TLS) | jkzhao's blog </title>
</head>
<a href="https://github.com/you"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"></a>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  



  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?c179eb46ac47d3b4b1b9203b82ee5821";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>








  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <a href="https://github.com/jkzhao"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"></a>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">jkzhao's blog</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">学习 总结 思考</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-guestbook">
          <a href="/guestbook" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            留言
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="#" class="st-search-show-outputs">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <form class="site-search-form">
  <input type="text" id="st-search-input" class="st-search-input st-default-search-input" />
</form>

<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
    (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
    e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

  _st('install', 'CziK4aDdRyzFJrfygnHH','2.0.0');
</script>



    </div>
  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                二进制方式部署Kubernetes 1.6.0集群(开启TLS)
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-09-15T14:06:09+08:00" content="2017-09-15">
              2017-09-15
            </time>
            
              <span class="post-updated">
              &nbsp; | &nbsp; 更新于
              <time itemprop="dateUpdated" datetime="2017-11-19T14:01:35+08:00" content="2017-11-19">
              2017-11-19
              </time>
              </span>
            
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/容器编排/" itemprop="url" rel="index">
                    <span itemprop="name">容器编排</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/09/15/二进制方式部署Kubernetes-1-6-0集群-开启TLS/" class="leancloud_visitors" data-flag-title="二进制方式部署Kubernetes 1.6.0集群(开启TLS)">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="Kubernetes简介"><a href="#Kubernetes简介" class="headerlink" title="Kubernetes简介"></a>Kubernetes简介</h2><p>Kubernetes是谷歌开源的容器集群管理系统，是Google多年大规模容器管理技术Borg的 开源版本，主要功能包括:</p>
<ul>
<li>基于容器的应用部署、维护和滚动升级</li>
<li>负载均衡和服务发现</li>
<li>跨机器和跨地区的集群调度<a id="more"></a></li>
<li>自动伸缩</li>
<li>无状态服务和有状态服务 </li>
<li>广泛的Volume支持 </li>
<li>插件机制保证扩展性</li>
</ul>
<p>之前尝试使用kubeadm自动化部署集群，使用yum去安装kubeadm等工具，但是不翻墙的情况下，这种方式在国内几乎是不可能安装成功的。于是改为采用二进制文件部署Kubernetes集群，同时开启了集群的TLS安全认证。本篇实践是参照opsnull的文章<a href="https://mp.weixin.qq.com/s/bvCZUl6LQhlqDVv_TNeDFg" target="_blank" rel="external">创建 kubernetes 各组件 TLS 加密通信的证书和秘钥</a>，结合公司的实际情况进行部署的。</p>
<h2 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h2><table>
<thead>
<tr>
<th>主机名</th>
<th>操作系统版本</th>
<th>IP地址</th>
<th>角色</th>
<th>安装软件</th>
</tr>
</thead>
<tbody>
<tr>
<td>node1</td>
<td>CentOS 7.0</td>
<td>172.16.7.151</td>
<td>Kubernetes Master、Node</td>
<td>etcd 3.2.7、kube-apiserver、kube-scheduler、kube-controller-manager、kubelet、kube-proxy、etcd 3.2.7、flannel 0.7.1、docker 1.12.6</td>
</tr>
<tr>
<td>node2</td>
<td>CentOS 7.0</td>
<td>172.16.7.152</td>
<td>Kubernetes Node</td>
<td>kubelet、kube-proxy、flannel 0.7.1、etcd 3.2.7、docker 1.12.6</td>
</tr>
<tr>
<td>node3</td>
<td>CentOS 7.0</td>
<td>172.16.7.153</td>
<td>Kubernetes Node</td>
<td>kubelet、kube-proxy、flannel 0.7.1、etcd 3.2.7、docker 1.12.6</td>
</tr>
<tr>
<td>spark32</td>
<td>CentOS 7.0</td>
<td>172.16.206.32</td>
<td>Harbor</td>
<td>docker-ce 17.06.1、docker-compose 1.15.0、harbor-online-installer-v1.1.2.tar</td>
</tr>
</tbody>
</table>
<p>spark32主机是harbor私有镜像仓库，关于harbor的安装部署见之前的博客《企业级Docker Registry——Harbor搭建和使用》。</p>
<h2 id="创建TLS加密通信的证书和密钥"><a href="#创建TLS加密通信的证书和密钥" class="headerlink" title="创建TLS加密通信的证书和密钥"></a>创建TLS加密通信的证书和密钥</h2><p>kubernetes各组件需要使用TLS证书对通信进行加密，这里我使用CloudFlare的PKI工具集<a href="https://github.com/cloudflare/cfssl" target="_blank" rel="external">cfssl</a>来生成CA和其它证书。<br>生成的CA证书和密钥文件如下：</p>
<ul>
<li>ca-key.pem</li>
<li>ca.pem</li>
<li>kubernetes-key.pem</li>
<li>kubernetes.pem</li>
<li>kube-proxy.pem</li>
<li>kube-proxy-key.pem</li>
<li>admin.pem</li>
<li>admin-key.pem</li>
</ul>
<p>各组件使用证书的情况如下：</p>
<ul>
<li>etcd：使用ca.pem、kubernetes-key.pem、kubernetes.pem；</li>
<li>kube-apiserver：使用ca.pem、kubernetes-key.pem、kubernetes.pem；</li>
<li>kubelet：使用ca.pem；</li>
<li>kube-proxy：使用ca.pem、kube-proxy-key.pem、kube-proxy.pem；</li>
<li>kubectl：使用ca.pem、admin-key.pem、admin.pem</li>
</ul>
<p>kube-controller、kube-scheduler当前需要和kube-apiserver部署在同一台机器上且使用非安全端口通信，故不需要证书。</p>
<h3 id="安装CFSSL"><a href="#安装CFSSL" class="headerlink" title="安装CFSSL"></a>安装CFSSL</h3><p>有两种方式安装，一是二进制源码包安装，二是使用go命令安装。<br>1.方式一：二进制源码包安装</p>
<pre><code># wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
# chmod +x cfssl_linux-amd64
# mv cfssl_linux-amd64 /root/local/bin/cfssl

# wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
# chmod +x cfssljson_linux-amd64
# mv cfssljson_linux-amd64 /root/local/bin/cfssljson

# wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64
# chmod +x cfssl-certinfo_linux-amd64
# mv cfssl-certinfo_linux-amd64 /root/local/bin/cfssl-certinfo

# export PATH=/root/local/bin:$PATH
</code></pre><p>2.方式二：使用go命令安装<br>安装go(需要go 1.6+)：</p>
<pre><code># 下载地址：https://golang.org/dl/
[root@node1 ~]# cd /usr/local/
[root@node1 local]# wget https://storage.googleapis.com/golang/go1.9.linux-amd64.tar.gz
[root@node1 local]# tar zxf go1.9.linux-amd64.tar.gz
[root@node1 local]# vim /etc/profile
# Go
export GO_HOME=/usr/local/go
export PATH=$GO_HOME/bin:$PATH 
[root@node1 local]# source /etc/profile
# 查看版本信息
[root@node1 local]# go version
go version go1.9 linux/amd64
</code></pre><p>安装cfssl:</p>
<pre><code>[root@node1 local]# go get -u github.com/cloudflare/cfssl/cmd/...
[root@node1 local]# ls /root/go/bin/
cfssl  cfssl-bundle  cfssl-certinfo  cfssljson  cfssl-newkey  cfssl-scan  mkbundle  multirootca
[root@node1 local]# mv /root/go/bin/* /usr/local/bin/
</code></pre><h3 id="创建CA"><a href="#创建CA" class="headerlink" title="创建CA"></a>创建CA</h3><p>1.创建 CA 配置文件</p>
<pre><code>[root@node1 local]# mkdir /opt/ssl
[root@node1 local]# cd /opt/ssl/
[root@node1 ssl]# cfssl print-defaults config &gt; config.json
[root@node1 ssl]# cfssl print-defaults csr &gt; csr.json
# 创建CA配置文件
[root@node1 ssl]# vim ca-config.json
{
  &quot;signing&quot;: {
    &quot;default&quot;: {
      &quot;expiry&quot;: &quot;8760h&quot;
    },
    &quot;profiles&quot;: {
      &quot;kubernetes&quot;: {
        &quot;usages&quot;: [
            &quot;signing&quot;,
            &quot;key encipherment&quot;,
            &quot;server auth&quot;,
            &quot;client auth&quot;
        ],
        &quot;expiry&quot;: &quot;8760h&quot;
      }
    }
  }
}
</code></pre><p>部分字段说明：</p>
<ul>
<li><strong>ca-config.json：</strong>可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个 profile；</li>
<li><strong>signing：</strong>表示该证书可用于签名其它证书；生成的 ca.pem 证书中 CA=TRUE；</li>
<li><strong>server auth：</strong>表示client可以用该 CA 对server提供的证书进行验证；</li>
<li><strong>client auth：</strong>表示server可以用该 CA 对client提供的证书进行验证。</li>
</ul>
<p>2.创建 CA 证书签名请求</p>
<pre><code>[root@node1 ssl]# vim ca-csr.json
{
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;System&quot;
    }
  ]
}
</code></pre><p>部分字段说明：</p>
<ul>
<li><strong>“CN”：</strong>Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法；</li>
<li><strong>“O”：</strong>Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)；</li>
</ul>
<p>3.生成 CA 证书和私钥</p>
<pre><code>[root@node1 ssl]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca
2017/09/10 04:22:13 [INFO] generating a new CA key and certificate from CSR
2017/09/10 04:22:13 [INFO] generate received request
2017/09/10 04:22:13 [INFO] received CSR
2017/09/10 04:22:13 [INFO] generating key: rsa-2048
2017/09/10 04:22:13 [INFO] encoded CSR
2017/09/10 04:22:13 [INFO] signed certificate with serial number 348968532213237181927470194452366329323573808966
[root@node1 ssl]# ls ca*
ca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem
</code></pre><h3 id="创建-Kubernetes-证书"><a href="#创建-Kubernetes-证书" class="headerlink" title="创建 Kubernetes 证书"></a>创建 Kubernetes 证书</h3><p>1.创建 kubernetes 证书签名请求</p>
<pre><code>[root@node1 ssl]# vim kubernetes-csr.json
{
    &quot;CN&quot;: &quot;kubernetes&quot;,
    &quot;hosts&quot;: [
      &quot;127.0.0.1&quot;,
      &quot;172.16.7.151&quot;,
      &quot;172.16.7.152&quot;,
      &quot;172.16.7.153&quot;,
      &quot;172.16.206.32&quot;,
      &quot;10.254.0.1&quot;,
      &quot;kubernetes&quot;,
      &quot;kubernetes.default&quot;,
      &quot;kubernetes.default.svc&quot;,
      &quot;kubernetes.default.svc.cluster&quot;,
      &quot;kubernetes.default.svc.cluster.local&quot;
    ],
    &quot;key&quot;: {
        &quot;algo&quot;: &quot;rsa&quot;,
        &quot;size&quot;: 2048
    },
    &quot;names&quot;: [
        {
            &quot;C&quot;: &quot;CN&quot;,
            &quot;ST&quot;: &quot;BeiJing&quot;,
            &quot;L&quot;: &quot;BeiJing&quot;,
            &quot;O&quot;: &quot;k8s&quot;,
            &quot;OU&quot;: &quot;System&quot;
        }
    ]
}
</code></pre><p>部分字段说明：</p>
<ul>
<li>如果 hosts 字段不为空则需要指定授权使用该证书的 IP 或域名列表，由于该证书后续被 etcd 集群和 kubernetes master 集群使用，所以上面分别指定了 etcd 集群、kubernetes master 集群的主机 IP；</li>
<li>还需要添加kube-apiserver注册的名为 kubernetes 服务的 IP（一般是 kue-apiserver 指定的 service-cluster-ip-range 网段的第一个IP，如 10.254.0.1)</li>
</ul>
<p>2.生成 kubernetes 证书和私钥</p>
<pre><code>[root@node1 ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes
2017/09/10 07:44:27 [INFO] generate received request
2017/09/10 07:44:27 [INFO] received CSR
2017/09/10 07:44:27 [INFO] generating key: rsa-2048
2017/09/10 07:44:27 [INFO] encoded CSR
2017/09/10 07:44:27 [INFO] signed certificate with serial number 695308968867503306176219705194671734841389082714
[root@node1 ssl]# ls kubernetes*
kubernetes.csr  kubernetes-csr.json  kubernetes-key.pem  kubernetes.pem
</code></pre><p>或者直接在命令行上指定相关参数：</p>
<pre><code># echo &apos;{&quot;CN&quot;:&quot;kubernetes&quot;,&quot;hosts&quot;:[&quot;&quot;],&quot;key&quot;:{&quot;algo&quot;:&quot;rsa&quot;,&quot;size&quot;:2048}}&apos; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes -hostname=&quot;127.0.0.1,172.16.7.151,172.16.7.152,172.16.7.153,172.16.206.32,10.254.0.1,kubernetes,kubernetes.default&quot; - | cfssljson -bare kubernetes
</code></pre><h3 id="创建-Admin-证书"><a href="#创建-Admin-证书" class="headerlink" title="创建 Admin 证书"></a>创建 Admin 证书</h3><p>1.创建 admin 证书签名请求</p>
<pre><code>[root@node1 ssl]# vim admin-csr.json
{
  &quot;CN&quot;: &quot;admin&quot;,
  &quot;hosts&quot;: [],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;system:masters&quot;,
      &quot;OU&quot;: &quot;System&quot;
    }
  ]
}
</code></pre><p>说明：</p>
<ul>
<li>后续 kube-apiserver 使用 RBAC 对客户端(如 kubelet、kube-proxy、Pod)请求进行授权；</li>
<li>kube-apiserver 预定义了一些 RBAC 使用的 RoleBindings，如 cluster-admin 将 Groupsystem:masters 与 Role cluster-admin 绑定，该 Role 授予了调用kube-apiserver 的所有 API的权限；</li>
<li>OU 指定该证书的 Group 为 system:masters，kubelet 使用该证书访问 kube-apiserver时 ，由于证书被 CA 签名，所以认证通过，同时由于证书用户组为经过预授权的system:masters，所以被授予访问所有 API 的权限。</li>
</ul>
<p>2.生成 admin 证书和私钥</p>
<pre><code>[root@node1 ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin
2017/09/10 20:01:05 [INFO] generate received request
2017/09/10 20:01:05 [INFO] received CSR
2017/09/10 20:01:05 [INFO] generating key: rsa-2048
2017/09/10 20:01:05 [INFO] encoded CSR
2017/09/10 20:01:05 [INFO] signed certificate with serial number 580169825175224945071583937498159721917720511011
2017/09/10 20:01:05 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable for websites. For more information see the Baseline Requirements for the Issuance and Management of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 (&quot;Information Requirements&quot;).
[root@node1 ssl]# ls admin*
admin.csr  admin-csr.json  admin-key.pem  admin.pem
</code></pre><h3 id="创建-Kube-Proxy-证书"><a href="#创建-Kube-Proxy-证书" class="headerlink" title="创建 Kube-Proxy 证书"></a>创建 Kube-Proxy 证书</h3><p>1.创建 kube-proxy 证书签名请求</p>
<pre><code>[root@node1 ssl]# vim kube-proxy-csr.json
{
  &quot;CN&quot;: &quot;system:kube-proxy&quot;,
  &quot;hosts&quot;: [],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;System&quot;
    }
  ]
}
</code></pre><p>说明：</p>
<ul>
<li>CN 指定该证书的 User 为 system:kube-proxy；</li>
<li>kube-apiserver 预定义的 RoleBinding cluster-admin 将User system:kube-proxy 与 Rolesystem:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限。</li>
</ul>
<p>2.生成 kube-proxy 客户端证书和私钥</p>
<pre><code>[root@node1 ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes  kube-proxy-csr.json | cfssljson -bare kube-proxy
2017/09/10 20:07:55 [INFO] generate received request
2017/09/10 20:07:55 [INFO] received CSR
2017/09/10 20:07:55 [INFO] generating key: rsa-2048
2017/09/10 20:07:55 [INFO] encoded CSR
2017/09/10 20:07:55 [INFO] signed certificate with serial number 655306618453852718922516297333812428130766975244
2017/09/10 20:07:55 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable for websites. For more information see the Baseline Requirements for the Issuance and Management of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 (&quot;Information Requirements&quot;).
[root@node1 ssl]# ls kube-proxy*
kube-proxy.csr  kube-proxy-csr.json  kube-proxy-key.pem  kube-proxy.pem
</code></pre><h3 id="校验证书"><a href="#校验证书" class="headerlink" title="校验证书"></a>校验证书</h3><p>以校验Kubernetes证书为例。</p>
<h4 id="使用openssl命令校验证书"><a href="#使用openssl命令校验证书" class="headerlink" title="使用openssl命令校验证书"></a>使用openssl命令校验证书</h4><pre><code>[root@node1 ssl]# openssl x509 -noout -text -in kubernetes.pem 
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number:
            79:ca:bb:84:73:15:b1:db:aa:24:d7:a3:60:65:b0:55:27:a7:e8:5a
    Signature Algorithm: sha256WithRSAEncryption
        Issuer: C=CN, ST=BeiJing, L=BeiJing, O=k8s, OU=System, CN=kubernetes
        Validity
            Not Before: Sep 10 11:39:00 2017 GMT
            Not After : Sep 10 11:39:00 2018 GMT
        Subject: C=CN, ST=BeiJing, L=BeiJing, O=k8s, OU=System, CN=kubernetes
...
        X509v3 extensions:
            X509v3 Key Usage: critical
                Digital Signature, Key Encipherment
            X509v3 Extended Key Usage: 
                TLS Web Server Authentication, TLS Web Client Authentication
            X509v3 Basic Constraints: critical
                CA:FALSE
            X509v3 Subject Key Identifier: 
                79:48:C1:1B:81:DD:9C:75:04:EC:B6:35:26:5E:82:AA:2E:45:F6:C5
            X509v3 Subject Alternative Name: 
                DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster, DNS:kubernetes.default.svc.cluster.local, IP Address:127.0.0.1, IP Address:172.16.7.151, IP Address:172.16.7.152, IP Address:172.16.7.153, IP Address:172.16.206.32, IP Address:10.254.0.1
...
</code></pre><p>【说明】：</p>
<ul>
<li>确认 Issuer 字段的内容和 ca-csr.json 一致；</li>
<li>确认 Subject 字段的内容和 kubernetes-csr.json 一致；</li>
<li>确认 X509v3 Subject Alternative Name 字段的内容和 kubernetes-csr.json 一致；</li>
<li>确认 X509v3 Key Usage、Extended Key Usage 字段的内容和 ca-config.json 中 kubernetesprofile 一致。</li>
</ul>
<h4 id="使用-Cfssl-Certinfo-命令校验"><a href="#使用-Cfssl-Certinfo-命令校验" class="headerlink" title="使用 Cfssl-Certinfo 命令校验"></a>使用 Cfssl-Certinfo 命令校验</h4><pre><code>[root@node1 ssl]# cfssl-certinfo -cert kubernetes.pem
{
  &quot;subject&quot;: {
    &quot;common_name&quot;: &quot;kubernetes&quot;,
    &quot;country&quot;: &quot;CN&quot;,
    &quot;organization&quot;: &quot;k8s&quot;,
    &quot;organizational_unit&quot;: &quot;System&quot;,
    &quot;locality&quot;: &quot;BeiJing&quot;,
    &quot;province&quot;: &quot;BeiJing&quot;,
    &quot;names&quot;: [
      &quot;CN&quot;,
      &quot;BeiJing&quot;,
      &quot;BeiJing&quot;,
      &quot;k8s&quot;,
      &quot;System&quot;,
      &quot;kubernetes&quot;
    ]
  },
  &quot;issuer&quot;: {
    &quot;common_name&quot;: &quot;kubernetes&quot;,
    &quot;country&quot;: &quot;CN&quot;,
    &quot;organization&quot;: &quot;k8s&quot;,
    &quot;organizational_unit&quot;: &quot;System&quot;,
    &quot;locality&quot;: &quot;BeiJing&quot;,
    &quot;province&quot;: &quot;BeiJing&quot;,
    &quot;names&quot;: [
      &quot;CN&quot;,
      &quot;BeiJing&quot;,
      &quot;BeiJing&quot;,
      &quot;k8s&quot;,
      &quot;System&quot;,
      &quot;kubernetes&quot;
    ]
  },
  &quot;serial_number&quot;: &quot;695308968867503306176219705194671734841389082714&quot;,
  &quot;sans&quot;: [
    &quot;kubernetes&quot;,
    &quot;kubernetes.default&quot;,
    &quot;kubernetes.default.svc&quot;,
    &quot;kubernetes.default.svc.cluster&quot;,
    &quot;kubernetes.default.svc.cluster.local&quot;,
    &quot;127.0.0.1&quot;,
    &quot;172.16.7.151&quot;,
    &quot;172.16.7.152&quot;,
    &quot;172.16.7.153&quot;,
    &quot;172.16.206.32&quot;,
    &quot;10.254.0.1&quot;
  ],
  &quot;not_before&quot;: &quot;2017-09-10T11:39:00Z&quot;,
  &quot;not_after&quot;: &quot;2018-09-10T11:39:00Z&quot;,
  &quot;sigalg&quot;: &quot;SHA256WithRSA&quot;,
  &quot;authority_key_id&quot;: &quot;&quot;,
  &quot;subject_key_id&quot;: &quot;79:48:C1:1B:81:DD:9C:75:4:EC:B6:35:26:5E:82:AA:2E:45:F6:C5&quot;,
...
</code></pre><h3 id="分发证书"><a href="#分发证书" class="headerlink" title="分发证书"></a>分发证书</h3><p>将生成的证书和秘钥文件（后缀名为.pem）拷贝到所有机器的 /etc/kubernetes/ssl 目录下备用:</p>
<pre><code>[root@node1 ssl]# mkdir -p /etc/kubernetes/ssl
[root@node1 ssl]# cp *.pem /etc/kubernetes/ssl
[root@node1 ssl]# scp -p *.pem root@172.16.7.152:/etc/kubernetes/ssl/
[root@node1 ssl]# scp -p *.pem root@172.16.7.153:/etc/kubernetes/ssl/
[root@node1 ssl]# scp -p *.pem root@172.16.206.32:/etc/kubernetes/ssl/
</code></pre><h2 id="下载和配置-kubectl-kubecontrol-命令行工具"><a href="#下载和配置-kubectl-kubecontrol-命令行工具" class="headerlink" title="下载和配置 kubectl(kubecontrol) 命令行工具"></a>下载和配置 kubectl(kubecontrol) 命令行工具</h2><h3 id="下载kubectl"><a href="#下载kubectl" class="headerlink" title="下载kubectl"></a>下载kubectl</h3><pre><code>[root@node1 local]# wget https://dl.k8s.io/v1.6.0/kubernetes-client-linux-amd64.tar.gz
[root@node1 local]# tar zxf kubernetes-client-linux-amd64.tar.gz
[root@node1 local]# cp kubernetes/client/bin/kube* /usr/bin/
[root@node1 local]# chmod +x /usr/bin/kube*
</code></pre><h3 id="创建-kubectl-kubeconfig-文件"><a href="#创建-kubectl-kubeconfig-文件" class="headerlink" title="创建 kubectl kubeconfig 文件"></a>创建 kubectl kubeconfig 文件</h3><pre><code>[root@node1 local]# cd /etc/kubernetes/
[root@node1 kubernetes]# export KUBE_APISERVER=&quot;https://172.16.7.151:6443&quot;
# 设置集群参数
[root@node1 kubernetes]# kubectl config set-cluster kubernetes \
&gt; --certificate-authority=/etc/kubernetes/ssl/ca.pem \
&gt; --embed-certs=true \
&gt; --server=${KUBE_APISERVER}
Cluster &quot;kubernetes&quot; set.
# 设置客户端认证参数
[root@node1 kubernetes]# kubectl config set-credentials admin \
&gt; --client-certificate=/etc/kubernetes/ssl/admin.pem \
&gt; --embed-certs=true \
&gt; --client-key=/etc/kubernetes/ssl/admin-key.pem
User &quot;admin&quot; set.
# 设置上下文参数
[root@node1 kubernetes]# kubectl config set-context kubernetes \
&gt; --cluster=kubernetes \
&gt; --user=admin
Context &quot;kubernetes&quot; set
# 设置默认上下文
[root@node1 kubernetes]# kubectl config use-context kubernetes
Switched to context &quot;kubernetes&quot;.
[root@node1 kubernetes]# ls ~/.kube/config 
/root/.kube/config
</code></pre><p>【说明】：</p>
<ul>
<li>admin.pem 证书 OU 字段值为 system:masters，kube-apiserver 预定义的 RoleBinding cluster-admin 将 Groupsystem:masters 与 Role cluster admin 绑定，该 Role 授予了调用kube-apiserver 相关 API 的权限；</li>
<li>生成的 kubeconfig 被保存到 ~/.kube/config 文件。</li>
</ul>
<h2 id="创建-kubeconfig-文件"><a href="#创建-kubeconfig-文件" class="headerlink" title="创建 kubeconfig 文件"></a>创建 kubeconfig 文件</h2><p>kubelet、kube-proxy 等 Node 机器上的进程与 Master 机器的 kube-apiserver 进程通信时需要认证和授权.<br>kubernetes 1.4 开始支持由 kube-apiserver 为客户端生成 TLS 证书的 TLS Bootstrapping 功能，这样就不需要为每个客户端生成证书了；该功能当前仅支持为 kubelet 生成证书。</p>
<h3 id="创建-TLS-Bootstrapping-Token"><a href="#创建-TLS-Bootstrapping-Token" class="headerlink" title="创建 TLS Bootstrapping Token"></a>创建 TLS Bootstrapping Token</h3><h4 id="Token-auth-file"><a href="#Token-auth-file" class="headerlink" title="Token auth file"></a>Token auth file</h4><p>Token可以是任意的包涵128 bit的字符串，可以使用安全的随机数发生器生成。</p>
<pre><code>[root@node1 ssl]# export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d &apos; &apos;)
[root@node1 ssl]# cat &gt; token.csv &lt;&lt;EOF
&gt; ${BOOTSTRAP_TOKEN},kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot;
&gt; EOF
</code></pre><p>将token.csv发到所有机器（Master 和 Node）的 /etc/kubernetes/ 目录。</p>
<pre><code>[root@node1 ssl]# cp token.csv /etc/kubernetes/
[root@node1 ssl]# scp -p token.csv root@172.16.7.152:/etc/kubernetes/
[root@node1 ssl]# scp -p token.csv root@172.16.7.153:/etc/kubernetes/
</code></pre><h4 id="创建-kubelet-bootstrapping-kubeconfig-文件"><a href="#创建-kubelet-bootstrapping-kubeconfig-文件" class="headerlink" title="创建 kubelet bootstrapping kubeconfig 文件"></a>创建 kubelet bootstrapping kubeconfig 文件</h4><pre><code>[root@node1 ssl]# cd /etc/kubernetes
[root@node1 kubernetes]# export KUBE_APISERVER=&quot;https://172.16.7.151:6443&quot;
# 设置集群参数
[root@node1 kubernetes]# kubectl config set-cluster kubernetes \
&gt; --certificate-authority=/etc/kubernetes/ssl/ca.pem \
&gt; --embed-certs=true \
&gt; --server=${KUBE_APISERVER} \
&gt; --kubeconfig=bootstrap.kubeconfig
Cluster &quot;kubernetes&quot; set.
# 设置客户端认证参数
[root@node1 kubernetes]# kubectl config set-credentials kubelet-bootstrap \
&gt; --token=${BOOTSTRAP_TOKEN} \
&gt; --kubeconfig=bootstrap.kubeconfig
User &quot;kubelet-bootstrap&quot; set.
# 设置上下文参数
[root@node1 kubernetes]# kubectl config set-context default \
&gt; --cluster=kubernetes \
&gt; --user=kubelet-bootstrap \
&gt; --kubeconfig=bootstrap.kubeconfig
Context &quot;default&quot; created.
# 设置默认上下文
[root@node1 kubernetes]# kubectl config use-context default --kubeconfig=bootstrap.kubeconfig
Switched to context &quot;default&quot;.
</code></pre><p>【说明】：</p>
<ul>
<li>–embed-certs 为 true 时表示将 certificate-authority 证书写入到生成的 bootstrap.kubeconfig 文件中；</li>
<li>设置客户端认证参数时没有指定秘钥和证书，后续由 kube-apiserver 自动生成。</li>
</ul>
<h3 id="创建-kube-proxy-kubeconfig-文件"><a href="#创建-kube-proxy-kubeconfig-文件" class="headerlink" title="创建 kube-proxy kubeconfig 文件"></a>创建 kube-proxy kubeconfig 文件</h3><pre><code>[root@node1 kubernetes]# export KUBE_APISERVER=&quot;https://172.16.7.151:6443&quot;
# 设置集群参数
[root@node1 kubernetes]# kubectl config set-cluster kubernetes \
&gt; --certificate-authority=/etc/kubernetes/ssl/ca.pem \
&gt; --embed-certs=true \
&gt; --server=${KUBE_APISERVER} \
&gt; --kubeconfig=kube-proxy.kubeconfig
Cluster &quot;kubernetes&quot; set.
# 设置客户端认证参数
[root@node1 kubernetes]# kubectl config set-credentials kube-proxy \
&gt; --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \
&gt; --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \
&gt; --embed-certs=true \
&gt; --kubeconfig=kube-proxy.kubeconfig
User &quot;kube-proxy&quot; set.
# 设置上下文参数
[root@node1 kubernetes]# kubectl config set-context default \
&gt; --cluster=kubernetes \
&gt; --user=kube-proxy \
&gt; --kubeconfig=kube-proxy.kubeconfig
Context &quot;default&quot; created.
# 设置默认上下文
[root@node1 kubernetes]# kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
Switched to context &quot;default&quot;.
</code></pre><p>【说明】：</p>
<ul>
<li>设置集群参数和客户端认证参数时 –embed-certs 都为 true，这会将 certificate-authority、client-certificate 和client-key 指向的证书文件内容写入到生成的 kube-proxy.kubeconfig 文件中；</li>
<li>kube-proxy.pem 证书中 CN 为 system:kube-proxy，kube-apiserver 预定义的 RoleBinding cluster-admin 将Usersystem:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限。</li>
</ul>
<h3 id="分发-kubeconfig-文件"><a href="#分发-kubeconfig-文件" class="headerlink" title="分发 kubeconfig 文件"></a>分发 kubeconfig 文件</h3><p>将两个 kubeconfig 文件分发到所有 Node 机器的 /etc/kubernetes/ 目录。</p>
<pre><code>[root@node1 kubernetes]# scp -p bootstrap.kubeconfig root@172.16.7.152:/etc/kubernetes/
[root@node1 kubernetes]# scp -p kube-proxy.kubeconfig root@172.16.7.152:/etc/kubernetes/
[root@node1 kubernetes]# scp -p bootstrap.kubeconfig root@172.16.7.153:/etc/kubernetes/
[root@node1 kubernetes]# scp -p kube-proxy.kubeconfig root@172.16.7.153:/etc/kubernetes/
</code></pre><h2 id="创建高可用-etcd-集群"><a href="#创建高可用-etcd-集群" class="headerlink" title="创建高可用 etcd 集群"></a>创建高可用 etcd 集群</h2><p>etcd 是 CoreOS 团队发起的开源项目，基于 Go 语言实现，做为一个分布式键值对存储，通过分布式锁，leader选举和写屏障(write barriers)来实现可靠的分布式协作。<br>kubernetes系统使用etcd存储所有数据。<br>CoreOS官方推荐集群规模5个为宜，我这里使用了3个节点。</p>
<h3 id="安装配置etcd集群"><a href="#安装配置etcd集群" class="headerlink" title="安装配置etcd集群"></a>安装配置etcd集群</h3><p>搭建etcd集群有3种方式，分别为Static, etcd Discovery, DNS Discovery。Discovery请参见<a href="https://coreos.com/etcd/docs/latest/op-guide/clustering.html" target="_blank" rel="external">官网</a>。这里仅以Static方式展示一次集群搭建过程。</p>
<p>首先请做好3个节点的时间同步。</p>
<p><strong>1.TLS 认证文件</strong><br>需要为 etcd 集群创建加密通信的 TLS 证书，这里复用以前创建的 kubernetes 证书。</p>
<pre><code>[root@node1 ssl]# cp ca.pem kubernetes-key.pem kubernetes.pem /etc/kubernetes/ssl
</code></pre><p>这步在之前做过，可以忽略。【注意】:kubernetes 证书的 hosts 字段列表中包含上面三台机器的 IP，否则后续证书校验会失败。</p>
<p><strong>2.下载二进制文件</strong><br>到 <a href="https://github.com/coreos/etcd/releases" target="_blank" rel="external">https://github.com/coreos/etcd/releases</a> 页面下载最新版本的二进制文件，并上传到/usr/local/目录下。</p>
<pre><code>[root@node1 local]# tar xf etcd-v3.2.7-linux-amd64.tar
[root@node1 local]# mv etcd-v3.2.7-linux-amd64/etcd* /usr/local/bin/
</code></pre><p>etcd集群中另外两台机器也需要如上操作。</p>
<p><strong>3.创建 etcd 的 systemd unit 文件</strong><br>配置文件模板如下，注意替换 ETCD_NAME 和 INTERNAL_IP 变量的值。</p>
<pre><code>[root@node1 ~]# cat etcd.service
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
EnvironmentFile=-/etc/etcd/etcd.conf
ExecStart=/usr/local/bin/etcd \
  --name ${ETCD_NAME} \
  --cert-file=/etc/kubernetes/ssl/kubernetes.pem \
  --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
  --peer-cert-file=/etc/kubernetes/ssl/kubernetes.pem \
  --peer-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
  --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \
  --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \
  --initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \
  --listen-peer-urls https://${INTERNAL_IP}:2380 \
  --listen-client-urls https://${INTERNAL_IP}:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://${INTERNAL_IP}:2379 \
  --initial-cluster-token etcd-cluster-0 \
  --initial-cluster node1=https://172.16.7.151:2380,node2=https://172.16.7.152:2380,node3=https://172.16.7.153:2380 \
  --initial-cluster-state new \
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
</code></pre><p>针对上面几个配置参数做下简单的解释：</p>
<ul>
<li>–name：方便理解的节点名称，默认为default，在集群中应该保持唯一，可以使用 hostname</li>
<li>–data-dir：服务运行数据保存的路径，默认为 ${name}.etcd</li>
<li>–snapshot-count：指定有多少事务（transaction）被提交时，触发截取快照保存到磁盘</li>
<li>–heartbeat-interval：leader 多久发送一次心跳到 followers。默认值是 100ms</li>
<li>–eletion-timeout：重新投票的超时时间，如果 follow 在该时间间隔没有收到心跳包，会触发重新投票，默认为 1000 ms</li>
<li>–listen-peer-urls：和同伴通信的地址，比如 <a href="http://ip:2380" target="_blank" rel="external">http://ip:2380</a><br>如果有多个，使用逗号分隔。需要所有节点都能够访问，所以不要使用 localhost！</li>
<li>–listen-client-urls：对外提供服务的地址：比如 <a href="http://ip:2379,http://127.0.0.1:2379，客户端会连接到这里和" target="_blank" rel="external">http://ip:2379,http://127.0.0.1:2379，客户端会连接到这里和</a> etcd 交互</li>
<li>–advertise-client-urls：对外公告的该节点客户端监听地址，这个值会告诉集群中其他节点</li>
<li>–initial-advertise-peer-urls：该节点同伴监听地址，这个值会告诉集群中其他节点</li>
<li>–initial-cluster：集群中所有节点的信息，格式为 node1=<a href="http://ip1:2380,node2=http://ip2:2380,…。" target="_blank" rel="external">http://ip1:2380,node2=http://ip2:2380,…。</a><br>注意：这里的 node1 是节点的 –name 指定的名字；后面的 ip1:2380 是 –initial-advertise-peer-urls 指定的值</li>
<li>–initial-cluster-state：新建集群的时候，这个值为new；假如已经存在的集群，这个值为 existing</li>
<li>–initial-cluster-token：创建集群的token，这个值每个集群保持唯一。这样的话，如果你要重新创建集群，即使配置和之前一样，也会再次生成新的集群和节点 uuid；否则会导致多个集群之间的冲突，造成未知的错误</li>
</ul>
<p>所有以–init开头的配置都是在bootstrap集群的时候才会用到，后续节点的重启会被忽略。</p>
<p>node1主机：</p>
<pre><code>[root@node1 local]# mkdir -p /var/lib/etcd
[root@node1 local]# cd /etc/systemd/system/
[root@node1 system]# vim etcd.service
...
ExecStart=/usr/local/bin/etcd \
  --name node1 \
...
  --initial-advertise-peer-urls https://172.16.7.151:2380 \
  --listen-peer-urls https://172.16.7.151:2380 \
  --listen-client-urls https://172.16.7.151:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://172.16.7.151:2379 \
  --initial-cluster-token etcd-cluster-0 \
...

[root@node1 system]# scp -p etcd.service root@172.16.7.152:/etc/systemd/system/
[root@node1 system]# scp -p etcd.service root@172.16.7.153:/etc/systemd/system/
</code></pre><p>node2主机：</p>
<pre><code>[root@node2 ~]# mkdir -p /var/lib/etcd /var/lib/etcd
[root@node2 ~]# vim /etc/systemd/system/etcd.service
...
ExecStart=/usr/local/bin/etcd \
  --name node2 \
...
  --initial-advertise-peer-urls https://172.16.7.152:2380 \
  --listen-peer-urls https://172.16.7.152:2380 \
  --listen-client-urls https://172.16.7.152:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://172.16.7.152:2379 \
  --initial-cluster-token etcd-cluster-0 \
...
</code></pre><p>node3主机：</p>
<pre><code>[root@node3 ~]# mkdir -p /var/lib/etcd /var/lib/etcd
[root@node3 ~]# vim /etc/systemd/system/etcd.service
...
ExecStart=/usr/local/bin/etcd \
  --name node2 \
...
  --initial-advertise-peer-urls https://172.16.7.153:2380 \
  --listen-peer-urls https://172.16.7.153:2380 \
  --listen-client-urls https://172.16.7.153:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://172.16.7.153:2379 \
  --initial-cluster-token etcd-cluster-0 \
...
</code></pre><p>【说明】：</p>
<ul>
<li>指定 etcd 的工作目录为 /var/lib/etcd，数据目录为 /var/lib/etcd，需在启动服务前创建这两个目录；</li>
<li>为了保证通信安全，需要指定 etcd 的公私钥(cert-file和key-file)、Peers 通信的公私钥和 CA 证书(peer-cert-file、peer-key-file、peer-trusted-ca-file)、客户端的CA证书（trusted-ca-file）；</li>
<li>创建 kubernetes.pem 证书时使用的 kubernetes-csr.json 文件的 hosts 字段包含所有 etcd 节点的 INTERNAL_IP，否则证书校验会出错；</li>
<li>–initial-cluster-state 值为 new 时，–name 的参数值必须位于 –initial-cluster 列表中。</li>
</ul>
<h3 id="启动-etcd-服务"><a href="#启动-etcd-服务" class="headerlink" title="启动 etcd 服务"></a>启动 etcd 服务</h3><p>集群中的节点都执行以下命令：</p>
<pre><code># systemctl daemon-reload
# systemctl enable etcd
# systemctl start etcd
</code></pre><h3 id="验证服务"><a href="#验证服务" class="headerlink" title="验证服务"></a>验证服务</h3><p>etcdctl 是一个命令行客户端，它能提供一些简洁的命令，供用户直接跟 etcd 服务打交道，而无需基于 HTTP API 方式。这在某些情况下将很方便，例如用户对服务进行测试或者手动修改数据库内容。我们也推荐在刚接触 etcd 时通过 etcdctl 命令来熟悉相关的操作，这些操作跟 HTTP API 实际上是对应的。<br>在etcd集群任意一台机器上执行如下命令：</p>
<p>1.查看集群健康状态</p>
<pre><code>[root@node1 system]# etcdctl --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/kubernetes/ssl/kubernetes.pem --key-file=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints &quot;https://172.16.7.151:2379&quot; cluster-health            
member 31800ab6b566b2b is healthy: got healthy result from https://172.16.7.151:2379
member 9a0745d96695eec6 is healthy: got healthy result from https://172.16.7.153:2379
member e64edc68e5e81b55 is healthy: got healthy result from https://172.16.7.152:2379
cluster is healthy
</code></pre><p>结果最后一行为 cluster is healthy 时表示集群服务正常。</p>
<p>2.查看集群成员，并能看出哪个是leader节点</p>
<pre><code>[root@node1 system]# etcdctl --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/kubernetes/ssl/kubernetes.pem --key-file=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints &quot;https://172.16.7.151:2379&quot; member list
31800ab6b566b2b: name=node1 peerURLs=https://172.16.7.151:2380 clientURLs=https://172.16.7.151:2379 isLeader=false
9a0745d96695eec6: name=node3 peerURLs=https://172.16.7.153:2380 clientURLs=https://172.16.7.153:2379 isLeader=false
e64edc68e5e81b55: name=node2 peerURLs=https://172.16.7.152:2380 clientURLs=https://172.16.7.152:2379 isLeader=true
</code></pre><p>3.删除一个节点</p>
<pre><code># 如果你想更新一个节点的IP(peerURLS)，首先你需要知道那个节点的ID
# etcdctl --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/kubernetes/ssl/kubernetes.pem --key-file=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints &quot;https://172.16.7.151:2379&quot; member list
31800ab6b566b2b: name=node1 peerURLs=https://172.16.7.151:2380 clientURLs=https://172.16.7.151:2379 isLeader=false
9a0745d96695eec6: name=node3 peerURLs=https://172.16.7.153:2380 clientURLs=https://172.16.7.153:2379 isLeader=false
e64edc68e5e81b55: name=node2 peerURLs=https://172.16.7.152:2380 clientURLs=https://172.16.7.152:2379 isLeader=true
# 删除一个节点
# etcdctl --endpoints &quot;http://192.168.2.210:2379&quot; member remove 9a0745d96695eec6
</code></pre><h2 id="部署-kubernetes-master"><a href="#部署-kubernetes-master" class="headerlink" title="部署 kubernetes master"></a>部署 kubernetes master</h2><p>kubernetes master 节点包含的组件：</p>
<ul>
<li>kube-apiserver</li>
<li>kube-scheduler</li>
<li>kube-controller-manager</li>
</ul>
<p>目前这三个组件需要部署在同一台机器上。<br>kube-scheduler、kube-controller-manager 和 kube-apiserver 三者的功能紧密相关；<br>同时只能有一个 kube-scheduler、kube-controller-manager 进程处于工作状态，如果运行多个，则需要通过选举产生一个 leader。</p>
<h3 id="TLS-证书文件"><a href="#TLS-证书文件" class="headerlink" title="TLS 证书文件"></a>TLS 证书文件</h3><p>检查之前生成的证书。</p>
<pre><code>[root@node1 kubernetes]# ls /etc/kubernetes/ssl
admin-key.pem  admin.pem  ca-key.pem  ca.pem  kube-proxy-key.pem  kube-proxy.pem  kubernetes-key.pem  kubernetes.pem
</code></pre><h3 id="下载二进制文件"><a href="#下载二进制文件" class="headerlink" title="下载二进制文件"></a>下载二进制文件</h3><p>有两种下载方式：<br>方式一：从 <a href="https://github.com/kubernetes/kubernetes/releases" target="_blank" rel="external">github release</a> 页面下载发布版 tarball，解压后再执行下载脚本。</p>
<pre><code>[root@node1 local]# cd /opt/
[root@node1 opt]# wget https://github.com/kubernetes/kubernetes/releases/download/v1.6.0/kubernetes.tar.gz
[root@node1 opt]# tar zxf kubernetes.tar.gz 
[root@node1 opt]# cd kubernetes/
[root@node1 kubernetes]# ./cluster/get-kube-binaries.sh
Kubernetes release: v1.6.0
Server: linux/amd64  (to override, set KUBERNETES_SERVER_ARCH)
Client: linux/amd64  (autodetected)

Will download kubernetes-server-linux-amd64.tar.gz from https://storage.googleapis.com/kubernetes-release/release/v1.6.0
Will download and extract kubernetes-client-linux-amd64.tar.gz from https://storage.googleapis.com/kubernetes-release/release/v1.6.0
Is this ok? [Y]/n
y
...
</code></pre><p>方式二：从 CHANGELOG页面 下载 client 或 server tarball 文件<br>server 的 tarball kubernetes-server-linux-amd64.tar.gz 已经包含了 client(kubectl) 二进制文件，所以不用单独下载kubernetes-client-linux-amd64.tar.gz文件</p>
<pre><code>wget https://dl.k8s.io/v1.6.0/kubernetes-server-linux-amd64.tar.gz
tar -xzvf kubernetes-server-linux-amd64.tar.gz
...
cd kubernetes
tar -xzvf  kubernetes-src.tar.gz
</code></pre><p>将二进制文件拷贝到指定路径：</p>
<pre><code>[root@node1 kubernetes]# pwd
/opt/kubernetes
[root@node1 kubernetes]# cd server/
[root@node1 server]# tar zxf kubernetes-server-linux-amd64.tar.gz
[root@node1 server]# cp -r kubernetes/server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet} /usr/local/bin/
</code></pre><h3 id="配置和启动-kube-apiserver"><a href="#配置和启动-kube-apiserver" class="headerlink" title="配置和启动 kube-apiserver"></a>配置和启动 kube-apiserver</h3><h4 id="创建-kube-apiserver的service配置文件"><a href="#创建-kube-apiserver的service配置文件" class="headerlink" title="创建 kube-apiserver的service配置文件"></a>创建 kube-apiserver的service配置文件</h4><p>在/usr/lib/systemd/system/下创建kube-apiserver.service，内容如下：</p>
<pre><code># cd /usr/lib/systemd/system/
# vim kube-apiserver.service
[Unit]
Description=Kubernetes API Service
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
After=etcd.service

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/apiserver
ExecStart=/usr/local/bin/kube-apiserver \
        $KUBE_LOGTOSTDERR \
        $KUBE_LOG_LEVEL \
        $KUBE_ETCD_SERVERS \
        $KUBE_API_ADDRESS \
        $KUBE_API_PORT \
        $KUBELET_PORT \
        $KUBE_ALLOW_PRIV \
        $KUBE_SERVICE_ADDRESSES \
        $KUBE_ADMISSION_CONTROL \
        $KUBE_API_ARGS
Restart=on-failure
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
</code></pre><p>/etc/kubernetes/config文件的内容为：</p>
<pre><code># vim /etc/kubernetes/config
###
# kubernetes system config
#
# The following values are used to configure various aspects of all
# kubernetes services, including
#
#   kube-apiserver.service
#   kube-controller-manager.service
#   kube-scheduler.service
#   kubelet.service
#   kube-proxy.service
# logging to stderr means we get it in the systemd journal
KUBE_LOGTOSTDERR=&quot;--logtostderr=true&quot;

# journal message level, 0 is debug
KUBE_LOG_LEVEL=&quot;--v=0&quot;

# Should this cluster be allowed to run privileged docker containers
KUBE_ALLOW_PRIV=&quot;--allow-privileged=true&quot;

# How the controller-manager, scheduler, and proxy find the apiserver
#KUBE_MASTER=&quot;--master=http://domainName:8080&quot;                             
KUBE_MASTER=&quot;--master=http://172.16.7.151:8080&quot;
</code></pre><p>该配置文件同时被kube-apiserver、kube-controller-manager、kube-scheduler、kubelet、kube-proxy使用。</p>
<p>创建apiserver配置文件/etc/kubernetes/apiserver：</p>
<pre><code># vim /etc/kubernetes/apiserver
###
## kubernetes system config
##
## The following values are used to configure the kube-apiserver
##
#
## The address on the local server to listen to.
#KUBE_API_ADDRESS=&quot;--insecure-bind-address=sz-pg-oam-docker-test-001.tendcloud.com&quot;
KUBE_API_ADDRESS=&quot;--advertise-address=172.16.7.151 --bind-address=172.16.7.151 --insecure-bind-address=172.16.7.151&quot;
#
## The port on the local server to listen on.
#KUBE_API_PORT=&quot;--port=8080&quot;
#
## Port minions listen on
#KUBELET_PORT=&quot;--kubelet-port=10250&quot;
#
## Comma separated list of nodes in the etcd cluster
KUBE_ETCD_SERVERS=&quot;--etcd-servers=https://172.16.7.151:2379,https://172.16.7.152:2379,https://172.16.7.153:2379&quot;
#
## Address range to use for services
KUBE_SERVICE_ADDRESSES=&quot;--service-cluster-ip-range=10.254.0.0/16&quot;
#
## default admission control policies
KUBE_ADMISSION_CONTROL=&quot;--admission-control=ServiceAccount,NamespaceLifecycle,NamespaceExists,LimitRanger,ResourceQuota&quot;
#
## Add your own!
KUBE_API_ARGS=&quot;--authorization-mode=RBAC --runtime-config=rbac.authorization.k8s.io/v1beta1 --kubelet-https=true --experimental-bootstrap-token-auth --token-auth-file=/etc/kubernetes/token.csv --service-node-port-range=30000-32767 --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem --client-ca-file=/etc/kubernetes/ssl/ca.pem --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem --etcd-cafile=/etc/kubernetes/ssl/ca.pem --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem --enable-swagger-ui=true --apiserver-count=3 --audit-log-maxage=30 --audit-log-maxbackup=3 --audit-log-maxsize=100 --audit-log-path=/var/lib/audit.log --event-ttl=1h&quot;
</code></pre><p>【说明】：</p>
<ul>
<li>–authorization-mode=RBAC 指定在安全端口使用 RBAC 授权模式，拒绝未通过授权的请求；</li>
<li>kube-scheduler、kube-controller-manager 一般和 kube-apiserver 部署在同一台机器上，它们使用非安全端口和 kube-apiserver通信;</li>
<li>kubelet、kube-proxy、kubectl 部署在其它 Node 节点上，如果通过安全端口访问 kube-apiserver，则必须先通过 TLS 证书认证，再通过 RBAC 授权；</li>
<li>kube-proxy、kubectl 通过在使用的证书里指定相关的 User、Group 来达到通过 RBAC 授权的目的；</li>
<li>如果使用了 kubelet TLS Boostrap 机制，则不能再指定 –kubelet-certificate-authority、–kubelet-client-certificate 和 –kubelet-client-key 选项，否则后续 kube-apiserver 校验 kubelet 证书时出现 ”x509: certificate signed by unknown authority“ 错误；</li>
<li>–admission-control 值必须包含 ServiceAccount；</li>
<li>–bind-address 不能为 127.0.0.1；</li>
<li>runtime-config配置为rbac.authorization.k8s.io/v1beta1，表示运行时的apiVersion；</li>
<li>–service-cluster-ip-range 指定 Service Cluster IP 地址段，该地址段不能路由可达；</li>
<li>缺省情况下 kubernetes 对象保存在 etcd /registry 路径下，可以通过 –etcd-prefix 参数进行调整。</li>
</ul>
<h4 id="启动kube-apiserver"><a href="#启动kube-apiserver" class="headerlink" title="启动kube-apiserver"></a>启动kube-apiserver</h4><pre><code># systemctl daemon-reload
# systemctl enable kube-apiserver
# systemctl start kube-apiserver
</code></pre><p>启动过程中可以观察日志：</p>
<pre><code># tail -f /var/log/message
</code></pre><h3 id="配置和启动-kube-controller-manager"><a href="#配置和启动-kube-controller-manager" class="headerlink" title="配置和启动 kube-controller-manager"></a>配置和启动 kube-controller-manager</h3><h4 id="创建-kube-controller-manager-的service配置文件"><a href="#创建-kube-controller-manager-的service配置文件" class="headerlink" title="创建 kube-controller-manager 的service配置文件"></a>创建 kube-controller-manager 的service配置文件</h4><p>在/usr/lib/systemd/system/下创建kube-controller-manager.service，内容如下：</p>
<pre><code># pwd
/usr/lib/systemd/system
# vim kube-controller-manager.service
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/controller-manager
ExecStart=/usr/local/bin/kube-controller-manager \
            $KUBE_LOGTOSTDERR \
            $KUBE_LOG_LEVEL \
            $KUBE_MASTER \
            $KUBE_CONTROLLER_MANAGER_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
</code></pre><p>创建kube-controller-manager配置文件/etc/kubernetes/controller-manager：</p>
<pre><code># vim /etc/kubernetes/controller-manager
###
# The following values are used to configure the kubernetes controller-manager

# defaults from config and apiserver should be adequate

# Add your own!
KUBE_CONTROLLER_MANAGER_ARGS=&quot;--address=127.0.0.1 --service-cluster-ip-range=10.254.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem  --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem --root-ca-file=/etc/kubernetes/ssl/ca.pem --leader-elect=true&quot;
</code></pre><p>【说明】：</p>
<ul>
<li>–service-cluster-ip-range 参数指定 Cluster 中 Service 的CIDR范围，该网络在各 Node 间必须路由不可达，必须和 kube-apiserver 中的参数一致；</li>
<li>–cluster-signing-* 指定的证书和私钥文件用来签名为 TLS BootStrap 创建的证书和私钥；</li>
<li>–root-ca-file 用来对 kube-apiserver 证书进行校验，指定该参数后，才会在Pod 容器的 ServiceAccount 中放置该 CA 证书文件；</li>
<li><p>–address 值必须为 127.0.0.1，因为当前 kube-apiserver 期望 scheduler 和 controller-manager 在同一台机器，否则：</p>
<pre><code># kubectl get componentstatuses
NAME                 STATUS      MESSAGE                                                                                        ERROR
scheduler            Unhealthy   Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: getsockopt: connection refused   
controller-manager   Healthy     ok                                                                                             
etcd-2               Unhealthy   Get http://172.20.0.113:2379/health: malformed HTTP response &quot;\x15\x03\x01\x00\x02\x02&quot;        
etcd-0               Healthy     {&quot;health&quot;: &quot;true&quot;}                                                                             
etcd-1               Healthy     {&quot;health&quot;: &quot;true&quot;}
</code></pre></li>
</ul>
<p>参考：<a href="https://github.com/kubernetes-incubator/bootkube/issues/64" target="_blank" rel="external">https://github.com/kubernetes-incubator/bootkube/issues/64</a></p>
<h4 id="启动-kube-controller-manager"><a href="#启动-kube-controller-manager" class="headerlink" title="启动 kube-controller-manager"></a>启动 kube-controller-manager</h4><pre><code># systemctl daemon-reload
# systemctl enable kube-controller-manager
# systemctl start kube-controller-manager
</code></pre><h3 id="配置和启动-kube-scheduler"><a href="#配置和启动-kube-scheduler" class="headerlink" title="配置和启动 kube-scheduler"></a>配置和启动 kube-scheduler</h3><h4 id="创建-kube-scheduler的serivce配置文件"><a href="#创建-kube-scheduler的serivce配置文件" class="headerlink" title="创建 kube-scheduler的serivce配置文件"></a>创建 kube-scheduler的serivce配置文件</h4><p>在/usr/lib/systemd/system/下创建kube-scheduler.serivce，内容如下：</p>
<pre><code># pwd
/usr/lib/systemd/system
# vim kube-scheduler.service
[Unit]
Description=Kubernetes Scheduler Plugin
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/scheduler
ExecStart=/usr/local/bin/kube-scheduler \
            $KUBE_LOGTOSTDERR \
            $KUBE_LOG_LEVEL \
            $KUBE_MASTER \
            $KUBE_SCHEDULER_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
</code></pre><p>创建kube-scheduler配置文件/etc/kubernetes/scheduler：</p>
<pre><code># vim /etc/kubernetes/scheduler
###
# kubernetes scheduler config

# default config should be adequate

# Add your own!
KUBE_SCHEDULER_ARGS=&quot;--leader-elect=true --address=127.0.0.1&quot;
</code></pre><p>【说明】：</p>
<ul>
<li>–address 值必须为 127.0.0.1，因为当前 kube-apiserver 期望 scheduler 和 controller-manager 在同一台机器。</li>
</ul>
<h4 id="启动-kube-scheduler"><a href="#启动-kube-scheduler" class="headerlink" title="启动 kube-scheduler"></a>启动 kube-scheduler</h4><pre><code># systemctl daemon-reload
# systemctl enable kube-scheduler
# systemctl start kube-scheduler
</code></pre><h3 id="验证-master-节点功能"><a href="#验证-master-节点功能" class="headerlink" title="验证 master 节点功能"></a>验证 master 节点功能</h3><pre><code># kubectl get componentstatuses
NAME                 STATUS    MESSAGE              ERROR
scheduler            Healthy   ok                   
controller-manager   Healthy   ok                   
etcd-0               Healthy   {&quot;health&quot;: &quot;true&quot;}   
etcd-1               Healthy   {&quot;health&quot;: &quot;true&quot;}   
etcd-2               Healthy   {&quot;health&quot;: &quot;true&quot;}              
</code></pre><h2 id="部署kubernetes-node节点"><a href="#部署kubernetes-node节点" class="headerlink" title="部署kubernetes node节点"></a>部署kubernetes node节点</h2><p>kubernetes node 节点包含如下组件：</p>
<ul>
<li>Docker 1.12.6</li>
<li>Flanneld</li>
<li>kubelet</li>
<li>kube-proxy</li>
</ul>
<h3 id="安装Docker"><a href="#安装Docker" class="headerlink" title="安装Docker"></a>安装Docker</h3><p>参见之前的文章《Docker镜像和容器》。</p>
<h3 id="安装配置Flanneld"><a href="#安装配置Flanneld" class="headerlink" title="安装配置Flanneld"></a>安装配置Flanneld</h3><h4 id="Flannel介绍"><a href="#Flannel介绍" class="headerlink" title="Flannel介绍"></a>Flannel介绍</h4><ul>
<li>Flannel是CoreOS团队针对Kubernetes设计的一个网络规划服务，简单来说，它的功能是让集群中的不同节点主机创建的Docker容器都具有全集群唯一的虚拟IP地址。</li>
<li>在默认的Docker配置中，每个节点上的Docker服务会分别负责所在节点容器的IP分配。这样导致的一个问题是，不同节点上容器可能获得相同的内外IP地址。</li>
<li>Flannel的设计目的就是为集群中的所有节点重新规划IP地址的使用规则，从而使得不同节点上的容器能够获得“同属一个内网”且”不重复的”IP地址，并让属于不同节点上的容器能够直接通过内网IP通信。</li>
<li>Flannel实质上是一种“覆盖网络(overlay network)”，也就是将TCP数据包装在另一种网络包里面进行路由转发和通信，目前已经支持udp、vxlan、host-gw、aws-vpc、gce和alloc路由等数据转发方式，默认的节点间数据通信方式是UDP转发。</li>
</ul>
<p>在Flannel的GitHub页面有如下的一张原理图：<br><img src="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/1.png" alt=""></p>
<ul>
<li>数据从源容器中发出后，经由所在主机的docker0虚拟网卡转发到flannel0虚拟网卡，这是个P2P的虚拟网卡，flanneld服务监听在网卡的另外一端。（Flannel通过ETCD服务维护了一张节点间的路由表）；</li>
<li>源主机的flanneld服务将原本的数据内容UDP封装后根据自己的路由表投递给目的节点的flanneld服务，数据到达后被解包，然后直接进入目的节点的flannel0虚拟网卡，然后被转发到目的主机的docker0虚拟网卡；</li>
<li>最后就像本机容器通信一样由docker0路由到目标容器，这样整个数据包的传递就完成了。</li>
</ul>
<h4 id="安装配置flannel"><a href="#安装配置flannel" class="headerlink" title="安装配置flannel"></a>安装配置flannel</h4><p>我这里使用yum安装，安装的版本是0.7.1。集群中的3台node都需要安装配置flannel。</p>
<pre><code># yum install -y flannel
# rpm -ql flannel
/etc/sysconfig/flanneld
/run/flannel
/usr/bin/flanneld
/usr/bin/flanneld-start
/usr/lib/systemd/system/docker.service.d/flannel.conf
/usr/lib/systemd/system/flanneld.service
/usr/lib/tmpfiles.d/flannel.conf
/usr/libexec/flannel
/usr/libexec/flannel/mk-docker-opts.sh
...
</code></pre><p>修改flannel配置文件：</p>
<pre><code># vim /etc/sysconfig/flanneld 
# Flanneld configuration options  

# etcd url location.  Point this to the server where etcd runs
FLANNEL_ETCD_ENDPOINTS=&quot;https://172.16.7.151:2379,https://172.16.7.152:2379,https://172.16.7.153:2379&quot;

# etcd config key.  This is the configuration key that flannel queries
# For address range assignment
FLANNEL_ETCD_PREFIX=&quot;/kube-centos/network&quot;

# Any additional options that you want to pass
#FLANNEL_OPTIONS=&quot;&quot;
FLANNEL_OPTIONS=&quot;-etcd-cafile=/etc/kubernetes/ssl/ca.pem -etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem -etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem&quot;
</code></pre><p>【说明】：        </p>
<ul>
<li>etcd的地址FLANNEL_ETCD_ENDPOINT</li>
<li>etcd查询的目录，包含docker的IP地址段配置。FLANNEL_ETCD_PREFIX        </li>
</ul>
<h4 id="在etcd中初始化flannel网络数据"><a href="#在etcd中初始化flannel网络数据" class="headerlink" title="在etcd中初始化flannel网络数据"></a>在etcd中初始化flannel网络数据</h4><p>多个node上的Flanneld依赖一个etcd cluster来做集中配置服务，etcd保证了所有node上flanned所看到的配置是一致的。同时每个node上的flanned监听etcd上的数据变化，实时感知集群中node的变化。</p>
<p>执行下面的命令为docker分配IP地址段：</p>
<pre><code># etcdctl --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/kubernetes/ssl/kubernetes.pem --key-file=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints &quot;https://172.16.7.151:2379&quot; mkdir /kube-centos/network
# etcdctl --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/kubernetes/ssl/kubernetes.pem --key-file=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints &quot;https://172.16.7.151:2379&quot; mk /kube-centos/network/config &apos;{&quot;Network&quot;: &quot;172.30.0.0/16&quot;, &quot;SubnetLen&quot;: 24, &quot;Backend&quot;: { &quot;Type&quot;: &quot;vxlan&quot; }}&apos;
{&quot;Network&quot;: &quot;172.30.0.0/16&quot;, &quot;SubnetLen&quot;: 24, &quot;Backend&quot;: { &quot;Type&quot;: &quot;vxlan&quot; }}
</code></pre><h4 id="启动flannel"><a href="#启动flannel" class="headerlink" title="启动flannel"></a>启动flannel</h4><p>集群中的3台node都启动flannel：</p>
<pre><code># systemctl daemon-reload
# systemctl start flanneld
</code></pre><p>启动完成后，会在/run/flannel/目录下生成两个文件，以node1为例：</p>
<pre><code># ls /run/flannel/         
docker  subnet.env
# cd /run/flannel/
[root@node1 flannel]# cat docker 
DOCKER_OPT_BIP=&quot;--bip=172.30.51.1/24&quot;
DOCKER_OPT_IPMASQ=&quot;--ip-masq=true&quot;
DOCKER_OPT_MTU=&quot;--mtu=1450&quot;
DOCKER_NETWORK_OPTIONS=&quot; --bip=172.30.51.1/24 --ip-masq=true --mtu=1450&quot;
# cat subnet.env 
FLANNEL_NETWORK=172.30.0.0/16
FLANNEL_SUBNET=172.30.51.1/24
FLANNEL_MTU=1450
FLANNEL_IPMASQ=false
</code></pre><p>现在查询etcd中的内容可以看到：</p>
<pre><code># etcdctl --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/kubernetes/ssl/kubernetes.pem --key-file=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints &quot;https://172.16.7.151:2379&quot; ls /kube-centos/network/subnets
/kube-centos/network/subnets/172.30.51.0-24
/kube-centos/network/subnets/172.30.29.0-24
/kube-centos/network/subnets/172.30.19.0-24
</code></pre><p>设置docker0网桥的IP地址(集群中node节点都需要设置)：</p>
<pre><code># source /run/flannel/subnet.env
# ifconfig docker0 $FLANNEL_SUBNET
</code></pre><p>这样docker0和flannel网桥会在同一个子网中，查看node1主机网卡：</p>
<pre><code>docker0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt;  mtu 1500
        inet 172.30.51.1  netmask 255.255.255.0  broadcast 172.30.51.255
flannel.1: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1450
        inet 172.30.51.0  netmask 255.255.255.255  broadcast 0.0.0.0
</code></pre><p>重启docker：</p>
<pre><code># systemctl restart docker
</code></pre><p><strong>【注意】:经过测试，docker 17.06.1-ce版本重启后，docker0网桥又会被重置为172.17.0.1，docker 1.12.6版本测试是不会有问题的。</strong></p>
<p>如果想重新设置flannel，先停止flanneld，清理etcd里的数据，然后 ifconfig flannel.1 down，然后启动flanneld，会重新生成子网，并up flannel.1网桥设备。</p>
<h4 id="测试跨主机容器通信"><a href="#测试跨主机容器通信" class="headerlink" title="测试跨主机容器通信"></a>测试跨主机容器通信</h4><p>分别在node1和node2上启动一个容器，然后ping对方容器的地址：</p>
<pre><code>[root@node1 flannel]# docker run -i -t centos /bin/bash
[root@38be151deb71 /]# yum install net-tools -y
[root@38be151deb71 /]# ifconfig
eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1450
        inet 172.30.51.2  netmask 255.255.255.0  broadcast 0.0.0.0

[root@node2 flannel]# docker run -i -t centos /bin/bash
[root@90e85c215fda /]# yum install net-tools -y
[root@90e85c215fda /]# ifconfig
eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1450
        inet 172.30.29.2  netmask 255.255.255.0  broadcast 0.0.0.0
[root@90e85c215fda /]# ping 172.16.51.2  
PING 172.16.51.2 (172.16.51.2) 56(84) bytes of data.
64 bytes from 172.16.51.2: icmp_seq=1 ttl=254 time=1.00 ms
64 bytes from 172.16.51.2: icmp_seq=2 ttl=254 time=1.29 ms
</code></pre><h4 id="补充：下载二进制包安装flannel"><a href="#补充：下载二进制包安装flannel" class="headerlink" title="补充：下载二进制包安装flannel"></a>补充：下载二进制包安装flannel</h4><p>从官网 <a href="https://github.com/coreos/flannel/releases" target="_blank" rel="external">https://github.com/coreos/flannel/releases</a> 下载的flannel release 0.7.1，并将下载的文件上传到服务器的/opt/flannel/目录下。</p>
<pre><code># mkdir flannel
# cd flannel/
# tar xf flannel-v0.7.1-linux-amd64.tar  
# ls
flanneld  flannel-v0.7.1-linux-amd64.tar  mk-docker-opts.sh  README.md
</code></pre><p>mk-docker-opts.sh是用来Generate Docker daemon options based on flannel env file。<br>执行 ./mk-docker-opts.sh -i 将会生成如下两个文件环境变量文件。</p>
<p>Flannel的<a href="https://github.com/coreos/flannel/blob/master/Documentation/running.md" target="_blank" rel="external">文档</a>中有写Docker Integration：<br>Docker daemon accepts –bip argument to configure the subnet of the docker0 bridge. It also accepts –mtu to set the MTU for docker0 and veth devices that it will be creating.<br>Because flannel writes out the acquired subnet and MTU values into a file, the script starting Docker can source in the values and pass them to Docker daemon:</p>
<pre><code>source /run/flannel/subnet.env
docker daemon --bip=${FLANNEL_SUBNET} --mtu=${FLANNEL_MTU} &amp;
</code></pre><p>Systemd users can use EnvironmentFile directive in the .service file to pull in /run/flannel/subnet.env</p>
<h3 id="安装和配置-kubelet"><a href="#安装和配置-kubelet" class="headerlink" title="安装和配置 kubelet"></a>安装和配置 kubelet</h3><p>kubelet 启动时向 kube-apiserver 发送 TLS bootstrapping 请求，需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper cluster 角色(role)， 然后 kubelet 才能有权限创建认证请求(certificate signing requests)：</p>
<pre><code># cd /etc/kubernetes
[root@node1 kubernetes]# kubectl create clusterrolebinding kubelet-bootstrap \
&gt; --clusterrole=system:node-bootstrapper \
&gt; --user=kubelet-bootstrap
clusterrolebinding &quot;kubelet-bootstrap&quot; created  
</code></pre><p><strong>【注意】：以上这步只需要在kubernetes node集群中的一台执行一次就可以了。</strong><br>【说明】：</p>
<ul>
<li>–user=kubelet-bootstrap 是在 /etc/kubernetes/token.csv 文件中指定的用户名，同时也写入了/etc/kubernetes/bootstrap.kubeconfig 文件。</li>
</ul>
<h4 id="下载最新的-kubelet-和-kube-proxy-二进制文件"><a href="#下载最新的-kubelet-和-kube-proxy-二进制文件" class="headerlink" title="下载最新的 kubelet 和 kube-proxy 二进制文件"></a>下载最新的 kubelet 和 kube-proxy 二进制文件</h4><p>这个在之前安装kubernetes master时已经下载好了二进制文件，只需要复制到相应目录即可。</p>
<pre><code>[root@node1 kubernetes]# cd /opt/kubernetes/server/kubernetes/server/bin/
[root@node1 bin]# scp -p kubelet root@172.16.7.152:/usr/local/bin/
[root@node1 bin]# scp -p kube-proxy root@172.16.7.152:/usr/local/bin/
[root@node1 bin]# scp -p kubelet root@172.16.7.153:/usr/local/bin/
[root@node1 bin]# scp -p kube-proxy root@172.16.7.153:/usr/local/bin/
</code></pre><h4 id="配置kubelet"><a href="#配置kubelet" class="headerlink" title="配置kubelet"></a>配置kubelet</h4><p>以下操作需要在集群的kubernetes node节点上都要运行，下面以node1服务器为例：</p>
<p><strong>1.创建 kubelet 的service配置文件：</strong><br>在/usr/lib/systemd/system/下创建文件kubelet.serivce：</p>
<pre><code># vim /usr/lib/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/kubelet
ExecStart=/usr/local/bin/kubelet \
            $KUBE_LOGTOSTDERR \
            $KUBE_LOG_LEVEL \
            $KUBELET_API_SERVER \
            $KUBELET_ADDRESS \
            $KUBELET_PORT \
            $KUBELET_HOSTNAME \
            $KUBE_ALLOW_PRIV \
            $KUBELET_POD_INFRA_CONTAINER \
            $KUBELET_ARGS
Restart=on-failure

[Install]
WantedBy=multi-user.target
</code></pre><p><strong>2.创建kubelet配置文件</strong><br>创建kubelet工作目录（必须创建，否则kubelet启动不了）：</p>
<pre><code># mkdir /var/lib/kubelet
</code></pre><p>创建kubelet配置文件：</p>
<pre><code># vim /etc/kubernetes/kubelet
###
## kubernetes kubelet (minion) config
#
## The address for the info server to serve on (set to 0.0.0.0 or &quot;&quot; for all interfaces)
KUBELET_ADDRESS=&quot;--address=172.16.7.151&quot;
#
## The port for the info server to serve on
#KUBELET_PORT=&quot;--port=10250&quot;
#
## You may leave this blank to use the actual hostname
KUBELET_HOSTNAME=&quot;--hostname-override=172.16.7.151&quot;
#
## location of the api-server
KUBELET_API_SERVER=&quot;--api-servers=http://172.16.7.151:8080&quot;
#
## pod infrastructure container
#KUBELET_POD_INFRA_CONTAINER=&quot;--pod-infra-container-image=sz-pg-oam-docker-hub-001.tendcloud.com/library/pod-infrastructure:rhel7&quot;
KUBELET_POD_INFRA_CONTAINER=&quot;--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure&quot;
#
## Add your own!
KUBELET_ARGS=&quot;--cgroup-driver=systemd --cluster-dns=10.254.0.2 --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig --require-kubeconfig --cert-dir=/etc/kubernetes/ssl --cluster-domain=cluster.local. --hairpin-mode promiscuous-bridge --serialize-image-pulls=false&quot;
</code></pre><p>【注意】：将配置文件中的IP地址更改为你的每台node节点的IP地址（除了–api-servers=<a href="http://172.16.7.151:8080这个ip地址是不用改的）。" target="_blank" rel="external">http://172.16.7.151:8080这个ip地址是不用改的）。</a><br>【说明】：</p>
<ul>
<li>–address 不能设置为 127.0.0.1，否则后续 Pods 访问 kubelet 的 API 接口时会失败，因为 Pods 访问的 127.0.0.1 指向自己而不是 kubelet；<br>如果设置了 –hostname-override 选项，则 kube-proxy 也需要设置该选项，否则会出现找不到 Node 的情况；</li>
<li>KUBELET_POD_INFRA_CONTAINER=”–pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure”，这个是一个基础容器，每一个Pod启动的时候都会启动一个这样的容器。如果你的本地没有这个镜像，kubelet会连接外网把这个镜像下载下来。最开始的时候是在Google的registry上，因此国内因为GFW都下载不了导致Pod运行不起来。现在每个版本的Kubernetes都把这个镜像打包，你可以提前传到自己的registry上，然后再用这个参数指定。</li>
<li>–experimental-bootstrap-kubeconfig 指向 bootstrap kubeconfig 文件，kubelet 使用该文件中的用户名和 token 向 kube-apiserver 发送 TLS Bootstrapping 请求；</li>
<li>管理员通过了 CSR 请求后，kubelet 自动在 –cert-dir 目录创建证书和私钥文件(kubelet-client.crt 和 kubelet-client.key)，然后写入 –kubeconfig 文件；</li>
<li>建议在 –kubeconfig 配置文件中指定 kube-apiserver 地址，如果未指定 –api-servers 选项，则必须指定 –require-kubeconfig 选项后才从配置文件中读取 kube-apiserver 的地址，否则 kubelet 启动后将找不到 kube-apiserver (日志中提示未找到 API Server），kubectl get nodes 不会返回对应的 Node 信息;</li>
<li>–cluster-dns 指定 kubedns 的 Service IP(可以先分配，后续创建 kubedns 服务时指定该 IP)，–cluster-domain 指定域名后缀，这两个参数同时指定后才会生效。</li>
</ul>
<h4 id="启动kubelet"><a href="#启动kubelet" class="headerlink" title="启动kubelet"></a>启动kubelet</h4><pre><code># systemctl daemon-reload
# systemctl enable kubelet
# systemctl start kubelet
</code></pre><h4 id="通过-kubelet-的-TLS-证书请求"><a href="#通过-kubelet-的-TLS-证书请求" class="headerlink" title="通过 kubelet 的 TLS 证书请求"></a>通过 kubelet 的 TLS 证书请求</h4><p>kubelet 首次启动时向 kube-apiserver 发送证书签名请求，必须通过后 kubernetes 系统才会将该 Node 加入到集群。</p>
<p>1.查看未授权的 CSR 请求</p>
<pre><code># kubectl get csr 
NAME        AGE       REQUESTOR           CONDITION
csr-fv3bj   49s       kubelet-bootstrap   Pending
</code></pre><p>2.通过 CSR 请求</p>
<pre><code># kubectl certificate approve csr-fv3bj
certificatesigningrequest &quot;csr-fv3bj&quot; approved
[root@node1 kubernetes]# kubectl get csr
NAME        AGE       REQUESTOR           CONDITION
csr-fv3bj   42m       kubelet-bootstrap   Approved,Issued
# kubectl get nodes
NAME           STATUS    AGE       VERSION
172.16.7.151   Ready     18s       v1.6.0
</code></pre><p>3.查看自动生成的 kubelet kubeconfig 文件和公私钥</p>
<pre><code>[root@node1 kubernetes]# ls -l /etc/kubernetes/kubelet.kubeconfig
-rw-------. 1 root root 2215 Sep 13 09:04 /etc/kubernetes/kubelet.kubeconfig
[root@node1 kubernetes]# ls -l /etc/kubernetes/ssl/kubelet*
-rw-r--r--. 1 root root 1046 Sep 13 09:04 /etc/kubernetes/ssl/kubelet-client.crt
-rw-------. 1 root root  227 Sep 13 09:02 /etc/kubernetes/ssl/kubelet-client.key
-rw-r--r--. 1 root root 1111 Sep 13 09:04 /etc/kubernetes/ssl/kubelet.crt
-rw-------. 1 root root 1675 Sep 13 09:04 /etc/kubernetes/ssl/kubelet.key
</code></pre><p>在集群中其它的kubernetes node节点上操作完成后，查看集群kubernetes node情况如下：</p>
<pre><code># kubectl get csr
NAME        AGE       REQUESTOR           CONDITION
csr-5n72m   3m        kubelet-bootstrap   Approved,Issued
csr-clwzj   16m       kubelet-bootstrap   Approved,Issued
csr-fv3bj   4h        kubelet-bootstrap   Approved,Issued
# kubectl get nodes
NAME           STATUS    AGE       VERSION
172.16.7.151   Ready     4h        v1.6.0
172.16.7.152   Ready     6m        v1.6.0
172.16.7.153   Ready     12s       v1.6.0
</code></pre><p>【问题】：切记每台node节点上的kubelet配置文件/etc/kubernetes/kubelet中的ip地址要改正确，否则会出现加入不了的情况。我在将node1节点的/etc/kubernetes/kubelet远程复制到node2节点上，没有修改ip，直接启动了，配置文件中写的ip地址是node1的ip地址，这就造成了node2节点并没有加入进来。采取的恢复操作是：</p>
<pre><code>[root@node2 ~]# systemctl stop kubelet
[root@node2 ~]# cd /etc/kubernetes
[root@node2 kubernetes]# rm -f kubelet.kubeconfig
[root@node2 kubernetes]# rm -rf ~/.kube/cache
# 修改/etc/kubernetes/kubelet中的ip地址
[root@node2 kubernetes]# vim /etc/kubernetes/kubelet
[root@node2 ~]# systemctl start kubelet
</code></pre><p>这样，再次启动kubelet时，kube-apiserver才收到证书签名请求。</p>
<h3 id="配置-kube-proxy"><a href="#配置-kube-proxy" class="headerlink" title="配置 kube-proxy"></a>配置 kube-proxy</h3><p>上面已经把kube-proxy复制到了kubernetes node节点的/usr/local/bin/目录下了，下面开始做配置。每台kubernetes node节点都需要做如下的操作。</p>
<p><strong>1.创建 kube-proxy 的service配置文件</strong><br>在/usr/lib/systemd/system/目录下创建kube-proxy.service：</p>
<pre><code># vim /usr/lib/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/proxy
ExecStart=/usr/local/bin/kube-proxy \
            $KUBE_LOGTOSTDERR \
            $KUBE_LOG_LEVEL \
            $KUBE_MASTER \
            $KUBE_PROXY_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
</code></pre><p><strong>2.创建kube-proxy配置文件/etc/kubernetes/proxy</strong><br>【注意】：需要修改每台kubernetes node的ip地址。以下以node1主机为例：</p>
<pre><code># vim /etc/kubernetes/proxy
###
# kubernetes proxy config

# default config should be adequate

# Add your own!
KUBE_PROXY_ARGS=&quot;--bind-address=172.16.7.151 --hostname-override=172.16.7.151 --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig --cluster-cidr=10.254.0.0/16&quot;
</code></pre><p>【说明】：</p>
<ul>
<li>–hostname-override 参数值必须与 kubelet 的值一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 iptables 规则；</li>
<li>kube-proxy 根据 –cluster-cidr 判断集群内部和外部流量，指定 –cluster-cidr 或 –masquerade-all 选项后 kube-proxy 才会对访问 Service IP 的请求做 SNAT；</li>
<li>–kubeconfig 指定的配置文件嵌入了 kube-apiserver 的地址、用户名、证书、秘钥等请求和认证信息；</li>
<li>预定义的 RoleBinding cluster-admin 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限。</li>
</ul>
<p><strong>3.启动 kube-proxy</strong></p>
<pre><code># systemctl daemon-reload
# systemctl enable kube-proxy
# systemctl start kube-proxy
</code></pre><h3 id="验证测试"><a href="#验证测试" class="headerlink" title="验证测试"></a>验证测试</h3><p>创建一个niginx的service试一下集群是否可用。</p>
<pre><code># kubectl run nginx --replicas=2 --labels=&quot;run=load-balancer-example&quot; --image=docker.io/nginx:latest --port=80
deployment &quot;nginx&quot; created                   
# kubectl expose deployment nginx --type=NodePort --name=example-service     
service &quot;example-service&quot; exposed
# kubectl describe svc example-service
Name:                   example-service
Namespace:              default
Labels:                 run=load-balancer-example
Annotations:            &lt;none&gt;
Selector:               run=load-balancer-example
Type:                   NodePort
IP:                     10.254.67.61
Port:                   &lt;unset&gt; 80/TCP
NodePort:               &lt;unset&gt; 32201/TCP
Endpoints:              172.30.32.2:80,172.30.87.2:80
Session Affinity:       None
Events:                 &lt;none&gt;

# kubectl get all
NAME                        READY     STATUS    RESTARTS   AGE
po/nginx-1931613429-nlsj1   1/1       Running   0          5m
po/nginx-1931613429-xr7zk   1/1       Running   0          5m

NAME                  CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
svc/example-service   10.254.67.61   &lt;nodes&gt;       80:32201/TCP   1m
svc/kubernetes        10.254.0.1     &lt;none&gt;        443/TCP        5h

NAME           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deploy/nginx   2         2         2            2           5m

NAME                  DESIRED   CURRENT   READY     AGE
rs/nginx-1931613429   2         2         2         5m

# curl &quot;10.254.67.61:80&quot; 
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre><p>浏览器输入172.16.7.151:32201或172.16.7.152:32201或者172.16.7.153:32201都可以得到nginx的页面。<br><img src="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/Kubernetes/2.png" alt=""></p>
<p>查看运行的容器（在node1和node2上分别运行了一个pod）：</p>
<pre><code># docker ps
CONTAINER ID        IMAGE                                                                                     COMMAND                  CREATED             STATUS              PORTS               NAMES
7d2ef8e34e43        docker.io/nginx@sha256:fc6d2ef47e674a9ffb718b7ac361ec4e421e3a0ef2c93df79abbe4e9ffb5fa08   &quot;nginx -g &apos;daemon off&quot;   40 minutes ago      Up 40 minutes                           k8s_nginx_nginx-1931613429-xr7zk_default_c628f12f-9912-11e7-9acc-005056b7609a_0
5bbb98fba623        registry.access.redhat.com/rhel7/pod-infrastructure                                       &quot;/usr/bin/pod&quot;           42 minutes ago      Up 42 minutes                           k8s_POD_nginx-1931613429-xr7zk_default_c628f12f-9912-11e7-9acc-005056b7609a_0
</code></pre><p>如果想删除刚才创建的deployment：</p>
<pre><code># kubectl get deployments
NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx     2         2         2            0           2m
# kubectl delete deployment nginx
deployment &quot;nginx&quot; deleted
</code></pre><h2 id="安装和配置-kube-dns-插件"><a href="#安装和配置-kube-dns-插件" class="headerlink" title="安装和配置 kube-dns 插件"></a>安装和配置 kube-dns 插件</h2><h3 id="kube-dns是什么"><a href="#kube-dns是什么" class="headerlink" title="kube-dns是什么"></a>kube-dns是什么</h3><p>刚才在上一步中创建了个Nginx deployment，得到了两个运行nginx服务的Pod。待Pod运行之后查看一下它们的IP，并在k8s集群内通过podIP和containerPort来访问Nginx服务。<br>获取Pod IP：</p>
<pre><code># kubectl get pod -o yaml -l run=load-balancer-example|grep podIP 
    podIP: 172.30.32.2
    podIP: 172.30.87.2
</code></pre><p>然后在Kubernetes集群的任一节点上就可以通过podIP在k8s集群内访问Nginx服务了。</p>
<pre><code># curl &quot;172.30.32.2:80&quot;
</code></pre><p>但是这样存在几个问题：</p>
<ul>
<li>每次收到获取podIP太扯了，总不能每次都要手动改程序或者配置才能访问服务吧，要怎么提前知道podIP呢？</li>
<li>Pod在运行中可能会重建，Pod的IP地址会随着Pod的重启而变化,并 不建议直接拿Pod的IP来交互</li>
<li>如何在多个Pod中实现负载均衡嘞？</li>
</ul>
<p>使用k8s Service就可以解决。Service为一组Pod(通过labels来选择)提供一个统一的入口，并为它们提供负载均衡和自动服务发现。<br>所以紧接着就创建了个service：</p>
<pre><code># kubectl expose deployment nginx --type=NodePort --name=example-service
</code></pre><p>创建之后，仍需要获取Service的Cluster-IP，再结合Port访问Nginx服务。<br>获取IP：</p>
<pre><code># kubectl get service example-service
NAME              CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
example-service   10.254.67.61   &lt;nodes&gt;       80:32201/TCP   1h
</code></pre><p>在集群内访问Service：</p>
<pre><code># curl &quot;10.254.67.61:80&quot; 
</code></pre><p>而在Kubernetes cluster外面，则只能通过<a href="http://node-ip:32201来访问。" target="_blank" rel="external">http://node-ip:32201来访问。</a><br>虽然Service解决了Pod的服务发现和负载均衡问题，但存在着类似的问题：不提前知道Service的IP，还是需要改程序或配置啊。kube-dns就是用来解决上面这个问题的。<br>kube-dns可以解决Service的发现问题，k8s将Service的名称当做域名注册到kube-dns中，通过Service的名称就可以访问其提供的服务。也就是说其他应用能够直接使用服务的名字，不需要关心它实际的 ip 地址，中间的转换能够自动完成。名字和 ip 之间的转换就是 DNS 系统的功能。<br>kubu-dns 服务不是独立的系统服务，而是一种 addon ，作为插件来安装的，不是 kubernetes 集群必须的（但是非常推荐安装）。可以把它看做运行在集群上的应用，只不过这个应用比较特殊而已。</p>
<h3 id="安装配置kube-dns"><a href="#安装配置kube-dns" class="headerlink" title="安装配置kube-dns"></a>安装配置kube-dns</h3><p>官方的yaml文件目录：<a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns。" target="_blank" rel="external">https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns。</a><br>kube-dns 有两种配置方式，在 1.3 之前使用 etcd + kube2sky + skydns 的方式，在 1.3 之后可以使用 kubedns + dnsmasq 的方式。<br>该插件直接使用kubernetes部署，实际上kube-dns插件只是运行在kube-system命名空间下的Pod，完全可以手动创建它。官方的配置文件中包含以下镜像：</p>
<pre><code>gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.1
gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.1
gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.1
</code></pre><h4 id="下载yaml文件"><a href="#下载yaml文件" class="headerlink" title="下载yaml文件"></a>下载yaml文件</h4><p>从 <a href="https://github.com/rootsongjc/follow-me-install-kubernetes-cluster/tree/master/manifests/kubedns" target="_blank" rel="external">https://github.com/rootsongjc/follow-me-install-kubernetes-cluster/tree/master/manifests/kubedns</a> 下载 kubedns-cm.yaml、kubedns-sa.yaml、kubedns-controller.yaml和kubedns-svc.yaml这4个文件下来，并上传到/opt/kube-dns/目录下。</p>
<pre><code># mkdir /opt/kube-dns
# cd /opt/kube-dns/
# ls kubedns-*
kubedns-cm.yaml  kubedns-controller.yaml  kubedns-sa.yaml  kubedns-svc.yaml
</code></pre><p>修改kubedns-controller.yaml文件，将其中的镜像地址改为时速云的地址：</p>
<pre><code>index.tenxcloud.com/jimmy/k8s-dns-dnsmasq-nanny-amd64:1.14.1
index.tenxcloud.com/jimmy/k8s-dns-kube-dns-amd64:1.14.1
index.tenxcloud.com/jimmy/k8s-dns-sidecar-amd64:1.14.1
</code></pre><ul>
<li>kubeDNS：提供了原来 kube2sky + etcd + skyDNS 的功能，可以单独对外提供 DNS 查询服务</li>
<li>dnsmasq： 一个轻量级的 DNS 服务软件，可以提供 DNS 缓存功能。kubeDNS 模式下，dnsmasq 在内存中预留一块大小（默认是 1G）的地方，保存当前最常用的 DNS 查询记录，如果缓存中没有要查找的记录，它会到 kubeDNS 中查询，并把结果缓存起来。</li>
</ul>
<h4 id="系统预定义的-RoleBinding"><a href="#系统预定义的-RoleBinding" class="headerlink" title="系统预定义的 RoleBinding"></a>系统预定义的 RoleBinding</h4><p>预定义的 RoleBinding system:kube-dns 将 kube-system 命名空间的 kube-dns ServiceAccount 与 system:kube-dns Role 绑定， 该 Role 具有访问 kube-apiserver DNS 相关 API 的权限。</p>
<pre><code># kubectl get clusterrolebindings system:kube-dns -o yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: &quot;true&quot;
  creationTimestamp: 2017-09-14T00:46:08Z
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kube-dns
  resourceVersion: &quot;56&quot;
  selfLink: /apis/rbac.authorization.k8s.io/v1beta1/clusterrolebindingssystem%3Akube-dns
  uid: 18fa2aff-98e6-11e7-a153-005056b7609a
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kube-dns
subjects:
- kind: ServiceAccount
  name: kube-dns
  namespace: kube-system
</code></pre><p>kubedns-controller.yaml 中定义的 Pods 时使用了 kubedns-sa.yaml 文件定义的 kube-dns ServiceAccount，所以具有访问 kube-apiserver DNS 相关 API 的权限。</p>
<h4 id="配置-kube-dns-ServiceAccount"><a href="#配置-kube-dns-ServiceAccount" class="headerlink" title="配置 kube-dns ServiceAccount"></a>配置 kube-dns ServiceAccount</h4><p>无需修改。</p>
<h4 id="配置-kube-dns-服务"><a href="#配置-kube-dns-服务" class="headerlink" title="配置 kube-dns 服务"></a>配置 kube-dns 服务</h4><pre><code># diff kubedns-svc.yaml.base kubedns-svc.yaml
30c30
&lt;   clusterIP: __PILLAR__DNS__SERVER__
---
&gt;   clusterIP: 10.254.0.2
</code></pre><p>【说明】：</p>
<ul>
<li>spec.clusterIP = 10.254.0.2，即明确指定了 kube-dns Service IP，这个 IP 需要和 kubelet 的 –cluster-dns 参数值一致。</li>
</ul>
<h4 id="配置-kube-dns-Deployment"><a href="#配置-kube-dns-Deployment" class="headerlink" title="配置 kube-dns Deployment"></a>配置 kube-dns Deployment</h4><pre><code># diff kubedns-controller.yaml.base kubedns-controller.yaml
</code></pre><p>【说明】：</p>
<ul>
<li>使用系统已经做了 RoleBinding 的 kube-dns ServiceAccount，该账户具有访问 kube-apiserver DNS 相关 API 的权限。</li>
</ul>
<h4 id="执行所有定义文件"><a href="#执行所有定义文件" class="headerlink" title="执行所有定义文件"></a>执行所有定义文件</h4><pre><code># pwd
/opt/kube-dns
# ls
kubedns-cm.yaml  kubedns-controller.yaml  kubedns-sa.yaml  kubedns-svc.yaml
# kubectl create -f .
configmap &quot;kube-dns&quot; created
deployment &quot;kube-dns&quot; created
serviceaccount &quot;kube-dns&quot; created
service &quot;kube-dns&quot; created
</code></pre><p>在3台node节点上查看生成的kube-dns相关pod和container：</p>
<pre><code>[root@node2 ~]# docker ps
CONTAINER ID        IMAGE                                                                                                                           COMMAND                  CREATED             STATUS              PORTS               NAMES
9b1dbfde7eac        index.tenxcloud.com/jimmy/k8s-dns-sidecar-amd64@sha256:947271f3e08b1fd61c4b26478f08d3a8f10bbca90d4dec067e3b33be08066970         &quot;/sidecar --v=2 --log&quot;   4 hours ago         Up 4 hours                              k8s_sidecar_kube-dns-351402727-6vnsj_kube-system_efb96c05-9928-11e7-9acc-005056b7609a_0
a455dc0a9b55        index.tenxcloud.com/jimmy/k8s-dns-dnsmasq-nanny-amd64@sha256:b253876345427dbd626b145897be51d87bfd535e2cd5d7d166deb97ea37701f8   &quot;/dnsmasq-nanny -v=2 &quot;   4 hours ago         Up 4 hours                              k8s_dnsmasq_kube-dns-351402727-6vnsj_kube-system_efb96c05-9928-11e7-9acc-005056b7609a_0
7f18c10c8d60        index.tenxcloud.com/jimmy/k8s-dns-kube-dns-amd64@sha256:94426e872d1a4a0cf88e6c5cd928a1acbe1687871ae5fe91ed751593aa6052d3        &quot;/kube-dns --domain=c&quot;   4 hours ago         Up 4 hours                              k8s_kubedns_kube-dns-351402727-6vnsj_kube-system_efb96c05-9928-11e7-9acc-005056b7609a_0
a6feb213296b        registry.access.redhat.com/rhel7/pod-infrastructure                                                                             &quot;/usr/bin/pod&quot;           4 hours ago         Up 4 hours                              k8s_POD_kube-dns-351402727-6vnsj_kube-system_efb96c05-9928-11e7-9acc-005056b7609a_0
</code></pre><h3 id="检查-kube-dns-功能"><a href="#检查-kube-dns-功能" class="headerlink" title="检查 kube-dns 功能"></a>检查 kube-dns 功能</h3><p>上面是通过 kubectl run 来启动了第一个Pod，但是并不支持所有的功能。使用kubectl run在设定很复杂的时候需要非常长的一条语句，敲半天也很容易出错，也没法保存，在碰到转义字符的时候也经常会很抓狂，所以更多场景下会使用yaml或者json文件，而使用kubectl create或者delete就可以利用这些yaml文件。通过 kubectl create -f file.yaml 来创建资源。kubectl run 并不是直接创建一个Pod，而是先创建一个Deployment资源 (replicas=1)，再由Deployment来自动创建Pod。</p>
<p>新建一个 Deployment：</p>
<pre><code>[root@node1 kube-dns]# vim my-nginx.yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: my-nginx
spec:
  replicas: 2
  template:
    metadata:
      labels:
        run: my-nginx
    spec:
      containers:
      - name: my-nginx
        image: docker.io/nginx:latest                                  
        ports:
        - containerPort: 80
# kubectl create -f my-nginx.yaml
deployment &quot;my-nginx&quot; created
</code></pre><p>Export 该 Deployment，生成 my-nginx 服务:</p>
<pre><code># kubectl expose deploy my-nginx 
service &quot;my-nginx&quot; exposed
# kubectl get services --all-namespaces |grep my-nginx
default       my-nginx          10.254.34.181   &lt;none&gt;        80/TCP          26s
</code></pre><p>创建另一个 Pod，查看 /etc/resolv.conf 是否包含 kubelet 配置的 –cluster-dns 和 –cluster-domain，是否能够将服务my-nginx 解析到 Cluster IP 10.254.34.181。</p>
<pre><code>[root@node1 kube-dns]# vim dns-test-busybox.yaml
apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: default
spec:
  containers:
  - image: busybox
    command:
      - sleep
      - &quot;3600&quot;
    imagePullPolicy: IfNotPresent
    name: busybox
  restartPolicy: Always
[root@node1 kube-dns]# kubectl create -f dns-test-busybox.yaml
pod &quot;busybox&quot; created
[root@node1 kube-dns]# kubectl exec -ti busybox -- nslookup kubernetes.default
Server:    10.254.0.2
Address 1: 10.254.0.2 kube-dns.kube-system.svc.cluster.local

Name:      kubernetes.default
Address 1: 10.254.0.1 kubernetes.default.svc.cluster.local

kubectl exec -ti busybox -- ping my-nginx
PING my-nginx (10.254.34.181): 56 data bytes

kubectl exec -ti busybox -- ping kubernetes
PING kubernetes (10.254.0.1): 56 data bytes

kubectl exec -ti busybox -- ping kube-dns.kube-system.svc.cluster.local
PING kube-dns.kube-system.svc.cluster.local (10.254.0.2): 56 data bytes
</code></pre><p>从结果来看，service名称可以正常解析。<br>另外，使用kubernetes的时候建议不要再用docker命令操作。</p>

      
    </div>

    <div>
      
        
      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Kubernetes/" rel="tag"><i class="fa fa-tag"></i>Kubernetes</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/09/08/企业级Docker-Registry-——-Harbor搭建和使用/" rel="next" title="企业级Docker Registry —— Harbor搭建和使用">
                <i class="fa fa-chevron-left"></i> 企业级Docker Registry —— Harbor搭建和使用
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/09/19/Kubernetes1-6集群上-开启了TLS-安装Dashboard/" rel="prev" title="Kubernetes1.6集群上(开启了TLS)安装Dashboard">
                Kubernetes1.6集群上(开启了TLS)安装Dashboard <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


      <div id="lv-container" data-id="city" data-uid="MTAyMC8yODkyNi81NDk1"></div>

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.jpg"
               alt="Zhao Jiankai" />
          <p class="site-author-name" itemprop="name">Zhao Jiankai</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">66</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">19</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">23</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/jkzhao" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/3566507667/profile?rightmod=1&wvr=6&mod=personinfo&is_all=1" target="_blank" title="微博">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  微博
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              友情链接
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.importnew.com/" title="ImportNew" target="_blank">ImportNew</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Kubernetes简介"><span class="nav-number">1.</span> <span class="nav-text">Kubernetes简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#环境信息"><span class="nav-number">2.</span> <span class="nav-text">环境信息</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#创建TLS加密通信的证书和密钥"><span class="nav-number">3.</span> <span class="nav-text">创建TLS加密通信的证书和密钥</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#安装CFSSL"><span class="nav-number">3.1.</span> <span class="nav-text">安装CFSSL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#创建CA"><span class="nav-number">3.2.</span> <span class="nav-text">创建CA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#创建-Kubernetes-证书"><span class="nav-number">3.3.</span> <span class="nav-text">创建 Kubernetes 证书</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#创建-Admin-证书"><span class="nav-number">3.4.</span> <span class="nav-text">创建 Admin 证书</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#创建-Kube-Proxy-证书"><span class="nav-number">3.5.</span> <span class="nav-text">创建 Kube-Proxy 证书</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#校验证书"><span class="nav-number">3.6.</span> <span class="nav-text">校验证书</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#使用openssl命令校验证书"><span class="nav-number">3.6.1.</span> <span class="nav-text">使用openssl命令校验证书</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#使用-Cfssl-Certinfo-命令校验"><span class="nav-number">3.6.2.</span> <span class="nav-text">使用 Cfssl-Certinfo 命令校验</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#分发证书"><span class="nav-number">3.7.</span> <span class="nav-text">分发证书</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#下载和配置-kubectl-kubecontrol-命令行工具"><span class="nav-number">4.</span> <span class="nav-text">下载和配置 kubectl(kubecontrol) 命令行工具</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#下载kubectl"><span class="nav-number">4.1.</span> <span class="nav-text">下载kubectl</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#创建-kubectl-kubeconfig-文件"><span class="nav-number">4.2.</span> <span class="nav-text">创建 kubectl kubeconfig 文件</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#创建-kubeconfig-文件"><span class="nav-number">5.</span> <span class="nav-text">创建 kubeconfig 文件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#创建-TLS-Bootstrapping-Token"><span class="nav-number">5.1.</span> <span class="nav-text">创建 TLS Bootstrapping Token</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Token-auth-file"><span class="nav-number">5.1.1.</span> <span class="nav-text">Token auth file</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#创建-kubelet-bootstrapping-kubeconfig-文件"><span class="nav-number">5.1.2.</span> <span class="nav-text">创建 kubelet bootstrapping kubeconfig 文件</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#创建-kube-proxy-kubeconfig-文件"><span class="nav-number">5.2.</span> <span class="nav-text">创建 kube-proxy kubeconfig 文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#分发-kubeconfig-文件"><span class="nav-number">5.3.</span> <span class="nav-text">分发 kubeconfig 文件</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#创建高可用-etcd-集群"><span class="nav-number">6.</span> <span class="nav-text">创建高可用 etcd 集群</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#安装配置etcd集群"><span class="nav-number">6.1.</span> <span class="nav-text">安装配置etcd集群</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#启动-etcd-服务"><span class="nav-number">6.2.</span> <span class="nav-text">启动 etcd 服务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#验证服务"><span class="nav-number">6.3.</span> <span class="nav-text">验证服务</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#部署-kubernetes-master"><span class="nav-number">7.</span> <span class="nav-text">部署 kubernetes master</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#TLS-证书文件"><span class="nav-number">7.1.</span> <span class="nav-text">TLS 证书文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#下载二进制文件"><span class="nav-number">7.2.</span> <span class="nav-text">下载二进制文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#配置和启动-kube-apiserver"><span class="nav-number">7.3.</span> <span class="nav-text">配置和启动 kube-apiserver</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#创建-kube-apiserver的service配置文件"><span class="nav-number">7.3.1.</span> <span class="nav-text">创建 kube-apiserver的service配置文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#启动kube-apiserver"><span class="nav-number">7.3.2.</span> <span class="nav-text">启动kube-apiserver</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#配置和启动-kube-controller-manager"><span class="nav-number">7.4.</span> <span class="nav-text">配置和启动 kube-controller-manager</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#创建-kube-controller-manager-的service配置文件"><span class="nav-number">7.4.1.</span> <span class="nav-text">创建 kube-controller-manager 的service配置文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#启动-kube-controller-manager"><span class="nav-number">7.4.2.</span> <span class="nav-text">启动 kube-controller-manager</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#配置和启动-kube-scheduler"><span class="nav-number">7.5.</span> <span class="nav-text">配置和启动 kube-scheduler</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#创建-kube-scheduler的serivce配置文件"><span class="nav-number">7.5.1.</span> <span class="nav-text">创建 kube-scheduler的serivce配置文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#启动-kube-scheduler"><span class="nav-number">7.5.2.</span> <span class="nav-text">启动 kube-scheduler</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#验证-master-节点功能"><span class="nav-number">7.6.</span> <span class="nav-text">验证 master 节点功能</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#部署kubernetes-node节点"><span class="nav-number">8.</span> <span class="nav-text">部署kubernetes node节点</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#安装Docker"><span class="nav-number">8.1.</span> <span class="nav-text">安装Docker</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#安装配置Flanneld"><span class="nav-number">8.2.</span> <span class="nav-text">安装配置Flanneld</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Flannel介绍"><span class="nav-number">8.2.1.</span> <span class="nav-text">Flannel介绍</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#安装配置flannel"><span class="nav-number">8.2.2.</span> <span class="nav-text">安装配置flannel</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#在etcd中初始化flannel网络数据"><span class="nav-number">8.2.3.</span> <span class="nav-text">在etcd中初始化flannel网络数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#启动flannel"><span class="nav-number">8.2.4.</span> <span class="nav-text">启动flannel</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#测试跨主机容器通信"><span class="nav-number">8.2.5.</span> <span class="nav-text">测试跨主机容器通信</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#补充：下载二进制包安装flannel"><span class="nav-number">8.2.6.</span> <span class="nav-text">补充：下载二进制包安装flannel</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#安装和配置-kubelet"><span class="nav-number">8.3.</span> <span class="nav-text">安装和配置 kubelet</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#下载最新的-kubelet-和-kube-proxy-二进制文件"><span class="nav-number">8.3.1.</span> <span class="nav-text">下载最新的 kubelet 和 kube-proxy 二进制文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#配置kubelet"><span class="nav-number">8.3.2.</span> <span class="nav-text">配置kubelet</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#启动kubelet"><span class="nav-number">8.3.3.</span> <span class="nav-text">启动kubelet</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#通过-kubelet-的-TLS-证书请求"><span class="nav-number">8.3.4.</span> <span class="nav-text">通过 kubelet 的 TLS 证书请求</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#配置-kube-proxy"><span class="nav-number">8.4.</span> <span class="nav-text">配置 kube-proxy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#验证测试"><span class="nav-number">8.5.</span> <span class="nav-text">验证测试</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#安装和配置-kube-dns-插件"><span class="nav-number">9.</span> <span class="nav-text">安装和配置 kube-dns 插件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#kube-dns是什么"><span class="nav-number">9.1.</span> <span class="nav-text">kube-dns是什么</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#安装配置kube-dns"><span class="nav-number">9.2.</span> <span class="nav-text">安装配置kube-dns</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#下载yaml文件"><span class="nav-number">9.2.1.</span> <span class="nav-text">下载yaml文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#系统预定义的-RoleBinding"><span class="nav-number">9.2.2.</span> <span class="nav-text">系统预定义的 RoleBinding</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#配置-kube-dns-ServiceAccount"><span class="nav-number">9.2.3.</span> <span class="nav-text">配置 kube-dns ServiceAccount</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#配置-kube-dns-服务"><span class="nav-number">9.2.4.</span> <span class="nav-text">配置 kube-dns 服务</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#配置-kube-dns-Deployment"><span class="nav-number">9.2.5.</span> <span class="nav-text">配置 kube-dns Deployment</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#执行所有定义文件"><span class="nav-number">9.2.6.</span> <span class="nav-text">执行所有定义文件</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#检查-kube-dns-功能"><span class="nav-number">9.3.</span> <span class="nav-text">检查 kube-dns 功能</span></a></li></ol></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhao Jiankai</span>
</div>

<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  本站访客数:<span id="busuanzi_value_site_uv"></span>
</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>

<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共173.8k字</span>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.0.1"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  



  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  


  
  
  

  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("r9OTvh5qdm5WfVnhJBm4XoP9-gzGzoHsz", "VAES8qziiwbdUq0IzdQVj5xD");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

<!--    -->
</body>
</html>
