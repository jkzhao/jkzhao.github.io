<!doctype html>



  


<html class="theme-next pisces use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Lobster Two:300,300italic,400,400italic,700,700italic|consolas:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="大数据，Hadoop," />





  <link rel="alternate" href="/atom.xml" title="jkzhao's blog" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico?v=5.0.1" />






<meta name="description" content="关于Apache Hadoop的介绍以本地模式、伪分布式安装部署请参见上篇博文，本篇博文主要记录完全分布式部署，并实现NameNode高可用和ResourceManager高可用。 环境规划">
<meta name="keywords" content="大数据，Hadoop">
<meta property="og:type" content="article">
<meta property="og:title" content="安装部署Apache Hadoop (完全分布式模式并且实现NameNode HA和ResourceManager HA)">
<meta property="og:url" content="http://yoursite.com/2016/08/07/安装部署Apache Hadoop (完全分布式模式并且实现NameNode HA和ResourceManager HA)/index.html">
<meta property="og:site_name" content="jkzhao's blog">
<meta property="og:description" content="关于Apache Hadoop的介绍以本地模式、伪分布式安装部署请参见上篇博文，本篇博文主要记录完全分布式部署，并实现NameNode高可用和ResourceManager高可用。 环境规划">
<meta property="og:image" content="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/hadoop/32.png">
<meta property="og:image" content="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/hadoop/33.png">
<meta property="og:image" content="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/hadoop/34.png">
<meta property="og:image" content="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/hadoop/35.png">
<meta property="og:image" content="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/hadoop/36.png">
<meta property="og:image" content="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/hadoop/37.png">
<meta property="og:image" content="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/hadoop/38.png">
<meta property="og:image" content="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/hadoop/39.png">
<meta property="og:image" content="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/hadoop/40.png">
<meta property="og:image" content="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/hadoop/41.png">
<meta property="og:image" content="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/hadoop/42.png">
<meta property="og:image" content="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/hadoop/43.png">
<meta property="og:image" content="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/hadoop/44.png">
<meta property="og:image" content="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/hadoop/45.png">
<meta property="og:image" content="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/hadoop/46.png">
<meta property="og:updated_time" content="2016-08-15T02:09:44.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="安装部署Apache Hadoop (完全分布式模式并且实现NameNode HA和ResourceManager HA)">
<meta name="twitter:description" content="关于Apache Hadoop的介绍以本地模式、伪分布式安装部署请参见上篇博文，本篇博文主要记录完全分布式部署，并实现NameNode高可用和ResourceManager高可用。 环境规划">
<meta name="twitter:image" content="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/hadoop/32.png">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 6287775856811050000,
      author: 'Author'
    }
  };
</script>

  <title> 安装部署Apache Hadoop (完全分布式模式并且实现NameNode HA和ResourceManager HA) | jkzhao's blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">jkzhao's blog</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">学习 总结 思考</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-guestbook">
          <a href="/guestbook" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            留言
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="#" class="st-search-show-outputs">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <form class="site-search-form">
  <input type="text" id="st-search-input" class="st-search-input st-default-search-input" />
</form>

<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
    (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
    e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

  _st('install', 'CziK4aDdRyzFJrfygnHH','2.0.0');
</script>



    </div>
  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                安装部署Apache Hadoop (完全分布式模式并且实现NameNode HA和ResourceManager HA)
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-08-07T16:51:08+08:00" content="2016-08-07">
              2016-08-07
            </time>
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/08/07/安装部署Apache Hadoop (完全分布式模式并且实现NameNode HA和ResourceManager HA)/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/08/07/安装部署Apache Hadoop (完全分布式模式并且实现NameNode HA和ResourceManager HA)/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>关于Apache Hadoop的介绍以本地模式、伪分布式安装部署请参见上篇博文，本篇博文主要记录完全分布式部署，并实现NameNode高可用和ResourceManager高可用。</p>
<h1 id="环境规划"><a href="#环境规划" class="headerlink" title="环境规划"></a>环境规划</h1><a id="more"></a>
<table>
<thead>
<tr>
<th>主机名</th>
<th>IP地址</th>
<th>操作系统版本</th>
<th>安装软件</th>
<th>运行的进程</th>
</tr>
</thead>
<tbody>
<tr>
<td>hadoop16</td>
<td>172.16.206.16</td>
<td>CentOS 7.2</td>
<td>JDK1.7、hadoop-2.7.2</td>
<td>NameNode、DFSZKFailoverController(zkfc)、ResourceManager</td>
</tr>
<tr>
<td>hadoop26</td>
<td>172.16.206.26</td>
<td>CentOS 6.5</td>
<td>JDK1.7、hadoop-2.7.2</td>
<td>NameNode、DFSZKFailoverController(zkfc)、ResourceManager</td>
</tr>
<tr>
<td>hadoop27</td>
<td>172.16.206.27</td>
<td>CentOS 6.5</td>
<td>JDK1.7、hadoop-2.7.2、Zookeeper</td>
<td>DataNode、NodeManager、JournalNode、QuorumPeerMain</td>
</tr>
<tr>
<td>hadoop28</td>
<td>172.16.206.28</td>
<td>CentOS 6.5</td>
<td>JDK1.7、hadoop-2.7.2、Zookeeper</td>
<td>DataNode、NodeManager、JournalNode、QuorumPeerMain</td>
</tr>
<tr>
<td>hadoop29</td>
<td>172.16.206.29</td>
<td>CentOS 6.5</td>
<td>JDK1.7、hadoop-2.7.2、Zookeeper</td>
<td>DataNode、NodeManager、JournalNode、QuorumPeerMain</td>
</tr>
</tbody>
</table>
<p><strong>注意：</strong>这里由于机器紧张，将NameNode和ResourceManager安装在一台机器上。在hadoop16主机上安装NameNode和ResourceManager使其处于active状态，在hadoop26上安装NameNode和ResourceManager使其处于standby状态。</p>
<p>环境拓扑：<br><img src="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/hadoop/32.png" alt=""><br><strong>注意：</strong>这里由于实验环境，所以将NameNode和ResourceManager放在了一起，生产环境下应该将NameNode和ResourceManager放在单独的机器上。<br><br>Hadoop2.0官方提供了两种HDFS HA的解决方案，一种是NFS，另一种是QJM。这两种共享数据的方案，NFS是操作系统层面的，JournalNode是hadoop层面的，这里我们使用简单的QJM集群进行数据共享。在该方案中，主备NameNode之间通过一组JournalNode同步元数据信息，一条数据只要成功写入多数JournalNode即认为写入成功。通常配置奇数个JournalNode。<br><br>这里还配置了一个zookeeper集群(27,28,29主机)，用于ZKFC（DFSZKFailoverController）故障转移，当Active NameNode挂掉了，会自动切换Standby NameNode和ResourceManager为standby状态。同时27,28,29主机作为DataNode节点。</p>
<h1 id="配置集群各节点hosts文件"><a href="#配置集群各节点hosts文件" class="headerlink" title="配置集群各节点hosts文件"></a>配置集群各节点hosts文件</h1><p>在各节点，编辑hosts文件，配置好各节点主机名和ip地址的对应关系：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># vim /etc/hosts</div><div class="line">172.16.206.16 hadoop16</div><div class="line">172.16.206.26 hadoop26</div><div class="line">172.16.206.27 hadoop27</div><div class="line">172.16.206.28 hadoop28</div><div class="line">172.16.206.29 hadoop29</div></pre></td></tr></table></figure></p>
<h1 id="安装JDK1-7"><a href="#安装JDK1-7" class="headerlink" title="安装JDK1.7"></a>安装JDK1.7</h1><p><strong>Hadoop Java Versions</strong> <br><br>Version 2.7 and later of Apache Hadoop requires Java 7. It is built and tested on both OpenJDK and Oracle (HotSpot)’s JDK/JRE. <br><br>Earlier versions (2.6 and earlier) support Java 6.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"># mkdir /usr/java</div><div class="line"># tar zxf /usr/local/jdk-7u80-linux-x64.gz -C /usr/java/</div><div class="line"># vim /etc/profile</div><div class="line">export JAVA_HOME=/usr/java/jdk1.7.0_80</div><div class="line">export PATH=$JAVA_HOME/bin:$PATH</div><div class="line">export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</div><div class="line"># source /etc/profile</div></pre></td></tr></table></figure></p>
<h1 id="安装依赖包ssh和rsync"><a href="#安装依赖包ssh和rsync" class="headerlink" title="安装依赖包ssh和rsync"></a>安装依赖包ssh和rsync</h1><p>对于Redhat/CentOS系列的，安装系统时一般都会默认安装openssh软件，里面包含了ssh客户端和ssh服务端，所以先检查下这个软件包是否安装了：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># yum list all openssh</div></pre></td></tr></table></figure></p>
<p>如果没有安装，安装：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># yum install -y openssh</div></pre></td></tr></table></figure></p>
<p>在检查rsync软件包是否安装：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># yum list all rsync</div></pre></td></tr></table></figure></p>
<h1 id="各节点时间同步"><a href="#各节点时间同步" class="headerlink" title="各节点时间同步"></a>各节点时间同步</h1><p>采用NTP(Network Time Protocol)方式来实现, 选择一台机器, 作为集群的时间同步服务器, 然后分别配置服务端和集群其他机器。我这里以hadoop16机器时间为准，其他机器同这台机器时间做同步。</p>
<h2 id="NTP服务端"><a href="#NTP服务端" class="headerlink" title="NTP服务端"></a>NTP服务端</h2><ol>
<li><p>安装ntp服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># yum install ntp -y</div></pre></td></tr></table></figure>
</li>
<li><p>配置/etc/ntp.conf，这边采用本地机器作为时间的原点<br>注释server列表：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">server 0.centos.pool.ntp.org iburst</div><div class="line">server 1.centos.pool.ntp.org iburst</div><div class="line">server 2.centos.pool.ntp.org iburst</div><div class="line">server 3.centos.pool.ntp.org iburst</div></pre></td></tr></table></figure>
</li>
</ol>
<p>添加如下内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">server 127.127.1.0 prefer</div><div class="line">fudge 127.127.1.0 stratum 8</div><div class="line">logfile /var/log/ntp.log</div></pre></td></tr></table></figure></p>
<ol>
<li><p>启动ntpd服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># systemctl start ntpd</div></pre></td></tr></table></figure>
</li>
<li><p>查看ntp服务状态</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># systemctl status ntpd</div></pre></td></tr></table></figure>
</li>
<li><p>加入开机启动</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># systemctl enable ntpd</div></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="NTP客户端"><a href="#NTP客户端" class="headerlink" title="NTP客户端"></a>NTP客户端</h2><ol>
<li><p>安装ntp</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># yum install ntpdate</div></pre></td></tr></table></figure>
</li>
<li><p>配置crontab任务主动同步</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># crontab -e</div><div class="line">*/10 * * * * /usr/sbin/ntpdate 172.16.206.16;hwclock -w</div></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="安装Zookeeper集群"><a href="#安装Zookeeper集群" class="headerlink" title="安装Zookeeper集群"></a>安装Zookeeper集群</h1><p><strong>注意：</strong>对于Zookeeper集群的话，官方推荐的最小节点数为3个。</p>
<h2 id="安装配置zk"><a href="#安装配置zk" class="headerlink" title="安装配置zk"></a>安装配置zk</h2><ol>
<li>配置zk节点的hosts文件<br>配置zk节点的hosts文件：配置3台机器的ip地址和主机名的对应关系。上面已经做过了。这里选择3台安装zk：hadoop27，hadoop28，hadoop29。</li>
<li>解压安装配置第一台zk<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"># cd /usr/local/</div><div class="line"># tar zxf zookeeper-3.4.6.tar.gz</div><div class="line"># cd zookeeper-3.4.6</div><div class="line">创建快照日志存放目录：</div><div class="line"># mkdir dataDir</div><div class="line">创建事务日志存放目录：</div><div class="line"># mkdir dataLogDir</div></pre></td></tr></table></figure>
</li>
</ol>
<p><strong>注意：</strong>如果不配置dataLogDir，那么事务日志也会写在dataDir目录中。这样会严重影响zk的性能。因为在zk吞吐量很高的时候，产生的事务日志和快照日志太多。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"># cd conf</div><div class="line"># mv zoo_sample.cfg zoo.cfg</div><div class="line"># vim zoo.cfg</div><div class="line"># 存放数据文件</div><div class="line">dataDir=/usr/local/zookeeper-3.4.6/dataDir</div><div class="line"># 存放日志文件</div><div class="line">dataLogDir=/usr/local/zookeeper-3.4.6/dataLogDir</div><div class="line"># zookeeper cluster，2888为选举端口，3888为心跳端口</div><div class="line">server.1=hadoop27:2888:3888</div><div class="line">server.2=hadoop28:2888:3888</div><div class="line">server.3=hadoop29:2888:3888</div></pre></td></tr></table></figure></p>
<p><img src="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/hadoop/33.png" alt=""><br>在我们配置的dataDir指定的目录下面，创建一个myid文件，里面内容为一个数字，用来标识当前主机，conf/zoo.cfg文件中配置的server.X中X为什么数字，则myid文件中就输入这个数字： <br><br>hadoop27主机：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># echo &quot;1&quot; &gt; /usr/local/zookeeper-3.4.6/dataDir/myid</div></pre></td></tr></table></figure></p>
<ol>
<li>远程复制第一台的zk到另外两台上，并修改myid文件为2和3<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># cd /usr/local/</div><div class="line"># scp -rp zookeeper-3.4.6 root@172.16.206.28:/usr/local/</div><div class="line"># echo &quot;2&quot; &gt; /usr/local/zookeeper-3.4.6/dataDir/myid</div><div class="line"># scp -rp zookeeper-3.4.6 root@172.16.206.29:/usr/local/</div><div class="line"># echo &quot;3&quot; &gt; /usr/local/zookeeper-3.4.6/dataDir/myid</div></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="启动和关闭zk"><a href="#启动和关闭zk" class="headerlink" title="启动和关闭zk"></a>启动和关闭zk</h2><p>在ZooKeeper集群的每个结点上，执行启动ZooKeeper服务的脚本，如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@hadoop27 ~]# /usr/local/zookeeper-3.4.6/bin/zkServer.sh start</div><div class="line">[root@hadoop28 ~]# /usr/local/zookeeper-3.4.6/bin/zkServer.sh start</div><div class="line">[root@hadoop29 ~]# /usr/local/zookeeper-3.4.6/bin/zkServer.sh start</div></pre></td></tr></table></figure></p>
<p>查看启动的进程：<br><img src="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/hadoop/34.png" alt=""><br><strong>停止zk命令：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># /usr/local/zookeeper-3.4.6/bin/zkServer.sh stop</div></pre></td></tr></table></figure></p>
<h2 id="测试zk集群"><a href="#测试zk集群" class="headerlink" title="测试zk集群"></a>测试zk集群</h2><p>可以通过ZooKeeper的脚本来查看启动状态，包括集群中各个结点的角色（或是Leader，或是Follower）：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@hadoop27 ~]# /usr/local/zookeeper-3.4.6/bin/zkServer.sh status</div><div class="line">JMX enabled by default</div><div class="line">Using config: /usr/local/zookeeper-3.4.6/bin/../conf/zoo.cfg</div><div class="line">Mode: follower</div></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@hadoop28 ~]# /usr/local/zookeeper-3.4.6/bin/zkServer.sh status</div><div class="line">JMX enabled by default</div><div class="line">Using config: /usr/local/zookeeper-3.4.6/bin/../conf/zoo.cfg</div><div class="line">Mode: leader</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@hadoop29 ~]# /usr/local/zookeeper-3.4.6/bin/zkServer.sh status</div><div class="line">JMX enabled by default</div><div class="line">Using config: /usr/local/zookeeper-3.4.6/bin/../conf/zoo.cfg</div><div class="line">Mode: follower</div></pre></td></tr></table></figure>
<p>通过上面状态查询结果可见，hadoop28是集群的Leader，其余的两个结点是Follower。<br><br>另外，可以通过客户端脚本，连接到ZooKeeper集群上。对于客户端来说，ZooKeeper是一个整体，连接到ZooKeeper集群实际上感觉在独享整个集群的服务，所以，你可以在任何一个结点上建立到服务集群的连接。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line">[root@hadoop29 ~]# /usr/local/zookeeper-3.4.6/bin/zkCli.sh -server hadoop27:2181</div><div class="line">Connecting to localhost:2181</div><div class="line">2016-07-18 21:26:57,647 [myid:] - INFO  [main:Environment@100] - Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT</div><div class="line">2016-07-18 21:26:57,650 [myid:] - INFO  [main:Environment@100] - Client environment:host.name=hadoop29</div><div class="line">2016-07-18 21:26:57,650 [myid:] - INFO  [main:Environment@100] - Client environment:java.version=1.7.0_80</div><div class="line">2016-07-18 21:26:57,652 [myid:] - INFO  [main:Environment@100] - Client environment:java.vendor=Oracle Corporation</div><div class="line">2016-07-18 21:26:57,652 [myid:] - INFO  [main:Environment@100] - Client environment:java.home=/usr/java/jdk1.7.0_80/jre</div><div class="line">2016-07-18 21:26:57,652 [myid:] - INFO  [main:Environment@100] - Client environment:java.class.path=/usr/local/zookeeper-3.4.6/bin/../build/classes:/usr/local/zookeeper-3.4.6/bin/../build/lib/*.jar:/usr/local/zookeeper-3.4.6/bin/../lib/slf4j-log4j12-1.6.1.jar:/usr/local/zookeeper-3.4.6/bin/../lib/slf4j-api-1.6.1.jar:/usr/local/zookeeper-3.4.6/bin/../lib/netty-3.7.0.Final.jar:/usr/local/zookeeper-3.4.6/bin/../lib/log4j-1.2.16.jar:/usr/local/zookeeper-3.4.6/bin/../lib/jline-0.9.94.jar:/usr/local/zookeeper-3.4.6/bin/../zookeeper-3.4.6.jar:/usr/local/zookeeper-3.4.6/bin/../src/java/lib/*.jar:/usr/local/zookeeper-3.4.6/bin/../conf:.:/usr/java/jdk1.7.0_80/lib/dt.jar:/usr/java/jdk1.7.0_80/lib/tools.jar</div><div class="line">2016-07-18 21:26:57,652 [myid:] - INFO  [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib</div><div class="line">2016-07-18 21:26:57,653 [myid:] - INFO  [main:Environment@100] - Client environment:java.io.tmpdir=/tmp</div><div class="line">2016-07-18 21:26:57,653 [myid:] - INFO  [main:Environment@100] - Client environment:java.compiler=&lt;NA&gt;</div><div class="line">2016-07-18 21:26:57,653 [myid:] - INFO  [main:Environment@100] - Client environment:os.name=Linux</div><div class="line">2016-07-18 21:26:57,653 [myid:] - INFO  [main:Environment@100] - Client environment:os.arch=amd64</div><div class="line">2016-07-18 21:26:57,653 [myid:] - INFO  [main:Environment@100] - Client environment:os.version=2.6.32-431.el6.x86_64</div><div class="line">2016-07-18 21:26:57,653 [myid:] - INFO  [main:Environment@100] - Client environment:user.name=root</div><div class="line">2016-07-18 21:26:57,653 [myid:] - INFO  [main:Environment@100] - Client environment:user.home=/root</div><div class="line">2016-07-18 21:26:57,653 [myid:] - INFO  [main:Environment@100] - Client environment:user.dir=/root</div><div class="line">2016-07-18 21:26:57,654 [myid:] - INFO  [main:ZooKeeper@438] - Initiating client connection, connectString=localhost:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@279ac931</div><div class="line">[root@hadoop29 ~]# /usr/local/zookeeper-3.4.6/bin/zkCli.sh -server hadoop27:2181</div><div class="line">Connecting to hadoop27:2181</div><div class="line">2016-07-18 21:29:48,216 [myid:] - INFO  [main:Environment@100] - Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT</div><div class="line">2016-07-18 21:29:48,219 [myid:] - INFO  [main:Environment@100] - Client environment:host.name=hadoop29</div><div class="line">2016-07-18 21:29:48,219 [myid:] - INFO  [main:Environment@100] - Client environment:java.version=1.7.0_80</div><div class="line">2016-07-18 21:29:48,221 [myid:] - INFO  [main:Environment@100] - Client environment:java.vendor=Oracle Corporation</div><div class="line">2016-07-18 21:29:48,221 [myid:] - INFO  [main:Environment@100] - Client environment:java.home=/usr/java/jdk1.7.0_80/jre</div><div class="line">2016-07-18 21:29:48,221 [myid:] - INFO  [main:Environment@100] - Client environment:java.class.path=/usr/local/zookeeper-3.4.6/bin/../build/classes:/usr/local/zookeeper-3.4.6/bin/../build/lib/*.jar:/usr/local/zookeeper-3.4.6/bin/../lib/slf4j-log4j12-1.6.1.jar:/usr/local/zookeeper-3.4.6/bin/../lib/slf4j-api-1.6.1.jar:/usr/local/zookeeper-3.4.6/bin/../lib/netty-3.7.0.Final.jar:/usr/local/zookeeper-3.4.6/bin/../lib/log4j-1.2.16.jar:/usr/local/zookeeper-3.4.6/bin/../lib/jline-0.9.94.jar:/usr/local/zookeeper-3.4.6/bin/../zookeeper-3.4.6.jar:/usr/local/zookeeper-3.4.6/bin/../src/java/lib/*.jar:/usr/local/zookeeper-3.4.6/bin/../conf:.:/usr/java/jdk1.7.0_80/lib/dt.jar:/usr/java/jdk1.7.0_80/lib/tools.jar</div><div class="line">2016-07-18 21:29:48,221 [myid:] - INFO  [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib</div><div class="line">2016-07-18 21:29:48,221 [myid:] - INFO  [main:Environment@100] - Client environment:java.io.tmpdir=/tmp</div><div class="line">2016-07-18 21:29:48,221 [myid:] - INFO  [main:Environment@100] - Client environment:java.compiler=&lt;NA&gt;</div><div class="line">2016-07-18 21:29:48,221 [myid:] - INFO  [main:Environment@100] - Client environment:os.name=Linux</div><div class="line">2016-07-18 21:29:48,221 [myid:] - INFO  [main:Environment@100] - Client environment:os.arch=amd64</div><div class="line">2016-07-18 21:29:48,222 [myid:] - INFO  [main:Environment@100] - Client environment:os.version=2.6.32-431.el6.x86_64</div><div class="line">2016-07-18 21:29:48,222 [myid:] - INFO  [main:Environment@100] - Client environment:user.name=root</div><div class="line">2016-07-18 21:29:48,222 [myid:] - INFO  [main:Environment@100] - Client environment:user.home=/root</div><div class="line">2016-07-18 21:29:48,222 [myid:] - INFO  [main:Environment@100] - Client environment:user.dir=/root</div><div class="line">2016-07-18 21:29:48,223 [myid:] - INFO  [main:ZooKeeper@438] - Initiating client connection, connectString=hadoop27:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@194d62f1</div><div class="line">Welcome to ZooKeeper!</div><div class="line">2016-07-18 21:29:48,245 [myid:] - INFO  [main-SendThread(hadoop27:2181):ClientCnxn$SendThread@975] - Opening socket connection to server hadoop27/172.16.206.27:2181. Will not attempt to authenticate using SASL (unknown error)</div><div class="line">2016-07-18 21:29:48,249 [myid:] - INFO  [main-SendThread(hadoop27:2181):ClientCnxn$SendThread@852] - Socket connection established to hadoop27/172.16.206.27:2181, initiating session</div><div class="line">JLine support is enabled</div><div class="line">[zk: hadoop27:2181(CONNECTING) 0] 2016-07-18 21:29:48,356 [myid:] - INFO  [main-SendThread(hadoop27:2181):ClientCnxn$SendThread@1235] - Session establishment complete on server hadoop27/172.16.206.27:2181, sessionid = 0x155fc2e082e0000, negotiated timeout = 30000</div><div class="line"></div><div class="line">WATCHER::</div><div class="line"></div><div class="line">WatchedEvent state:SyncConnected type:None path:null</div><div class="line"></div><div class="line">[zk: hadoop27:2181(CONNECTED) 0]</div></pre></td></tr></table></figure></p>
<p>输入quit，可以退出。</p>
<h2 id="脚本定期清理zk快照和日志文件"><a href="#脚本定期清理zk快照和日志文件" class="headerlink" title="脚本定期清理zk快照和日志文件"></a>脚本定期清理zk快照和日志文件</h2><p>正常运行过程中，ZK会不断地把快照数据和事务日志输出到dataDir和dataLogDir这两个目录，并且如果没有人为操作的话，ZK自己是不会清理这些文件的。<br><br>我这里采用脚本切割。将脚本上传到/usr/local/zookeeper-3.4.6/目录下。脚本内容如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">#!/bin/bash</div><div class="line">###Description:This script is used to clear zookeeper snapshot file and transaction logs.</div><div class="line">###Written by: jkzhao - jkzhao@wisedu.com</div><div class="line">###History: 2016-04-08 First release.</div><div class="line"></div><div class="line"># Snapshot file dir.</div><div class="line">dataDir=/usr/local/zookeeper-3.4.6/dataDir/version-2</div><div class="line"></div><div class="line"># Transaction logs dir.</div><div class="line">dataLogDir=/usr/local/zookeeper-3.4.6/dataLogDir/version-2</div><div class="line"></div><div class="line"># Reserved 5 files.</div><div class="line">COUNT=5</div><div class="line"></div><div class="line">ls -t $dataDir/snapshot.* | tail -n +$[$COUNT+1] | xargs rm -f</div><div class="line">ls -t $dataLogDir/log.* | tail -n +$[$COUNT+1] | xargs rm -f</div></pre></td></tr></table></figure></p>
<p>赋予脚本执行权限：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># chmod +x clean_zklog.sh</div></pre></td></tr></table></figure></p>
<p>配置周期性任务，每个星期日的0点0分执行：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># crontab -e</div><div class="line">0 0 * * 0 /usr/local/zookeeper-3.4.6/clean_zklog.sh</div></pre></td></tr></table></figure></p>
<p><strong>注意：</strong>所有zk节点都得配置脚本和周期性任务。</p>
<h1 id="添加Hadoop运行用户"><a href="#添加Hadoop运行用户" class="headerlink" title="添加Hadoop运行用户"></a>添加Hadoop运行用户</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># groupadd hadoop</div><div class="line"># useradd -g hadoop hadoop</div><div class="line"># echo &quot;wisedu&quot; | passwd --stdin hadoop &amp;&gt; /dev/null</div></pre></td></tr></table></figure>
<p><strong>注意：</strong>所有节点都得添加hadoop用户。</p>
<h1 id="配置主节点登录自己和其他节点不需要输入密码"><a href="#配置主节点登录自己和其他节点不需要输入密码" class="headerlink" title="配置主节点登录自己和其他节点不需要输入密码"></a>配置主节点登录自己和其他节点不需要输入密码</h1><p>这里的主节点指的是NameNode，ResourceManager。配置hadoop16主机(Active)登录hadoop16，hadoop26，hadoop27，hadoop28，hadoop29主机免密码。还要配置hadoop26主机(Standby)登录hadoop16，hadoop26,hadoop27，hadoop28，hadoop29主机免密码。 (也可以不配置，每个节点一个一个启动服务，最好不要这样做）。<br>hadoop用户登录shell：</p>
<ol>
<li><p>配置hadoop16主机(Active)登录hadoop16，hadoop26，hadoop27，hadoop28，hadoop29主机免密码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[hadoop@hadoop16 ~]$ ssh-keygen -t rsa -P &apos;&apos;</div><div class="line">[hadoop@hadoop16 ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop16</div><div class="line">[hadoop@hadoop16 ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop26</div><div class="line">[hadoop@hadoop16 ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop27</div><div class="line">[hadoop@hadoop16 ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop28</div><div class="line">[hadoop@hadoop16 ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop29</div></pre></td></tr></table></figure>
</li>
<li><p>配置hadoop26主机(Standby)登录hadoop16，hadoop26，hadoop27，hadoop28，hadoop29主机免密码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[hadoop@hadoop26 ~]$ ssh-keygen -t rsa -P &apos;&apos;</div><div class="line">[hadoop@hadoop26 ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop16</div><div class="line">[hadoop@hadoop26 ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop26</div><div class="line">[hadoop@hadoop26 ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop27</div><div class="line">[hadoop@hadoop26 ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop28</div><div class="line">[hadoop@hadoop26 ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop29</div></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="安装hadoop"><a href="#安装hadoop" class="headerlink" title="安装hadoop"></a>安装hadoop</h1><h2 id="安装配置master节点-hadoop16主机"><a href="#安装配置master节点-hadoop16主机" class="headerlink" title="安装配置master节点(hadoop16主机)"></a>安装配置master节点(hadoop16主机)</h2><ol>
<li><p>将安装包上传至//usr/local目录下并解压</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[root@hadoop16 ~]# cd /usr/local/</div><div class="line">[root@hadoop16 local]# tar zxf hadoop-2.7.2.tar.gz</div><div class="line">[root@hadoop16 local]# ln -sv hadoop-2.7.2 hadoop</div><div class="line">[root@hadoop16 local]# cd hadoop</div><div class="line">[root@hadoop16 hadoop]# mkdir logs</div><div class="line">[root@hadoop16 hadoop]# chmod g+w logs</div><div class="line">[root@hadoop16 hadoop]# chown -R hadoop:hadoop ./*</div><div class="line">[root@hadoop16 hadoop]# chown -R hadoop:hadoop /usr/local/hadoop-2.7.2</div></pre></td></tr></table></figure>
</li>
<li><p>配置hadoop环境变量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@hadoop16 hadoop]# vim /etc/profile</div><div class="line"># HADOOP</div><div class="line">export HADOOP_HOME=/usr/local/hadoop</div><div class="line">export PATH=$PATH:$&#123;HADOOP_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/sbin</div></pre></td></tr></table></figure>
</li>
<li><p>修改文件hadoop-env.sh和yarn-env.sh<br>Hadoop的各守护进程依赖于JAVA_HOME环境变量，可在这两个文件中配置特定的JAVA环境。此处仅需要修改hadoop-env.sh文件。此外，Hadoop大多数守护进程默认使用的堆大小为1GB，但现实应用中，可能需要对其各类进程的堆内存大小做出调整，这只需要编辑这两个文件中的相关环境变量值即可，例如HADOOP_HEAPSIZE、HADOOP_JOB_HISTORY_HEAPSIZE、JAVA_HEAP_SIZE和YARN_HEAP_SIZE等。<br><br>hadoop用户登录shell，或者root用户登录，su - hadoop。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hadoop@hadoop16 ~]$ vim /usr/local/hadoop/etc/hadoop/hadoop-env.sh</div></pre></td></tr></table></figure>
</li>
</ol>
<p><img src="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/hadoop/35.png" alt=""></p>
<ol>
<li>修改配置文件<br>hadoop用户登录shell，或者root用户登录，su - hadoop。<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hadoop@hadoop16 ~]$ cd /usr/local/hadoop/etc/hadoop/</div></pre></td></tr></table></figure>
</li>
</ol>
<p><strong>修改core-site.xml，</strong>该文件包含了NameNode主机地址以及其监听RPC端口等信息，对于伪分布式模式的安装来说，其主机地址是localhost；对于完全分布式中master节点的主机名称或者ip地址；如果配置NameNode是HA，指定HDFS的nameservice为一个自定义名称，然后在hdfs-site.xml配置NameNode节点的主机信息。NameNode默认的RPC端口是8020。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">        &lt;!-- 指定hdfs的nameservice为ns1 --&gt;</div><div class="line">        &lt;property&gt;</div><div class="line">                &lt;name&gt;fs.defaultFS&lt;/name&gt;</div><div class="line">                &lt;value&gt;hdfs://ns1&lt;/value&gt;</div><div class="line">        &lt;/property&gt;</div><div class="line">        &lt;!-- 指定hadoop临时目录 --&gt;</div><div class="line">        &lt;property&gt;</div><div class="line">                &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</div><div class="line">                &lt;value&gt;/usr/local/hadoop/tmp&lt;/value&gt;</div><div class="line">        &lt;/property&gt;</div><div class="line">        &lt;!-- 指定zookeeper地址 --&gt;</div><div class="line">        &lt;property&gt;</div><div class="line">                &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;</div><div class="line">                &lt;value&gt;hadoop27:2181,hadoop28:2181,hadoop29:2181&lt;/value&gt;</div><div class="line">        &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure></p>
<p><strong>修改hdfs-site.xml，</strong>该文件主要用于配置HDFS相关的属性，例如复制因子（即数据块的副本数）、NN和DN用于存储数据的目录等。数据块的副本数对于伪分布式的Hadoop应该为1，完全分布式模式下默认数据副本是3份。在这个配置文件中还可以配置NN和DN用于存储的数据的目录。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div></pre></td><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">        &lt;!--指定hdfs的nameservice为ns1，需要和core-site.xml中的保持一致 --&gt;</div><div class="line">        &lt;property&gt;</div><div class="line">                &lt;name&gt;dfs.nameservices&lt;/name&gt;</div><div class="line">                &lt;value&gt;ns1&lt;/value&gt;</div><div class="line">        &lt;/property&gt;</div><div class="line">        &lt;!-- ns1下面有两个NameNode，分别是nn1，nn2 --&gt;</div><div class="line">        &lt;property&gt;</div><div class="line">                &lt;name&gt;dfs.ha.namenodes.ns1&lt;/name&gt;</div><div class="line">                &lt;value&gt;nn1,nn2&lt;/value&gt;</div><div class="line">        &lt;/property&gt;</div><div class="line">        &lt;!-- nn1的RPC通信地址 --&gt;</div><div class="line">        &lt;property&gt;</div><div class="line">                &lt;name&gt;dfs.namenode.rpc-address.ns1.nn1&lt;/name&gt;</div><div class="line">                &lt;value&gt;hadoop16:9000&lt;/value&gt;</div><div class="line">        &lt;/property&gt;</div><div class="line">        &lt;!-- nn1的http通信地址 --&gt;</div><div class="line">        &lt;property&gt;</div><div class="line">                &lt;name&gt;dfs.namenode.http-address.ns1.nn1&lt;/name&gt;</div><div class="line">                &lt;value&gt;hadoop16:50070&lt;/value&gt;</div><div class="line">        &lt;/property&gt;</div><div class="line">        &lt;!-- nn2的RPC通信地址 --&gt;</div><div class="line">        &lt;property&gt;</div><div class="line">                &lt;name&gt;dfs.namenode.rpc-address.ns1.nn2&lt;/name&gt;</div><div class="line">                &lt;value&gt;hadoop26:9000&lt;/value&gt;</div><div class="line">        &lt;/property&gt;</div><div class="line">        &lt;!-- nn2的http通信地址 --&gt;</div><div class="line">        &lt;property&gt;</div><div class="line">                &lt;name&gt;dfs.namenode.http-address.ns1.nn2&lt;/name&gt;</div><div class="line">                &lt;value&gt;hadoop26:50070&lt;/value&gt;</div><div class="line">        &lt;/property&gt;</div><div class="line">        &lt;!-- 指定NameNode的元数据在JournalNode上的存放位置 --&gt;</div><div class="line">        &lt;property&gt;</div><div class="line">                &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;</div><div class="line">                &lt;value&gt;qjournal://hadoop27:8485;hadoop28:8485;hadoop29:8485/ns1&lt;/value&gt;</div><div class="line">        &lt;/property&gt;</div><div class="line">        &lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt;</div><div class="line">        &lt;property&gt;</div><div class="line">                &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;</div><div class="line">                &lt;value&gt;/usr/local/hadoop/journaldata&lt;/value&gt;</div><div class="line">        &lt;/property&gt;</div><div class="line">        &lt;!-- 开启NameNode失败自动切换 --&gt;</div><div class="line">        &lt;property&gt;</div><div class="line">                &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;</div><div class="line">                &lt;value&gt;true&lt;/value&gt;</div><div class="line">        &lt;/property&gt;</div><div class="line">        &lt;!-- 配置失败自动切换实现方式 --&gt;</div><div class="line">        &lt;property&gt;</div><div class="line">                &lt;name&gt;dfs.client.failover.proxy.provider.ns1&lt;/name&gt;</div><div class="line">                &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;</div><div class="line">        &lt;/property&gt;</div><div class="line">        &lt;!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行--&gt;</div><div class="line">        &lt;property&gt;</div><div class="line">                &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;</div><div class="line">                &lt;value&gt;</div><div class="line">                        sshfence</div><div class="line">                        shell(/bin/true)</div><div class="line">                &lt;/value&gt;</div><div class="line">        &lt;/property&gt;</div><div class="line">        &lt;!-- 使用sshfence隔离机制时需要ssh免登陆 --&gt;</div><div class="line">        &lt;property&gt;</div><div class="line">                &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;</div><div class="line">                &lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt;</div><div class="line">        &lt;/property&gt;</div><div class="line">        &lt;!-- 配置sshfence隔离机制超时时间 --&gt;</div><div class="line">        &lt;property&gt;</div><div class="line">                &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;</div><div class="line">                &lt;value&gt;30000&lt;/value&gt;</div><div class="line">        &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure></p>
<p><strong>注意：</strong>如果需要其它用户对hdfs有写入权限，还需要在hdfs-site.xml添加一项属性定义。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">     &lt;name&gt;dfs.permissions&lt;/name&gt;</div><div class="line">     &lt;value&gt;false&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure></p>
<p><strong>修改mapred-site.xml，</strong>该文件用于配置集群的MapReduce framework，此处应该指定yarn，另外的可用值还有local和classic。mapred-site.xml默认是不存在，但有模块文件mapred-site.xml.template，只需要将其复制mapred-site.xml即可。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[hadoop@hadoop16 hadoop]$ cp mapred-site.xml.template mapred-site.xml</div><div class="line">[hadoop@hadoop16 hadoop]$ vim mapred-site.xml</div></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">	&lt;!-- 指定mr框架为yarn方式 --&gt;</div><div class="line">	&lt;property&gt;</div><div class="line">		&lt;name&gt;mapreduce.framework.name&lt;/name&gt;</div><div class="line">		&lt;value&gt;yarn&lt;/value&gt;</div><div class="line">	&lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
<p><strong>修改yarn-site.xml，</strong>该文件用于配置YARN进程及YARN的相关属性。首先需要指定ResourceManager守护进程的主机和监听的端口，对于伪分布式模型来来讲，其主机为localhost，默认的端口是8032；其次需要指定ResourceManager使用的scheduler，以及NodeManager的辅助服务。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">	&lt;!-- 开启RM高可用 --&gt;</div><div class="line">	&lt;property&gt;</div><div class="line">	   &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;</div><div class="line">	   &lt;value&gt;true&lt;/value&gt;</div><div class="line">	&lt;/property&gt;</div><div class="line">	&lt;!-- 指定RM的cluster id --&gt;</div><div class="line">	&lt;property&gt;</div><div class="line">	   &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;</div><div class="line">	   &lt;value&gt;yrc&lt;/value&gt;</div><div class="line">	&lt;/property&gt;</div><div class="line">	&lt;!-- 指定RM的名字 --&gt;</div><div class="line">	&lt;property&gt;</div><div class="line">	   &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;</div><div class="line">	   &lt;value&gt;rm1,rm2&lt;/value&gt;</div><div class="line">	&lt;/property&gt;</div><div class="line">	&lt;!-- 分别指定RM的地址 --&gt;</div><div class="line">	&lt;property&gt;</div><div class="line">	   &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;</div><div class="line">	   &lt;value&gt;hadoop16&lt;/value&gt;</div><div class="line">	&lt;/property&gt;</div><div class="line">	&lt;property&gt;</div><div class="line">	   &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;</div><div class="line">	   &lt;value&gt;hadoop26&lt;/value&gt;</div><div class="line">	&lt;/property&gt;</div><div class="line">	&lt;!-- 指定zk集群地址 --&gt;</div><div class="line">	&lt;property&gt;</div><div class="line">	   &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;</div><div class="line">	   &lt;value&gt;hadoop27:2181,hadoop28:2181,hadoop29:2181&lt;/value&gt;</div><div class="line">	&lt;/property&gt;</div><div class="line">	&lt;property&gt;</div><div class="line">	   &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</div><div class="line">	   &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</div><div class="line">	&lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure></p>
<p><strong>修改slaves，</strong>该文件存储了当前集群的所有slave节点的列表，对于伪分布式模型，其文件内容仅应该是你localhost，这也的确是这个文件的默认值。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[hadoop@hadoop16 hadoop]$ vim slaves</div><div class="line">hadoop27</div><div class="line">hadoop28</div><div class="line">hadoop29</div></pre></td></tr></table></figure></p>
<h2 id="安装配置其他节点"><a href="#安装配置其他节点" class="headerlink" title="安装配置其他节点"></a>安装配置其他节点</h2><p><strong>注意：</strong>这里由于节点数目少，没有使用ansible等自动化工具。 <br><br><strong>重复操作解压、配置环境变量，参照前面。</strong> <br><br>Hadoop集群的各节点配置文件都是一样的，我们可以将master节点上的配置文件scp到其他节点上：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[hadoop@hadoop16 ~]$ scp -p /usr/local/hadoop/etc/hadoop/* hadoop@hadoop26:/usr/local/hadoop/etc/hadoop/</div><div class="line">[hadoop@hadoop16 ~]$ scp -p /usr/local/hadoop/etc/hadoop/* hadoop@hadoop27:/usr/local/hadoop/etc/hadoop/</div><div class="line">[hadoop@hadoop16 ~]$ scp -p /usr/local/hadoop/etc/hadoop/* hadoop@hadoop28:/usr/local/hadoop/etc/hadoop/</div><div class="line">[hadoop@hadoop16 ~]$ scp -p /usr/local/hadoop/etc/hadoop/* hadoop@hadoop29:/usr/local/hadoop/etc/hadoop/</div></pre></td></tr></table></figure></p>
<h1 id="启动hadoop"><a href="#启动hadoop" class="headerlink" title="启动hadoop"></a>启动hadoop</h1><p><strong>注意：</strong>请严格按照下面的步骤启动。</p>
<h2 id="启动Zookeeper集群"><a href="#启动Zookeeper集群" class="headerlink" title="启动Zookeeper集群"></a>启动Zookeeper集群</h2><p>分别在hadoop27、hadoop28、hadoop29上启动zk，前面已经启动好了，不再重复。</p>
<h2 id="启动journalnode"><a href="#启动journalnode" class="headerlink" title="启动journalnode"></a>启动journalnode</h2><p>hadoop用户登录shell，分别在在hadoop27、hadoop28、hadoop29上执行：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hadoop@hadoop27 ~]$ /usr/local/hadoop/sbin/hadoop-daemon.sh start journalnode</div></pre></td></tr></table></figure></p>
<p>运行jps命令检验，hadoop27、hadoop28、hadoop29上多了JournalNode进程。</p>
<h2 id="格式化HDFS"><a href="#格式化HDFS" class="headerlink" title="格式化HDFS"></a>格式化HDFS</h2><p>在HDFS的NN启动之前需要先初始化其用于存储数据的目录，可以在hdfs-site.xml配置文件中使用dfs.namenode.name.dir属性定义HDFS元数据持久存储路径，默认为${hadoop.tmp.dir}/dfs/name，这里是存放在JournalNode中；dfs.datanode.data.dir属性定义DataNode用于存储数据块的目录路径，默认为${hadoop.tmp.dir}/dfs/data。如果指定的目录不存在，格式化命令会自动创建之；如果事先存在，请确保其权限设置正确，此时格式化操作会清除其内部的所有数据并重新建立一个新的文件系统。 <br><br>在hadoop16(Active)上执行命令:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hadoop@hadoop16 ~]$ hdfs namenode -format</div></pre></td></tr></table></figure></p>
<p><img src="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/hadoop/36.png" alt=""><br>格式化后会在根据core-site.xml中的hadoop.tmp.dir配置生成个文件，这里我配置的是/usr/local/hadoop/tmp。<br><br>启动hadoop16主机上的NameNode：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hadoop@hadoop16 ~]$ hadoop-daemon.sh start namenode</div></pre></td></tr></table></figure></p>
<p>然后在hadoop26(Standby)主机上执行如下命令，同步hadoop16主机上的NameNode元数据信息：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hadoop@hadoop26 ~]$ hdfs namenode –bootstrapStandby</div></pre></td></tr></table></figure></p>
<p>同步完成后，停止hadoop16主机上的NameNode：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hadoop@hadoop16 ~]$ hadoop-daemon.sh stop namenode</div></pre></td></tr></table></figure></p>
<p>这里如果不启动Active的NameNode，就在Standby主机上同步，会报如下的错误：<br><img src="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/hadoop/37.png" alt=""><br>这是因为没有启动active namenode，因为standby namenode是通过active namenode的9000端口通讯的。若active namenode没有启动，则9000没有程序监听提供服务。 <br><br>当然也可以不启动Active NameNode就进行同步元数据信息，就是直接用命令拷贝Active主机上的元数据信息目录到Standby主机上，但是不建议这么做：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hadoop@hadoop16 hadoop]$ scp -r tmp/ hadoop@hadoop26:/usr/local/hadoop</div></pre></td></tr></table></figure></p>
<h2 id="格式化ZKFC-仅在hadoop16上执行即可"><a href="#格式化ZKFC-仅在hadoop16上执行即可" class="headerlink" title="格式化ZKFC(仅在hadoop16上执行即可)"></a>格式化ZKFC(仅在hadoop16上执行即可)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hadoop@hadoop16 ~]$ hdfs zkfc -formatZK</div></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/hadoop/38.png" alt=""></p>
<h2 id="启动HDFS-在hadoop16上执行"><a href="#启动HDFS-在hadoop16上执行" class="headerlink" title="启动HDFS(在hadoop16上执行)"></a>启动HDFS(在hadoop16上执行)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hadoop@hadoop16 ~]$ /usr/local/hadoop/sbin/start-dfs.sh</div></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/hadoop/39.png" alt=""><br>可以在各主机执行jps，查看启动的进程：<br><img src="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/hadoop/40.png" alt=""><br><img src="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/hadoop/41.png" alt=""><br><img src="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/hadoop/42.png" alt=""></p>
<h2 id="启动YARN"><a href="#启动YARN" class="headerlink" title="启动YARN"></a>启动YARN</h2><p><strong>注意：</strong>还是在hadoop16上执行start-yarn.sh，这是因为没有把namenode和resourcemanager分开，生产环境需要把他们分开，他们分开了就要分别在不同的机器上启动。<br><br><strong>启动yarn(在hadoop16上)：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hadoop@hadoop16 ~]$ /usr/local/hadoop/sbin/start-yarn.sh</div></pre></td></tr></table></figure></p>
<p><img src="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/hadoop/43.png" alt=""><br><strong>启动yarn standby(在hadoop26上)：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hadoop@hadoop26 ~]$  /usr/local/hadoop/sbin/yarn-daemon.sh start resourcemanager</div></pre></td></tr></table></figure></p>
<p>可以在各节点执行jps，查看启动的进程：<br><img src="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/hadoop/44.png" alt=""><br><img src="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/hadoop/45.png" alt=""><br><img src="https://raw.githubusercontent.com/jkzhao/MarkdownPictures/master/hadoop/46.png" alt=""></p>
<h1 id="停止hadoop"><a href="#停止hadoop" class="headerlink" title="停止hadoop"></a>停止hadoop</h1><p>停止HDFS集群：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hadoop@hadoop16 ~]$ stop-dfs.sh</div></pre></td></tr></table></figure></p>
<p>停止YARN集群：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hadoop@hadoop16 ~]$ stop-yarn.sh</div></pre></td></tr></table></figure></p>
<p>停止ResourceManager(Standby)：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[hadoop@hadoop26 ~]$ yarn-daemon.sh stop resourcemanager</div></pre></td></tr></table></figure></p>

      
    </div>

    <div>
      
        
      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/大数据，Hadoop/" rel="tag">#大数据，Hadoop</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2016/09/06/vpn简介及openvpn搭建/" rel="next" title="vpn简介及openvpn搭建">
                <i class="fa fa-chevron-left"></i> vpn简介及openvpn搭建
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2016/08/04/安装部署Apache Hadoop (本地模式和伪分布式)/" rel="prev" title="安装部署Apache Hadoop (本地模式和伪分布式)">
                安装部署Apache Hadoop (本地模式和伪分布式) <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="2016/08/07/安装部署Apache Hadoop (完全分布式模式并且实现NameNode HA和ResourceManager HA)/"
           data-title="安装部署Apache Hadoop (完全分布式模式并且实现NameNode HA和ResourceManager HA)" data-url="http://yoursite.com/2016/08/07/安装部署Apache Hadoop (完全分布式模式并且实现NameNode HA和ResourceManager HA)/">
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.jpg"
               alt="Zhao Jiankai" />
          <p class="site-author-name" itemprop="name">Zhao Jiankai</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">9</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">7</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/jkzhao" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/3566507667/profile?rightmod=1&wvr=6&mod=personinfo&is_all=1" target="_blank" title="微博">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  微博
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.zhihu.com/people/ju-feng-93" target="_blank" title="知乎">
                  
                    <i class="fa fa-fw fa-zhihu"></i>
                  
                  知乎
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              友情链接
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.importnew.com/" title="ImportNew" target="_blank">ImportNew</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#环境规划"><span class="nav-number">1.</span> <span class="nav-text">环境规划</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#配置集群各节点hosts文件"><span class="nav-number">2.</span> <span class="nav-text">配置集群各节点hosts文件</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#安装JDK1-7"><span class="nav-number">3.</span> <span class="nav-text">安装JDK1.7</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#安装依赖包ssh和rsync"><span class="nav-number">4.</span> <span class="nav-text">安装依赖包ssh和rsync</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#各节点时间同步"><span class="nav-number">5.</span> <span class="nav-text">各节点时间同步</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#NTP服务端"><span class="nav-number">5.1.</span> <span class="nav-text">NTP服务端</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#NTP客户端"><span class="nav-number">5.2.</span> <span class="nav-text">NTP客户端</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#安装Zookeeper集群"><span class="nav-number">6.</span> <span class="nav-text">安装Zookeeper集群</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#安装配置zk"><span class="nav-number">6.1.</span> <span class="nav-text">安装配置zk</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#启动和关闭zk"><span class="nav-number">6.2.</span> <span class="nav-text">启动和关闭zk</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#测试zk集群"><span class="nav-number">6.3.</span> <span class="nav-text">测试zk集群</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#脚本定期清理zk快照和日志文件"><span class="nav-number">6.4.</span> <span class="nav-text">脚本定期清理zk快照和日志文件</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#添加Hadoop运行用户"><span class="nav-number">7.</span> <span class="nav-text">添加Hadoop运行用户</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#配置主节点登录自己和其他节点不需要输入密码"><span class="nav-number">8.</span> <span class="nav-text">配置主节点登录自己和其他节点不需要输入密码</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#安装hadoop"><span class="nav-number">9.</span> <span class="nav-text">安装hadoop</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#安装配置master节点-hadoop16主机"><span class="nav-number">9.1.</span> <span class="nav-text">安装配置master节点(hadoop16主机)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#安装配置其他节点"><span class="nav-number">9.2.</span> <span class="nav-text">安装配置其他节点</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#启动hadoop"><span class="nav-number">10.</span> <span class="nav-text">启动hadoop</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#启动Zookeeper集群"><span class="nav-number">10.1.</span> <span class="nav-text">启动Zookeeper集群</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#启动journalnode"><span class="nav-number">10.2.</span> <span class="nav-text">启动journalnode</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#格式化HDFS"><span class="nav-number">10.3.</span> <span class="nav-text">格式化HDFS</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#格式化ZKFC-仅在hadoop16上执行即可"><span class="nav-number">10.4.</span> <span class="nav-text">格式化ZKFC(仅在hadoop16上执行即可)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#启动HDFS-在hadoop16上执行"><span class="nav-number">10.5.</span> <span class="nav-text">启动HDFS(在hadoop16上执行)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#启动YARN"><span class="nav-number">10.6.</span> <span class="nav-text">启动YARN</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#停止hadoop"><span class="nav-number">11.</span> <span class="nav-text">停止hadoop</span></a></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhao Jiankai</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.0.1"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"jkzhao"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
      
      <script src="/vendors/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
      <script src="/js/src/hook-duoshuo.js"></script>
    
  





  
  
  

  

  

</body>
</html>
